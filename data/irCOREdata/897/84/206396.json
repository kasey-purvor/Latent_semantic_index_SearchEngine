{"doi":"10.1109\/TNN.2006.872352","coreId":"206396","oai":"oai:eprints.lse.ac.uk:14969","identifiers":["oai:eprints.lse.ac.uk:14969","10.1109\/TNN.2006.872352"],"title":"The influence of oppositely classified examples on the generalization complexity of Boolean functions","authors":["Anthony, Martin","Franco, Leonardo"],"enrichments":{"references":[{"id":17227724,"title":"A theorem on sensitivity and applications in private computation,\u201d in","authors":[],"date":"1999","doi":"10.1145\/301250.301340","raw":"A. G\u00e1l and A. Ros\u00e9n, \u201cA theorem on sensitivity and applications in private computation,\u201d in Proc. 31st ACM Symp. Theory of Computing, 1999, pp. 348\u2013357.","cites":null},{"id":17227706,"title":"Almost optimal lower bounds for small depth circuits,\u201d","authors":[],"date":"1989","doi":"10.1145\/12130.12132","raw":"J. Hastad, \u201cAlmost optimal lower bounds for small depth circuits,\u201d Adv. Comput. Res., vol. 5, pp. 143\u2013170, 1989.","cites":null},{"id":17227734,"title":"Antipredictable sequences: harder to predict thanrandomsequences,\u201dNeuralComput.,vol.10,pp.2219\u20132230,1998.","authors":[],"date":null,"doi":"10.1162\/089976698300017043","raw":"H. Zhu and W. Kinzel, \u201cAntipredictable sequences: harder to predict thanrandomsequences,\u201dNeuralComput.,vol.10,pp.2219\u20132230,1998.","cites":null},{"id":17227708,"title":"Circuit Complexity and Neural Networks.","authors":[],"date":"1994","doi":"10.1002\/(sici)1099-0526(199803\/04)3:4<59::aid-cplx11>3.3.co;2-d","raw":"I. Parberry, Circuit Complexity and Neural Networks. Cambridge, MA: MIT Press, 1994.","cites":null},{"id":17227719,"title":"Computational complexity for physicists,\u201d","authors":[],"date":"2002","doi":"10.1109\/5992.998639","raw":"S. Mertens, \u201cComputational complexity for physicists,\u201d Comput. Sci. Eng., vol. 4, pp. 31\u201347, 2002.","cites":null},{"id":17227714,"title":"Constant depth circuits, Fourier transform and learnability,\u201d","authors":[],"date":"1993","doi":"10.1145\/174130.174138","raw":"N. Linial, Y. Mansour, and N. Nisan, \u201cConstant depth circuits, Fourier transform and learnability,\u201d J. ACM, vol. 40, pp. 607\u2013620, 1993.","cites":null},{"id":17227720,"title":"Depth-size tradeoffs for neural computation,\u201d","authors":[],"date":"1991","doi":"10.1109\/12.106225","raw":"K. Y. Siu, V. P. Roychowdhury, and T. Kailath, \u201cDepth-size tradeoffs for neural computation,\u201d IEEE Trans. Comput., vol. 40, no. 12, pp. 1402\u20131412, Dec. 1991.","cites":null},{"id":17227709,"title":"Generalization ability of Boolean functions implemented in feedforward neural networks,\u201d Neurocomputing,","authors":[],"date":"2006","doi":"10.1016\/j.neucom.2006.01.025","raw":"L. Franco, \u201cGeneralization ability of Boolean functions implemented in feedforward neural networks,\u201d Neurocomputing, 2006, to be published.","cites":null},{"id":17227711,"title":"Generalization and selection of examples in feedforward neural networks,\u201d","authors":[],"date":"2000","doi":"10.1162\/089976600300014999","raw":", \u201cGeneralization and selection of examples in feedforward neural networks,\u201d Neural Comput., vol. 12, pp. 2405\u20132426, 2000.","cites":null},{"id":17227712,"title":"Generalization properties of modular networks implementing the parity function,\u201d","authors":[],"date":"2001","doi":"10.1109\/72.963767","raw":", \u201cGeneralization properties of modular networks implementing the parity function,\u201d IEEE Trans. Neural Netw., vol. 12, no. 6, pp. 1306\u20131313, Nov. 2001.","cites":null},{"id":17227725,"title":"Improved learning of AC functions,\u201d in","authors":[],"date":"1991","doi":"10.1016\/b978-1-55860-213-7.50032-8","raw":"M. L. Furst, J. C. Jackson, and S. W. Smith, \u201cImproved learning of AC functions,\u201d in Proc. 4th Annu. Workshop on Computational Learning Theory, 1991, pp. 317\u2013325.","cites":null},{"id":17227743,"title":"Learning capability and storage capacity of two-hiddenlayer feedforward networks,\u201d","authors":[],"date":"2003","doi":"10.1109\/tnn.2003.809401","raw":"G.-B. Huang, \u201cLearning capability and storage capacity of two-hiddenlayer feedforward networks,\u201d IEEE Trans. Neural Netw., vol. 14, no. 2, pp. 274\u2013281, Mar. 2003.","cites":null},{"id":17227717,"title":"Minimization of Boolean complexity in human concept learning,\u201d","authors":[],"date":"2000","doi":null,"raw":"J. Feldman, \u201cMinimization of Boolean complexity in human concept learning,\u201d Nature, vol. 407, pp. 572\u2013573, 2000.","cites":null},{"id":17227723,"title":"Minimizing the average query complexityoflearningmonotoneBooleanfunctions,\u201dINFORMSJ.Comput.,","authors":[],"date":"2002","doi":null,"raw":"V. I. Torvik and E. Triantaphyllou, \u201cMinimizing the average query complexityoflearningmonotoneBooleanfunctions,\u201dINFORMSJ.Comput., vol. 14, pp. 142\u2013172, 2002.","cites":null},{"id":17227733,"title":"Networks: A Comprehensive Foundation:","authors":[],"date":"1994","doi":"10.1142\/s0129065794000372","raw":"S.Haykin, Neural Networks: A Comprehensive Foundation: Macmillan, 1994.","cites":null},{"id":17227729,"title":"Neural models and spectral methods,\u201d","authors":[],"date":"1994","doi":"10.1007\/978-1-4615-2696-4_1","raw":"V. P. Roychowdhury, K. Y. Siu, and A. Orlitsky, \u201cNeural models and spectral methods,\u201d in Theoretical Advances in Neural Computation and Learning, V. P. Roychowdhury, K. Y. Siu, and A. Orlitsky, Eds. Boston, MA: Kluwer Academic, 1994.","cites":null},{"id":17227731,"title":"Neural Network Learning: Theoretical Foundations.","authors":[],"date":"1999","doi":"10.1017\/cbo9780511624216.022","raw":"M. Anthony and P. L. Bartlett, Neural Network Learning: Theoretical Foundations. Cambridge, U.K.: Cambridge Univ. Press, 1999.","cites":null},{"id":17227710,"title":"Non-glassy ground state in a long-range antiferromagnetic frustrated model in the hypercubic cell,\u201d","authors":[],"date":"2004","doi":"10.1016\/j.physa.2003.10.011","raw":"L. Franco and S. A. Cannas, \u201cNon-glassy ground state in a long-range antiferromagnetic frustrated model in the hypercubic cell,\u201d Physica A, vol. 332, pp. 337\u2013348, 2004.","cites":null},{"id":17227721,"title":"On specifying Boolean functions by labeled examples,\u201d","authors":[],"date":"1995","doi":"10.1016\/0166-218x(94)00007-z","raw":"M. Anthony, G. Brightwell, and J. Shawe-Taylor, \u201cOn specifying Boolean functions by labeled examples,\u201d Discrete Appl. Math., vol. 61, pp. 1\u201325, 1995.","cites":null},{"id":17227736,"title":"On the computational complexity of ising spin glasses,\u201d","authors":[],"date":null,"doi":"10.1088\/0305-4470\/15\/10\/028","raw":"F. Barahona, \u201cOn the computational complexity of ising spin glasses,\u201d J. Phys. A: Math. Gen., vol. 15, pp. 3241\u20133253, 1982.","cites":null},{"id":17227738,"title":"Periodic symmetric functions, serial addition, and multiplication with neural networks,\u201d","authors":[],"date":"1998","doi":"10.1109\/72.728356","raw":"S. D. Cotofana and S. Vassiliadis, \u201cPeriodic symmetric functions, serial addition, and multiplication with neural networks,\u201d IEEE Trans. Neural Netw., vol. 9, no. 6, pp. 1118\u20131128, Nov. 1998.","cites":null},{"id":17227727,"title":"Polynomial threshold functions, AC functions, and spectral norms,\u201d","authors":[],"date":"1992","doi":"10.1137\/0221003","raw":"J. Bruck and R. Smolensky, \u201cPolynomial threshold functions, AC functions, and spectral norms,\u201d SIAM J. Comput., vol. 21, pp. 33\u201342, 1992.","cites":null},{"id":17227741,"title":"Randomness and mathematical proof,\u201d","authors":[],"date":"1975","doi":"10.1038\/scientificamerican0575-47","raw":"G. Chaitin, \u201cRandomness and mathematical proof,\u201d Scientif. Amer., vol. 232, pp. 47\u201352, 1975.","cites":null},{"id":17227728,"title":"Slicing the hypercube,\u201d in Surveys","authors":[],"date":"1993","doi":"10.1017\/cbo9780511662089.009","raw":"M. Saks, \u201cSlicing the hypercube,\u201d in Surveys in Combinatorics (Invited Talks of the 1993 British Combinatorial Conf.). Cambridge, U.K.: Cambridge Univ. Press, 1993, pp. 211\u2013255.","cites":null},{"id":17227739,"title":"Solving arithmetic problems using feedforward neural networks,\u201d","authors":[],"date":"1965","doi":"10.1016\/s0925-2312(97)00069-6","raw":"L. Franco and S. A. Cannas, \u201cSolving arithmetic problems using feedforward neural networks,\u201d Neurocomput., vol. 18, pp. 61\u201379, 1965.","cites":null},{"id":17227730,"title":"Statistical Learning Theory.","authors":[],"date":"1998","doi":"10.1007\/978-1-4757-3264-1","raw":"V. N. Vapnik, Statistical Learning Theory. New York: Wiley, 1998.","cites":null},{"id":17227718,"title":"Statistical mechanics methods and phase transitions in optimization problems,\u201d","authors":[],"date":"2001","doi":"10.1016\/s0304-3975(01)00149-9","raw":"O. C. Martin, R. Monasson, and R. Zecchina, \u201cStatistical mechanics methods and phase transitions in optimization problems,\u201d Theor. Comput. Sci., vol. 265, pp. 3\u201367, 2001.","cites":null},{"id":17227737,"title":"Statistical mechanics, three-dimensionality and NP-completeness I. Universality of intractability for the partition function of the Ising model accross nonplanar lattices,\u201d in","authors":[],"date":"2000","doi":"10.1145\/335305.335316","raw":"S. Istrail, \u201cStatistical mechanics, three-dimensionality and NP-completeness I. Universality of intractability for the partition function of the Ising model accross nonplanar lattices,\u201d in Proc. 32nd ACM Symp. Theory of Computing (STOC 2000), 2000, pp. 87\u201396.","cites":null},{"id":17227735,"title":"The anisotropic kagome antiferromagnet: a topical spin glass?,\u201d","authors":[],"date":"1993","doi":"10.1051\/jp1:1993104","raw":"P. Chandra, P. Coleman, and I. Ritchey, \u201cThe anisotropic kagome antiferromagnet: a topical spin glass?,\u201d J. Phys. I, vol. 3, pp. 591\u2013610, 1993.","cites":null},{"id":17227715,"title":"The average sensitivity of bounded-depth circuits,\u201d","authors":[],"date":"1997","doi":"10.1016\/s0020-0190(97)00131-2","raw":"R. B. Boppana, \u201cThe average sensitivity of bounded-depth circuits,\u201d Inf. Process. Lett., vol. 63, pp. 257\u2013261, 1997.","cites":null},{"id":17227716,"title":"The average sensitivity of square-freeness,\u201d","authors":[],"date":"2000","doi":"10.1007\/pl00001600","raw":"A. Bernasconi, C. Damm, and I. Shparlinski, \u201cThe average sensitivity of square-freeness,\u201d Comput. Complex., vol. 9, pp. 39\u201351, 2000.","cites":null},{"id":17227705,"title":"The Complexity of Boolean Functions.","authors":[],"date":"1987","doi":"10.1007\/3-540-18170-9_185","raw":"I. Wegener, The Complexity of Boolean Functions. New York: Wiley, 1987.","cites":null},{"id":17227713,"title":"The in\ufb02uence of variables on Boolean functions,\u201d in","authors":[],"date":"1988","doi":"10.1109\/sfcs.1988.21923","raw":"J. Kahn, G. Kalai, and N. Linial, \u201cThe in\ufb02uence of variables on Boolean functions,\u201d in Proc. 29th IEEE Symp. Foundations of Computer Science (FOCS\u201988), 1988, pp. 68\u201380.","cites":null},{"id":17227744,"title":"The sample complexity of pattern classi\ufb01cation with neural networks: the size of the weights is more important than the size of the network,\u201d","authors":[],"date":"1998","doi":"10.1109\/18.661502","raw":"P. L. Bartlett, \u201cThe sample complexity of pattern classi\ufb01cation with neural networks: the size of the weights is more important than the size of the network,\u201d IEEE Trans. Inf. Theory, vol. 44, no. 2, pp. 525\u2013536, Mar. 1998. LeonardoFranco(M\u201906)wasborninTucum\u00e1n,Argentina. He received the M.S. and Ph.D. degrees in physics from the University of C\u00f3rdoba, Argentina, where he analyzed the generalization properties of feed-forward neural networks. He then became a Postdoctoral Fellow to SISSA, Trieste, Italy, where he became involved in computational neuroscience. He then moved to the University ofOxford,U.K.,asa ResearchScientistwhereheapplied and developed information theory methods to the analysis of neuronal recordings. He is currently withM\u00e1lagaUniversity,Spain,asaRam\u00f3nyCajalresearcherworkingonneural networks, their applications to biomedical problems, and also in computational neuroscience. He has authored approximately 25 publications in journals and international conferences. Martin Anthony was raised in Paisley, Scotland. He received the B.Sc. degree in mathematics from the University of Glasgow, Scotland, and the Ph.D. degree, also in mathematics from the University of London, U.K. Since 1990, he has been on the Mathematics faculty of the London School of Economics (LSE). He is now Professor of Mathematics and Convener of the Mathematics Department at LSE. He has published widely on the mathematical theory of machine learning and neural networks, including Computational Learning Theory: An Introduction (with N. L. Biggs), Neural Network Learning: Theoretical Foundations (with P. L. Bartlett), and Discrete Mathematics of Neural Networks: Selected Topics.","cites":null},{"id":17227740,"title":"Three approaches to the quantitative de\ufb01nition of information,\u201d","authors":[],"date":"1965","doi":"10.1080\/00207166808803030","raw":"A. Kolmogorov, \u201cThree approaches to the quantitative de\ufb01nition of information,\u201d Probl. Info. Transmiss., vol. 1, pp. 1\u201317, 1965.","cites":null},{"id":17227742,"title":"V.Deolalikar,\u201cMappingBooleanfunctionswithneuralnetworkshaving binary weights and zero thresholds,\u201d","authors":[],"date":"2001","doi":"10.1109\/72.925568","raw":"V.Deolalikar,\u201cMappingBooleanfunctionswithneuralnetworkshaving binary weights and zero thresholds,\u201d IEEE Trans. Neural Netw., vol. 12, no. 3, pp. 639\u2013642, May 2001.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2006-05","abstract":null,"downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/206396.pdf","fullTextIdentifier":"http:\/\/eprints.lse.ac.uk\/14969\/1\/__Libfile_repository_Content_Martin%2C%20A_The%20influence%20of%20oppositely%20classified%20examples%20on%20the%20generalization%20complexity%20of%20Boolean%20functions%28lsero%29.pdf","pdfHashValue":"e69a37165f7fdb02ebcc5c3af7c0ce9ba6902685","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lse.ac.uk:14969<\/identifier><datestamp>\n      2012-11-08T16:07:41Z<\/datestamp><setSpec>\n      74797065733D4445505453:4C53452D4D41<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lse.ac.uk\/14969\/<\/dc:relation><dc:title>\n        The influence of oppositely classified examples on the generalization complexity of Boolean functions<\/dc:title><dc:creator>\n        Anthony, Martin<\/dc:creator><dc:creator>\n        Franco, Leonardo<\/dc:creator><dc:subject>\n        QA76 Computer software<\/dc:subject><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2006-05<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lse.ac.uk\/14969\/1\/__Libfile_repository_Content_Martin%2C%20A_The%20influence%20of%20oppositely%20classified%20examples%20on%20the%20generalization%20complexity%20of%20Boolean%20functions%28lsero%29.pdf<\/dc:identifier><dc:identifier>\n          Anthony, Martin and Franco, Leonardo  (2006) The influence of oppositely classified examples on the generalization complexity of Boolean functions.  IEEE Transactions on Neural Networks, 17 (3).  pp. 578-590.  ISSN 1045-9227     <\/dc:identifier><dc:relation>\n        http:\/\/ieeexplore.ieee.org\/xpl\/RecentIssue.jsp?punumber=72<\/dc:relation><dc:relation>\n        10.1109\/TNN.2006.872352<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lse.ac.uk\/14969\/","http:\/\/ieeexplore.ieee.org\/xpl\/RecentIssue.jsp?punumber=72","10.1109\/TNN.2006.872352"],"year":2006,"topics":["QA76 Computer software"],"subject":["Article","PeerReviewed"],"fullText":"  \nAntony Martin and Leonardo Franco  \nThe influence of oppositely classified examples on \nthe generalization complexity of Boolean functions \n \nArticle (Published version) \n(Refereed) \n \n \nAnthony, Martin and Franco, Leonardo (2006) The influence of oppositely classified examples on \nthe generalization complexity of Boolean functions. Ieee transactions on neural networks, 17 (3). \npp. 578-590. ISSN 1045-9227  \n \nDOI: 10.1109\/TNN.2006.872352 \n \n \u00a9 2006 IEEE \n \nThis version available at: http:\/\/eprints.lse.ac.uk\/14969\/ \nAvailable in LSE Research Online: October 2012  \n \nLSE has developed LSE Research Online so that users may access research output of the \nSchool. Copyright \u00a9 and Moral Rights for the papers on this site are retained by the individual \nauthors and\/or other copyright owners. Users may download and\/or print one copy of any \narticle(s) in LSE Research Online to facilitate their private study or for non-commercial research. \nYou may not engage in further distribution of the material or use it for any profit-making activities \nor any commercial gain. You may freely distribute the URL (http:\/\/eprints.lse.ac.uk) of the LSE \nResearch Online website.  \n578 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 3, MAY 2006\nThe Influence of Oppositely Classified Examples on\nthe Generalization Complexity of Boolean Functions\nLeonardo Franco, Member, IEEE, and Martin Anthony\nAbstract\u2014In this paper, we analyze Boolean functions using a re-\ncently proposed measure of their complexity. This complexity mea-\nsure, motivated by the aim of relating the complexity of the func-\ntions with the generalization ability that can be obtained when the\nfunctions are implemented in feed-forward neural networks, is the\nsum of a number of components. We concentrate on the case in\nwhich we use the first two of these components. The first is related\nto the \u201caverage sensitivity\u201d of the function and the second is, in a\nsense, a measure of the \u201crandomness\u201d or lack of structure of the\nfunction. In this paper, we investigate the importance of using the\nsecond term in the complexity measure, and we consider to what\nextent these two terms suffice as an indicator of how difficult it is to\nlearn a Boolean function. We also explore the existence of very com-\nplex Boolean functions, considering, in particular, the symmetric\nBoolean functions.\nIndex Terms\u2014Average sensitivity, Boolean functions, com-\nplexity, generalization, learning, randomness, symmetric func-\ntions.\nI. INTRODUCTION\nA. Background\nTHE complexity of Boolean functions is one of the centraland classical topics in the theory of computation. Scien-\ntists have long tried to classify Boolean functions according\nto various complexity measures, such as the minimal size of\nBoolean circuits needed to compute specific functions [1]\u2013[3].\nFranco [4], [5] introduced a complexity measure for Boolean\nfunctions that appears to be related to the generalization error\nwhen learning the functions by neural networks. This com-\nplexity measure has been derived from results showing that the\ngeneralization ability obtained for Boolean functions and for\nthe number of examples (or similarly queries) needed to learn\nthe functions when implemented in neural networks is related\nto the number of pairs of examples that are similar (close with\nrespect to the Hamming distance), but have opposite outputs\n[6], [7]. (The Hamming distance between two binary vectors\nof length is simply the number of components\non which they differ.) When only the bordering (or boundary)\nexamples (those at Hamming distance 1) are considered, the\nManuscript received January 13, 2004; revised September 29, 2005. This\nwork was supported by CICYT (Espa\u00f1a) by Grant TIN2005-02984 (that in-\ncludes FEDER funds). The work of L. Franco was supported by the Ram\u00f3n y\nCajal Programme of the Ministerio de Educaci\u00f3n y Ciencia (Espa\u00f1a). The work\nof M. Anthony was supported in part by the IST Programme of the European\nCommunity, under the PASCAL Network of Excellence, IST-2002-506778.\nL. Franco is with the Departamento de Lenguajes y Ciencias de la Com-\nputaci\u00f3n, Universidad de M\u00e1laga, Campus de Teatinos S\/N, 29071, M\u00e1laga,\nSpain (e-mail: lfranco@lcc.uma.es).\nM. Anthony is with the Department of Mathematics, London School of Eco-\nnomics and Political Sciences, London WC2A 2AE, U.K.\nDigital Object Identifier 10.1109\/TNN.2006.872352\ncomplexity measure becomes equivalent to average sensitivity,\na measure introduced by Kahn et al. [8]. Average sensitivity\nhas been linked by Linial et al. [9] to the complexity of learning\nin the probabilistic \u201cPAC\u201d model of learning introduced by\nValiant [10]; and many results about the average sensitivity of\nBoolean functions have been obtained for different classes of\nBoolean functions [11], [12]. It has been shown in [4], and is\nfurther analyzed in this paper, that terms that account for the\nnumber of pairs of opposite examples at Hamming distance\nlarger than 1 are important in obtaining a better match between\nthe complexity of different kinds of Boolean functions and the\nobserved generalization ability.\nKnowing and characterizing which functions are the most\ncomplex has a number of implications. It could help us to un-\nderstand what functions can be most easily learned. (For human\nlearning, the difficulty of learning has been linked to function\ncomplexity in [13].) In physics, a link has been established be-\ntween the complexity measure and the Hamiltonian of spin sys-\ntems [5], and a correspondence has been shown to exist between\nthe most complex Boolean functions and the ground state of\nmagnetic systems. It is worth noting that in recent years physics\nand computational complexity theory have both benefited from\ntheir interaction, and the analogy just mentioned offers a new\npoint of contact between the disciplines. (For an introduction\nto some of the relationships between statistical mechanics and\ncomputational complexity, see [14] and [15].) In this way, the\nstudy of the complexity measure and of the most complex func-\ntions that it defines are of interest in a number of disciplinary\ncontexts, including mathematical properties of Boolean func-\ntions, the physics of magnetic systems, and learning in real and\nartificial systems.\nB. Overview of the Paper\nIn Section II, we introduce the complexity measure that is the\nfocus of this paper. In its most general form, for a Boolean func-\ntion , the complexity is a weighted\nsum of terms, , where can be interpreted,\nbroadly speaking, as indicating how many pairs of inputs to\nthe Boolean function that are Hamming distance apart have\ndifferent output values of the function. We briefly indicate that\nsome of the terms of this complexity measure can be related to\npreviously-studied concepts in the theory of Boolean functions,\nsuch as average sensitivity and the Fourier coefficients.\nIn Section III, we report on some experiments conducted to\ninvestigate the usefulness of the second-order complexity term\n(used in conjunction with ) and it is demonstrated that\nthe complexity measure seems, for\nthe families of functions considered, to correlate well with the\n1045-9227\/$20.00 \u00a9 2006 IEEE\nFRANCO AND ANTHONY: GENERALIZATION COMPLEXITY OF BOOLEAN FUNCTIONS 579\ngeneralization error when the functions are learned using neural\nnetwork architectures.\nSection IV contains a theoretical analysis of the complexity\nof functions possessing some \u201cirrelevant attributes,\u201d meaning\nthat the functions do not depend on all the coordinates of\ntheir inputs. We then look closely at a particular case of such\nfunctions, namely those corresponding to the parity function\non some subset of the attributes. Here, we find that the cor-\nrespondence between the complexity and generalization\nerror is not extremely precise, and this therefore provides some\nevidence that, in some cases, it may be wise to use additional,\nhigher-order, terms such as . This class of functions with\nirrelevant attributes is, however, rather small and we do find,\nempirically, a stronger correspondence between and the\ngeneralization error when considering a larger class of func-\ntions derived from the parity functions, with some irrelevant\nattributes, but also a degree of \u201crandomness\u201d\u2019 in their definition.\nSections V and VI consider a well studied class of Boolean\nfunctions known as the symmetric functions. These are func-\ntions whose output depends only on the number of ones in the\ninput (or, equally, these are the functions whose values are pre-\nserved under any permutation of the coordinates of the inputs).\nThis class has been much-studied in a number of contexts: Many\ngeneral results on sample and computational complexity have\nbeen obtained [1], [4], [16]. Section V presents some theoret-\nical bounds on the complexity of these functions. Section VI\ninvestigates how the complexity of symmetric functions can be\napproximated by examining only the inputs that have a number\nof ones close to .\nThe paper finishes with some discussion, conclusions, and\nsuggestions for future work.\nII. THE COMPLEXITY MEASURE AND ITS INTERPRETATION\nA. A Complexity Measure for Boolean Functions\nIn its most general form, the Boolean function complexity\nmeasure considered here consists of a sum of terms, , each\nof which accounts for the number of neighboring examples at a\ngiven Hamming distance having different outputs (that is, dif-\nferent values of the function). The complexity measure can be\nwritten in a general form as\n(1)\nwhere are constant values that weight how pairs of oppo-\nsitely classified examples (that is, elements of with dif-\nferent outputs) at Hamming distance contribute to the total\ncomplexity, and is the number of input bits. In all sections of\nthe paper, apart from where is clearly indicated, the complexity\nmeasure used was equal to (i.e., only the\nfirst two terms of (1) were used and the value of used was\nequal to 1).\nEach term has a normalization factor that takes into ac-\ncount both the number of neighboring examples at Hamming\ndistance and the total number of examples. Explic-\nitly, if the examples are enumerated as , then\n(2)\nwhere is the number of examples at Hamming distance\nfrom a given example, equal to the Binomial coefficient .\nThus, may be interpreted as the probability, uniformly\nover choice of example , and uniformly over the choice of an\nexample at Hamming distance from , that .\nB. Relationship to Other Measures Associated With Boolean\nFunctions\nThe first term, is proportional to the number of bordering\n(or boundary) examples of the function ; that is, those with\nan immediate neighbor having opposite outputs. Equivalently,\nit is the probability that flipping a uniformly chosen bit in a\nuniformly chosen example will change the output of . This is\nproportional to the average sensitivity of the function\n[8], [9], [17]. For a Boolean function on variables and\n, the sensitivity of at , which we shall denote , is\nthe number of neighbors of (that is, differing\nfrom only in one entry) such that . The average\nsensitivity is defined to be the average, over all elements of\n, of the sensitivities, .\nThe average sensitivity is also related to the notion of the\n\u201cinfluence\u201d of a variable; see [8], for instance. The influence of\nvariable is defined to be the proportion of such\nthat if is obtained from by changing the th entry of , we\nhave . It can be seen that is the sum of the\ninfluences of the variables. The connection between and\nthe average sensitivity is that .\nThe number of bordering examples (equivalently, the average\nsensitivity or ) has been shown to be related to the general-\nization ability that can be obtained when Boolean functions are\nimplemented in neural networks [6], to the number of exam-\nples needed to obtain perfect generalization [6], [7], to a bound\non the number of examples needed to specify a linearly sepa-\nrable function [18], and to the query complexity of monotone\nBoolean functions [19]. Moreover, links between the sensitivity\nof a Boolean function and its learnability in the PAC sense have\nbeen established [9] and many results regarding the average sen-\nsitivity of Boolean functions have been derived [8], [11], [12],\n[20].\nThe complexity measures can also be related to another\nimportant idea in the theory of Boolean functions, namely the\nFourier coefficients of . Fourier (or harmonic) analysis of\nBoolean functions has recently proven to be extremely useful\nin a number of areas, such as learning theory ([9], [21] for\ninstance) and circuit complexity [22]. (References [23] and\n[24] provide good surveys of the harmonic analysis of Boolean\nfunctions and its uses.) It is shown in [8] that the average\nsensitivity of a Boolean function , and hence the complexity\nmeasure may be written in terms of the Fourier coef-\nficients of . Furthermore, results of Kahn, Kalai, and Linial\nalso show that the higher-order complexity terms can be\nexpressed in terms of the Fourier coefficients.\n580 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 3, MAY 2006\nFig. 1. Generalization ability versus first term (top graph), second term\n(middle graph), and first plus second terms (bottom graph) of the complexity\nmeasure for three different classes of functions. Each point in the graphs\ncorresponds to the average obtained across 30 generated functions with\napproximately the complexity shown. (See text for more details.).\nIII. IMPORTANCE OF THE SECOND-ORDER TERM OF THE\nCOMPLEXITY MEASURE\nThe second-order term has been shown to be relevant in order\nto produce an accurate match between the complexity measure\nand the observed generalization ability when the functions\nare implemented in neural networks [4]. Experimental results\nindicate that the first-order complexity term alone does\nnot give as good a correspondence as does the combination\n.\nFig. 1 (top) shows the generalization ability versus the first-\norder complexity obtained from simulations performed for\nthree different classes of functions. Each point in the graphs\nof Fig. 1 corresponds to the average obtained across 30 gener-\nated functions with approximately the complexity shown. The\nfirst class of functions (indicated as in the\nfigure) was generated by modifications of the constant, identi-\ncally-1, function, producing functions with first-order complex-\nities between 0 and 0.5. These were generated as a func-\ntion of a parameter in the following way: For every example,\na random uniform number in the range [0, 1] was selected and\nthen compared to the value of . If the random value was smaller\nthan then the output of the function on that example was ran-\ndomly selected with equal probability to be 0 or 1. Thus, for each\nexample, with probability , the output is randomly chosen, and\nwith probability the output is 1. The second set of functions\n( . in the figure) was generated in the same\nway but through random modifications of the parity function,\nto obtain functions with a complexity between 1 and 0.5. (The\nparity function is the function that has output 1 on an example\nprecisely when the example has an odd number of entries equal\nto 1.) The third set of functions ( in the\nfigure) was generated as follows: Starting with the parity func-\ntion, for each positive example (that is, an example with output\n1 on the parity function), the output is changed to 0 with prob-\nability . This yields functions with complexities ranging from\n1 to 0, all of which, except the initial parity function, are unbal-\nanced (in the sense that the number of outputs equal to 0 and\n1 are different). The classes of functions span a range of com-\nplexities according to the way they are generated. In particular,\nfor the case of the first term of the complexity measure, , the\nmaximum possible range of from 0 to 1 is covered only when\nthe class includes the constant function (with all outputs having\nthe same value), for which , and the parity function or\nits complementary one, the only functions with complexity\nequals to 1.\nFig. 1 (top graph) shows, for each of these three classes, the\ngeneralization ability computed by simulations performed in a\nneural network architecture with inputs and eight neu-\nrons in the hidden layer trained with backpropagation, using half\nof the total number of examples for training, one fourth for val-\nidation and one fourth for testing the generalization ability (by\nwhich we mean the proportion of the test set that is classified\ncorrectly by the network after training). The generalization error\nis measured at the point at which the validation error achieved\nits minimum value. In Fig. 1 (middle) the generalization ability\nis plotted against the second-order term of the complexity mea-\nsure, , and it can be observed that the general behavior of\nthe generalization seems uncorrelated to this second term (Note\nthe different scale in this graph in comparison to the other two).\nBut when we plot, in figure (bottom), the generalization ability\nversus better agreement is obtained for the three dif-\nferent classes of Boolean functions. The discrepancy observed\nin the generalization ability in Fig. 1 for functions with sim-\nilar complexity appears to be almost totally corrected when\nis used. The shape of the curves in Fig. 1 (middle graph)\nis better understood if we consider at the same time the behavior\nof the generalization ability as a function of , as the general-\nization ability is much more correlated to than to when\nand are considered separately. In particular for the case of the\nfunctions labeled . the shape of the curve\nis understood by following the curve in Fig. 1 (middle graph)\nstarting from the point where generalization ability equals to 1\nand equals to 0 in clockwise sense and consider that along\nFRANCO AND ANTHONY: GENERALIZATION COMPLEXITY OF BOOLEAN FUNCTIONS 581\nFig. 2. Complexity terms C (top) and C (bottom) versus the parameter p\nthat controls randomness for the class of functions used in Fig. 1. For the first\ntwo classes of functions the amount of randomness is directly proportional to p\nwhile for the third class the maximum randomness is achieved for p = 0:5. A\ngood agreement between the amount of randomness and C is obtained.\nthis trajectory the value of monotonically increases Fig. 1\n(top graph) and thus, as expected, the generalization ability de-\ncreases.\nWe also show the values of and as a function of the\nparameter that controls the amount of randomness in the func-\ntions in Fig. 2 (top and middle). The first (second) class of func-\ntions starts with the constant (parity) function when and\nends in a totally random function when . We see that\nincreases (decreases) and increases with , as expected. For\nthe third class of functions the maximum amount of randomness\ncorresponds to 0.5, at which point all of the outputs that were\noriginally 1 are changed to 0 with probability 0.5. (Note that, for\nthis class of functions, the value of is proportional to the frac-\ntion of output bits 1 remaining in the definition of the function.)\nThe results for the three different classes of functions pre-\nsented in Fig. 2 (middle) show a clear correlation between\nthe amount of randomness or lack of structure (or regularity)\npresent in the Boolean functions and the value of . The term\nincludes the contribution of pairs of examples at Hamming\ndistance 2 and belonging to the same \u201clevel\u201d (that is, having\nthe same weight, the number of bits that are ON) and also from\nexamples that are at Hamming distance 2 from each other and\nhave weights differing by 2. It is worth noting that the first\ncontribution, from examples in the same level (i.e., of the same\nweight) measures how \u201cnonsymmetric\u201d the function is. (For\nsymmetric functions, all examples of a particular weight have\nidentical outputs.)\nWe have also investigated the behavior of the generaliza-\ntion error versus the complexity value of the functions when\nFig. 3. Generalization ability versus the complexity C (first and second\nterms included) for Boolean functions with N = 14 inputs created starting\nfrom the constant function and adding random modifications. The three curves\nshown were computed for different sizes of the training set while the size of the\nvalidation and generalization sets was held constant. It is indicated in the figure\nlegend the fraction of examples used in each set. (See text for more details.)\nThe error bars indicate the standard deviation of the mean computed over sets\nof 10 functions.\ndifferent sizes of sets are used for the training, validation and\ngeneralization measurement procedures. To analyze this, we\nran simulations using the set of Boolean functions generated\nby random modifications of the constant function (previously\nnamed ) that generates a set of Boolean\nfunctions with complexity in the range from\n0 to 1 for the case of inputs. The total number of\ndifferent input-output pairs (examples) is equal in this case\nto . In Fig. 3 the results are shown for three\ndifferent analyzed cases. The upper curve in Fig. 3 corresponds\nto the case of using half of the total number of examples for\ntraining (8192), one fourth (4096) for validation, and one\nfourth for measuring the generalization error. The middle curve\nwas constructed using [1\/8, 1\/4, 1\/4] of the total number of\nexamples, corresponding to 2048, 4096, and 4096 examples.\nThe lower curve was computed using 128 examples for training\nand the same size sets for the validation and generalization sets\n(indicated by [1\/128, 1\/4, 1\/4]). The figure indicates that the\nmonotonic relationship between complexity and generalization\nability is preserved for different selection of the training set\nsizes. The figure also shows that as the fraction of examples\nfor training diminishes the generalization ability is reduced,\nsomething that, of course, is to be expected from standard\nmodels of generalization (such as discussed in [25] and [26]),\nas a reduction in training sample size implies a reduction in\nknowledge about the target function. The results presented in\nFig. 3 show that when a comparison between different functions\nis done the size of the training set used has to be approximately\nthe same for all the different functions, as the size of the\ntraining set clearly affects the level of generalization that can be\nobtained. Thus, if the complexity of a function is measured it\nis not straightforward to predict the level of generalization that\ncan be expected as the size of the training set used plays also\nan important role. Nevertheless, the complexity value obtained\ncan be used as a comparison against other functions that will\nbe trained with the same size training set.\nAnother indication of the importance of the second-order\ncomplexity term comes from the fact that when only is\nconsidered, the functions with highest complexity turn out to\n582 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 3, MAY 2006\nbe the very well known parity function and its complement [4],\n[27], yet, as we shall see, numerical simulations have shown\nthat there are functions which are more complex to implement\non a neural network (in the sense that the generalization error\nis higher). Indeed, for this class of very complex functions the\ngeneralization error obtained is greater than 0.5 (which is what\nwould be expected for random functions). In [4] it has been\nshown that the average generalization error over a whole set\nof functions using the same architecture is 0.5, indicating that\nthere exist functions for which the generalization error is higher\nthan 0.5. Similar results have been obtained for time series\nimplemented by perceptrons by Zhu & Kinzel [28].\nIV. A METHOD FOR FINDING VERY COMPLEX BOOLEAN\nFUNCTIONS AND GROUND STATES\nA. Irrelevant Attributes\nIn this section, we investigate how to find Boolean functions\nwith a high complexity. One technique that seems to be\nuseful is to consider functions having a number of irrelevant at-\ntributes. Such an approach is motivated by considerations from\nstatistical mechanics. In [4], [5] an analogy is established be-\ntween the Boolean function complexity measure and the Hamil-\ntonian of magnetic systems. This analogy implies that there is a\ncorrespondence between the ground state of magnetic systems\nand the most complex functions. Ground states of many mag-\nnetic systems have been observed often to have a certain type\nof order (short or long range order) and it is a subject of contro-\nversy under which conditions this order does not arise [5], [29].\nIn some cases the ordered ground state consists of two equal\nsize antiferromagnetic domains, corresponding in the language\nof Boolean functions to a parity function on variables,\nwith the th variable being irrelevant. Finding the ground states\nof magnetic systems is a complicated task only rigorously un-\ndertaken in very few cases. It has been shown that in most cases\nthe problem of rigorously establishing that a state is the ground\nstate is computationally intractable [30], [31].\nA Boolean function is said to have irrelevant at-\ntributes if there are indexes such that the\nvalue of the function does not depend\non . For the sake of simplicity, let us suppose\nthese \u201cirrelevant attributes\u201d are .\nThen, the value of is determined entirely by its projection\nonto the relevant attributes, given, in this case, by\n(The choice of 0 for the last coordinates here is arbitrary, the\npoint being that the value of is independent of these.)\nB. Complexity With Irrelevant Attributes\nThe complexity of can be related to that of as\nfollows.\nTheorem 4.1: With this notation, we have\n(3)\nProof: To see why (3) holds, it is useful to recall the prob-\nabilistic interpretations of and . For , and\n, let denote the example obtained from by\n\u201cflipping\u201d the th component\u2014that is, by changing from 0 to\n1 or from 1 to 0\u2014and let be the example obtained from\nby flipping components and . Then, we have the following,\nwhere all probabilities indicated are uniform over choice of\nand over the choice of components to be flipped\nFrom these, (3) follows.\nConsider the -dimensional Boolean functions defined as the\nparity function on variables for . The\ncomplexity of these functions including the first- and second-\norder terms, and , can be written in terms of as\n(4)\n(5)\nThis follows from (3), because the functions concerned have\nirrelevant attributes, and because the projection onto the\nrelevant attributes is the parity function on vari-\nables, having and . It can also be seen\ndirectly: The two terms in (4) represent the fraction of pairs of\nexamples with opposite outputs at Hamming distances 1 and\n2, respectively. To find the function of this particular type with\nhighest complexity, we take the derivative of with re-\nspect to (assuming, temporarily, that is a continuous pa-\nrameter)\n(6)\nFRANCO AND ANTHONY: GENERALIZATION COMPLEXITY OF BOOLEAN FUNCTIONS 583\nFig. 4. (Top graph) Complexity (C + C ) for Boolean functions that\nimplement the parity function onN \u0000A variables as a function of the fraction\nof relevant variables (1 \u0000 (A=N)), for different values of N . (Bottom graph)\nGeneralization error versus the fraction of relevant variables for the case of the\nparity function on (N \u0000 A) bits.\nAssuming that , and calculating the maximizing value\n, the root of (6), we obtain\n(7)\nThe complexity of the corresponding function is\n(8)\nIn particular, for the case we obtain\nwhich yields, when is an integer, a Boolean function\nwith complexity\n(9)\nThe complexity of the function found is larger than 1.125 for\nany , indicating that we have found a very complex function.\n(For comparison, we note that the complexity of\nthe parity function and of a random function is approximately\n1.0.) As mentioned in Section IV-A, the problem of finding the\nmost complex functions is analogous to finding the ground state\nof magnetic systems, that can be rigorously analyzed only in\nfew cases. It is a specially difficult problem to analyze when\ncompeting (or frustrating) interactions exists, as is the case of\nthe complexity measure comprising at least two terms. Our re-\nsults show that there exists functions with a complexity\nof at least 1.125 but we can not prove that these are\nthe maximum complexity functions. An exhaustive analysis of\nall functions with inputs showed that the most complex\nfunction in that case had equals to 1.25 [4].\nC. Empirical Investigation\nWe empirically study the generalization error obtained when\nBoolean functions are implemented on feed-forward neural net-\nworks, to analyze its correlation with the complexity measure.\nWe show in Fig. 4 the values of the complexity (top)\nand generalization error (bottom) of parity functions that de-\npend on variables for the cases and . The\nbehavior of the generalization error (Fig. 4, bottom) follows ap-\nproximately the shape of the curve obtained for the complexity\n(Fig. 4, top) but only approximately. The com-\nplexity reaches a maximum value for a value of\napproximately equal to 0.7 (Fig. 4, top). The results from the\nsimulations for and show a generalization error\nhaving an upward trend, as the fraction of relevant variables in-\ncreases, reaching a maximum when there are no irrelevant vari-\nables. This might be explained by the fact these functions are\nquite regular and maybe easier to implement than other func-\ntions with the same amount of randomness. We also note that\nif the third term (or higher-order terms) is considered in the\ncomplexity measure then the prediction is that a local maximum\nvalue for the complexity as a function of the frac-\ntion of relevant variables no longer exists. (More information\nabout this is given later in this section.) We analyze in the dis-\ncussion what criteria to use when considering how many terms\nto include in the definition of the complexity measure.\nTo continue with the empirical analysis, we decided to look\nat some related functions with a greater element of randomness\nin their definition. We considered functions that implement the\nparity function of variables, where the final variables\non any given input example were subject to random alteration:\nEach of the final variables of an example were, with proba-\nbility 0.5, left unchanged, and with probability 0.5, were set to 0.\nOn each example, then, the function constructed computed the\nparity function of variables on that example, where\nis a value between 0 and , distributed according to a bino-\nmial distribution with mean . The set of functions found is\na complex one for which the generalization error can, for some\nvalues of , be larger than 0.5.\nThe simulations were performed in one hidden layer archi-\ntectures, trained with backpropagation, using half of the total\nnumber of examples for training, one fourth for validation and\nthe remaining fourth to measure the generalization error. The\nvalidation set was used to prevent overfitting, i.e., the valida-\ntion error is constantly monitored and at its minimum value the\ngeneralization error is measured. The training was done using\nstandard backpropagation with momentum with learning rate\nequal to 0.25 and momentum constant equal to 0.2. The number\nof neurons in the single hidden layer used in the architectures\nwere equal to the number of inputs. (Other possibilities were\nexplored and no significant differences were found.) The error\nbars plotted show the standard error of the mean (SME), when\n50 averages were taken. The SME decreases as the number of\ninput bits, , increases and for fixed it was approximately\n584 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 3, MAY 2006\nconstant as a function of , except for the case , in which\ncase the SME was normally larger.\nIn Fig. 5 the values of the complexity (top) and\ngeneralization error (bottom) obtained for the Boolean functions\nthat implement the parity function on variables (in\nthe sense described earlier) are shown for the cases of 8, 10,\n14. The generalization error is larger than 0.5 for some values of\n. Fig. 5 (bottom) indicates that a correlation is found\nwith the computed values (from the numerical simulations) for\nthe complexity Fig. 5 (top). Furthermore, by using the\nempirical value obtained for the maximum of the generalization\nerror as a function of we estimate, through (7), a\nvalue for . From the results, plotted in Fig. 5 (bottom), we\nobtain values of equal to 7\/8 and 8\/10 for 8 and\n10, respectively, that we correct to 7.5\/8 and 9\/10 because the\nfunctions were created by using the extra variables indicated\nby as mentioned before and thus can be considered to\nbe equivalent for extra variables. These values lead to\nequals to 0.5 and 0.5625 for the cases and ,\nrespectively (the simulations for the case do not show\na clear maximum value for the generalization error and this is\nwhy we did not do the estimation for this case.\nWe can also find functions for which is very high, inde-\npendently of . In this case, we repeat the procedure used in\n(4) to find that functions defined as the parity on variables\nhave a complexity equal to\n(10)\nwhich is larger than 0.5 for all . For the case , this\ngives a function with and we have exhaustively ver-\nified that this is the largest value that can be obtained for any\nBoolean function with . For cases where , we do\nnot know how close the value obtained for is to its maximum\npossible; and, as aforementioned, it could be that the demonstra-\ntion that this value corresponds to the functions with maximum\ncomplexity is intractable (Istrail, 2000). For symmetric Boolean\nfunctions, however, as we observe in Section V, the maximum\nvalue that can be achieved for is 0.5.\nD. Higher-Order Complexity Terms\nSuppose that is a Boolean function of variables, of which\nare irrelevant. Using the same notation as earlier, for\n, the th complexity term of can be related to the\ncomplexity terms (for ) of as follows.\nTheorem 4.2: With this notation\n(11)\nwhere and .\nProof: Again, we use the probabilistic interpretation of\n. For , and , let de-\nnote the element of obtained from by \u201cflipping\u201d the\ncomponents for \u2014that is, by changing these from 0\nto 1 or from 1 to 0. For , the number of relevant attributes\nFig. 5. (Top graph) Complexity (C +C ) and bottom graph) Generalization\nerror vs the fraction of relevant variables (1 \u0000 (A=N)) for Boolean functions\ndescribed in the text as the parity on (N \u0000A+ \u000e) variables for different values\nof N .\nin (which is ) has to be at least\n(since there are irrelevant attributes); and, the number of rel-\nevant attributes can be no more than (since\nthere are relevant attributes in total). In the Theorem\nstatement, if then should be considered equal to\n0. Then we have the following, where all probabilities indicated\nare uniform over choice of and over the choice of coordinate\nset of cardinality\nwhich is as required.\nConsider now, as earlier, the function defined as the parity\nfunction on variables, where is between 0 and .\n(So, the case is the usual parity function, and the case\nis a function that is constant (either identically 1 or\nidentically 0).\nFRANCO AND ANTHONY: GENERALIZATION COMPLEXITY OF BOOLEAN FUNCTIONS 585\nCorollary 4.3: Suppose that and that is the\nparity function on variables (having irrelevant at-\ntributes). Then, for\nwhere and\n.\nThis follows from (11), because the projection of onto\nthe relevant attributes is the parity function on\nvariables, having if is odd, and\notherwise.\nFor , let\nbe the complexity of when all complexity terms\nup to order have been included. We commented that in con-\nsidering , the parity func-\ntions on attributes have a complexity measure\nthat increases to a maximum and then decreases again, as the\nnumber of relevant attributes is increased (that is, as is de-\ncreased). However, this behavior appears not to be present when\nwe consider ; and also when we consider\nthe full complexity . In considering\nthese \u201cfuller\u201d measures of complexity, it appears that as the\nnumber of relevant attributes is increased, the complexity in-\ncreases monotonically, so that the parity function itself (with\nno irrelevant attributes) has the highest complexity among the\nfunctions. This perhaps explains the empirical results\non the generalization error for these functions (Fig. 4, bottom),\nsomething that we noted could not be explained simply by the\nfirst- and second-order complexity measure . We have\nseen that the first-order complexity is often an inadequate\nmeasure of complexity and that this can be corrected by adding\nthe second-order term. Now we see, additionally, that, for some\nclasses of function, even higher-order complexity terms are re-\nquired to reveal a correspondence with generalization error.\nV. COMPLEXITY OF SYMMETRIC BOOLEAN FUNCTIONS\nAn important class of Boolean functions is the class of sym-\nmetric functions, those for which the output depends only on the\nnumber of input bits ON (or, equivalently, on the weight of the\nexample). This class includes many important functions, such\nas the parity function and the majority function, and many re-\nsults regarding different properties of these functions have been\nobtained [1], [6], [7], [16], [32], [33]. We first determine inde-\npendently the maximum values of and that such func-\ntions can achieve and then by using an approximation, in which\nwe consider only the input examples with a balanced or almost\nbalanced number of input bits ON and OFF, we analyze which\nsymmetric functions have high complexity measure.\nFor the case of it is trivial to see that the parity function\nand its complement, for which , are the only Boolean\nfunctions for which is maximum, and they are symmetric.\nThe maximum possible value for among symmetric func-\ntions is 0.5, as we now show.\nFor a given number of input bits, , we organize the exam-\nples in levels according to the number of bits ON (number of\nbits equal to 1), . The number of OFF bits in a given ex-\nFig. 6. Poset for N = 4.\nample is then equal to . A useful picture to see\nhow the examples are organized in levels is the poset diagram\n(see Fig. 6) where neighboring examples at a Hamming distance\n1 are linked by bonds or edges. (If we identify examples with\nsubsets of , the poset is the power set of with respect to\nset inclusion.) In Fig. 6 the poset is shown for the case .\nThe number of examples at Hamming distance 2\nfrom any given example is . For any example , this\nnumber may be decomposed as\n(12)\nwhere is the number of examples at distance 2 in the\nsame level of the poset as , and is the number of\nexamples at distance 2 and in a different level (either level\nor ). Now, and depend only on the level\nof the poset to which belongs. Explicitly, for all in level\n(13)\nas this is the number of examples in the same level that have one\ndifferent bit ON and one different bit OFF but the same total\nnumber of bits ON. For in level\n(14)\nas this is the number of different examples that can be obtained\nfrom by flipping two ON bits to OFF or by flipping two OFF\nbits to ON. (The binomial coefficient is interpreted as 0 if\n.)\nFor a symmetric Boolean function, examples in the same level\nhave the same output. It follows that, in considering the com-\nplexity measure , only examples at distance 2 and in a dif-\nferent level need be considered. Therefore, if denotes level\nof the poset, we have\n(15)\n(16)\n(17)\nwhere\n(18)\n586 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 3, MAY 2006\nFig. 7. Triangle created from (18) starting from N = 2 up to N = 12. The\nith coefficient C in row j (j = 2 being the first row) has value C =\nC(i; j) = (1= ) ( + )= ((i + (j \u0000 i) \u0000 j)=(j \u0000 j)) .\nis times the maximum possible contribution to from\nthe examples in level .\nThe numbers of (18) are indicated (for to )\nin the triangular array of Fig. 7.\nIt is apparent that, in this triangle, an entry in a given row\ncan be obtained by adding the two elements that are located\nabove it in the preceding row. It can also be seen that the sum\nof the numbers in each row is double that of the sum in the\npreceding row. Both these observations are easily verified. It can\nbe checked that for any and\n(19)\nAdditionally, the sum of the numbers in row can be seen to\nbe as follows. We note that\nsince both sides of this identity are different expressions for the\nnumber of pairs where\nand . (Clearly, for each choice of there\nare possible , giving the right-hand side. But the same\nquantity can be calculated as follows. Choose some subset of\nand then some 2-subset of . This second approach\ngives the left-hand side.) Also\nsince both sides are the number of pairs where\n, and . It follows that\n(20)\nThis shows, in particular, that for any symmetric Boolean\nfunction, we have\n(21)\nThe maximum value of 0.5 for can be achieved for any\nby the symmetric functions with the property that the outputs\nchosen for the different levels alternate between 0 and 1 every\ntwo levels.\nVI. APPROXIMATING THE COMPLEXITY OF SYMMETRIC\nBOOLEAN FUNCTIONS\nWe now analyze approximately the -complexity of sym-\nmetric functions, where . The idea\nof the approximation is to focus on the middle layers of the\nposet, these being the largest, to compute locally the complexity\naround these layers.\nThe middle layer (in the case of even ) and the middle two\nlayers (in the case of odd ) are the most populated ones. Since\nthe complexity measure that we are considering involves exam-\nples up to Hamming distance 2, to compute our approximation,\nwe consider only the layers within distance 2 of these largest\nones. We first consider the case even and base our analysis,\ntherefore, on the levels , , , ,\n. We analyze the validity of the approximation by con-\nsidering the fraction of pairs of examples at Hamming distance\n1 (those contributing to ) that are taken into account by the\napproximation in proportion to the total number of pairs of ex-\namples at Hamming distance 1.\nFraction of pairs in the approx.\npairs\nTotal number of pairs (22)\nwhere pairs indicates the number of pairs between levels\nand . For the case of (22) leads to\nFraction of pairs in the approx. (23)\nStatistical tests of significance for the approximation were\ncarried for the cases and and they are detailed\nlater.\nWe express the approximated complexity of the symmetric\nBoolean functions in terms of the \u201cinteractions\u201d of the examples\nat Hamming distance 1 and 2 of these five levels and introduce a\nfunction that will account for this value. ,\nwhere and reflect the\nvalues of the interactions between the different levels consid-\nered. Explicitly, reflects the value of the interaction at Ham-\nming distance 1 between levels and and be-\ntween levels and . That is, takes value 0 if\nthe Boolean function assigns the same output to all these three\nlayers; it has value 1 if, for one of these pairs of layers, the out-\nputs are different and for the other pair they are equal; and it\nhas value 2 if for each pair of layers, the outputs are different.\nSimilarly, reflects the value of the interaction at Hamming\ndistance 1 between levels and and between\nlevels and . The parameters and\ndescribe distance-2 interactions: reflects the value of the in-\nteraction at Hamming distance 2 between levels and\n, and accounts for the interaction at Hamming dis-\ntance 2 between the middle level and levels .\nFRANCO AND ANTHONY: GENERALIZATION COMPLEXITY OF BOOLEAN FUNCTIONS 587\nTABLE I\nTHE ASSIGNMENTS OF VALUES TO THE MIDDLE FIVE LAYERS OF THE POSET\nAND THE CORRESPONDING PARAMETER VECTORS\nNow we approximate and assess numerically, for all config-\nurations of assignments of output values to these middle layers,\nthe complexity of symmetric functions whose outputs are con-\nsistent with these. Without any loss of generality we assume out-\nputs of examples in level to be 1. By symmetry, we then\nneed only consider the ten configurations of output values to\nthe five layers that are shown in the first column of Table I.\nHere, the assignment means, for example, that\nlayer is assigned 1, layer is assigned 0, and so\non. We also indicate, in the second column, the corresponding\nparameters .\nWe use two different ways of measuring the com-\nplexity in these five central layers. These approximations\nare made by two functions and of the parameters\n. These are given (for , 2) by\nwhere measures the complexity, relativized to these layers,\nand , are two approximations for the complexity, locally\naround these layers. The function is defined by\n(24)\nThe approximation is defined as follows:\n(25)\nHere, the leading factor of 1\/2 reflects the fact that, for sym-\nmetric functions, can be no more than 1\/2 (as shown in the\nprevious section). The function is given by\n(26)\nwhere , the number of pairs of examples in these five layers\nwhich are at distance 2 and in the same layer is\n(27)\nGenerally, we would expect to underestimate , because al-\nthough accounts for all distance-2 pairs within the same layer,\nin these five layers, does not account for possible distance-2\ninteractions between points of these five layers and points out-\nside these layers. (There is no term corresponding to a possible\ninteraction between layer and layer , and so\non.)\nTable II shows the values of , , , and for the\nconfigurations of interest (see Table I), for the case .\n(The first column indicates the appropriate parameter values.) In\nthe final column of the table, we give the mean values of , ,\nover all symmetric functions on variables which\nextend the given configuration on the five central layers. So, for\ninstance, for the last entry of the first column, we consider all\nthose symmetric Boolean functions on which assign\nvalues 1, 0, 1, 0, 1 to layers 5, 6, 7, 8, 9 (respectively)\u2014that\nis, all those that extend the pattern of the central\nlayers\u2014and we compute the mean values of , , and\nover all such functions.\nTable II shows the mean values of the complexity measures\nfor extensions of the given configurations of the middle layers.\nMore information is provided by the distribution of these. For\ninstance, Fig. 8 shows the distribution of complexities ,\nand for extensions of the configuration [cor-\nresponding to the third row of Table II with equal to (2, 1,\n0, 1)]. As noted in Table II, in this case the means of ,\nand are (respectively) 0.709, 0.194, and 0.903. The corre-\nsponding standard deviations are 0.067, 0.047, and 0.082. The\nminimum values of each are 0.576, 0.097 and 0.733, and the\nmaximum values are 0.843, 0.290, and 1.0, respectively. We\nalso assessed statistically how good the approximations of\nand using only examples from the five middle layers were\nin relationship to the real values. For the case of we\ncomputed the regression value between the true complexity of\nall the symmetric Boolean functions and their respective ap-\nproximated value, finding the Pearson correlation coefficient to\nbe highly significant and for and\n, respectively. For the case we computed the cor-\nrelation between the values given by the approximations and\nthe complexity values of all the symmetric Boolean functions\ncompatible with each of the 10 different function approxima-\ntions defined in Table I ( functions were considered for each\nof the cases) to find values of the Pearson correlation equal to\nand for and , respectively. The\ncorrelation was statistically significant in all cases .\nSo far we have considered the case of even . To analyze\nthe case of odd , we would consider the four most populated\nlevels with . Suppose (without loss\nof generality) that a Boolean function assigns value 1 to the\nexamples in level . If then alternates its values be-\ntween immediate layers of the poset, we obtain the parity func-\ntion or its complement, for which and . Another\ninteresting configuration is that in which assigns value 1 to ex-\namples in layer and then alternates its values every\ntwo layers. As we have seen, this gives the maximum possible\nvalue (for a symmetric function) of , namely . The\nfunction also has , as we now show. Suppose that\nis of the form for a positive integer . (A very similar\n588 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 3, MAY 2006\nTABLE II\nAPPROXIMATIONS TO THE COMPLEXITIES C , C AND C OF FUNCTIONS HAVING A GIVEN CONFIGURATION OF OUTPUTS ON THE MIDDLE FIVE LAYERS (IN THE\nCASE N = 14). (SEE TEXT AND TABLE I.) THE LAST COLUMN SHOWS THE MEAN VALUES OF COMPLEXITIES OF ALL FUNCTIONS CONSISTENT WITH THESE\nGIVEN OUTPUT PATTERNS OF THE MIDDLE LAYERS. APPROXIMATIONS AND MEANS OF C COMPLEXITIES ARE HIGHLIGHTED IN BOLD\nargument can be made if, instead, is of the form .) The\ncontributions to are of two types: Those arising from, for\neach example in layer , the neighbors in layer (for\n) and (equally) those arising from, for each example\nin layer , the neighbors in layer . These two\ntypes of contribution are equal, since each is the total number of\nedges between layers and . We, therefore, have\n(28)\nNow, we can argue combinatorially, as follows:\nis equal to the number of pairs where is of even\ncardinality and . (For, there are subsets of cardi-\nnality and, for each, there are choices for .) Now, suppose\nthat . Let be any element of not equal to . Then\nthe subsets of containing can be partitioned into\npairs where runs through all subsets of\ncontaining but not containing . Since precisely one member\nof each such pair is of even cardinality, and since there are\nchoices for , it follows that the number of pairs where\nand is even is exactly . Hence\nas claimed.\nVII. DISCUSSION AND CONCLUSION\nThe ability to generalize is a very interesting and important\nproperty of many intelligent systems like humans and neural\nnetworks, and a lot of effort has been devoted to its under-\nstanding. We analyzed in this paper a recently proposed mea-\nsure for the complexity of Boolean functions related to the dif-\nficulty of generalization when neural networks are trained using\nexamples of the function. We studied the first-order ( ) and\nsecond-order ( ) terms of the complexity measure and demon-\nstrated the importance of the second term in obtaining accurate\ncomparisons with the generalization error. Furthermore, we an-\nalyzed the relationship between and the amount of random-\nness introduced in three different classes of functions and found\na very clear correlation, suggesting that the second-order com-\nplexity term can be used as an estimate of the randomness ex-\nisting in a Boolean function. This is, we believe, an important\nfeature, since measuring randomness is a very complicated and\ndelicate matter [34], [35], and also because it is important to\nquantify randomness in applications such as cryptography [12].\nBy using an assumption on the nature of the most complex\nfunctions based on some results from statistical mechanics,\nwe have been able to obtain very complex Boolean functions,\nwith a complexity larger than 1. We showed empirically that\nthe difficulty of generalization for these functions was related\nto the complexity measure (once the functions were modified\nby adding a controlled element of randomness). We have\nseen in the experiments in Section III that the use of the first\nand second term were enough to get a nice match between\ngeneralization and complexity for different classes of functions\nbut when in Section IV we analyzed the parity function using\nsome irrelevant variables we found that the two first terms\nwere not enough and that higher-order terms were needed to\nobtain an accurate match. Our opinion is that when analyzing\nlarge classes of functions, in which the values are averaged\nacross many different functions around a certain complexity\nthe two first terms are enough, but when the analysis is done for\nvery few functions, as is the case of the functions analyzed in\nSection III it might be necessary to include terms of third-order\nor higher, but this is an issue that needs further investigations.\nRegarding the values of the constants that weigh the dif-\nferent terms in the complexity measure in relationship to the\nfirst term, we have been able to estimate the value of to be\nclose to 0.5 (the estimations were 0.5 and 0.5625 for\nand 10 inputs, respectively). We note that the estimation relies\non some approximations, and so they should be not considered\nas definitive values, and that is why we take a conservative\napproach in some parts of this paper and use the value of\nused in previous analysis [4], [5] in which was equal to 1.\nThis value permits, as demonstrated in Section III, to obtain a\ngood match between generalization error and complexity for\ndifferent classes of functions. We have also observed in the\ndifferent studies carried out in this paper and in [4], [5] that\nfor large classes of functions higher-order terms normally have\na similar value to the first and second, in particular the first,\nthird, fifth terms tends to have similar values and the same for\nthe even terms (second, fourth, sixth, etc.). The consequence\nof the previous observations in relationship to the values of the\nconstants would be that consecutive terms will have to be\nweighted by an amount proportional to the relationship between\nfirst and second terms (i.e, ; ) but\nwe do not have yet experimental or theoretical justification for\na proper choice of their absolute values. As higher-order terms\nFRANCO AND ANTHONY: GENERALIZATION COMPLEXITY OF BOOLEAN FUNCTIONS 589\nFig. 8. Number of functions compatible with the values of the middle layer\nbeing 10100, and having givenC , C and C = C +C complexities (top,\nmiddle, bottom histograms, respectively).\nweight the effect of examples at larger Hamming distances the\nlogic indicates that their weight should be not higher than those\nfor the first and second terms.\nFor the class of symmetric Boolean functions, we first ob-\ntained a general bound on the maximum value that the second-\norder term of the complexity can take and, secondly, by focusing\non the most populated levels of inputs, we found approximate\nvalues for the complexity of certain symmetric functions (and,\nin particular, we were able to obtain an indication that some\ncomplex symmetric functions existed). We have performed sta-\ntistical tests that showed that these approximations compared\nwell (for and ) with the computationally cal-\nculated actual values of the complexities.\nAs a whole, the results presented in this paper show that the\ncomplexity measure introduced in [4] can be used to charac-\nterize different classes of Boolean functions in relationship to\nthe complexity of generalization, and we think that this may lead\nto new lines of research contributing to a better understanding of\nthe difficulty of learning Boolean functions by neural networks\nand in particular to the study of how changes in the neural ar-\nchitecture affect the computability of functions [36], [37].\nClassical statistical learning theory (see [25] or [26], for ex-\nample) suggests that the difference between generalization error\nand training error (that is, between the error on a test set and\non the training set) can be bounded (with high probability) by\na quantity that depends on the size of the network (more pre-\ncisely, its VC-dimension) and is independent of the function\nbeing learned. In particular, this leads to an upper bound on the\ngeneralization error that increases as the training error increases.\nIt is possible that a perceived correlation between complexity\nand generalization error is in large part due to a correlation be-\ntween complexity and training error (a higher complexity indi-\ncating that it is more difficult to \u201cfit\u201d the network to the func-\ntion). More recently, certain nonuniform bounds depending on\nthe sizes of the weights in the network after training have been\nobtained [38]. (These weights can depend on the particular func-\ntion being learned, and in this sense are nonuniform.) As men-\ntioned, the classical VC-dimension based results from statistical\nlearning theory give a (high-probability) upper bound on the\ndifference between generalization and training errors, and this\nbound depends on the size of the network, but not on the func-\ntion. We conjecture that such a uniform bound can be replaced\nby one that involves not only the size of the network, but also\nthe complexity of the particular Boolean function being learned.\nFurther experiments would indicate whether this is the case.\nWe are currently exploring different extensions of this work,\nsuch as the generalization of the measure to continuous input\nfunctions, the use of the complexity measure for individual pat-\nterns to improve learning, the construction of neural networks\narchitectures for restricted classes of Boolean functions, the\nuse of the second term of the complexity measure to estimate\nrandomness, and also applications to the statistical mechanics\nof magnetic systems. In particular, we think that the complexity\nmeasure will be an important element in order to study how\nchanges in the architecture (number of hidden layers and\nnumber of neurons on each layer) affect the generalization\nability and we are currently doing such analysis on the class of\nsymmetric Boolean functions.\nACKNOWLEDGMENT\nFruitful discussions with Dr. S. A. Cannas are gratefully ac-\nknowledged. The authors also acknowledge valuable comments\nand suggestions from three anonymous reviewers.\n590 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 3, MAY 2006\nREFERENCES\n[1] I. Wegener, The Complexity of Boolean Functions. New York: Wiley,\n1987.\n[2] J. Hastad, \u201cAlmost optimal lower bounds for small depth circuits,\u201d Adv.\nComput. Res., vol. 5, pp. 143\u2013170, 1989.\n[3] I. Parberry, Circuit Complexity and Neural Networks. Cambridge,\nMA: MIT Press, 1994.\n[4] L. Franco, \u201cGeneralization ability of Boolean functions implemented in\nfeedforward neural networks,\u201d Neurocomputing, 2006, to be published.\n[5] L. Franco and S. A. Cannas, \u201cNon-glassy ground state in a long-range\nantiferromagnetic frustrated model in the hypercubic cell,\u201d Physica A,\nvol. 332, pp. 337\u2013348, 2004.\n[6] , \u201cGeneralization and selection of examples in feedforward neural\nnetworks,\u201d Neural Comput., vol. 12, pp. 2405\u20132426, 2000.\n[7] , \u201cGeneralization properties of modular networks implementing\nthe parity function,\u201d IEEE Trans. Neural Netw., vol. 12, no. 6, pp.\n1306\u20131313, Nov. 2001.\n[8] J. Kahn, G. Kalai, and N. Linial, \u201cThe influence of variables on Boolean\nfunctions,\u201d in Proc. 29th IEEE Symp. Foundations of Computer Science\n(FOCS\u201988), 1988, pp. 68\u201380.\n[9] N. Linial, Y. Mansour, and N. Nisan, \u201cConstant depth circuits, Fourier\ntransform and learnability,\u201d J. ACM, vol. 40, pp. 607\u2013620, 1993.\n[10] L. G. Valiant, \u201cA Theory of the learnable,\u201d Commun. ACM, vol. 27, pp.\n1134\u20131142, 1984.\n[11] R. B. Boppana, \u201cThe average sensitivity of bounded-depth circuits,\u201d Inf.\nProcess. Lett., vol. 63, pp. 257\u2013261, 1997.\n[12] A. Bernasconi, C. Damm, and I. Shparlinski, \u201cThe average sensitivity of\nsquare-freeness,\u201d Comput. Complex., vol. 9, pp. 39\u201351, 2000.\n[13] J. Feldman, \u201cMinimization of Boolean complexity in human concept\nlearning,\u201d Nature, vol. 407, pp. 572\u2013573, 2000.\n[14] O. C. Martin, R. Monasson, and R. Zecchina, \u201cStatistical mechanics\nmethods and phase transitions in optimization problems,\u201d Theor.\nComput. Sci., vol. 265, pp. 3\u201367, 2001.\n[15] S. Mertens, \u201cComputational complexity for physicists,\u201d Comput. Sci.\nEng., vol. 4, pp. 31\u201347, 2002.\n[16] K. Y. Siu, V. P. Roychowdhury, and T. Kailath, \u201cDepth-size tradeoffs\nfor neural computation,\u201d IEEE Trans. Comput., vol. 40, no. 12, pp.\n1402\u20131412, Dec. 1991.\n[17] M. Ben-Or and N. Linial, \u201cCollective coin flipping,\u201d in Randomness and\nComputation, S. Micali, Ed. New York: Academic, 1989, pp. 91\u2013115.\n[18] M. Anthony, G. Brightwell, and J. Shawe-Taylor, \u201cOn specifying\nBoolean functions by labeled examples,\u201d Discrete Appl. Math., vol. 61,\npp. 1\u201325, 1995.\n[19] V. I. Torvik and E. Triantaphyllou, \u201cMinimizing the average query com-\nplexity of learning monotone Boolean functions,\u201d INFORMS J. Comput.,\nvol. 14, pp. 142\u2013172, 2002.\n[20] A. G\u00e1l and A. Ros\u00e9n, \u201cA theorem on sensitivity and applications in\nprivate computation,\u201d in Proc. 31st ACM Symp. Theory of Computing,\n1999, pp. 348\u2013357.\n[21] M. L. Furst, J. C. Jackson, and S. W. Smith, \u201cImproved learning ofAC\nfunctions,\u201d in Proc. 4th Annu. Workshop on Computational Learning\nTheory, 1991, pp. 317\u2013325.\n[22] J. Bruck and R. Smolensky, \u201cPolynomial threshold functions, AC\nfunctions, and spectral norms,\u201d SIAM J. Comput., vol. 21, pp. 33\u201342,\n1992.\n[23] M. Saks, \u201cSlicing the hypercube,\u201d in Surveys in Combinatorics (Invited\nTalks of the 1993 British Combinatorial Conf.). Cambridge, U.K.:\nCambridge Univ. Press, 1993, pp. 211\u2013255.\n[24] V. P. Roychowdhury, K. Y. Siu, and A. Orlitsky, \u201cNeural models\nand spectral methods,\u201d in Theoretical Advances in Neural Computa-\ntion and Learning, V. P. Roychowdhury, K. Y. Siu, and A. Orlitsky,\nEds. Boston, MA: Kluwer Academic, 1994.\n[25] V. N. Vapnik, Statistical Learning Theory. New York: Wiley, 1998.\n[26] M. Anthony and P. L. Bartlett, Neural Network Learning: Theoretical\nFoundations. Cambridge, U.K.: Cambridge Univ. Press, 1999.\n[27] S. Haykin, Neural Networks: A Comprehensive Foundation: Macmillan,\n1994.\n[28] H. Zhu and W. Kinzel, \u201cAntipredictable sequences: harder to predict\nthan random sequences,\u201d Neural Comput., vol. 10, pp. 2219\u20132230, 1998.\n[29] P. Chandra, P. Coleman, and I. Ritchey, \u201cThe anisotropic kagome anti-\nferromagnet: a topical spin glass?,\u201d J. Phys. I, vol. 3, pp. 591\u2013610, 1993.\n[30] F. Barahona, \u201cOn the computational complexity of ising spin glasses,\u201d\nJ. Phys. A: Math. Gen., vol. 15, pp. 3241\u20133253, 1982.\n[31] S. Istrail, \u201cStatistical mechanics, three-dimensionality and NP-com-\npleteness I. Universality of intractability for the partition function of\nthe Ising model accross nonplanar lattices,\u201d in Proc. 32nd ACM Symp.\nTheory of Computing (STOC 2000), 2000, pp. 87\u201396.\n[32] S. D. Cotofana and S. Vassiliadis, \u201cPeriodic symmetric functions, serial\naddition, and multiplication with neural networks,\u201d IEEE Trans. Neural\nNetw., vol. 9, no. 6, pp. 1118\u20131128, Nov. 1998.\n[33] L. Franco and S. A. Cannas, \u201cSolving arithmetic problems using feed-\nforward neural networks,\u201d Neurocomput., vol. 18, pp. 61\u201379, 1965.\n[34] A. Kolmogorov, \u201cThree approaches to the quantitative definition of in-\nformation,\u201d Probl. Info. Transmiss., vol. 1, pp. 1\u201317, 1965.\n[35] G. Chaitin, \u201cRandomness and mathematical proof,\u201d Scientif. Amer., vol.\n232, pp. 47\u201352, 1975.\n[36] V. Deolalikar, \u201cMapping Boolean functions with neural networks having\nbinary weights and zero thresholds,\u201d IEEE Trans. Neural Netw., vol. 12,\nno. 3, pp. 639\u2013642, May 2001.\n[37] G.-B. Huang, \u201cLearning capability and storage capacity of two-hidden-\nlayer feedforward networks,\u201d IEEE Trans. Neural Netw., vol. 14, no. 2,\npp. 274\u2013281, Mar. 2003.\n[38] P. L. Bartlett, \u201cThe sample complexity of pattern classification with\nneural networks: the size of the weights is more important than the size\nof the network,\u201d IEEE Trans. Inf. Theory, vol. 44, no. 2, pp. 525\u2013536,\nMar. 1998.\nLeonardo Franco (M\u201906) was born in Tucum\u00e1n, Ar-\ngentina. He received the M.S. and Ph.D. degrees in\nphysics from the University of C\u00f3rdoba, Argentina,\nwhere he analyzed the generalization properties of\nfeed-forward neural networks.\nHe then became a Postdoctoral Fellow to SISSA,\nTrieste, Italy, where he became involved in computa-\ntional neuroscience. He then moved to the University\nof Oxford, U.K., as a Research Scientist where he ap-\nplied and developed information theory methods to\nthe analysis of neuronal recordings. He is currently\nwith M\u00e1laga University, Spain, as a Ram\u00f3n y Cajal researcher working on neural\nnetworks, their applications to biomedical problems, and also in computational\nneuroscience. He has authored approximately 25 publications in journals and\ninternational conferences.\nMartin Anthony was raised in Paisley, Scotland.\nHe received the B.Sc. degree in mathematics from\nthe University of Glasgow, Scotland, and the Ph.D.\ndegree, also in mathematics from the University of\nLondon, U.K.\nSince 1990, he has been on the Mathematics\nfaculty of the London School of Economics (LSE).\nHe is now Professor of Mathematics and Convener\nof the Mathematics Department at LSE. He has\npublished widely on the mathematical theory of\nmachine learning and neural networks, including\nComputational Learning Theory: An Introduction (with N. L. Biggs), Neural\nNetwork Learning: Theoretical Foundations (with P. L. Bartlett), and Discrete\nMathematics of Neural Networks: Selected Topics.\n"}