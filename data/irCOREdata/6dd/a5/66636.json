{"doi":"10.1007\/11561071_66","coreId":"66636","oai":"oai:dro.dur.ac.uk.OAI2:650","identifiers":["oai:dro.dur.ac.uk.OAI2:650","10.1007\/11561071_66"],"title":"Finding frequent patterns in a string in sublinear time.","authors":["Berenbrink,  P.","Ergun,  F.","Friedetzky,  T."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":["Brodal,  G. S.","Leonardi,  S."],"datePublished":"2005-10","abstract":"We consider the problem of testing whether (a large part of) a given string X of length n over some finite alphabet is covered by multiple occurrences of some (unspecified) pattern Y of arbitrary length in the combinatorial property testing model. Our algorithms randomly query a sublinear number of positions of X, and run in sublinear time in n. We first focus on finding patterns of a given length, and then discuss finding patterns of unspecified length","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66636.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/650\/1\/650.pdf","pdfHashValue":"cb754cbbc15f58192e2e1c83043f03e3abd7d998","publisher":"Springer","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:650<\/identifier><datestamp>\n      2015-03-31T11:36:45Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Finding frequent patterns in a string in sublinear time.<\/dc:title><dc:creator>\n        Berenbrink,  P.<\/dc:creator><dc:creator>\n        Ergun,  F.<\/dc:creator><dc:creator>\n        Friedetzky,  T.<\/dc:creator><dc:description>\n        We consider the problem of testing whether (a large part of) a given string X of length n over some finite alphabet is covered by multiple occurrences of some (unspecified) pattern Y of arbitrary length in the combinatorial property testing model. Our algorithms randomly query a sublinear number of positions of X, and run in sublinear time in n. We first focus on finding patterns of a given length, and then discuss finding patterns of unspecified length.<\/dc:description><dc:publisher>\n        Springer<\/dc:publisher><dc:source>\n        Brodal,  G. S. & Leonardi,  S. (Eds.).  Algorithms - ESA 2005 : 13th Annual European Symposium, 3-6 October 2005, Palma de Mallorca, Spain ; proceedings. Berlin: Springer, pp. 746-757, Lecture notes in computer science(3669)<\/dc:source><dc:contributor>\n        Brodal,  G. S.<\/dc:contributor><dc:contributor>\n        Leonardi,  S.<\/dc:contributor><dc:date>\n        2005-10<\/dc:date><dc:type>\n        Book chapter<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:650<\/dc:identifier><dc:identifier>\n        issn:0302-9743<\/dc:identifier><dc:identifier>\n        issn: 1611-3349<\/dc:identifier><dc:identifier>\n        doi:10.1007\/11561071_66<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/650\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1007\/11561071_66<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/650\/1\/650.pdf<\/dc:identifier><dc:rights>\n        The final publication is available at Springer via http:\/\/dx.doi.org\/10.1007\/11561071_66<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["0302-9743"," 1611-3349","issn: 1611-3349","issn:0302-9743"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2005,"topics":[],"subject":["Book chapter","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n04 November 2008\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nBerenbrink, P. and Ergun, F. and Friedetzky, T. (2005) \u2019Finding frequent patterns in a string in sublinear\ntime.\u2019, in Algorithms - ESA 2005 : 13th Annual European Symposium, 3-6 October 2005, Palma de Mallorca,\nSpain ; proceedings. Berlin: Springer, pp. 746-757. Lecture notes in computer science. (3669).\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1007\/1156107166\nPublisher\u2019s copyright statement:\nThe original publication is available at www.springerlink.com\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\nFinding Frequent Patterns in a String in\nSublinear Time\nPetra Berenbrink1, Funda Ergun2, and Tom Friedetzky3\n1 School of Computing Science, Simon Fraser University, Burnaby, B.C., V5A 1S6,\nCanada, WWW: http:\/\/www.cs.sfu.ca\/~petra\/\n2 School of Computing Science, Simon Fraser University, Burnaby, B.C., V5A 1S6,\nCanada, WWW: http:\/\/www.cs.sfu.ca\/~funda\/\n3 Department of Computer Science, Durham University, Durham, DH1 3LE, U.K.,\nWWW: http:\/\/www.dur.ac.uk\/tom.friedetzky\/\nAbstract. We consider the problem of testing whether (a large part\nof) a given string X of length n over some finite alphabet is covered by\nmultiple occurrences of some (unspecified) pattern Y of arbitrary length\nin the combinatorial property testing model. Our algorithms randomly\nquery a sublinear number of positions of X, and run in sublinear time in\nn. We first focus on finding patterns of a given length, and then discuss\nfinding patterns of unspecified length.\n1 Introduction\nThe problem of finding frequent occurrences of patterns in a string comes up in\nmany areas such as telecommunications, e-commerce, and databases, where the\napplications generate long data streams to be analyzed. An example from data\nmining is efficient handling of iceberg queries, that is, identifying those objects in\na data stream which occur with frequency over a threshold. In property testing of\nstrings, testing whether a string consists of back-to-back repetitions of the same\npatterns is called periodicity testing. Usually, the efforts to efficiently identify\nsuch trends in data are hampered by the large size of the data, which can be too\nlarge to fit into main memory and to be efficiently analyzable, even by a linear\ntime algorithm.\nIn this work, we are interested in detecting frequent repetitions of a pattern\n(of any size) in a string of length n in time sublinear in n. In contrast to previ-\nous work, the pattern boundaries are unrestricted, which, while more realistic,\ncomplicates matters. We explore this problem in the combinatorial property test-\ning model ([10, 3]), and first obtain an algorithm which distinguishes between\nstrings which are mostly covered with occurrences of one pattern of given size k\nand those that do not contain a large number of repeated patterns in o(k) time.\nWe then generalize our result to detecting repetitions of patterns of unspecified\nlength, in o(n) time. In both cases, if a frequent pattern exists, the algorithm\ncan implicitly return a (likely approximate) copy of the pattern.\nThe fact that patterns can occur anywhere in the string means that there\ncan be a linear number of possible patterns of linear size in a given string,\nand comparing them to one another can easily lead to inefficient algorithms.\nTo handle this, we represent each pattern as a short \u201csketch\u201d or a \u201csignature\u201d.\nHowever, there can still be a linear number of signatures sharing one location.\nTo deal with this, we use a sparse representation of our data which is based on\nthe few locations sampled. We show that this small amount of information is\nsufficient to make conclusions about repeated trends in the input stream.\nOur Results. We first present an algorithm which, given a string X and a length\nk, tests in O(\n\u221a\nkpolylogk) time if there exists a pattern Y of length k that covers\nX (notice that Y is not given); allowing the occurrences of the pattern to overlap.\nWe say that Y approximately covers an \u03b1-fraction of X if there exists a set of\nsubstrings Z = {Z1, . . . , Zj} of X (each of length k) where h(Zi, Y ) \u2264 \u01ebk for\nall Zi \u2208 Z and at least \u03b1n locations of X are covered by some Zi \u2208 Z. Here\nh(Zi, Y ) is the Hamming distance between Y and Zi. If a pattern of length k\nexists that covers all of X , with probability 1\u2212o(1) our algorithm outputs Pass.\nIf there is no pattern Y of length k that covers an \u03b1-fraction of X , \u03b1, \u01eb \u2208 (0, 1),\nit outputs Fail with probability 1\u2212 o(1).\nNext, we give an algorithmwhich, givenX and k, tests in time O(\n\u221a\nkpolylogk)\nwhether there is a string of a length \u2113 \u2208 [\u03b4k, . . . , k] covering an \u03b1-fraction of X ;\nit outputs Fail if there is no pattern Y of length k that approximately covers\nan (1 \u2212 \u03b2)\u03b1-fraction of X , with probability 3\/4, with certain restrictions on\n\u03b1, \u03b2 and \u01eb. We use this variant to test if there is a pattern of any length that\ncovers an \u03b1-fraction of X , or if no pattern exists that approximately covers a\n(1\u2212 \u03b2)\u03b1-fraction of X .\nRelated Work. Combinatorial property testing was first defined by [10, 3]. For an\noverview of results see [9] and the references therein. Two recent related results\ntesting whether a string is close to periodic are [2, 7]. They assume that the size\nof the period is fixed so that position in which the period (corresponding to\n\u201cpatterns\u201d) appear is fixed, too. This means that it is more or less clear which\npositions should be sampled. In our case, since a pattern can be anywhere, we\nneed to make sure that we have samples in all possible places that a pattern\ncan be 4. Another related result tests in sublinear time whether two strings have\nlarge edit distance ([1]).\nThere have also been sublinear space streaming results. Iceberg queries for\nidentifying objects that appear in more than a fraction of a string are explored\nin [6]. Periodicity testing is investigated using sketches in [5] where the running\ntime is considered in terms of memory accesses. These techniques differ from\nours that they are not bound by sublinear time, but are expected to return more\naccurate answers.\n2 Preliminaries\nGiven string X of length n, let X [i] refer to the ith character of X . r = [i : j]\ndenotes {i, i+ 1, . . . , j}, where i and j are called respectively the left and right\n4 To achieve this, we use an extra stage of sampling where one of the stages makes\nsure that two copies of the same pattern will be correctly aligned.\nendpoints of r. X [i : j] denotes the substring of X starting at location i and\nending at j. [n] is short for [1 : n] = {1, . . . , n}.\nGiven a location i in X , we say that a length k substring (or pattern) Y\n\u201ccovers\u201d, \u201ccontains\u201d, or \u201cappears around\u201d i if and only if Y = X [j : j + k \u2212 1]\nsuch that i \u2208 [j : j+k\u22121]. Here Y only refers to the contents of the substring, and\nthus, can be repeated elsewhere in X . We call each such repetition an occurrence.\nh(X,Y ) denotes the Hamming distance between X and Y .\n3 Finding frequent patterns of given length k\nWe first consider finding patterns of length exactly k.\n3.1 Length exactly k\nWe now formally define the problem of testing for frequent patterns. Formally,\ngiven a string X of length n and 1 \u2264 k \u2264 \u03b1n, we would like to have an algorithm\nwith the following behavior.\n\u2013 If there is a pattern Y of length k which covers all locations of X , then the\nalgorithm returns Pass with probability 1\u2212 o(1).\n\u2013 If there is no pattern Y of length k such that a set of substrings Z =\n{Z1, . . . , Zj} of X (of length k) exist where h(Zi, Y ) \u2264 \u01ebk for all Zi \u2208 Z and\nat least \u03b1n locations of X are covered by some Zi \u2208 Z, then the algorithm\nreturns Fail with probability 1\u2212 o(1).\nNote that the occurrences of the pattern can overlap. For a visual intuition on\nthe Fail condition, consider a scheme to mark an X with respect to a pattern\nY of length k. For j = 1, . . . , n\u2212 k + 1, if h(Y,X [j : j + k \u2212 1]) \u2264 \u01ebk then mark\nlocations X [j], . . . , X [j + k \u2212 1]. In the end, if some X [i] remains unmarked,\nthen there exists no substring Z of X of length k that covers location i such\nthat h(Z, Y ) \u2264 \u01ebk. The Fail condition holds if and only if the marking of X\nwith respect to Y results in at least (1 \u2212 \u03b1)n unmarked locations for any Y of\nof length k.\nConsider X = abcabcaabcaabca. Y = abca covers X fully, where the a in\nlocation 4 is covered by two overlapping copies of Y , thus the algorithm should\nreturn Pass. Now let X \u2032 = abcdbaddacdedbcbe. Substrings abcd, dbad, dacd,\ndbcb, cover all but two characters of X \u2032, and each has Hamming distance 1 to\nY = dbcd; thus the algorithm can return either Pass of Fail. Later, we will show\nhow our results translate into the non-overlapping case, where our definition of\ndistance will be equivalent to the usual one.\nThe approach of randomly choosing a few locations in X and checking\nwhether there is a pattern which covers all of these locations is not straightfor-\nward to implement in sublinear time, for two reasons. First, even if two random\nlocations i and j lie in two occurrences of the same pattern, they are likely to be\nin different positions within the two occurrences, with k2 possible location pairs.\nThis hurdle is not present in periodicity testing where the pattern boundaries\nare fixed. Second, the fact that locations p1 and p2, as well as p1 and p3 occur\nwithin the same pattern does not imply that p2 and p3 do. Two patterns can\nshare \u0398(k) many patterns of length k; thus, finding one shared by all of the\nsample points in o(n) time is nontrivial.\nA Three-Stage Sampling Approach We now present our approach which\ntackles the above problems in time o(k). To do that, we will use sampling and\nkeep small summaries of our samples in \u201csignatures\u201d. Let \u2113p = \u0398(polylogn),\n\u2113s = \u0398(\n\u221a\nk log log k), \u2113t = \u0398(polylogn), with large enough constants hidden in\nthe \u0398. Our sampling has three stages, where Stages 1 and 2 obtain primary and\nsecondary locations and Stage 3 the actual samples.\nStage 1: Construct set P = {p0, p1, . . . , p\u2113p}, of primary locations, where each\npi is chosen independently and uniformly at random (i.u.r.) from [n].\nStage 2: For each pi \u2208 P , construct set Si of secondary locations, said to be\nowned by pi, of the form Si = {si,0, si,1, . . . , si,ls}, 5 where each si,j \u2208 Si is\nchosen i.u.r. from [pi \u2212 k : pi + k]. 6\nStage 3: Construct a sorted list of locations T = t1t2 . . . t\u2113t where the ti are\npicked i.u.r. from [\u22122k : 2k] and are in ascending order. Now consider any\nsecondary location si,j \u2208 Si. Obtain samples Ti,j = si,j+t1, si,j+t2, . . . , si,j+t\u2113t ;\nthese will be owned by si,j . The elements of Tij are uniformly distributed in\n[si,j \u2212 2k : si,j + 2k]; furthermore, the locations of the samples relative to any\nsecondary location s that owns them is identical across all s (Fig. 1).\nstring X\n3.\ndistributed around s5,1\nT5,2 T5,3 T5,4\n\u201creal samples\u201dT5,1, samples owned by s5,1 \u2208 S5\naccording to T ; |T5,1| = \u2113t\nprimary location (say) p5 \u2208 P , |P | = \u2113p + 1\n1.\n2. secondary locations s5,j \u2208 S5 owned by p5, |S5| = \u2113s, i.u.r. from [p5 \u2212 k, p5 + k]\nFig. 1. Example for primary location p5; steps (1,2,3) indicate order of selection\nTemplates and Signatures. We will represent each substring of length k ofX with\na short signature. To do this, decompose T into sublists which we call templates,\neach of which contains the offsets to obtain samples to form a signature.\nDefinition 1. A list \u03c4 is said to be a template (of T) if for some \u22122k < i \u2264 k+1,\n\u03c4 , \u03c4 is the maximal sublist of T whose elements are in the range [i : i+ k \u2212 1].\nThe following lemma shows templates are large enough.\nLemma 1. Let \u2113t = 24c\u00af log k for some large enough constant c. With probability\nat least 1\u2212 o(1\/k) every template consists of at least c\u00af log k characters.\nProof. Consider the probability that there exists a j \u2208 [\u22122k : 2k \u2212 j + 1] such\nthat there are fewer than c\u00af log k elements in T whose values are in [j : j+ k\u2212 1].\nTo do this, partition the interval [\u22122k : 2k] into 12 subintervals of length k\/3.\n5 We will drop subscripts later when they are obvious.\n6 If pi < k (pi > n\u2212 k) then the area [1 : pi + k] ([pi \u2212 k : n]) is sampled.\nAny interval [j : j + k \u2212 1] will fully contain such subinterval. The expected\nnumber of elements of T in a subinterval is 2c\u00af log k, which is a lower bound\non the expected length of a signature. Using Chernoff bounds (see e.g. [4]) the\nprobability that a particular subinterval will have fewer than c\u00af log k samples is\nat most e\u2212c\u00af log k = o(1\/k). Thus, the probability that at least one subinterval\nwill have fewer than that many elements is also at most o(1\/k).\nThe next observation holds by symmetry, showing that the sample points are\nuniformly distributed over a template.\nObservation 1 For an interval r = [i : i + k \u2212 1] for \u22122k \u2264 i \u2264 k + 1, given\nthat the template representing r contains p locations, the set consisting of these\np locations is uniformly distributed in the subsets of size p of {i, . . . , i+ k \u2212 1}.\nUsing the elements of a template as offsets with respect to a secondary loca-\ntion to obtain actual samples, we obtain a signature:\nDefinition 2. Let s = sl,m be a secondary location and \u03c4 = ti, ti+1, . . . , tj be a\ntemplate representing some interval [u : u + k \u2212 1] for \u22122k \u2264 u < k + 1. The\nsignature corresponding to \u03c4 with respect to sl,m is sig\u03c4 (l,m) = X [s+ ti], X [s+\nti+1], . . . , X [s+ tj ], representing the interval [s+ u : s+ u+ k \u2212 1].\nLet T = \u03c41, \u03c42, . . . denote the list of all templates of T . Below we show that\nthere are not too many distinct templates. This imposes an O(\n\u221a\nk \u00b7 polylogk)\nbound on the total number of signatures generated. The proof is omitted.\nLemma 2. |T | \u2264 2\u2113t. Furthermore, the total number of signatures generated\nfrom X from the locations and samples obtained as above is at most 2\u2113t\u2113s(\u2113p+1).\nSince there are many more intervals of length k than templates, we now build\na succinct representation of their correspondance.\nDefinition 3. Let \u03c4 be a template. Let Q = {i | \u2212 2k \u2264 i \u2264 k and interval [i :\ni+ k\u2212 1]induces \u03c4}. The range of \u03c4 , r(\u03c4), is [a : b], with a and b as the left and\nright endpoints of Q.\nThe notion of the range of a template extends naturally to the range of a\nsignature. Let si,j be any secondary location and [a : b] be the range of some\ntemplate \u03c4 . Then the range of sg = sig\u03c4 (i, j), denoted r(sg), is [a+si,j : b+si,j ].\nWe observe below how to compute the range of a template (the range of a\nsignature is computed similarly). Let t0 = \u22122k \u2212 1 and tlt+1 = 2k + 1.\nObservation 2 Let \u03c4 = ti, ti+1, . . . , tj be a template. Then, r(\u03c4) = [max{ti\u22121+\n1, tj \u2212 k + 1} : min{ti, tj+1 \u2212 k}].\nThe Basic Sampling Algorithm Our algorithm consists of two phases. In\nthe initialization phase we construct data structure D with signatures related to\nthe first primary location, p0. In the next phase we compare signatures of other\nprimary locations, to those already considered. If we identify a pattern which\noccurs around all our primary locations, we return Pass. In what follows, let c\nbe a sufficiently large constant.\nInitialization Phase:\nObtain sets P, S of primary, secondary locations, T of offsets, and T of templates\nIf there exists a template \u03c4 \u2208 T with less than c log k sample points return Fail\nSet D = \u03c6; G = \u03c6;\nFor each secondary location s0,i for 1 \u2264 i \u2264 \u2113s and each template \u03c4 \u2208 T\nsg = sig\u03c4 (0, i)\nlet r = r(\u03c4 ) \u2229 [p0 \u2212 s0,i \u2212 k + 1 : p0 \u2212 s0,i]\nR = {r}\nif sg does not exist in D, insert < sg,R > in D;\notherwise let R\u2032 be the range of the entry found in D for sg\nchange the range of the entry for sg in D to R\u2032 \u222aR\nG = G \u222a R\nThe operation taking intersection of the ranges ensures that substrings which do\nnot intersect with p0 are not considered\n7.\nIterative Phase:\nFor m = 1 to \u2113p do\nD\u2032 = \u03c6; G\u2032 = \u03c6;\nFill out D\u2032 with signatures around pm as D was filled above for p0\nFor each signature sg in D\u2032 with range R\nIf sg exists in D with range R\u2032, G\u2032 = G\u2032 \u222aR\nG = G \u2229G\u2032\nOutput: If D 6= \u03c6 return Pass, otherwise return Fail.\nData Stuctures. Data structures D and D\u2032 store modified signatures and their\nranges. A modified signature is obtained (from, say, sg = sig\u03c4(i, j)) tagging\nsg with a prefix, namely the smallest index in \u03c4 . This ensures two matching\n(modified) signatures will come from the same template. Each node contains\na signature and its current range set R, representing the (candidate) frequent\nsubstrings which have this signature. Both D,D\u2032 and R can be implemented\nby using any standard data structure that supports linear time construction\nand logarithmic time search and updates, as well as constant time prev and\nnext operations. G and G\u2032 store ranges in a similar way. Inserting a range into\nR can take linear time due to the deletions of small ranges during merging.\nThe deletion of a range can be charged to its insertion, maintaining logarithmic\namortized insertion and deletion times. The union and intersection operations\nall are performed in logarithmic time per range.\nAnalysis of the Algorithm\nTheorem 3. Let X be a string of length n and parameter k be such that 1 \u2264\nk \u2264 \u03b1n. Let \u2113p = c \u00b7 log k, \u2113s = c\u2032\n\u221a\nk log k and let \u2113t = 24c\u00af log k with sufficiently\nlarge constants c, c\u2032, c\u00af.\n(a) If there is a pattern Y of length k that covers 100% of X, then the algorithm\nreturns Pass with probability at least 1\u2212 o(1).\n7 When we take a union of ranges, ranges which touch or overlap are merged.\n(b) If there is no pattern Y of length k such that at least \u03b1n characters of X\ncan be covered by substrings Z1, . . . , Zw (of length |Y |) where h(Zi, Y ) \u2264 \u01ebk\nthen the algorithm returns Fail with probability at least 1\u2212 o(1).\nThe algorithm runs in O(\n\u221a\nkpolylogk) time and space.\nProof. We start with the proof of Part a. The runtime analysis is submitted in\nthis short version.\n(a): If the Pass condition is satisfied, we can get an outcome of Fail if one the\ntwo following cases happens.\n(i) For some pi, we do not have a pair of \u201cwell aligned\u201d secondary samples s, s\n\u2032\nbelonging to p0 and pi respectively. By assumption, we have a copy of a string\nY covering p0 and one covering pi. To detect that these two copies are identi-\ncal, we need to get identical signatures from them, for which we need to have\nsecondary locations s, s\u2032 with identical relative locations w.r.t. the first and the\nsecond copy of Y respectively. By the birthday paradox (see [8], Page 45), the\nprobability that we will not have such a \u201cwell aligned\u201d pair of secondary loca-\ntions for one particular pi is at most e\n\u2212\u2113s(\u2113s\u22121)\/2k = e\u2212(c\n\u20322k log k\u2212\n\u221a\nc\u2032k log k)\/2k \u2264\ne\u2212(c\n\u20322k log k)\/4k \u2264 k\u2212(c\u2032)2\/4 for c\u2032 \u2265 2\u221ac+ 1. Using the union bound, the proba-\nbility that this situation might arise for some pj is 1\/k.\n(ii) We get a Fail answer due to a signature which is smaller than the thresh-\nold. By Lemma 1 this can only happen with a probability of o(1). Thus, the\nprobability of an incorrect Fail answer is at most o(1).\n(b): If the Fail condition is satisfied, a Pass can be returned as as a result of\ntwo events, analyzed below.\n(i) Choice of primary locations: Call two substrings of size k Z1 and Z2 similar if\nh(Z1, Z2) \u2264 \u01ebk. With the Fail condition, a small number of the primary locations\np1, . . . , p\u2113p may be covered by substrings which are similar to one particular\nsubstring around p0. p0 is covered by at most k different substrings of length k;\nWLOG consider Y = X [p0 \u2212 k + 1 : p0]. Due to the Fail assumption, marking\nX w.r.t. Y will leave at least (1\u2212\u03b1)n positions unmarked. The probability that\na fixed primary location will fall on a marked position for a fixed string Y then\nis at most 1\u2212 \u03b1.\n(ii) Unlucky choice of templates: The signatures for two substrings Y and Y \u2032\ncan be identical even if Y and Y \u2032 differ in more that \u01ebk locations. This is a\nproblem only if the signatures are at least c\u00af log k characters long, since otherwise\nthe algorithm automatically returns Fail. (Note that for a match of signatures\nto be found, the two signatures must be generated from the same template,\nwhich guarantees that the two substrings are being compared at corresponding\nlocations.)\nNote that, due to how the signature of Y has been picked, the second state-\nment of Lemma 1 and the bound on the size of a signature, the samples in the\nsignature of Y correspond to a uniformly chosen subset of c\u00af log k samples from\nY . Assume that the signature for Y \u2032 has been obtained from the same tem-\nplate as that of Y (the opposite of this only helps us). For the two signatures\nto match, none of the samples in the signatures must be from locations where\nY and Y \u2032 differ. Since Y and Y \u2032 are not similar, the probability of this is at\nmost \u01ebc\u00af log k \u2264 1\/k3 . For any pair of primary locations p0 and pi we compare\nat most \u21132s \u00b7 2\u2113t signatures with each other (see Lemma 2). The probability to\nfind identical signatures for a pair of primary locations p0 and pi is at most\n(\u21132s \u00b7 2\u2113t) \u00b7 1k3 \u2264 1k . Since, given p0, there are at most k possible choices for Y , the\nprobability of a false negative\/false positive is at most k \u00b7((1\u2212 \u03b1) + 1k\n)\u2113p \u2264 o(1).\nWe now present a lemma relating the result with overlapping patterns to\nnon-overlapping patterns. The proof is omitted.\nLemma 3. If \u03b1n characters of a string X of length n are covered by overlapping\npatterns of length k, then at least \u03b1n\/2 characters are covered by non-overlapping\npatterns.\n3.2 Length approximately k\nIn this section we show how to test if any pattern of length in the range [\u03b4k : k]\nfor constant \u03b4 < 1 occurs over a large fraction of a given string X . We first\ndevelop a high level algorithm similar to that in Section 3.1. Later, we will use\nthis algorithm to find out if there is any pattern (of any size) which occurs\nfrequently in X .\nWe define our modified algorithm in terms of its differences from the algo-\nrithm in Section 3.1. First, a template is now defined as the maximal sublist of T\nwhose elements represent a range [i : i+ \u03b4k\u22121] (see Definition 1). Consequently,\na signature now spans an area of size \u03b4k.\nThe second change is in our data structures. In our previous algorithm, to\nidentify a pattern as frequent, we confirmed that it occurred around all our pri-\nmary locations. Here, we will check that a pattern occurs around a large number\nof primary locations. To count the occurrences of patterns around primary loca-\ntions, we replace D with DR, described below. In the new algorithm, at the end\nof the iterative section, rather than taking an intersection of the ranges (along\nwith signatures) found for the new pm with the existing candidates ranges in\nD, we now simply add the new ranges found to DR. (Which keeps track of how\nmany times a range has been added). At the end, if there is a particular pattern\nthat occurs around many of the primary locations, it will be witnessed by DR\nthat the signature and range representing the pattern have a large count (one\nfor each occurrence around a primary location).\nThe algorithm outputs Pass if there is a signature and corresponding range\n(thus, a pattern) found around at least \u03b1\u03b4\u2113p primary locations, for some con-\nstants \u03b1, \u03b4 < 1, according to the count obtained from DR.\nData Structure for the Modified Algorithm. We use a data structure DR to store\nranges in terms of their endpoints.DR is, likeD, a standard data structure. Each\nnode contains three fields: a value for an endpoint of a range, a count tracking\nthe times that an endpoint has been encountered, and a one bit field containing\nthe values left or right to qualify an endpoint. Here one can insert a range in\nlogarithmic time, output how many times each (sub)range has been inserted in\nlinear time for all of the ranges. For instance, if [2 : 8] and [6 : 14] have been\ninserted, DR has value 1 for [2 : 5] and [9 : 14], and 2 for the intersection, [6 : 8].\nTo insert a range [a, b], we first look for a in DR. If it is not found, we insert\n(a, left, 1) into DR. If a exists and the endpoint bit shows left, we increment\nthe count field for that entry; if the endpoint bit shows right we decrement the\ncount. We treat b similarly: if the value is not found, we insert (b, right, 1). If an\nentry exists and the endpoint is right, we increment the count; if the endpoint is\nleft, we decrement the count. If at any point the count at a node reaches zero,\nwe delete the node.\nTo obtain a count of the ranges, we use a range counter (initially set to 0),\nstarting from the smallest value in DR and following the next pointers. For every\nnode we see with endpoint left the we increment the range counter by the count\nin that node; for every node with endpoint right we decrement by the count\nin that node. The value of the counter between two nodes in DR represents\nhow many times the range delimited by the values in those two nodes has been\ninserted.\nAnalysis of the Algorithm In this section we will prove that the algorithm\nworks correctly. First we show that whenever there is a pattern of length k\ncovering an \u03b1-fraction of the string, then there is a pattern of length \u03b4k covering\nan \u03b1\u03b4-fraction. The proof is omitted.\nLemma 4. Let \u03b1 \u2208 (0, 1). Let X be a string of length n. Let \u03b4 \u2208 (0, 1).\n(a) Whenever there exists a pattern of length \u2113 with \u03b4k \u2264 \u2113 \u2264 k that covers at\nleast an \u03b1-fraction of C, then there also exists a pattern of length \u03b4k that\ncovers at least an (\u03b1\u03b4)-fraction of the string.\n(b) Whenever there exists a pattern Y of length \u2113 with \u03b4k \u2264 \u2113 \u2264 k such that at\nleast an \u03b1-fraction of X can be approximately covered by substrings Z1, . . . , Zj\n(of length |Y |) where h(Zi, Y ) \u2264 \u01ebk, then there also exist a pattern Y \u2032 of\nlength \u03b4k and substrings Z \u20321, . . . , Z\n\u2032\nj, such that least an (\u03b1\u03b4)-fraction of the\nstring can be covered by the Z \u20321, . . . , Z\n\u2032\nj with h(Z\n\u2032\ni, Y\n\u2032) \u2264 \u01eb\u2032\u03b4k where \u01eb\u2032 = \u01eb\/\u03b4.\nTheorem 4. Let k \u2265 100. Let \u03b1 \u2208 [45 , 1), \u03b2 \u2208 (0, 1) with \u03b1(1 \u2212 \u03b2) \u2264 23 . Let\n\u03b4 = 4041 . Let X be a string of length n. Let \u2113p = c \u00b7 log k, \u2113s = c\u2032\n\u221a\nk log k and let\n\u2113t = c\u00af log k for large enough constants c, c\n\u2032, c\u00af.\n(a) If there is a pattern Y of length \u2113 with \u03b4k \u2264 \u2113 \u2264 k that covers an \u03b1-fraction\nof X, then the algorithm returns Pass with probability at least 3\/4.\n(b) If there is no pattern Y of length \u2113 with \u03b4k \u2264 \u2113 \u2264 k such that at least an\n\u03b1(1\u2212\u03b2)-fraction of X can be covered by substrings Z1, . . . , Zj (of length |Y |)\nwhere h(Zi, Y ) \u2264 \u01ebk then the algorithm returns Fail with probability at least\n3\/4.\nThe algorithm runs in O(\n\u221a\nkpolylog k) time and space.\nProof. (a) We can get a Fail answer if one of the following three cases happen.\n(i) For some pi, we do not have a pair of \u201cwell aligned\u201d secondary samples s and\ns\u2032 belonging to p0 and pi respectively. Using the birthday paradox we can show\nthat the probability that there exists a primary location that we can not align\nto p0 is 1\u2212 o(1).\n(ii) We get a Fail due to a signature which is too small. Lemma 1 shows that\nthis will only happen with a probability of o(1).\n(iii) We get a Fail because not sufficiently many of our primary location posi-\ntions fall into the pattern. Using Lemma 4, the probability that a fixed primary\nlocation does hit the occurrence of the pattern is at least \u03b1\u03b4, thus p0 will not\nbe in the pattern with a probability of 1\u2212 \u03b1\u03b4. From the remaining \u2113p primary\nlocations, expected \u03b1\u03b4\u2113p samples will fall into an occurrence of the pattern. Us-\ning Chernoff bounds from [4], we can show that the probability that fewer than\n(1\u2212\u03b3)\u00b7\u03b1\u03b4\u2113p of p1, . . . , p\u2113p fall within an occurrence of the pattern is at most 1\/k.\nWe can make \u03b3 a constant as close to zero as we wish by making the constant c\n(the coefficient of log k in \u2113p) large enough.\nPutting things together, the probability that the algorithm outputs Fail in\nthe Pass case is is most o(1) + (1\u2212 \u03b1\u03b4) + 1\/k = o(1) + (1\u2212 (40\/41)\u03b1) + 1\/k =\no(1) + (1\/41)\u03b1+ 1\/k \u2264 1\/4 for k a large enough constant.\n(b) We now consider the probability of a Pass answer if the Fail condition is\nsatisfied. Notice that our algorithm now allows for finding patterns (of length\nbetween \u03b4k and k) that actually do not contain primary locations. As we choose\nsecondary locations within a \u00b1k radius around primary locations, a primary\nlocation may have a distance of up to (1\u2212\u03b4)k from an endpoint of an occurrence\nof the pattern, and still be able to identify the pattern as such. We refer to these\nregions of size (1 \u2212 \u03b4)k to the left and to the right of an occurrence as extra\nregions.\nConsider the modification of the marking game from Section 3.1, that marks\nall locations that allow for identifying occurrences of the pattern Y , by marking\nboth the occurrences of Y itself, as well as all the corresponding extra regions.\nIt is easy to see that if there does not exist a pattern of length \u2113 with \u03b4k \u2264 \u2113 \u2264 k\nthat covers an \u03b1(1 \u2212 \u03b2)-fraction of X , the modified marking scheme will mark\nat most \u03b1(1\u2212 \u03b2)n+ (\u03b1(1\u2212 \u03b2)n\/\u03b4k) \u00b7 2(1\u2212 \u03b4)k = \u03b1(1\u2212 \u03b2)n \u00b7 (2\/(40\/41)\u2212 1) =\n\u03b1(1\u2212\u03b2)n\u00b7(1+1\/20) locations. The first term, \u03b1(1\u2212\u03b2)n, is an upper bound on how\nmuch the actual pattern can cover, whereas the second term is an upper bound\non the number of occurrences of the pattern, multiplied with the size of the extra\nregions (of which there are two for every occurrence). Let \u00b5 = \u03b1(1\u2212\u03b2) (1 + 120\n)\n,\ni.e., the coefficient of n in the above expression.\nFix an occurrence Y of length \u03b4k that is identifiable by p0, i.e., p0 is contained\nin either Y itself, or in one of the two extra regions around Y . Notice that there\nare k + 2(1\u2212 \u03b4)k = (3 \u2212 2\u03b4)k many choices for Y . There are two cases that let\nthe algorithm find a pattern between p0 and some pi\n(i) Unlucky choice of primary locations: too many of the primary locations\np1, . . . , p\u2113p may be covered by substrings which are similar to Y . Hence, the prob-\nability that a primary location is close enough to an occurrence of the pattern\nY is at most \u00b5 (as defined above).\n(ii) Unlucky choice of templates: The signatures for two substrings Y and Y \u2032\ncan be identical even if Y and Y \u2032 differ in more that \u01eb\u2032k locations (see Lemma\n4, part (b)). Similar to Theorem 3, we can show that in this case the probability\nto find identical signatures for a pair of primary locations p0 and pi for i \u2208\n{1, . . . , \u2113p} is at most 1\/k.\nSimilar to the proof of Theorem 3 we can argue that the probability to\nfind identical signatures for a pair of primary locations p0 and pi is at most\n\u00b5+ 1\/k for a fixed pattern Y . Hence, the expected value for the counter of Y is\n(\u00b5+1\/k) \u00b7 \u2113p. Using Chernoff bounds [4], it is easy to show that the probability\nthat the algorithm finds more than (1 + \u03b3\u2032)(\u00b5 + 1\/k)\u2113p copies of Y , is at most\n1\/k3. Again, we can obtain a (constant) \u03b3\u2032 as close to zero as we wish by choosing\na sufficiently large value of c.\nFor the Pass and Fail case to be distinguishable, we need (1 + \u03b3\u2032)(\u00b5 +\n1\/k) = (1+ \u03b3\u2032)(\u03b1(1\u2212 \u03b2)(1 + 1\/20)+ 1\/k)+\u03bb \u2264 (1\u2212 \u03b3) \u00b7\u03b1\u03b4 for some (constant)\n\u03bb > 0. Choose c large enough such that (1 + \u03b3\u2032) \u2264 10099 . Since k \u2265 100, (1 +\n\u03b3\u2032)(\u03b1(1\u2212\u03b2)(1+1\/20)+1\/k) \u2264 71\/99. Furthermore, we can choose (1\u2212\u03b3) \u2265 4142 ,\nand thus (1 \u2212 \u03b3)\u03b1\u03b4 \u2265 41\u00b74\u00b74042\u00b75\u00b741 = 3242 = 1621 . Therefore, we indeed have a gap of\n\u03bb = 1621 \u2212 7199 = 31693 .\nSince there are at most k + (1 \u2212 \u03b4)k = (2 \u2212 \u03b4)k possible choices for Y , the\nprobability of a false negative\/false positive is at most (2\u2212 \u03b4)k \u00b7 1k3 = o(1).\nUsing several runs of the algorithm together with simple majority vote it is\neasy to strengthen the results such that the algorithm gives the right answers\nwith a polynomial small probability. In the following we refer to this algorithm\nas the reliable version of the algorithm that finds variable length patterns. The\nruntime is still O(\n\u221a\nkpolylogk).\nNote that the algorithm can be easily modified to answer Pass if, say, x\npercent of the string is covered with a pattern, and Fail if less than x \u2212 \u03b1\npercent of the string are covered with a pattern.\n4 Finding frequent patterns of unspecified length\nIn this part we will use the reliable version of the algorithm that finds variable\nlength patterns in order to search for all patterns that cover most parts of the\nstring. The new algorithm works in log1\/\u03b4 n rounds. In round i (1 \u2264 i \u2264 log1\/\u03b4 n),\nwe search for patterns of length \u2113 with \u03b4in \u2264 \u2113 \u2264 \u03b4i+1n.\nThe algorithm works on an output table which has an entry for every i with\n1 \u2264 i \u2264 log1\/\u03b4 n. It writes Pass (Fail) in position i of the array if the algorithm\noutputs Pass (Fail) in round i. We can prove the following Theorem.\nTheorem 5. Use the same definitions as in Theorem 4 and run the modified\nalgorithm for \u0398(log n) times per round. Furthermore, fix \u03b4 < 1 and choose r\nsuch that \u03b4r \u2265 100.\n(a) For every i \u2264 r such there exists a pattern Y of length \u2113 with \u03b4i+1n \u2264 \u2113 \u2264 \u03b4in\nthat covers an \u03b1-fraction of X, the algorithm writes a Pass into position i\nof the output array with a probability of 1\u2212 n\u22121.\n(b) For every i \u2264 r such there exists no pattern Y of length \u2113 with \u03b4in \u2264 \u2113 \u2264\n\u03b4i+1n such that at least an \u03b1(1\u2212\u03b2)-fraction of X can be covered by substrings\nZ1, . . . , Zj (of length |Y |) where h(Zi, Y ) \u2264 \u01ebk the algorithm writes a Fail\ninto position i of the output array with a probability of 1\u2212 n\u22121.\nThe algorithm runs in O(\n\u221a\nkpolylogk) time and space.\nProof. The proof follows directly from Theorem 4. The array has o(n) entries\nand for every i the algorithm answers Pass (Fail) correctly with a probability\nof 1\u2212 n\u22122.\n5 Conclusions\nIt is also possible to define an algorithm for which a constant number of primary\nlocations is sufficient, rather than O(log k) as in the previous sections. However,\nsince \u201cnothing is for free\u201d there is a bigger gap in the pattern length between\nthe Pass and Fail cases. Notice that for our algorithms a constant number of\nprimary locations is not enough since we essentially search for the k possible\npatterns of length k that contain the primary location p0. This means that, for\na fixed pattern Y which includes p0, the probability that all primary locations\nare contained in the same pattern has to be at most 1\/k for the Fail case. Since\nthe probability that a fixed primary location is contained in a fixed pattern\n(not a fixed occurrence of a pattern) is constant, we need log k many primary\nlocations. This algorithm will be presented in the full version. Unfortunately,\nit is in general not possible to determine the longest pattern occurring in the\nstring, whilst guaranteeing a probability for correctness of the answer, using our\nmodel. See the full version of this paper for a more detailed discussion\nReferences\n1. T. Batu, F. Ergun, J. Kilian, A. Magen, S. Raskhodnikova, R. Rubinfeld and R.\nSami. A sublinear algorithm for weakly approximating edit distance. STOC 2003,\n316\u2013324.\n2. F. Ergun, S. Muthukrishnan, and C. Sahinalp, Sublinear methods for detecting peri-\nodic trends in data streams. Latin American Symposium on Theoretical Informatics\n(LATIN), 2004.\n3. O. Goldreich, S. Goldwasser and D. Ron. Property testing and its connection to\nlearning and approximation, Journal of the ACM 45(4):653\u2013750, 1998.\n4. T. Hagerup and C. Ru\u00a8b. A Guided Tour of Chernoff Bounds. Information Pro-\ncessing Letters 33 (1989), pp. 305\u2013308.\n5. P. Indyk, N. Koudas and S. Muthukrishnan, Identifying Representative Trends in\nMassive Time Series Data Sets Using Sketches. Proc. VLDB 2000. 363\u2013372.\n6. R. Karp, S. Shenker, and C. Papadimitriou, A simple algorithm for finding frequent\nelements in streams and bags. ACM Trans. Database Syst. 28: 51-55 (2003)\n7. O. Lachish and I. Newman, Periodicity Testing. Proc. RANDOM 2005, to appear.\n8. R. Motwani and P. Raghavan, Randomized Algorithms. Cambridge University\nPress (1995)\n9. D. Ron, Property Testing (A Tutorial). Handbook of Randomization, 2000.\n10. R. Rubinfeld and M. Sudan, Robust Characterization of Polynomials with Appli-\ncations to Program Testing, SIAM Journal of Computing 25(2):252\u2013271, 1996.\n"}