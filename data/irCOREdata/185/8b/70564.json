{"doi":"10.1109\/35.685379","coreId":"70564","oai":"oai:eprints.lancs.ac.uk:11621","identifiers":["oai:eprints.lancs.ac.uk:11621","10.1109\/35.685379"],"title":"Supporting Adaptive Video Applications in Mobile Environments","authors":["Davies, Nigel","Finney, Joseph","Friday, Adrian","Scott, Andrew"],"enrichments":{"references":[{"id":16306546,"title":"Adaptation and Mobility in Wireless Information Systems.&quot;","authors":[],"date":null,"doi":"10.1109\/98.295355","raw":"Katz,  R.H.  &quot;Adaptation  and  Mobility  in  Wireless  Information  Systems.&quot;  IEEE  Personal Communications Vol. 1 No. 1, Pages 6-17.","cites":null},{"id":16306556,"title":"An ISM Band Spread Spectrum Local Area Network: WaveLAN,&quot; IEEE Workshop on Wireless Local Area Networks, Worcester Polytechnic Institute 140-145. [Yeadon,96]","authors":[],"date":"1996","doi":null,"raw":"&quot;An ISM Band Spread Spectrum Local Area Network: WaveLAN,&quot; IEEE Workshop on Wireless Local Area Networks, Worcester Polytechnic Institute 140-145. [Yeadon,96] N. Yeadon, F. Garcia, D. Hutchison and D. Shepherd, \u201cFilters: QoS Support Mechanisms for Multipeer Communications\u201d, IEEE Journal on Selected Areas in Computing (JSAC) special issue  on  Distributed  Multimedia  Systems  and  Technology,  Vol.  14,  No.  7,.  Pp.  1245-1262, September 1996.","cites":null},{"id":16306560,"title":"Are Disks in the Air Just Pie in the Sky?&quot;","authors":[],"date":"1994","doi":"10.1109\/mcsa.1994.512728","raw":"Zdonik, S., M. Franklin, R. Alonso, and S. Acharya. &quot;Are Disks in the Air Just Pie in the Sky?&quot; Proc. Workshop on Mobile Computing Systems and Applications, Santa Cruz, CA, U.S., 1994.","cites":null},{"id":16306550,"title":"GSM\/WaveLAN Integration.&quot;","authors":[],"date":"1997","doi":null,"raw":"Kriaras, I. &quot;GSM\/WaveLAN Integration.&quot; Proc. 4th Wireless Workshop, Lucent Bell Laboratories, Holmdel, U.S., 1997.","cites":null},{"id":16306541,"title":"Limbo: A Tuple Space Based Platform for Adaptive Mobile Applications.&quot;","authors":[],"date":"1997","doi":null,"raw":"Davies, N., S. Wade, A. Friday, and G. Blair. &quot;Limbo: A Tuple Space Based Platform for Adaptive  Mobile  Applications.&quot;  Proc.  Joint  International  Conference  on  Open  Distributed Processing and Distributed Platforms (ICODP\/ICDP '97), Toronto, Canada, Chapman and Hall, 1997.","cites":null},{"id":16306552,"title":"Mobile Multimedia collaborative project with Simoco International Ltd, Barclay Associates Ltd,","authors":[],"date":null,"doi":null,"raw":"Mobile Multimedia collaborative project with Simoco International Ltd, Barclay Associates Ltd, HW Communications Ltd, (EPSRC LINK PCP project GR\/K82024), further details: www.lancs.ac.uk\/computing\/research\/mpg\/most\/mm_project.html","cites":null},{"id":16306539,"title":"Programme DE\/RES - 0601, Subtechnical Committee (STC) RES 06, \u201cRadio Equipment Systems (RES); Trans-European Trunked Radio (TETRA); Voice plus Data (V+D), Designer's Guide\u201d,","authors":[],"date":"1995","doi":null,"raw":"ETSI Work Programme DE\/RES - 0601, Subtechnical Committee (STC) RES 06, \u201cRadio Equipment Systems (RES); Trans-European Trunked Radio (TETRA); Voice plus Data (V+D), Designer's Guide\u201d, May 1995.","cites":null},{"id":16306544,"title":"Rover: A Toolkit for Mobile Information Access.&quot;","authors":[],"date":"1995","doi":"10.1145\/224057.224069","raw":"Joseph, A., A. deLespinasse, J. Tauber, D. Gifford, and M.F. Kaashoek. &quot;Rover: A Toolkit for Mobile Information Access.&quot; Proc. 15th ACM Symposium on Operating System Principles (SOSP), Copper Mountain Resort, Colorado, U.S., ACM Press, Vol. 29, Pages 156-171. 3-6 December 1995.","cites":null},{"id":16306558,"title":"Supporting Video in Heterogeneous Environments.&quot;","authors":[],"date":"1998","doi":"10.1145\/330560.330853","raw":"Yeadon, N., N. Davies, A. Friday, and G.S. Blair. &quot;Supporting Video in Heterogeneous Environments.&quot; Proc. Symposium on Applied Computing, Atlanta, U.S., February 1998.","cites":null},{"id":16306548,"title":"The Evaluation of Video Layout Strategies on a HighBandwidth File Server.&quot;","authors":[],"date":null,"doi":"10.1007\/3-540-58404-8_21","raw":"Keeton,  K.,  and  R.  Katz.  &quot;The  Evaluation  of  Video  Layout  Strategies  on  a  HighBandwidth File Server.&quot; Proc. 4th International Workshop On Network and Operating System Support for Digital Audio and Video, Lancaster House, Lancaster, U.K., Pages 237-248.","cites":null},{"id":16306536,"title":"Very Low Bitrate Videocoding using H.263 and Foreseen Extensions\u201d,","authors":[],"date":"1996","doi":null,"raw":"G.  Bjontegaard,  \u201cVery  Low  Bitrate  Videocoding  using  H.263  and  Foreseen Extensions\u201d, Proceedings of European Conference on Multimedia Applications, Services and Techniques (ECMAST \u201996), Louvain-la-Neuve, Belgium, May 1996, pp 825-838.","cites":null},{"id":16306554,"title":"Video Over Wireless.&quot;","authors":[],"date":null,"doi":null,"raw":"Moura,  J.M.F.,  R.S.  Jasinchi,  H.  Shiojiri,  and  J.  Lin.  &quot;Video  Over  Wireless.&quot;  IEEE Personal Communications, 3 No. 1, Pages 44-54.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"1998-06","abstract":"The transmission of digital video over wireless networks is becoming a reality: it is now possible to construct working prototype systems which illustrate the benefits to be accrued from the integration of mobile computing and digital video. However, systems which deploy video in mobile environments must be able to adapt to changes in the quality of service of their underlying communications channel. The authors focus on the practical applications and implications of supporting adaptive video in mobile environments. In particular, we describe a testbed which supports multicast transmission of stored and live video sequences over both WaveLAN and GSM technologies. The testbed employs H.263 and MPEG encoding techniques and enables clients to freely roam between heterogeneous networks while maintaining video connectivity","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/70564.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/11621\/1\/ieee%2Dcomms%2Dadaptivevideo%2D1998.pdf","pdfHashValue":"93f372af851a7ec5434e8b4791dffffe3082f44c","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:11621<\/identifier><datestamp>\n      2018-01-24T00:04:20Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Supporting Adaptive Video Applications in Mobile Environments<\/dc:title><dc:creator>\n        Davies, Nigel<\/dc:creator><dc:creator>\n        Finney, Joseph<\/dc:creator><dc:creator>\n        Friday, Adrian<\/dc:creator><dc:creator>\n        Scott, Andrew<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        The transmission of digital video over wireless networks is becoming a reality: it is now possible to construct working prototype systems which illustrate the benefits to be accrued from the integration of mobile computing and digital video. However, systems which deploy video in mobile environments must be able to adapt to changes in the quality of service of their underlying communications channel. The authors focus on the practical applications and implications of supporting adaptive video in mobile environments. In particular, we describe a testbed which supports multicast transmission of stored and live video sequences over both WaveLAN and GSM technologies. The testbed employs H.263 and MPEG encoding techniques and enables clients to freely roam between heterogeneous networks while maintaining video connectivity.<\/dc:description><dc:date>\n        1998-06<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/35.685379<\/dc:relation><dc:identifier>\n        Davies, Nigel and Finney, Joseph and Friday, Adrian and Scott, Andrew (1998) Supporting Adaptive Video Applications in Mobile Environments. IEEE Communications Magazine, 36 (6). pp. 138-143. ISSN 0163-6804<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/11621\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1109\/35.685379","http:\/\/eprints.lancs.ac.uk\/11621\/"],"year":1998,"topics":["QA75 Electronic computers. Computer science"],"subject":["Journal Article","PeerReviewed"],"fullText":"SUPPORTING ADAPTIVE VIDEO APPLICATIONS IN MOBILE \nENVIRONMENTS \n \nNigel Davies, Joe Finney, Adrian Friday and Andrew Scott \n \nDistributed Multimedia Research Group, \nDepartment of Computing, \nLancaster University, \nLancaster, \nU.K. \n \ntelephone: +44 (0)1524 594337 \ne-mail: most@comp.lancs.ac.uk \n \nABSTRACT \n The transmission of digital video over wireless networks is becoming a \nreality: it is now possible to construct working prototype systems which \nillustrate the benefits to be accrued from the integration of mobile \ncomputing and digital video. However, systems which deploy video in \nmobile environments must be able to adapt to changes in the quality-of-\nservice (QoS) of their underlying communications channel. In this paper \nwe focus on the practical applications and implications of supporting \nadaptive video in mobile environments. In particular, we describe a testbed \nwhich supports multicast transmission of stored and live video sequences \nover both WaveLAN and GSM technologies. The testbed employs H.263 \nand MPEG encoding techniques and enables clients to freely roam between \nheterogeneous networks while maintaining video connectivity.  \n1. INTRODUCTION \nThis paper focuses on the practical issues associated with developing support for \nadaptive mobile video applications. While mobile video is currently not of a quality \nwhich would be acceptable to the average end-user, there are many vertical \napplication domains in which video can play an important role. As part of an on-going \nproject to develop adaptive mobile multimedia applications for the emergency \nservices (police, fire, ambulance) we have carried out an analysis of possible \napplications of mobile video within each service [MM,97]. In all cases we found \nexample application scenarios in which video could significantly assist the services in \ntheir work. For example, with the introduction of surveillance cameras in shops, \nshopping centres and, increasingly, city centre streets it would be beneficial if police \nofficers could access this information, either in stored form to enable them to identify \nsuspects or missing persons or as a live feed when trying to locate suspects. Other \nexample applications include ad-hoc video surveillance, incident monitoring, crowd \ncontrol, remote interrogation and recording and remotely monitoring arrest sequences \nfor later use in court. \nA key issue in supporting video in mobile environments is that of adaptation. \nAdaptive applications are able to react to rapid and significant fluctuations in their \nenvironment and, in particular, in the quality-of-service (QoS) offered by their \nunderlying communications channel. This ability to adapt is widely recognised as \nbeing crucial to the success of mobile applications [Katz,94] since it enables them to \nsurvive QoS fluctuations caused by both environmental factors and by users roaming \nbetween networking technologies (e.g. a user leaving a building and having to hand-\nover from a relatively high-bandwidth wireless LAN to a low-speed public access \nwireless WAN). \nIn this paper we describe our experiences of developing an experimental Video-\non-Demand system which supports adaptation for mobile clients. Our system uses IP \nmulticast to enable multiple clients to be supported in a heterogeneous environment. \nThe system supports client roaming across network overlays and uses QoS \ninformation together with a number of video encoding techniques to allow clients to \nmaintain video continuity despite network transitions. \nThe paper is structured as follows. Section 2 explores the issue of mobile video \nadaptation in more detail and presents a brief summary of existing work in the area \nfrom a Computer Science perspective. Section 3 describes our testbed in depth. In \nsection 4 we present an analysis of our experiences of developing and using the \ntestbed and section 5 contains our concluding remarks. \n2. TECHNIQUES FOR SUPPORTING ADAPTIVE VIDEO \nThe real-world applications described above illustrate the usefulness of video in \nmobile environments. However, research has demonstrated that the successful \ndeployment of mobile applications requires that they address the issue of adaptation \n[Katz,94]. More specifically, state-of-the-art mobile applications are required to \nmonitor and adapt to changes in the QoS available from their communications \ninfrastructure. The issue of adaptation is particularly significant for video applications \nbecause of the relatively high data rates involved. Consider, for example a user who is \nconnected via a typical wireless local area network (such as WaveLAN [Tuch,91]) \nand is watching a video clip stored on a local video server. In this case the \ninfrastructure could support the transmission and playback of a single high-quality \nMPEG-1 system stream (assuming relatively modest loading on the network). \nHowever, if the user leaves the area of WaveLAN coverage and establishes a GSM \n(or similar) connection to maintain connectivity the available bandwidth will drop \nfrom a nominal throughput of around 2 Mbps to 9.6 Kbps with the consequence that \none second of MPEG-1 video encoded at 1 Mbps will require nearly two minutes to \ndownload. This is clearly unacceptable and the system would need to adapt to ensure \nthat the user is able to continue watching the video, albeit at a reduced quality. \nThe importance of adaptation can also be demonstrated for applications which use \nonly a single communications technology. For example, applications which are \nsupported by radio systems based on the TETRA standard [ETSI,95] will need to \nadapt to the dynamic increases or decreases in bandwidth supported by this standard. \nA number of techniques have been developed to support adaptive video applications \nand these are briefly discussed in the following sections. \nUse of Video Filtering\/Transcoding Techniques \nAn obvious approach to creating an adaptive video application is to deploy system \nservices which can process a video stream to reduce its bandwidth requirements until \nthey match the available resources. Such system services are often referred to as \nproxies, filters or transcoders depending on the precise operations they perform on the \nvideo stream. For instance, such a service might drop frames, reduce the image size of \neach frame, remove colour information or re-code the video in a different format (e.g. \nthe Berkeley Motion-JPEG to H.261 transcoder). There has been a substantial body of \nwork in this area in the multimedia research community where the aim has typically \nbeen to support multiple recipients of a single video stream who have heterogeneous \nnetworking and display capabilities [Yeadon,96]. While such services appear to offer \na simple approach to solving the problems of adaptation they suffer from a number of \nserious drawbacks. Firstly, services which process video streams are inherently \nresource intensive. As a consequence, most services require a dedicated machine and \nare unable to process more than a small number of concurrent streams. Secondly, \nnodes in the network must be found which are prepared to run proxies on clients\u2019 \nbehalf. In some cases it may be possible to route the video via the client's 'home' \nmachine in which case this is less of a problem. However, in many cases this is likely \nto lead to unnecessary demands on network resources and increased latency. \nFurthermore, proxies are normally most effective when run close to the boundary \nbetween fixed and wireless networks. Finally, while some straightforward processing \nis possible for most image formats this can usually only provide adaptation within a \nrelatively narrow range. In the example used previously of a user switching from \nWaveLAN to GSM a complete change of format would probably be required which is \nbeyond the capabilities of most transcoders. \nUse of Multiple Video Streams \nFor systems which access stored video from a video server it is possible to address \nthe requirement for adaptation by storing multiple encodings of each video sequence \nusing a variety of compression techniques, data and frame rates. The client may \ntherefore pick the most appropriate stored version of a particular video stream to suit \ntheir requirements and current network QoS. Such an approach incurs the overhead of \nstoring multiple representations of each video sequence and, furthermore, clients \nwhich switch between representations must rely on the video server to maintain \nsynchronisation between the streams in order to reduce interruptions to the video \nsequence. However, the storage overheads and complexity may be offset by the \nreduced processing requirements as compared to proxy-based solutions since real-\ntime encoding, transcoding or filtering are no longer required. \nUse of Scalable Coding Schemes \nScalable coding schemes are designed to enable a video sequence to be played \nback at a number of different resolutions\/qualities depending on the available \nbandwidth. In essence, the video is stored as a base layer followed by one or more \noptional layers, each of which adds more detail to the image produced by the base \nlayer. Hence, clients can be sent the base layer and then as many optional layers as \nresources allow. These systems have obvious attractions for adaptive systems and \ntheir use has been proposed by a number of researchers in application domains \nranging from the development of QoS aware video servers [Keeton,93] to providing \nvideo over wireless networks [Moura,96]. \nDiscussion \nSupporting course grained adaptation caused by roaming between overlayed \nnetworks places new demands on video applications. In particular, the use of \ntranscoders and multiply encoded video streams requires that client applications be \nable to detect and process a range of encoded video formats. Furthermore, since an \nadaptive system may cause clients to experience a change of video format part-way \nthrough a video sequence it is important to minimise the time taken for the client \ndecoder to be brought up to state. This process is further complicated by the use of \nhierarchical video coding schemes such as MPEG-1 in which interdependencies exist \nbetween individual video frames. In order to decode a given frame the decoder may \nrequire earlier frames for reference or will be forced to wait until the next group of \npackets (which contain an intra-coded frame with no frame interdependencies). \nFurthermore, in the case of MPEG-1 system streams in particular, in which MPEG-1 \naudio and video streams are multiplexed together, the decoder will require the \nsequence start header in order to be able to successfully demultiplex the sub-streams. \nIn order to allow clients to handover an ongoing streamed video there are three \nbasic approaches (which do not require non-standard stream syntax): non-\ninterdependent (intra-coded or reference frames) must be transmitted relatively \nfrequently and the user must wait for the next reference to be transmitted, the \nimmediately preceding reference frame and any subsequent frames up to the current \npoint may be transmitted out of band to enable the client to 'catch up', or, in the case \nof live streams, the encoder may be forced to generate a new reference frame. The \nfirst option is undesirable since reference frames are generally larger than their \ndependants and thus more reference frames equals lower compression. The second \nand third options may cause a large burst of data when a client joins a video stream \nbut have the advantage that larger reference frames can be transmitted infrequently. \nThe second option also requires that the decoder is able to decode faster than the \ncurrent stream is transmitting (or else it would never catch up). \nThe choice of schemes is dependent upon the number of handovers which are \nrequired (i.e. how often clients join and leave a stream) and how long users are \nprepared to wait to start receiving video once the handover has taken place. For the \ntestbed described in the remainder of this paper we focus on the use of multiple \nencodings to support adaptation. \n3. A TEST-BED FOR ADAPTIVE VIDEO \nOur testbed was created as the result of a collaborative venture involving \nLancaster University and Lucent Technologies. The overall aim was to provide a \nvideo format independent demonstration environment which could illustrate the use \nof adaptive techniques to enable groups of users to view stored and live video \nsequences as they roamed between a relatively high bandwidth indoor network \n(WaveLAN) and a low bandwidth public access cellular network (GSM). Our \ndecision to use multiple encodings enables us to support course-grained adaptation \nand handovers by dynamically mapping client requests on to different instances of the \nsame video stream; each instance being encoded at a different bit-rate or using a \ndifferent encoding standard. \n3.1. Overall Architecture \nOur overall architecture is shown in figure 1. The network configuration consists \nof a fixed 10 Mbps Ethernet backbone with a WavePOINT I wireless bridge and a \ndial-in PPP server. The system services (Session Manager and Video Server) run on \nLinux 2.0.30 Workstations and the clients are a mixture of Windows\/NT 4.0 \nworkstations on the wired backbone and Windows 95 laptops connected via \nWaveLAN and GSM networks (using Nokia Card Phones). In this simple \nconfiguration with a single mobile sub-network we are able to run the infrastructure \nwithout requiring mobile-IP. The PPP server is connected to ISDN which allows a \nGSM data call to be established with minimal modem negotiation (and thus reduced \nlatency). Each client is pre-assigned two unique IP addresses, one permanent and one \nfor dial-up access. \nWindows Based Video Clients\n(WaveLAN and GSM interfaces)\nFixed Network \n(Ethernet)\nFixed Live Video Source (H.263)\nVideo Clients on \nFixed Network\nWaveLAN Wireless \nNetwork\nWireless Live Video \nSource (H.263)\nVideo Server\nISDN - GSM\nSession Manager\n \nFigure 1 : Experimental Infrastructure \n3.2. Session Manager \nThe primary role of session managers is to act as a point of coordination between \nclients and servers. All sources of video register with a session manager on \ninitialisation. This registration contains details of the video streams the source can \nsupport including a textual description of the stream and the different formats and bit-\nrates in which the stream is available. The session manager compiles the information \nfrom multiple servers into a session directory which is supplemented with status \ninformation on each stream (e.g. if it currently playing). The directory and associated \nstatus information is multicast periodically over a well known site-local multicast \naddress. \nClients receive the directory information and interact with the session manager \nbased on the information contained therein. At its simplest, this interaction comprises \nrequests to start playing a new video stream or join an existing active stream. The \nclient may specify upper and lower bounds on the QoS it is prepared to accept. For \nsimplicity the client may choose from a range of predefined alternatives from H.263 \nat 8 and 22 Kbps, H.261 at multiples of 64 Kbps, MPEG-1 and Motion-JPEG (only \nthe H.263 and MPEG-1 options are supported in the current system). \nThe session manager collates this QoS information and for each network \nmaintains an approximation of the utilised and available bandwidth, number of active \nstreams and a client list for each stream. Whenever a client requests to start or join a \nstream, stops listening to a stream, or affects a handover, the session manager is \nnotified. This QoS information is used by the session manager to perform simple \nadmission control of client requests. \nIn more detail, when the client requests to join an active stream its current \nnetwork interface (and hence implied available bandwidth) and QoS constraints are \nchecked against those of the active stream. If the client can support the stream the \nappropriate multicast endpoint is returned and the stream usage count incremented. \nWhen a new stream is to be started the session manager picks the highest quality \nvariant of the requested stream available subject to the estimated available bandwidth \non the client\u2019s network and the supplied QoS parameters. If any such stream exists the \nSession Manager starts the stream and returns the new endpoint to the client. \nIn addition, mobile clients notify the session manager of their currently active \nnetwork interface (either WaveLAN or GSM) when they initialise and when the state \nof their interface changes (due to a handover). This information allows session \nmanagers to match a suitable QoS variant of any stream the client has open to that \nclient\u2019s interface. For example, if a client is currently watching an MPEG stream via \nWaveLAN and hands over to GSM the session manager will determine if a lower \nQoS instance of the same stream is currently playing. If this is the case, the client will \nbe notified of the multicast address at which it can receive the transmission. If there is \nnot a lower QoS instance of the stream currently active, the session manager will \nrequest one from the appropriate video server, instruct the server to seek to the correct \nposition in the stream (to ensure a relatively seamless handover - see section 4) and \npass the address of this new stream to the client. \nWhenever a client wishes to perform an action on an active stream which may \naffect other clients, such as pausing the stream or seeking to a new position, that client \nmust first gain control of the stream from their session manager. Control is only \nallocated to one client at a time and must be relinquished as soon as the operation is \ncomplete. \nIn addition to simple floor management, session managers also implement the \nconcept of sibling relationships between active stream instances. To illustrate this, \nconsider two mobile clients both of which are watching the same video clip over \nWaveLAN and therefore have a high quality playback. One of these clients then loses \naccess to the WaveLAN and must resort to using GSM, and thus receives a much \nlower quality version of the video clip. However, from the users point of view, both \nclients are still viewing the same video clip. When such an event arises, the session \nmanager forms a sibling relationship between the high and low quality instances of \nthe stream. This means that the floor is now shared between both instances, and any \nmanipulations performed on one stream (seek, pause etc.) are reflected in the other. \nThis enables synchronisation between the instances to be maintained to the extent that \nif the client using GSM was to regain WaveLAN access they would be able to rejoin \nthe original high-quality stream. \nThe Session Manager trivially authenticates each client request by checking its \nunique client identifier against an access control list (more sophisticated security and \nauthentication mechanisms have not been an early focus of our work). \n3.3. Stored Video Sources \nStored video servers provide rate controlled playback of continuous media \nstreams to mobile clients. Accurate rate pacing (+\/- 400bps) of the playout is essential \nsince all video is multicast to potentially many clients and thus a closed loop back \nchannel to the server is undesirable. The video servers achieve rate pacing via a credit \nbased flow control scheme, where credit is assigned based on the difference between \nthe theoretical volume of data which should be transmitted (known since the video \nstreams are encoded at a constant bit rate) and the actual volume of data sent. The \nserver only sends a packet when it has accumulated sufficient credit to do so. \nInformation transmitted from video servers is multicast using UDP as a transport \nprotocol with a 2 byte application level header and a 1024 byte payload: \nP L D Media Seq. No. Data\n \nFigure 2 : Protocol Packet Header \nThis header allows the video server to specify the type of media contained in the \npacket, whether it contains priming information (P), if this is the last in a sequence of \npriming packets (L) and to signal a temporal discontinuity (D) \u2013 see section 4.4. A \nsequence number is also included to aid clients in detecting\/monitoring packet loss. \nThe video servers store multiple copies of each video clip, recorded at a range of \nconstant bit rates and typically using different encoding techniques. In addition to the \nstandard Open, Close, Pause and Seek operations provided by most video servers, a \nHandover operation is also supported which specifies the active stream to handover \nfrom and the QoS of the new stream. Upon receipt of a handover request, the video \nserver spawns a new stream instance, at the requested QoS, and at the same temporal \nposition as the original stream. To enable the video server to locate the correct \ntemporal position in the encoded video file we pre-processes the video files to \ngenerate an index track. The index track specifies the number of video frames in the \nfile, the offset into the encoded stream of the start of each frame and the type of each \nframe. \nIn addition to generating accurate positional information, video servers must also \nprovide priming information where needed to allow client decoders to be brought up \nto state when joining or handing over to an active stream. This information allows \nplayback of the new stream to commence as seamlessly as possible. The form of this \npriming information is dependent on the media type of the stream being primed. In \nthe case of MPEG-1 a valid system header is required to enable the codec to be able \nto successfully demultiplex the audio and video from the system stream. In the case of \nvery low bit rate encodings such as H.263 where intra coded frames (consisting of \nintra coded macroblocks) are usually infrequent, the previous intra coded frame is a \nprerequisite for achieving reasonable handover performance. Ideally, where \nbandwidth allows, the successive inter coded frames up until the current playback \npoint are also desirable (although this is highly dependent on the amount of motion in \nthe scene). \n3.4. Live Video Sources \nWe have implemented the live video sources as self-contained units capable of \nbeing used in an ad-hoc surveillance scenario such as that described in section 1. In \nmore detail, these units consist of a motherboard designed for embedded systems \nwork, a 200 MHz Pentium processor, WaveLAN card, hard disk and Connectix \nQuickCam mounted in a box 230x320x67 mm. The units run Linux and a version of \nthe Telenor H.263 encoder which has been tuned to offer increased performance \n(typically about double the frame rate). They are able to capture and encode video at \napproximately seven frames per second (at Q-CIF size) and the frame rate, image size \nand quantization factor can be adjusted by the session manager to provide limited \nsupport for adaptation. Details of our work on live video sources for ad-hoc \nsurveillance can be found in [Yeadon,98].  \n3.5. Client Software \nThe architecture of the client decoder is illustrated in figure 3.  \n \nFigure 3 : The Client Architecture \nThe client application initially spawns a session manager liaison thread which \nmonitors session directory broadcasts and status information pertaining to ongoing \nand potential video streams. The application is completely self configuring and \nrequires only to be able to monitor the multicast transmissions of the Session \nManager. \nThe received advertisements are translated into a viewing guide which can be \nbrowsed in a simple tabbed list box (see figure 4). The user may choose an active or \nan inactive stream and supply their QoS requirements (on a global or per-stream \nbasis), as described in section 3.2, using the \u2018Preferences\u2019 tab. \n \nFigure 4: The Client Application \nOnce admitted to viewing a particular video stream, the client spawns a \nconfiguration manager thread which builds a chain of objects for rendering the \nspecified video format. As format changes in the video are detected, new decoder \nobjects can be spawned and attached on the fly, allowing near seamless changes in \nvideo quality (and format). \nThe current client supports two video formats, H.263 and MPEG-1 system \nstreams. Our H.263 codec is based on the publicly available Telenor H.263 decoder \n[Bjontegaard,96] which has been ported to make it a freely instantiatable Visual C++ \nobject. MPEG support is via the Microsoft ActiveMovie run-time library. We found \nthat the MPEG codec shipped with ActiveMovie provided superior MPEG playback \nto both the Berkeley MPEG Video and Boston system stream player (both of which \nwe ported to Visual C++ to provide a fair comparison). \nIn essence, ActiveMovie is an object oriented architecture based on the concept of \na graph of filter components. Individual components (codecs, sources, translators and \nrenderer filters) are chained together to form a media type compatible sequence which \ndecodes and renders a given media type. In order to use ActiveMovie with a network \nsourced video stream rather than a local file it proved necessary to build a new source \nfilter component which can accept network streamed MPEG data and connect to the \nexisting ActiveMovie filter graph architecture. In our implementation this source \ncomponent can buffer up to a second of video to smooth out jitter in the playback. \nThe client makes use of the Lucent GSM\/WaveLAN network selection platform \nfor monitoring and initiating handovers between networks [Kriaras,97]. This platform \ninforms the client application when the host is starting to move out of coverage of the \nWaveLAN network and automatically establishes a GSM communications link. The \nclient can then initiate a soft handover to an alternative version of the video stream. \nThe advantage of a soft over hard handover is that we have sufficient warning to \nenable the priming information required by the decoders (as described in section 3.3) \nto be transferred over WaveLAN, speeding up the time taken for the decoder to begin \nplayback (clients ignore priming information that they do not require). \n4. Analysis \nWe have recently completed the implementation of the application and testbed \ndescribed in section 3. The system is capable of supporting multiple groups of clients \nviewing independent or related video streams with different QoS attributes. Clients \nthat roam between WaveLAN and GSM networks experience a handover to a \ndifferent QoS stream concurrent with their handover between networks. During a soft \nhandover (one in which both networks are available simultaneously for a brief period) \nwe have achieved quite effective results: MPEG video to H.263 can be achieved \npractically seamlessly (i.e. to the observer the handover is instantaneous). The reverse \ntransition takes approximately 7 seconds due to the prefetching and graph rendering \nprocess required by ActiveMovie. This overhead highlights a problem with using \nActiveMovie in this context: while the architecture allows run-time instantiation of a \nwide range of commercial and public domain codecs including H.261, H.263+ and \nIndio etc. many component filters require a sizeable amount of data to establish \nwhether or not they can process a given stream. This gives rise to wide ranging \ncomplications for network streamed applications during periods of adaptation when \nmultiple components must be instantiated and primed before video playback can \nresume. \nA secondary cause of delay when handing over to (or joining) an MPEG stream is \nthe need to accumulate a reasonable buffer to smooth out network jitter. We have \nfound that a buffer of approximately one second allows smooth playout of MPEG \nstreams (encoded at 1.2MBits) over WaveLAN in most circumstances.  \nOur approach of using multiple encodings at the servers has worked better than \nwe originally anticipated. Using simple index tracks it is relatively easy to seek to the \nsame temporal location in streams encoded using both MPEG and H.263 with \nsufficient accuracy for the purposes of supporting handovers. Furthermore, the \noverhead of maintaining a second low-quality version of an MPEG video is minimal: \nusually accounting for an increase of less than 1% in the total amount of data stored. \nFinally, since the video servers are not required to process the video information they \nare able to support a significantly greater number of clients than would be possible \nwith the same processing power using an approach based on, for example, video \nfiltering techniques or transcoding.  \nHowever, we were not able to meet our original aim of designing the video \nservers to be completely independent of the video encoding techniques used. This is \nbecause the servers must be able to parse the video streams to determine the \nappropriate priming information to send to clients during handovers. As the influence \nof the Internet and network streaming applications on standards become apparent this \nwill become less of an issue (for instance, MPEG-2 has been designed to facilitate late \nentry). \nThe use of Session Managers to provide overall QoS management and co-\nordination proved highly successful in our test domain but clearly does not scale well. \nHowever, it would be relatively simple to divide the session manager's roles into two \ndistinct components arriving at a more traditional architecture in which application \nlevel session managers maintain stream relationships on a per-group basis and QoS \nmanagers, possibly co-located with the video servers to determine the appropriate \nQoS at which video should be sent to a given client. \nFinally, in our current implementation we use an RPC package to exchange \ncontrol information between the different system components. While there have been \nnumerous attempts to adapt the RPC paradigm to operate in mobile environments \n(e.g. [Joseph,95]) we are of the opinion that RPC is poorly suited to this task. In \nparticular, the synchronous nature of the paradigm leads to problems when clients \nexperience extended periods of intermittent connectivity. We therefore intend to \nreplace the RPC package with an asynchronous distributed systems platform called \nL2imbo [Davies,97]. This platform provides asynchronous message passing and \nbuffering of messages over time to enable the system to survive periods of \ndisconnection. \n5. Concluding Remarks and Future Work \nIn order to operate effectively in mobile environments applications and system \ncomponents must be able to adapt to changes in their communications QoS. In this \npaper we have presented our experiences of developing a testbed and associated \napplication for adaptive video. The system enables clients to maintain video \nconnectivity while roaming between GSM and WaveLAN networks. Adaptation is \nsupported by storing multiple encodings of video streams and allowing clients to \nreceive different encodings based on their network QoS. The handover between \ndifferent encodings is optimised by the use of priming information which, when \ncombined with a soft handover between networking technologies, enables clients to \nexperience a near seamless handover from WaveLAN to GSM. \nIn the near future we are proposing to make a number of enhancements to our test \nenvironment. More specifically, we hope to modify our client so that it is based \nentirely on ActiveMovie components. In this way we will be able to rapidly \nincorporate new video codecs without the need for substantial implementation effort \nand will be able to more elegantly\nto integrate additional codecs, such as H.261 or H.263+, to provide us with moderate \nquality video at intermediate data rates which we expect to introduce a number of \ninteresting QoS management issues (particularly with respect to balancing client \nrequirements and the promotion\/demotion of stream quality on the fly). \nFurthermore, we are planning to replace the RPC based communications protocol \nwith the L2imbo distributed systems platform which supports asynchronous \ncommunications primitives and has been developed specifically for operation in a \nmobile environment. \nAcknowledgements \nThe testbed described in this paper was carried out in co-operation with Lucent \nTechnologies, U.K. The initial version of the continuous media video server was \ndeveloped by Kev Sherratt, the ActiveMovie socket component by Martin Dunmore, \nthe H.263 encoder and decoder software by Nick Yeadon and the ad-hoc surveillance \nunit by Matt Storey. \nReferences \n[Bjontegaard,96] G. Bjontegaard, \u201cVery Low Bitrate Videocoding using H.263 and Foreseen \nExtensions\u201d, Proceedings of European Conference on Multimedia Applications, Services and \nTechniques (ECMAST \u201996), Louvain-la-Neuve, Belgium, May 1996, pp 825-838.  \n[ETSI,95] ETSI Work Programme DE\/RES - 0601, Subtechnical Committee (STC) RES 06, \u201cRadio \nEquipment Systems (RES); Trans-European Trunked Radio (TETRA); Voice plus Data (V+D), \nDesigner's Guide\u201d, May 1995.  \n[Davies,97] Davies, N., S. Wade, A. Friday, and G. Blair. \"Limbo: A Tuple Space Based Platform for \nAdaptive Mobile Applications.\" Proc. Joint International Conference on Open Distributed \nProcessing and Distributed Platforms (ICODP\/ICDP '97), Toronto, Canada, Chapman and Hall, \n1997. \n[Joseph,95] Joseph, A., A. deLespinasse, J. Tauber, D. Gifford, and M.F. Kaashoek. \"Rover: A Toolkit \nfor Mobile Information Access.\" Proc. 15th ACM Symposium on Operating System Principles \n(SOSP), Copper Mountain Resort, Colorado, U.S., ACM Press, Vol. 29, Pages 156-171. 3-6 \nDecember 1995. \n[Katz,94] Katz, R.H. \"Adaptation and Mobility in Wireless Information Systems.\" IEEE Personal \nCommunications Vol. 1 No. 1, Pages 6-17. \n[Keeton,93] Keeton, K., and R. Katz. \"The Evaluation of Video Layout Strategies on a High-\nBandwidth File Server.\" Proc. 4th International Workshop On Network and Operating System \nSupport for Digital Audio and Video, Lancaster House, Lancaster, U.K., Pages 237-248.  \n[Kriaras,97] Kriaras, I. \"GSM\/WaveLAN Integration.\" Proc. 4th Wireless Workshop, Lucent Bell \nLaboratories, Holmdel, U.S., 1997. \n[MM,97] Mobile Multimedia collaborative project with Simoco International Ltd, Barclay Associates \nLtd, HW Communications Ltd, (EPSRC LINK PCP project GR\/K82024), further details: \n www.lancs.ac.uk\/computing\/research\/mpg\/most\/mm_project.html \n[Moura,96] Moura, J.M.F., R.S. Jasinchi, H. Shiojiri, and J. Lin. \"Video Over Wireless.\" IEEE \nPersonal Communications, 3 No. 1, Pages 44-54.  \n[Tuch,91] \"An ISM Band Spread Spectrum Local Area Network: WaveLAN,\" IEEE Workshop on \nWireless Local Area Networks, Worcester Polytechnic Institute 140-145. \n[Yeadon,96] N. Yeadon, F. Garcia, D. Hutchison and D. Shepherd, \u201cFilters: QoS Support Mechanisms \nfor Multipeer Communications\u201d, IEEE Journal on Selected Areas in Computing (JSAC) special \nissue on Distributed Multimedia Systems and Technology, Vol. 14, No. 7,. Pp. 1245-1262, \nSeptember 1996. \n[Yeadon,98] Yeadon, N., N. Davies, A. Friday, and G.S. Blair. \"Supporting Video in Heterogeneous \nEnvironments.\" Proc. Symposium on Applied Computing, Atlanta, U.S., February 1998. \n[Zdonik,94] Zdonik, S., M. Franklin, R. Alonso, and S. Acharya. \"Are Disks in the Air Just Pie in the \nSky?\" Proc. Workshop on Mobile Computing Systems and Applications, Santa Cruz, CA, U.S., \n1994. \n"}