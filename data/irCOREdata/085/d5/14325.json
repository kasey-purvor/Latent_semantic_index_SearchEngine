{"doi":"10.1080\/0968776970050106","coreId":"14325","oai":"oai:generic.eprints.org:229\/core5","identifiers":["oai:generic.eprints.org:229\/core5","10.1080\/0968776970050106"],"title":"Prospects for summative evaluation of CAL in higher education","authors":["Draper, Stephen"],"enrichments":{"references":[{"id":200740,"title":"Delivery trucks or groceries? More food for thought on whether media (will, may, can't) influence learning',","authors":[],"date":"1994","doi":"10.1007\/bf02299086","raw":"Ross, S.M. (1994), 'Delivery trucks or groceries? More food for thought on whether media (will, may, can't) influence learning', Introduction to special issue of Educational Technology Research and Development, 42 (2) 5-6.","cites":null},{"id":200739,"title":"Effective Computer-Based Learning of Introductory Economics - Some Results from an Evaluation of the Winecon Package, Discussion Paper in Economics No.","authors":[],"date":"1996","doi":null,"raw":"MacDonald, Z. and Shields, M. (1996), Effective Computer-Based Learning of Introductory Economics - Some Results from an Evaluation of the Winecon Package, Discussion Paper in Economics No. 96\/11, University of Leicester.","cites":null},{"id":200738,"title":"Integrative evaluation: an emerging role for classroom studies of CAL',","authors":[],"date":"1996","doi":"10.1016\/0360-1315(95)00068-2","raw":"Draper, S.W., Brown, M.I., Henderson, F.P. and McAteer, E. (1996), 'Integrative evaluation: an emerging role for classroom studies of CAL', Computers and Education, 26 (1-3), 17-32; and in Kibby, M.R. and Hartley, J.R. (eds.), Computer Assisted Learning: Selected Contributions from the CAL 95 Symposium, Oxford: Pergamon, 17-32.","cites":null},{"id":200736,"title":"Measuring learning resource use',","authors":[],"date":"1996","doi":"10.1016\/0360-1315(96)00017-6","raw":"Brown, M.I., Doughty, G.F., Draper, S.W., Henderson, F.P. and McAteer, E. (1996), 'Measuring learning resource use', Computers and Education, 27, 103-13.","cites":null},{"id":200737,"title":"Reconsidering research on learning from media',","authors":[],"date":"1983","doi":"10.3102\/00346543053004445","raw":"Clark, R.E. (1983), 'Reconsidering research on learning from media', Review of Educational Research, 53 (4) 445-59.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"1997","abstract":"Many developers and evaluators feel an external demand on them for summative evaluation of courseware. Problems soon emerge. One is that the CAL may not be used at all by students if it is not made compulsory. If one measures learning gains, how does one know that one is measuring the effect of the CAL or of the motivation in that situation? Such issues are the symptoms of the basic theoretical problem with summative evaluation, which is that CAL does not cause learning like turning on a tap, any more than a book does. Instead, it is one rather small factor in a complex situation. It is of course possible to do highly controlled experiments: for example to motivate the subjects in a standardized way. This should lead to measurements that are repeatable by other similar experiments. However they will be measurements that have little power to predict the outcome when the CAL is used in real courses. Hence the simple view of summative evaluation must be abandoned. Yet it is possible to gather useful information by studying how a piece of CAL is used in a real course and what the outcomes were. Although this does not guarantee the same outcomes for another purchaser, it is obviously useful to know that the CAL has been used successfully one or more times, and how it was used on those occasions. Such studies can also serve a different \u2018integrative\u2019 rather than summative function by pointing out failings of the CAL software and suggesting how to remedy them","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14325.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/229\/1\/ALT_J_Vol5_No1_1997_Prospects%20for%20summative%20evalua.pdf","pdfHashValue":"66fc01bd92fefc4436883ad9388850c9bb515688","publisher":"Universit of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:229<\/identifier><datestamp>\n      2011-04-04T09:22:16Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/229\/<\/dc:relation><dc:title>\n        Prospects for summative evaluation of CAL in higher education<\/dc:title><dc:creator>\n        Draper, Stephen<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        Many developers and evaluators feel an external demand on them for summative evaluation of courseware. Problems soon emerge. One is that the CAL may not be used at all by students if it is not made compulsory. If one measures learning gains, how does one know that one is measuring the effect of the CAL or of the motivation in that situation? Such issues are the symptoms of the basic theoretical problem with summative evaluation, which is that CAL does not cause learning like turning on a tap, any more than a book does. Instead, it is one rather small factor in a complex situation. It is of course possible to do highly controlled experiments: for example to motivate the subjects in a standardized way. This should lead to measurements that are repeatable by other similar experiments. However they will be measurements that have little power to predict the outcome when the CAL is used in real courses. Hence the simple view of summative evaluation must be abandoned. Yet it is possible to gather useful information by studying how a piece of CAL is used in a real course and what the outcomes were. Although this does not guarantee the same outcomes for another purchaser, it is obviously useful to know that the CAL has been used successfully one or more times, and how it was used on those occasions. Such studies can also serve a different \u2018integrative\u2019 rather than summative function by pointing out failings of the CAL software and suggesting how to remedy them.<\/dc:description><dc:publisher>\n        Universit of Wales Press<\/dc:publisher><dc:date>\n        1997<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/229\/1\/ALT_J_Vol5_No1_1997_Prospects%20for%20summative%20evalua.pdf<\/dc:identifier><dc:identifier>\n          Draper, Stephen  (1997) Prospects for summative evaluation of CAL in higher education.  Association for Learning Technology Journal, 5 (1).  pp. 33-39.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776970050106<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/229\/","10.1080\/0968776970050106"],"year":1997,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"Prospects for summative evaluation\nof CAL in higher education\nStephen W. Draper\nDepartment of Psychology, University of Glasgow\nMany developers and evaluators feel an external demand on them for summative evaluation of\ncourseware. Problems soon emerge. One is that the CAL may not be used at all by students if it is not\nmade compulsory. If one measures learning gains, how does one know that one is measuring the effect\nof the CAL or of the motivation in that situation? Such issues are the symptoms of the basic theoretical\nproblem with summative evaluation, which is that CAL does not cause learning like turning on a tap,\nany more than a book does. Instead, it is one rather small factor in a complex situation. It is of course\npossible to do highly controlled experiments: for example to motivate the subjects in a standardized\nway. This should lead to measurements that are repeatable by other similar experiments. However they\nwill be measurements that have little power to predict the outcome when the CAL is used in real\ncourses. Hence the simple view of summative evaluation must be abandoned Yet it is possible to gather\nuseful information by studying how a piece of CAL is used in a real course and what the outcomes\nwere. Although this does not guarantee the same outcomes for another purchaser, it is obviously useful\nto know that the CAL has been used successfully one or more times, and how it was used on those\noccasions. Such studies can also serve a different 'integrative' rather than summative function by\npointing out failings of the CAL software and suggesting how to remedy them.\nIntroduction\nSummative evaluation is evaluation done after software design and production is\ncomplete in order to establish its performance and properties. A prototypical case would\nbe the tables produced in the consumer magazine Which? comparing a considerable range\nof properties of alternative available products (for example, washing machines) that they\nhave measured in their own trials. Thus summative evaluation is not only done after\nproduction; it is typically about comparative measurements done to assist decisions\nconcerning purchase.\nMany developers and evaluators feel an external demand on them for summative\nevaluation of courseware. They feel they are being asked to prove that the software\n'works', to show that it is cost-effective, that it is durable, and that it is worth the price to\nthe purchaser. This is seen as a matter of testing the software, as in most software-\ndevelopment projects. Thus tests are done by using the software, and measuring various\n_\nStephen W. Draper Prospects for summative evaluation of CAL in higher education\noutcomes of its use (for example, how people think about it, what is learned). Sometimes\nthe software's performance is compared with some alternative such as no software or\ntraditional teaching.\nHowever, we know much less about the ingredients of successful teaching delivery than\nwe do about washing clothes, and this has important consequences for what we can learn\nfrom measurements and what we in fact want to find out.\nSymptoms of problems with the obvious approach\nIn our work on the TILT project (Doughty et al, 1995), we were soon struck by features\nthat cast doubt on the sense of our doing evaluation of this kind (Draper et al, 1994).\n1. The first is that the CAL may not be used at all by students if it is not made\ncompulsory (here I shall use the term 'CAL' to refer indiscriminately to any computer\nsoftware that might be introduced by teachers to support learning). This draws attention\nto the crucial role of motivation. If you measure learning gains, how do you know you are\nmeasuring the effect of the CAL or of the motivation in that situation? Certainly in the\ncase mentioned, the CAL alone produced no learning because it produced no usage:\nmotivation created by a teacher was crucial.\n2. Another issue is that of the actions of teachers, for instance engaging students in a\nSocratic dialogue based around the CAL software. Obviously, any learning gains would\nbe affected, and probably dominated, by the teacher's skill. But to evaluate software in\nthe absence of teachers is to measure a different situation from the most common one in\nhigher education, and furthermore one that would not get the most from the software;\nhence it is neither realistic (valid) nor constructive.\n3. We have observed student study strategies such as note-taking radically changed by\nshort remarks by the teacher, far more so than in a lecture. At least at the present time, it\nseems that CAL does not usually elicit a stable study strategy, while teachers can and do\ninfluence it in a big way. Since study strategies have a large effect on learning outcomes,\nagain it seems beside the point to look for measurements independent of these; rather, the\npoint would be to discover which study strategy is best for each piece of courseware and\nhow to ensure that students adopt it.\n4. Although some software is designed to be used once and never referred to again, much\ncourseware is like textbooks and is intended to serve as reference and revision material as\nwell as, or instead of, primary exposition. That means that the relevant tests of learning\nmust be delayed until after the examination. However, it is then hard to tell how much, if\nat all, the students depended on the courseware as opposed to alternative resources such\nas books. Any evaluation will be indicative not about the properties of the software but\nabout how the overall set of resources and student activities performed.\n5. As a corollary to this, if in fact students find the courseware useless, they are likely to\ncompensate by relying more on alternative resources (we might call such self-monitoring\nand correction 'auto-compensation'). In universities, poor teaching may often be masked\nby this, and final performance relatively little affected. On this view, studying the effect of\ncourseware in isolation is unrealistic, but overall performance depends mainly on the\n34\nALT-] Volume 5 Number I\nstudents' self-management rather than on any one resource. One could, however, attempt\nto study which resources students use and value (Brown et al, 1996).\n6. Halo effects can also be important, where a teacher's attitude to the technology may\nstrongly affect students in either positive or negative directions. While we have observed\nmarked effects of this kind on student attitudes, whether this matters for learning depends\nalso on whether student attitudes affect their use of the CAL software; if they have no\nalternative resource, it may not matter.\n7. Similarly, Hawthorne effects may occur, where the act of doing the evaluation may\naffect students by making them feel more valued, pay more attention to the CAL\nsoftware, and perhaps the subject matter it deals with. In addition to an effect on learners'\nattitudes, pre-tests given as part of an evaluation may well improve learning by\ncommunicating to the students what they should try to learn from the material.\nFurthermore, priming students to activate the relevant part of their theoretical knowledge\nis known to have a big effect on improving learning outcomes from laboratory classes and\nsimulations, whether this is done by the evaluation or by a deliberate part of the teaching\n(for example, pre-laboratory exercises). All one can evaluate is the combined effect of the\nCAL software, the evaluation, and the whole of the associated teaching and learning\nresources. This is fine from the point of view of improving learning and teaching, and\nindeed evaluation should probably be a permanent part of practice, but it again\nundermines the view that the effects of CAL can be studied as an independent topic.\nCAL is only part of an ensemble\nThe fundamental point is that CAL'does not cause learning, and in fact is not a major\ncause at all. Learning results from the combined effect of many important factors, and\ntypically, in universities, from multiple resources. Any realistic study or evaluation\nmeasures the combined effect of an ensemble. This does not mean evaluation studies are\nimpossible, but it does mean we need to think out what we really want to discover. We\ncannot expect to treat CAL like a washing machine: as a simple device whose\nperformance can be measured once in a standardized situation which will then tell us all\nwe need to know to decide whether and how to use it.\nIf we remember that testing a piece of CAL is essentially the same as testing a textbook\nwould be, then this seems obvious. It is also like considering the question: Is the 9.30\nGlasgow-Edinburgh train good for getting to Edinburgh? It is possible to imagine that\nthere could be something uniquely good or bad about that train and not others, but in\nfact usually the important factors are not the details of the train itself but how it fits into\npeople's overall travel needs and plans. People use trains only as part of wider plans, and\ntrains are mainly good or bad to the extent that they fit, or do not, into the success of\nthese wider plans. To do meaningful evaluation of CAL, we have to understand learners'\nwider plans and study them: what they are, what the main factors are that influence their\nsuccess, and where CAL fits into this.\nThe issues listed above are the symptoms of the basic theoretical problem with summative\nevaluation, which is that CAL does not cause learning like turning on a tap, any more\nthan a book does. Instead it is one rather small factor in a complex situation.\n35\nStephen W. Draper Prospects for summative evaluation of CAL in higher education\nIntegrative evaluation: the actual utility of summative evaluation\nIn the TILT project, we performed many evaluations on completed software in the\nclassroom. We found that our evaluation reports were often useful to teachers, but not\nfor summing up the properties of the software so much as for identifying specific\nproblems in the case being studied that the teachers would use to make changes, usually\nnot to the software but to some other aspect of the delivery, for example, modifying how\nthey introduced the software, or adding a section to a lecture. We thus realized that the\nvalue of our evaluation was not as summative evaluation of the software, but as\nformative evaluation of the overall teaching and learning situation. We called this\n'integrative evaluation' (Draper et al, 1996) because most of the changes made concerned\nimproving the embedding or integration of the software into the rest of the surrounding\ndelivery.\nExperience elsewhere seems to bear this out: often when CAL is first introduced there are\nsome problems, particularly if students feel they have been set adrift without the right kind\nof support and guidance. If evaluation is taken as measuring how good or bad the software\nis, it would have to record a partial failure, and would have to refrain from making\ncontributions to improvements. However, this is not how sensible participants actually use\nit: they use the evaluation to alert them to problems and quickly introduce improvements.\nThe next time the course is given, the evaluation can often verify the improvement, but its\nmost important role has been identifying problems and necessary improvements.\nConversely, when participants treat an evaluation as strictly summative, problems can\nresult. In an unpublished case, an evaluation was commissioned to help the institution\ndecide whether to adopt a substantial CAL package as part of a course. The report was\nthen interpreted strictly as supporting the adoption, but all its advice on the crucial\nintegration issues it identified was overlooked and no further evaluation was permitted.\nStandard student feedback later indicated substantial quality problems on the resulting\ncourse, apparently around those integration issues identified in the original study.\nWhat we really want, and what we can do\nEvaluation is worth doing only if it serves some purpose and leads to some useful action.\nThe previous section described how evaluations on completed software are often used to\nidentify integration issues and so improve the total teaching and learning. Are there, then,\nany other goals that a summative evaluation might be required to help us with? Yes - the\ngoals informally expressed as: Are we going to use it? How are we going to use it? An\nimportant decision that should be supported by information from evaluations, in\neducation as in consumer purchases, is whether or not to buy and adopt some product, in\nthis case a piece of CAL software. Relevant to this would be answers to whether students\ndo learn in courses adopting the CAL software, whether this worked in other institutions\n(particularly one's own), what it costs in resources to run the course with the CAL\nsoftware, and what one needs (and needs to know) in order to run such a course\nsuccessfully: a description of the whole teaching delivery including auxiliary materials\nwould thus be very desirable.\nThe fact that learning outcomes depend on the combination of many factors, of which the\nCAL software is only one, means that no single study can prove that the software will\n36\nALT-] Volume 5 Number I\nwork in any other situation; for example, it might work for the authors, but not work in\nthe different context of a new adopter. However, while this does mean that certainty is\nbeyond reach, it does not mean that evaluation is worthless for this. Imagine what you\nwould find useful and persuasive as evidence about whether to adopt a piece of CAL\nsoftware. The first important thing to discover is whether it has ever been used\nsuccessfully, in other words with satisfactory learning outcomes. Even in fields much\nbetter understood theoretically, such as building aircraft, the first use is crucial to\ndemonstrate that no crucial mistake has been made: one does not expect to be the first\nperson on an aircraft never used before. But unlike aircraft, the performance of CAL\ndepends to a great extent on the surrounding context of use, so tests with real students as\npart of a real course are much more convincing than tests in a laboratory with paid\nsubjects. Thus, even though certainty may be beyond reach, tests showing that the CAL\nsoftware can be successful are an important reason for and outcome of summative\nevaluation. Beyond that, the questions (see above) shade into issues of how best to use it.\nThe more these issues are identified explicitly and made available in reports or auxiliary\nmaterial for teachers, the better for those deciding whether to adopt it. Pure\ndemonstrations of possible success, and outcomes from integrative evaluation, can\ntogether serve the essential underlying goal of supporting decisions about whether to\nadopt the software.\nExperiments\nA related issue is the use of controlled experiments. Summative evaluations on consumer\ngoods are based on such controlled experiments, for example using the same standard\nload of dirty clothes and the same detergent for each washing machine compared. On the\nother hand, the useful results of integrative evaluation are often (though not always) the\nresult of open-ended observation or student feedback, identifying factors that had not\nbeen foreseen and so not systematically measured. Furthermore, all the points above\nabout the factors likely to be important in affecting outcomes suggest that we do not\nknow enough to control all the relevant factors. Few if any experiments, for instance,\nattempt to control the halo effect or students' uncertainty about study strategies with\nCAL.\nCan experiments be used meaningfully at all in CAL evaluation? This remains arguable.\nClark (1983) has contended that no meaningful experiments on whether learning is\naffected by the medium of instruction have been or could be done, because other factors\nmore plausible as the causative agent vary with the medium. While hotly debated in the\nliterature (Ross, 1994), his arguments have not been conclusively rebutted, and they apply\nalso to the use of experiments in evaluation. They would apply most strongly to prevent\nus from drawing generalizations such as that a piece of CAL software will always be\nsuccessful. Nevertheless, some experiments seem rather convincing, for example,\nMacDonald and Shields (1996), particularly when alternative ways of teaching are\ndirectly compared, for example lectures, and CAL with and without special worksheets.\nNote that such experiments can in part serve an integrative role by yielding information\non how best to use the software: there is no exclusive association between evaluation\nmethods (experiment versus open-ended observation) and the evaluation goal (summative\nor integrative). In the end, experiments are probably like other studies of CAL: they can\n37\nStephen W. Draper Prospects for summative evaluation of CAL in higher education\nshow that the CAL software was definitely part of successful learning in one case, and if\nno other cases have been reported, then this is in favour of the software. On the other\nhand, we remain aware that many factors may have been important, and experimental\nreports in the literature never describe them all. It could always be that one of these\nfactors was crucial, but will not be present if one tries to use CAL in one's own teaching;\nfor instance, freeing up lecturers (by substituting CAL for lectures) perhaps meant that\nwhile in the laboratory supervising CAL use, they performed tutorial interactions with\nstudents that were crucial, just to have something to do.\nConclusion\nWe can do evaluation that is summative in some senses but not in others. We can do\nevaluation at the end of the design cycle on completed software that will not be further\nmodified. We can do evaluation that provides evidence relevant both to deciding whether\nto use that piece of courseware for teaching, and on how best to use it. This latter\nevidence might be from careful comparative experiments or more formative style work\nwhich detects unforeseen problems that turn out to be important in successful use of the\nsoftware. But there is no prospect of doing evaluation that sums up a product once and\nfor all, measuring its essential properties in a way that will represent and predict its\nperformance in all other contexts.\nAcknowledgements\nThis paper stems from work on the TILT (Teaching with Independent Learning\nTechnologies) project, funded through the TLTP (Teaching and Learning Technology\nProgramme) by the UK university funding bodies (DENI, HEFCE, HEFCW, SHEFC)\nand by the University of Glasgow. The ideas come from collaboration with other\nmembers of the evaluation group, particularly Margaret Brown and Erica McAteer. The\nstudies mentioned here could not have been done without, in addition, the active\nparticipation of many members of the University teaching staff to whom I am grateful.\nReferences\nBrown, M.I., Doughty, G.F., Draper, S.W., Henderson, F.P. and McAteer, E. (1996),\n'Measuring learning resource use', Computers and Education, 27, 103-13.\nClark, R.E. (1983), 'Reconsidering research on learning from media', Review of\nEducational Research, 53 (4) 445-59.\nDoughty, G., Arnold, S., Barr, N., Brown, M.I., Creanor, L., Donnelly, P.J., Draper,\nS.W., Duffy, C., Durndell, H., Harrison, M., Henderson, F.P., Jessop, A., McAteer, E.,\nMilner, M., Neil, D.M., Pflicke, T., Pollock, M., Primrose, C , Richard, S., Sclater, N.,\nShaw, R., Tickner, S., Turner, I., van der Zwan, R. and Watt, H.D. (1995), Using\nLearning Technologies: Interim Conclusions from the TILT Project, TILT Project Report\nno.3, Robert Clark Centre, University of Glasgow.\nDraper, S.W., Brown, M.I., Edgerton, E., Henderson, F.P., McAteer, E., Smith, E.D.\n38\nALT-] Volume 5 Number \/\nand Watt, H.D. (1994), Observing and Measuring the Performance of Educational\nTechnology, TILT Project Report no.1, Robert Clark Centre, University of Glasgow.\nDraper, S.W., Brown, M.I., Henderson, F.P. and McAteer, E. (1996), 'Integrative\nevaluation: an emerging role for classroom studies of CAL', Computers and Education, 26\n(1-3), 17-32; and in Kibby, M.R. and Hartley, J.R. (eds.), Computer Assisted Learning:\nSelected Contributions from the CAL 95 Symposium, Oxford: Pergamon, 17-32.\nMacDonald, Z. and Shields, M. (1996), Effective Computer-Based Learning of\nIntroductory Economics - Some Results from an Evaluation of the Winecon Package,\nDiscussion Paper in Economics No. 96\/11, University of Leicester.\nRoss, S.M. (1994), 'Delivery trucks or groceries? More food for thought on whether\nmedia (will, may, can't) influence learning', Introduction to special issue of Educational\nTechnology Research and Development, 42 (2) 5-6.\n39\n"}