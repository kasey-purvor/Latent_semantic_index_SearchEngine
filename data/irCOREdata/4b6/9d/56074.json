{"doi":"10.1109\/WCRE.2001.957829","coreId":"56074","oai":"oai:eprints.lincoln.ac.uk:1455","identifiers":["oai:eprints.lincoln.ac.uk:1455","10.1109\/WCRE.2001.957829"],"title":"Reverse engineering to achieve maintainable WWW sites","authors":["Boldyreff, Cornelia","Kewish, Richard"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2001","abstract":"The growth of the World Wide Web and the accelerated development of web sites and associated web technologies has resulted in a variety of maintenance problems. The maintenance problems associated with web sites and the WWW are examined. It is argued that currently web sites and the WWW lack both data abstractions and structures that could facilitate maintenance. A system to analyse existing web sites and extract duplicated content and style is described here. In designing the system, existing Reverse Engineering techniques have been applied, and a case for further application of these techniques is made in order to prepare sites for their inevitable evolution in futur","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/56074.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/1455\/1\/13030249.pdf","pdfHashValue":"120270ce4237097fa4792890ab8b1c0025cb670f","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:1455<\/identifier><datestamp>\n      2013-03-13T08:27:03Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47343430<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47343030<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F50:6A6163735F50343133<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/1455\/<\/dc:relation><dc:title>\n        Reverse engineering to achieve maintainable WWW sites<\/dc:title><dc:creator>\n        Boldyreff, Cornelia<\/dc:creator><dc:creator>\n        Kewish, Richard<\/dc:creator><dc:subject>\n        G440 Human-computer Interaction<\/dc:subject><dc:subject>\n        G400 Computer Science<\/dc:subject><dc:subject>\n        P413 Publishing via the World Wide Web<\/dc:subject><dc:description>\n        The growth of the World Wide Web and the accelerated development of web sites and associated web technologies has resulted in a variety of maintenance problems. The maintenance problems associated with web sites and the WWW are examined. It is argued that currently web sites and the WWW lack both data abstractions and structures that could facilitate maintenance. A system to analyse existing web sites and extract duplicated content and style is described here. In designing the system, existing Reverse Engineering techniques have been applied, and a case for further application of these techniques is made in order to prepare sites for their inevitable evolution in future<\/dc:description><dc:date>\n        2001<\/dc:date><dc:type>\n        Conference or Workshop contribution<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/1455\/1\/13030249.pdf<\/dc:identifier><dc:identifier>\n          Boldyreff, Cornelia and Kewish, Richard  (2001) Reverse engineering to achieve maintainable WWW sites.  In: Eighth Working Conference on Reverse Engineering (WCRE'01), 2-5 Oct 2001, Stuttgart, Germany.  <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/WCRE.2001.957829<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/1455\/","http:\/\/dx.doi.org\/10.1109\/WCRE.2001.957829"],"year":2001,"topics":["G440 Human-computer Interaction","G400 Computer Science","P413 Publishing via the World Wide Web"],"subject":["Conference or Workshop contribution","PeerReviewed"],"fullText":" 1 \nReverse Engineering to Achieve Maintainable WWW Sites \n \nCornelia Boldyreff1  and Richard Kewish \nR.I.S.E. \nDepartment of Computer Science  \nUniversity of Durham  \nDurham, DH1 3LE, U.K. \n+44 191 374 2638 \ncornelia.boldyreff@durham.ac.uk \n                                                           \n1\n Corresponding author \n \n \nABSTRACT \nThe growth of the World Wide Web and the accelerated \ndevelopment of web sites and associated web technologies \nhas resulted in a variety of maintenance problems. The \nmaintenance problems associated with web sites and the \nWWW are examined.  \nIt is argued that currently web sites and the WWW lack \nboth data abstractions and structures that could facilitate \nmaintenance. A system to analyse existing web sites and \nextract duplicated content and style is described here. In \ndesigning the system, existing Reverse Engineering \ntechniques have been applied, and a case for further \napplication of these techniques is made in order to prepare \nsites for their inevitable evolution in future.  \n \nKeywords \nWeb site maintenance, web analysis, detection of \nduplicated web content, re-structuring, data abstraction \n1 BACKGROUND \nSince the World Wide Web (WWW, or web) first became \nwidely accepted, it has grown to be a huge information and \nstorage medium. The speed and breadth of its growth and \nits inherent complexity have meant that the web has not \nseen many of the rigorous development and maintenance \nprinciples applied to its contents and applications that \ntraditional Software Engineering applies to more \nconventional large scale software applications [1,2].  \n \nIn the 1970s and 80s, computer scientists started to \nconsider software as another engineered product and began \nto apply the same stringent design, production, and \nmaintenance procedures that more conventional \nengineering disciplines already employed. The web is now \nbeing considered in the same way [2] and concepts \ndeveloped in traditional Software Engineering, for \nexample, Lehman's Laws [3] are beginning to be studied \nwith respect to the development of web sites over time [1]. \nHowever, this does not address the issue of web site \nmaintenance which has not been as widely considered as \nthe development of a web site [4,5], but is equally \nimportant now that the web has grown so large and has \nbecome an integral part of modern society.  \nThe current web site format is a collection of files \ncontaining several different types of information that can be \nretrieved and viewed using a web browser. Typically \ndocuments are constructed using the Hypertext Markup \nLanguage (HTML) which defines the style, structure, and \ncontent of the web pages. These pages can also contain \nembedded elements and links to other files of the same or \ndifferent formats such as images or Java applets. \nOn a conventional web site, each page is stored as a \nseparate file in a hierarchical directory structure. These are \nstored on one or more servers that run software to \nimplement the Hypertext Transfer Protocol (HTTP) which \nprocesses client requests (e.g. from distributed web \nbrowsers) and returns the relevant files. However, the \nhaphazard manner in which the web has come into being \nhas had a profound influence on the its current state and as \na result much of the existing web is very hard to maintain \nand has not been subjected to systemic or routine \nmaintenance. \nThe current system of storage for web sites causes several \nproblems for maintaining the site; these are as follows: \n\u2022 the contents of individual files may be duplicated \nacross several pages, possibly causing inconsistent data \nthroughout the site; \n\u2022 the page styles may not be consistent across the site \ngiving an unprofessional look to the site and possibly \nmaking it difficult to navigate; \nProceedings of the Eighth Working Conference On Reverse Engineering (WCRE\u009201) \n0-7695-1303-4\/02 $17.00 \u00a9 2002 \u0001 IEEE \n 2 \n\u2022 links and references on web sites are notoriously \nvolatile so they require frequent checking and updating \n[6], a task that is impeded by decentralisation of the \nweb site contents and distribution of its maintenance. \nIt is now beginning to be accepted that a web site can be \nregarded in much the same way as a large-scale \nconventional software system [2,7], with many of the same \nmaintenance requirements as well as some of the \nspecification and design requirements. Therefore, in a \nsimilar manner to the way that software models and \nprogramming language structures have been adapted to \naccommodate our changing perception of software, a new \nmore easily maintained structure for web sites is required.  \nThe problems identified above can be ameliorated or solved \naltogether by the centralisation of web content storage and \nthe introduction of some basic, well established software \nmaintenance principles. In the remainder of this paper, \nmaintenance problems associated with the WWW are \nanalysed in greater depth in the following section. The \napplication of traditional reverse engineering techniques to \nexisting web sites is covered in a further section. The \nresults achieved are discussed and evaluated in a final \nsection, where future work is also considered. \n \n2 MAINTENANCE PROBLEMS ASSOCIATED \nWITH THE WWW \nSimilarities can easily be drawn between large web sites \nand large-scale software systems [1,2]. Large sites often \ncontain many thousands of lines of 'code' split into many \n'modules' stored in many places with large amounts of data \noften important to the owner of the site. \nIn most traditional software systems, the code is segregated \nfrom the data by data abstraction, and many systems are \ndata processors in which the program only interacts with \nthe data when that data is provided to it. However, due to \nthe design of HTML, in web documents, the data is marked \nup by tags to format both its structure and style of \npresentation. Thus, the data is an integral part of the 'code', \nand this complicates web maintenance. Others have noted \nthat the blurred distinction between data and software in the \nHTML model presents developers with some interesting \nand problematic consequences [1,7]. \nDue to the ease with which web pages can be generated \nusing a variety of tools (MS Frontpage, Netscape \nComposer, Macromedia Dreamweaver to name but a few), \nweb sites can be created by people with little or no formal \nknowledge of software engineering and in very little time. \nFor small scale personal web sites, it is not important that \nthe site is error free and up-to-date; however, for larger \ncorporate sites, this is often key to the businesses' on-line \nsuccess. \nDespite being young, the WWW has grown and is still \ngrowing rapidly with the result that modern sites are as \nlarge and complex as large-scale traditional software. In \n1996, the maintainers of the Microsoft web site1 estimated \nthat they maintained over a million pages with a predictable \nimpact on maintenance [8]. \nThe rapid growth of the WWW combined with the \nnecessity for businesses to quickly deploy the sites results \nin sites being produced with little or no design and no \nconcern for the maintenance issues. This has resulted in a \nstate of poorly maintained sites [9]. \nSpecific Problems Identified \nIn research carried out into the state of web sites [10,8,9], \nfour common problems has been identified; they are as \nfollows: \n\u2022 broken links, \n\u2022 incorrect or out-of-date data, \n\u2022 inconsistent information, and  \n\u2022 inconsistent style. \nSurveys of web users [4] have shown that inconsistency \nand inaccuracy of data rank alongside with broken links as \nmajor obstacles in the take-up of the web. Inconsistent style \nand poor navigation have also been cited as major problems \nthat discourage users from continuing to use a site [11]. \nResearch into these areas is developing alongside Human \nComputer Interaction and Usability research to try and \ndevelop guidelines and strategies that will help businesses \nto utilise the web more effectively [5]. \nBroken links \nThis especially common problem is due to links being \nexplicitly \"hard-wired\" into the HTML of a web page no \nlonger pointing to the target page and its contents. A \nbroken link is usually a consequence of one of the \nfollowing: \n\u2022 the linked site has moved or closed down; \n\u2022 the linked page has been removed or renamed; \n\u2022 the link has been incorrectly specified (i.e. it is mis-\nspelt or simply nonexistent); \n\u2022 the page exists, but the its access permissions have \nbeen wrongly set; \n\u2022 the page exists but the relevant information has \nchanged or been removed making the linked content \nno longer relevant to the originating page with a link to \nit. \nWays have been developed to minimise the first three \nsituations described above. These fall into two categories: \n                                                           \n1\n http:\/\/www.microsoft.com \nProceedings of the Eighth Working Conference On Reverse Engineering (WCRE\u009201) \n0-7695-1303-4\/02 $17.00 \u00a9 2002 \u0001 IEEE \n 3 \ndynamic link storage and link testing tools. \nDynamic storage usually consists of a database that stores \nthe links centrally [6]. The HTML page containing the link \nis dynamically generated with the data for the database \nwhen requested. This enables the maintainer to easily \nupdate all occurrences of a link in a site when the link \nchanges. It also simplifies the process of re-structuring a \nweb site. \nLink testing is carried out to find links that are no longer \nvalid [6]. Typical systems parse the HTLM document and \nsystematically test every link to ensure that a valid page \nexists at the end of the link. These tools can be used in \nconjunction with dynamic storage to provide more \ncomprehensive aid to maintainers. \nThe final two situations are almost impossible to prevent \nunless there is co-operation between the maintainers of the \npages at both ends of the link. Typically this will only be \npossible for links within a site. \nIncorrect or Out-of-date Information \nOne of the major reasons for businesses setting up web \nsites is that they have large amounts of data that changes \ntoo frequently for them to publish or disseminate \neffectively through standard media (e.g. booklets, mailings, \nretail outlets) [10]. By creating a web site, they can have \none central information point and concentrating on keeping \nit up-to-date and as accurate as possible. However, \nproblems of out-of-date information are still common [9] \nand many sites give no indication of the currency of their \ninformation, nor do they indicate the last time of up-date. \nInconsistent Information \nInconsistent information is separate from the previous \ncategory because, although it will invariably include \nincorrect information, in the case of inconsistent \ninformation, some of it may be correct. In a typical web \nsite, the same information may be stored in one or more \ndifferent places. If this information changes and the \nmaintainer doesn't up-date all occurrences of the \ninformation, the inconsistencies will be introduced into the \nsite [9]. Consequently some of the information will be \nincorrect and misleading [4]. \nThese problems of information incorrectness and \ninconsistency have been addressed by the creation of server \nside scripting systems [7] connected to databases that can \nbe used to dynamically generate pages containing the most \nrecent and up-to-date information obtained from the \ndatabase. This will be discussed in greater detail in the \nsection on problems using database storage with the \nWWW. \nInconsistent Style \nThis is a separate consideration from the design of the user \ninterface in so much as it is not concerned with the actual \nchoice of style used, but rather the consistent application of \nthe styles used. Research into Human Computer Interaction \nhas shown that systems should use a consistent and \nstandard style throughout [4]. However, it is not uncommon \nto find different fonts, font sizes, colours, backgrounds and \nother style variations within the same web site across \nseveral pages or even in single pages of the site. Obviously \nthis gives the site an unprofessional image as well as \ncausing problems for users with navigation and readability. \nThe Internet communities, and the standards bodies in \nparticular, have started to make great efforts to counter this \nproblem. For instance, the consortium in charge of \nstandardising the web, the W3C2, has introduced the \nconcept of Cascaded Style Sheets (CSS) which have been \nwidely accepted by web site authors and browser \ndevelopers alike. \nCSS enable the author or author tool to abstract the style, \nand to a lesser extent the structure, away from the contents \nof the HTML file. Dave Raggett, one of the leading \nproponents of CSS and a major contributor to the W3C \nrecommendation3, has demonstrated the effective \ntransformations that can be carried out by basic scripting \nand CSS [12]. By creating a CSS file for a collection of \npages and making small changes to that single file, it is \npossible to dramatically alter the appearance of all the \nHTML pages in a consistent manner. This feature is of \ngreat importance to maintainers [14] as it minimises their \nmaintenance efforts with respect to site style. \nRoot Causes of Web Site Maintenance Problems \nWhile the above problems all reflect poor maintenance \npractices, they are symptomatic of poor design decisions \nand a general lack of Software Engineering principles being \napplied in web developments. In addition, several features \nof the existing WWW architecture compound the situation. \nMost notable among these are: \n\u2022 forced duplication of files or data; \n\u2022 the file structure of web sites; and \n\u2022 the HTML format of combined code and data storage. \nDuplication of Files or Data \nIf the same information is required in two or more pages, it \nis common for that information to be duplicated and placed \non all the pages. HTML lacks an \"include\" directive \ncommon in many other languages. This problem is caused \nby the method of storage within HTML and forces the \ndesigner to choose between linking pages and copying the \ndata. Likewise if a file, typically an image file, is needed in \nseveral places and it is awkward to reference it in a single \nlocation (see The Structure of Web Sites below), then \n                                                           \n2\n http:\/\/www.w3c.org \n3\n http:\/\/www.w3c.org\/TR\/REC-CSS2 \nProceedings of the Eighth Working Conference On Reverse Engineering (WCRE\u009201) \n0-7695-1303-4\/02 $17.00 \u00a9 2002 \u0001 IEEE \n 4 \ncopies may be made and placed throughout the site. This \ncauses the problems associated with the failure to \nsuccessfully update all copies. \nThe File Structure of Web Sites \nWeb sites consist of files containing the site contents. The \npredominant file format is HTML, with some embedded \ncontent such as images and increasingly the inclusion of \nscripts both as part of the HTML document or as  a means \nof generating the HTML page. \nThe files, which can be likened to modules in a \nconventional software system, are usually stored in \ndirectories to further \"modularise\" the site into \ncomparmentalised blocks of similar data type or related \ncontent. When this approach is followed uniformly and \nsensibly across the site, it is very useful and effective in \ncreating a site whose content is easy to manage [1]. \nHowever, the huge scale of some web sites and the inherent \ncomplexity of so many interlinked files makes the \norganised decentralisation of the pages a complex task \nwhich, if not carried out carefully and with planning, can \nresult in illogical arrangements of files which quickly \nbecome too complicated to manage.                                                                                                                                                                \nFor instance, in a small site, it is sensible to store all images \nrequired on the site in an images directory and to use \nrelative URLs to reference them. A relative URL is \npreferable to an absolute URL because pages can easily be \nmoved around, even between web sites, without affecting \nthe links as long as the file structures are preserved. \nHowever, as the site grows and its file structure becomes \nmore differentiated, this can result in awkward use of \nrelative file referencing and the loss of the advantages \nassociated with relative URLs.  \nThe HTML format of combined code and data storage \nAs noted earlier, HTML files are a combination of data and \ntags. The tags control both the style and layout of pages. As \nthese contents are intermixed, a maintainer editing a page \nto alter content can inadvertently alter tags. \nIn summary, it is important to carefully design the file \nstructure associated with a web site and to consider the \nimplications of file contents on the maintainability of a site. \nThis is an area of web development that has received little \nstudy until recently. In the following section, consideration \nis given to this area and the proposed solution of using a \ncentralised data storage medium such as a database, as \nsuggested in [13], is investigated. \nProblems of using Database Storage with the WWW  \nOne of the main uses for databases on the WWW is for the \nstorage of company data such as customer or product \ndetails. This information may be stored in specialised \ndatabases designed and used solely for the company web \nsite, or it may utilise a connection to existing company \ndatabases to produce the same result [7]. Obviously it is \nwasteful to maintain the same information in more than one \ndatabase although for security reasons it may be sensible to \nmake one database a partial or complete replica of another. \nMany companies and organisations holding information in \nexisting databases have sought to integrate these data stores \nwith their web sites, rather than incur the penalties of \nhaving to maintain separate data sources. Dynamic page \ngenerating systems and associated scripting languages \nmake such integration possible although considerable re-\ndevelopment of existing web pages may be necessary. In \naddition, co-ordination of the database maintenance with \nthe web site maintenance is required.  \nDatabase systems can also be used to store more complex \ncomponents of web sites or references to these. Examples \nof these are databases to store links [6, 14]. These enable \nmaintainers to easily verify and update links throughout a \nsite. However, in the case of older web sites, some reverse \nengineering of the site may be required to ensure that \ncurrently replicated elements within the web site are \nidentified, abstracted, and, where appropriate, moved to a \ndatabase or subsumed into a scripting language program.  \n3 REVERSE ENGINEERING OF WEB SITES  \nThe databases used as discussed above are typically \nrelational databases. Typically little or no modification of \nthe existing database structures is required before \nintegration with the web; however, complex scripts or \nprograms may be required to extract, process and present \nthe relevant information. \nMany scripting languages have been developed to facilitate \ndynamic page creation. Several of these have had \nextensions built on to enable database access or have been \ndesigned with database integration in mind. An example of \nthe latter is Microsoft's Active Server Pages (ASP); and  an \nexample of the former is the perl scripting language.  \nDatabases employed in the fashion need not be closely \nintegrated into the Web architecture as the scripting \nlanguages are well enough developed to make an almost \nseamless integration.  \nGiven the investment of companies in their existing \ndatabases and current web pages, this research has \naddressed the possibility of reversing engineering existing \nweb pages to identify replicated contents that can usefully \nbe stored in databases where the databases will either \nalready exist or be specifically created for this purpose. \nThrough this research, the aim is to determine how to assist \ncompanies in developing a better basis for the future \nmaintenance of their web sites and to overcome some the \nproblems associated with web site maintenance discussed \nearlier. \nThe reverse engineering undertaken with respect to web \nsites has drawn on earlier work with the Reverse \nEngineering community dealing with code analysis and \nclone detection as a goal of the work is to detect and \nProceedings of the Eighth Working Conference On Reverse Engineering (WCRE\u009201) \n0-7695-1303-4\/02 $17.00 \u00a9 2002 \u0001 IEEE \n 5 \nParse \nTree \nAnalyser \nData \nStorage \nBody \nAnalyser \nStyle-\nSheet \nGenerator \nDynamic \nPage \nGeneration \nHead \nAnalyser \nHTML \nFile \nParser \nminimise duplication of web site content. In classical \nsoftware engineering, a clone is any piece of code that has \nbeen copied and possibly altered. Clones are characterised \nby a measure of similarity ranging from 100% for replicas \nto 0% where no similarity can be found. However, as Baker \nsuggests it is very hard to find two pieces of code that will \nnot have some similarity, the sophisticated part is finding \ncode that is a proper clone rather than simply being \ncoincidentally similar [15]. \nFor example, a designer might create several HTML pages \non a web site, all with the same head section by copying the \nhead from one page into all the others. This would create \nclones with 100% similarity. If subsequently, the titles of \neach page are altered, then the clones would not be \nidentical although they would still have a high similarity.  \nGiven that the alteration of web pages is not random and is \nlikely to involve identifiable components, such as the page \ntitles in the above example, the approach adopted here has \nbeen to parse and analyse the web pages adapting \napproaches for existing work in Reverse Engineering \napplied to software code. An approach employing metrics \nbased on [16] is ruled out by the current lack of appropriate \nmetrics defined for web site contents although this is \ncertainly something to consider in the future. \nArchitecture of the System Developed \nBelow is an overview of the high-level architecture of the \nsystem developed. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nTo achieve adequate coverage, it was decided that the \nparser within the system should handle pages that \nconformed to the HTML 4.0 specification with some \nadditional features such as Active Server Pages (ASP), \nscripting languages (e.g. Javascript and VBScript) and \nCascaded Style Sheets (CSS).  \nAlthough the W3C DOM Parser for HTML 4.0 was \navailable, our system is based on an HTML 3.2 compliant \nparser from the JavaCC distribution. Unlike the W3C \nparser, this is not a validating parser, so it can \naccommodate invalid HTML. It also produces useful parse-\ntree output for further processing and thus, is a better basis \nfor the replica detection. It is also easy to maintain and \nadapt due to its yacc-like design; extensions to HTML 3.2 \nhave been easily achieved through altering the grammar \nsupplied.  \nAfter parsing, the system stores a rationalised copy of the \nextracted HTML data in a central data storage with each \nunique element (e.g. Page titles separate from links) stored \nin a separate part of the repository. Content that cannot \neasily be stored (e.g. images) are stored in files within an \norganised directory structure. Rather than storing the details \nof how to reconstruct the pages, the original HTML page is \nmodified to use scripts that retrieve the data from storage \nand regenerate pages as required. These modified files \ncontain only the structure of the original HTML files, with \nall the content abstracted out into the data storage and the \nstyles in CSS files. The files are stored in an organised \ndirectory structure on the server. It is these pages that will \nbe accessed by the server, executed, and published when \nrequested. \nThis approach was chosen because it could be implemented \non top of existing web architecture. It also has the \nadvantage of creating new web pages and an associated \ndatabase that is both easy to maintain and limits the \nmaintainer to altering data at its source, thereby ensuring \nthat, because the pages are generated from source, they will \nalways have the most up-to-date data content available. The \ndatabase is supported by an organised directory structure \nthat is used to store the external files. Several types of files \nare stored by the system including applets, images, scripts, \nand CSS files. The directory structure is created within a \ndirectory specified by the user and consists of the following \ndirectories listed in the figure below. \n \n \n \n \n \n \nDirectory Structure for the Storage System \npublic_html \n \nASP      Binaries      CSS      Images      Log      Script \n \nJScript   Other   VBScript \nProceedings of the Eighth Working Conference On Reverse Engineering (WCRE\u009201) \n0-7695-1303-4\/02 $17.00 \u00a9 2002 \u0001 IEEE \n 6 \nDatabase Design \nHaving decided to use a relational database for the central \ndata storage, the tables and fields were designed. The first \nconsideration was given to what extra data should be stored \nin the database to facilitate easy maintenance in the future. \nFrom our earlier considerations, it was decided that the \nfollowing should be added: \n\u2022 the last date of update for every element as this enables \nthe maintainers to track information that is out-of-date \nby a given date. Adding finer grained time stamps such \nas the time within a day was not considered necessary, \nas most information does not change so frequently to \nwarrant this. \n\u2022 an identifier for the person who carried out the update. \nThis enables maintainers to confer with one another \nregarding changes that have been made and to ensure \ntheir work is co-ordinated in a consistent manner. \n \nA record of changes can then be made in enough detail for \nmaintainers to ascertain who changed what when. \nThus, the basic requirement for every table is that it should \nhave fields allowing the following: unique identification of \nevery entry; identification of the date of the last update; and \nidentification of the maintainer who last updated it. \nThe detailed record of changes made is written to text files \nrather than the database as this is primarily for auditing \npurposes. \nAs the system requires the maintainers to supply both a \nusername and password before changes can be made to the \ndatabase, it is possible to identify each maintainer uniquely. \nThe usernames are stored in a separate table with a unique \nID number, that is referenced by the other tables to identify \nthem.  \nThe other tables are as follows: Titles, Styles, Scripts, \nObjects, Links, Images, Content, Bases, ASP. These are \ndescribed in greater detail in the following section. \nWhat is stored in where? \nThere are three broad types of data extracted by the system, \neach requiring a slightly different method of storage. These \nare as follows: textual content, large text files, and binary \nfiles.  \nThe textual content of the site is the information that is \npresented to the user as text. This can be short strings, e.g. \nthe title, or long paragraphs of text, more commonly found \nin body. This content changes frequently and is very \nimportant to the site owner. Character Large Object \n(CLOB) storage is used for textual content. On most \ndatabase systems, each CLOB field can store up to 2Gb of \ntext. \nAs scripts and CSS files contain large amounts of textual \ndata. This data is imported into the HTML file by \nreference. Although CLOB fields could have been used, \nthis would introduce the problem of how to recognise when \nthe client needs the files and how to pass the content as if it \nwas a separate file. This is avoided by storing all linked \nfiles in the directory structure and storing a reference to the \nfile in the database. \nBinary files, i.e. Image files and Java applets, like the large \ntext files containing scripts and styles, are stored in the \ndirectory structure, with a reference in the database. \nContent Layout and Processing \nThe tags surrounding the content define the layout of the \nHTML page as it will displayed in the client browser. The \nintroduction of new tags and CSS into HTML4 has allowed \nsome of the structural layout to be taken out; however, \nHTML tags still produce most of the structure. \nOriginally it was decided that the structure should be \nextracted during analysis and stored separately along with \nthe style and contents; and that all these stored elements \nwould be used to recreate the page. However, after \nstudying a number of existing web page layouts, including \nsites with consistent structures, it was not possible to find \nenough similarities to be able to break the structure down \nsatisfactorily. For this reason, the system keeps the \nstructure in place as found in the existing web page and \nonly extracts the style and content. To facilitate this, the \nanalyser constructs an HTML file as it processes the parse-\ntree. The tags are analysed and then copied to the new file, \nwith the content that has been removed from them replaced \nby an embedded script statement for retrieving the data \nfrom the database. When the analyser has finished, the new \nweb page file is written to the directory structure at the top \nlevel.  \nTo detect and remove duplication, the system processes \neach piece of text as follows:  \n1. all excess white space is removed (as this is ignored by \nbrowsers),  \n2. a code is generated for the text,  \n3. a comparison is made with codes for pieces of text \nalready processed, and  \n4. a new element with its corresponding code is only \nstored if no match has been found. \nStatistics \nIt was felt that maintainers might find it useful to see some \ndetails of the web page analysis results and subsequent \nprocessing. Therefore, the system generates details of what \nwent on during the parsing, analysis and storage. \nA log file, with a time and date stamp for easy \nidentification, is created during every run. Logs of all files \nparsed are made along with a summary of the analysis. All \nerror messages reported to the user during parsing are also \nProceedings of the Eighth Working Conference On Reverse Engineering (WCRE\u009201) \n0-7695-1303-4\/02 $17.00 \u00a9 2002 \u0001 IEEE \n 7 \nlogged. \nAfter each parse, a breakdown is presented. It includes the \nfollowing counts: \n\u2022 count of embedded files identified and how many are \nnot already recorded, broken down into file types, e.g. \nimages and scripts; \n\u2022 count of links identified and how many were not new; \n\u2022 count of blocks of textual content found and how many \nof these were not new; and \n\u2022 count of files parsed and number of failures and \nproblems encountered. \nRecreation of the Web Pages \nAfter the system has been employed, the pages need to be \nre-generated from the new web pages and the database \nupon requests made for the new web pages by clients. \nThese new web pages contain scripts that access the \ndatabase. One basic function is used to extract the relevant \ndata; it simply retrieves contents from a specified table and \nfield. The returned string is appended to the end of the \nHTML stream being generated by the server as it interprets \neach script in the new web page. \nThere is an overhead for the script interpretation and the \ndatabase accesses at the page load time. This differs in \nindividual cases; and if the overhead is considered too \ngreat, it would be possible to recreate a new static web page \nafter processing. However, this would require strict \ndiscipline to ensure that no changes were made to the static \npages and changes were made through the new system with \nnew versions of the pages being regenerated after any \nchanges. Quite a sophisticated extension to the system \nwould be required to determine the impact of any change \non all the other pages in the site to ensure that changes were \nmade uniformly across a site. \nThe advantage of using the dynamic approach is that \nchanges made to the stored database associated with a set \nof web pages take effect immediately. Because all the data \nis stored within the database and no HTML pages are ever \nstored locally, the maintainers cannot make changes to the \npages without changing the database thereby ensuring the \nentire set of pages is uniformly updated. \n4 RESULTS OBTAINED \nThe system has been evaluated with a small sample of web \nsites:  \n1. Palatinate'99 - the web site of the student newspaper of \nthe University of Durham. A snapshot of the site was \ntaken in June 1999. The site consists of 121 HTML \nfiles in 26 directories with many scripts and linked \nfiles. \n2. SEG'99 - The Computer Science Department at \nDurham have a web site of material associated with the \nsecond year Software Engineering group project which \nis part of the Software Engineering module. A copy of \nthe site at June 1999 was studied. The site consists of \n60 HTML files in 23 directories with scripts and linked \nfiles. \n3. Palatinate'00 - This site replaced the Palatinate'99 site \nand is a re-designed site. The copy of the site studied \nwas made in March 2000. The new site consists of 34 \nHTML files in 12 directories. It contains no scripts but \ndoes contain many linked files. \n4. Personal Web Site 1 - This site was the homepage of a \nUniversity of Durham student. Although this type of \nsite would not normally be subject to formal \nmaintenance, it is a good example of a well designed \nsite with consistent features and use of scripts. The site \nis entirely HTML4 compliant and utilises CSS. It is \nwell maintained and organised. The site consists of 27 \nHTML files in 5 directories. \n5. Personal Web Site 2 - this is the old site of the same \nstudent. The copy studied has been replaced by \nPersonal Web Site 1. It uses a combination of scripts, \nframes and images; but has been constructed with little \nregard to design principles and exhibits poor layout. \nThe site consists of 16 HTML files in 2 directories. It \ncontains several linked files. \nDuplication Detected \nThe table below provides evidence of the level of \nduplication found and eliminated by the system. \nWeb site Number of \nPieces of \nContent Found \nAverage Number of \nPieces of Content \nStored \nPalatinate\u201999 907 355 \nSEG\u201999 637 245 \nPalatinate\u201900 357 186 \nPersonal Site 1 284 201 \nPersonal Site 2 102 91 \n \nNote that while the parser consistently identified the same \nnumber of pieces of data for each site analysed, \ninsignificant variations in the number of duplicated pieces \nwere found depending on the version of the system used, so \nan average is given above. \nThe figure below shows these results graphically. It can be \nseen that there is a direct correlation between the number of \npieces of content in the site and the amount of duplication \ndetected. The more data there is the more duplication has \nbeen detected. \nConversely the smaller a site is, the amount of duplication \nProceedings of the Eighth Working Conference On Reverse Engineering (WCRE\u009201) \n0-7695-1303-4\/02 $17.00 \u00a9 2002 \u0001 IEEE \n 8 \nis also smaller. This is shown by the narrowing gap \nbetween the two lines. For instance, on the larger \nPalatinate'99 site, only 355 pieces of data are stored from a \npossible 907. This means the system detected that almost \ntwo thirds of the data has been duplicated. However, on the \nsmaller site, the proportion was found to be much lower. \nThe second personal site has almost no duplicated data \n(only a tenth is duplicated) and the slightly larger and better \ndesigned personal site has only a third of its data \nduplicated. \n \nThis result is not surprising because the maintainers of \nsmall sites will be able to remember a higher proportion of \nthe site than the maintainers of large sites, so they are more \nlikely to realise when they are repeating the content.  \nOverall, the results show that the system does a good job of \nremoving duplication within a web site, and thus with \nduplication minimised, the new site should be easier to \nmaintain. Although there is an overhead, in generating the \npages dynamically in the present system, it would be \npossible to generate static pages form the databases and use \nthese provided that no maintenance was carried out on \nthese directly as all the advantages of removing duplicates \nwould be lost. \nAn extension to the system ensures timely up-date by \nmonitoring changes made to the web site content databases \nand informing the maintainers every time that a data item \nover a specified age has been accessed. This feature \nprovides a very useful service for the maintainers because it \ninforms them if they have neglected some content for a \nlong time. It also highlights contents that are being \naccessed most frequently; and, if necessary, they can focus \ntheir maintenance efforts on some preventative \nmaintenance to ease the future maintenance. \n5 CONCLUSIONS \nThis research has demonstrated that a significant reduction \nin the duplicated content of web sites can be achieved \nthrough application of the system described here. The \nsystem achieves this through a more rational re-structuring \nof the original web site pages and their contents. \nIdentifying common styles and other contents requires \nextensive analysis of the existing web pages. However, the \nrationalisation of the site achieved should in theory allow \nmaintainers to understand the contents of the web sites \nmore straightforwardly and to separate concerns of style \nfrom other types of information content during subsequent \nmaintenance of the site. Empirical evidence of such \nbenefits can only be obtained in the longer term. At present \nthe system developed has only been used in trials with a \nvery small number of web sites. Integration of such a \nsystem with one of the more popular web page \ndevelopment systems would provide the necessary \nemployment of the system by practitioners needed to form \nthe basis for a more in-depth evaluation. We are \nconsidering this as a future development of the system.  \nThe modest results achieved do clearly show that \ntechniques from conventional Reserve Engineering can be \napplied to alleviate some of the maintenance problems \nfound with existing web sites and the current storage \nsystems employed for web sites. Certainly this work has \nshown that there is scope for further research on the reverse \nengineering of web sites.  \nREFERENCES \n1.  Warren, PJ, Boldyreff C, Munro M, The Evolution of \nWebsites, International Workshop on Program \nComprehension, IEEE Computer Society Press, 1999. \n2.  Brereton P, Budgen D, Hamiliton G, Hypertext: The \nnext Maintenance Mountain, IEEE Computer, Vol. 31, \nNo. 12, pp. 49-55, 1998. \n3.  Lehman MM, Belady L, Program Evolution: Processes \nof Software Change, Academic Press, London, pp. \n247-274, 1985. \n4.  White MD, Abels EG, Hahn K, Identifying user-based \ncriteria for Web pages, Internet Research, Vol. 7, No. \n4, pp. 252-262, 1997. \n5.  White MD, Abels EG, Hahn K, User-based design \nprocess for Web sites, Internet Research, Vol. 8, No. 1, \npp. 39-50, 1998. \n6.  Arnold SC, An Architecture for Maintaining Link \nStructure of a Website, Proceedings of WSE'99, 1st \nAnnual Workshop on Web Site Evolution, pp. 9-11, \n1999. \n7.  Antoniol G, Canfora G, et al, Web Sites: Files, \nPrograms or Databases?, Proceedings of WSE'99, 1st \nAnnual Workshop on Web Site Evolution, pp. 6-8, \n1999. \n8.  Prevelakis V, Managing large WWW Sites, Internet \nAmount of Duplicated Data in Web Sites\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1 2 3 4 5\nNumber o f\nPieces of\nContent\nAverage\nNumber o f\nPieces of\nContent Stored\nProceedings of the Eighth Working Conference On Reverse Engineering (WCRE\u009201) \n0-7695-1303-4\/02 $17.00 \u00a9 2002 \u0001 IEEE \n 9 \nResearch: Electronic Networking Applications and \nPolicy, Vol. 9, No. 1, pp. 41-48, 1999. \n9. Warren, PJ, Boldyreff C, Munro M, Characterising \nEvolution in Web Sites, Proceedings of WSE'99, 1st \nAnnual Workshop on Web Site Evolution, pp. 46-48, \n1999. \n10. Aspden P, Katz J, Motivations for and barriers to \nInternet usage, Internet Research: Electronic \nNetworking Applications and Policy, Vol. 7, No. 3, pp. \n170-188, 1997. \n11. Nielsen, J, Designing Web Usability, new Riders \nPublishing, 2000. \n12. Raggett D, Adding Style and Behaviour to Web Pages \nwith a Dash of Spice, Computer Networks and ISDN \nSystems, Vol. 30, pp. 676-678, 1998. \n13. van Ossenbruggen J, et al, Requirements for \nMultimedia Markup and Style Sheets on the World \nWide Web, Computer Networks and ISDN Systems, \nVol. 30, pp. 694-696, 1998. \n14. Hartman JH, et al, Index-based Hyperlinks, Computer \nNetworks and ISDN Systems, Vol. 29, pp. 1129-1135, \n1997. \n15. Baker, SB, On Finding Duplication and Near-\nDuplication in Large Software Systems, Proceedings \nof the Working Conference on Reverse Engineering, \nIEEE Computer Society Press, 1995. \n16. Mayrand J, Leblanc C, Merlo EM, Experiment on the \nAutomatic Detection of Function Clones in a Software \nSystem Using Metrics, Proceedings of the International \nConference on Software Maintenance, IEEE Computer \nSociety Press, pp. 244-253, 1996. \n \n \nProceedings of the Eighth Working Conference On Reverse Engineering (WCRE\u009201) \n0-7695-1303-4\/02 $17.00 \u00a9 2002 \u0001 IEEE \n 10 \n \nProceedings of the Eighth Working Conference On Reverse Engineering (WCRE\u009201) \n0-7695-1303-4\/02 $17.00 \u00a9 2002 \u0001 IEEE \n"}