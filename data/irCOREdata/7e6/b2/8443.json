{"doi":"10.1145\/1710000","coreId":"8443","oai":"oai:eprints.brighton.ac.uk:3201","identifiers":["oai:eprints.brighton.ac.uk:3201","10.1145\/1710000"],"title":"GENEVAL: a proposal for shared-task evaluation in NLG","authors":["Reiter, Ehud","Belz, Anja"],"enrichments":{"references":[{"id":42748169,"title":"Bleu: A method for automatic evaluation of machine translation.","authors":[],"date":"2002","doi":"10.3115\/1073083.1073135","raw":"Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of ACL-2002, pages 311\u2013318.","cites":null},{"id":42748174,"title":"Building Natural Language Generation Systems.","authors":[],"date":"2000","doi":"10.1017\/cbo9780511519857","raw":"Ehud Reiter and Robert Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press.","cites":null},{"id":42748166,"title":"Comparing automatic and human evaluation of NLG systems.","authors":[],"date":"2006","doi":"10.3115\/1706269.1706298","raw":"Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In Proceedings of EACL-2006, pages 313\u2013320.","cites":null},{"id":42748164,"title":"Evaluation metrics for generation.","authors":[],"date":"2000","doi":"10.3115\/1118253.1118255","raw":"Srinavas Bangalore, Owen Rambow, and Steve Whittaker. 2000. Evaluation metrics for generation. In Proceedings of INLG-2000, pages 1\u20138.","cites":null},{"id":42748182,"title":"Exploiting a parallel text-data corpus.","authors":[],"date":"2003","doi":"10.1007\/978-1-4471-0649-4_14","raw":"Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin Yu. 2003. Exploiting a parallel text-data corpus. In Proceedings of Corpus Linguistics 2003, pages 734\u2013 743.","cites":null},{"id":42748168,"title":"Generating textual summaries of graphical time series data to support medical decision making in the neonatal intensive care unit.","authors":[],"date":"2005","doi":"10.1007\/s10877-005-0879-3","raw":"Anna Law, Yvonne Freer, Jim Hunter, Robert Logie, Neil McIntosh, and John Quinn. 2005. Generating textual summaries of graphical time series data to support medical decision making in the neonatal intensive care unit. Journal of Clinical Monitoring and Computing, 19:183\u2013194.","cites":null},{"id":42748181,"title":"Lessons from a failure: Generating tailored smoking cessation letters.","authors":[],"date":"2003","doi":"10.1016\/s0004-3702(02)00370-3","raw":"Ehud Reiter, Roma Robertson, and Liesl Osman. 2003. Lessons from a failure: Generating tailored smoking cessation letters. Artificial Intelligence, 144:41\u201358.","cites":null},{"id":42748165,"title":"Shared-task evaluations in HLT: Lessons for NLG.","authors":[],"date":"2006","doi":"10.3115\/1706269.1706297","raw":"Anja Belz and Adam Kilgarriff. 2006. Shared-task evaluations in HLT: Lessons for NLG. In Proceedings of INLG-2006.","cites":null},{"id":42748180,"title":"Should corpora texts be gold standards for NLG?","authors":[],"date":"2002","doi":null,"raw":"Ehud Reiter and Somayajulu Sripada. 2002. Should corpora texts be gold standards for NLG? In Proceedings of INLG-2002, pages 97\u2013104.","cites":null},{"id":42748167,"title":"The evolution of evaluation: Lessons from the Message Understanding Conferences. Computer Speech and Language,","authors":[],"date":"1998","doi":"10.1006\/csla.1998.0102","raw":"Lynette Hirschman. 1998. The evolution of evaluation: Lessons from the Message Understanding Conferences. Computer Speech and Language, 12:283\u2013 285.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2006-01-01","abstract":"We propose to organise a series of shared-task NL G events, where participants are asked to build systems with similar input\/output functionalities, and these systems are evaluated with a range of different evaluation techniques. The main purpose of these events is to allow us to compare different evaluation techniques, by correlating the results of different evaluations on the systems entered in the event","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"Association for Computational Linguistics","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.brighton.ac.uk:3201<\/identifier><datestamp>\n      2015-02-25T14:48:56Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51303030:51313030<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.brighton.ac.uk\/3201\/<\/dc:relation><dc:title>\n        GENEVAL: a proposal for shared-task evaluation in NLG<\/dc:title><dc:creator>\n        Reiter, Ehud<\/dc:creator><dc:creator>\n        Belz, Anja<\/dc:creator><dc:subject>\n        Q100 Linguistics<\/dc:subject><dc:description>\n        We propose to organise a series of shared-task NL G events, where participants are asked to build systems with similar input\/output functionalities, and these systems are evaluated with a range of different evaluation techniques. The main purpose of these events is to allow us to compare different evaluation techniques, by correlating the results of different evaluations on the systems entered in the events<\/dc:description><dc:publisher>\n        Association for Computational Linguistics<\/dc:publisher><dc:date>\n        2006-01-01<\/dc:date><dc:type>\n        Contribution to conference proceedings in the public domain<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        <\/dc:rights><dc:identifier>\n        http:\/\/eprints.brighton.ac.uk\/3201\/1\/geneval-final.pdf<\/dc:identifier><dc:identifier>\n          Reiter, Ehud and Belz, Anja  (2006) GENEVAL: a proposal for shared-task evaluation in NLG  In: Proceedings of the 4th International Conference on Natural Language Generation (INLG'06), July 2006, Sydney, Australia.     <\/dc:identifier><dc:relation>\n        http:\/\/delivery.acm.org\/10.1145\/1710000\/1706298\/p136-reiter.pdf?ip=194.81.199.46&acc=OPEN&CFID=112180454&CFTOKEN=65961523&__acm__=1339761274_6cf5a555b9fe51d596450d277df43186<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.brighton.ac.uk\/3201\/","http:\/\/delivery.acm.org\/10.1145\/1710000\/1706298\/p136-reiter.pdf?ip=194.81.199.46&acc=OPEN&CFID=112180454&CFTOKEN=65961523&__acm__=1339761274_6cf5a555b9fe51d596450d277df43186"],"year":2006,"topics":["Q100 Linguistics"],"subject":["Contribution to conference proceedings in the public domain","PeerReviewed"],"fullText":"GENEVAL: A Proposal for Shared-task Evaluation in NLG\nEhud Reiter\nUniversity of Aberdeen, UK\nereiter@csd.abdn.ac.uk\nAnja Belz\nUniversity of Brighton, UK\na.s.belz@brighton.ac.uk\nAbstract\nWe propose to organise a series of shared-\ntask NLG events, where participants are\nasked to build systems with similar in-\nput\/output functionalities, and these sys-\ntems are evaluated with a range of differ-\nent evaluation techniques. The main pur-\npose of these events is to allow us to com-\npare different evaluation techniques, by\ncorrelating the results of different evalua-\ntions on the systems entered in the events.\n1 Background\nEvaluation is becoming increasingly important in\nNatural Language Generation (NLG), as in most\nother areas of Natural Language Processing (NLP).\nNLG systems can be evaluated in many differ-\nent ways, with different associated resource re-\nquirements. For example, a large-scale task-\neffectiveness study with human subjects could last\nover a year and cost more than US$100,000 (Re-\niter et al., 2003); on the other hand, a small-scale\ncomparison of generated texts to human-written\nreference texts can be done in a manner of days.\nHowever, while the latter kind of study is very\nappealing in terms of cost and time, and cheap\nand reliable evaluation techniques would be very\nuseful for people developing and testing new NLG\ntechniques, it is only worth doing if we have rea-\nson to believe that its results tell us something\nabout how useful the generated texts are to real\nhuman users. It is not obvious that this is the case\n(Reiter and Sripada, 2002).\nPerhaps the best way to study the reliability of\ndifferent evaluation techniques, and more gener-\nally to develop a better empirical understanding of\nthe strengths and problems of different evaluation\ntechniques, is to perform studies where a range of\ndifferent evaluation techniques are used to evalu-\nate a set of NLG systems with similar functional-\nities. Correlating the results of the different eval-\nuation techniques will give us empirical insight as\nto how well these techniques work in practice.\nUnfortunately, few such studies have been car-\nried out, perhaps because (to date) few NLG sys-\ntems have been built with comparable functional-\nity (our own work in this area is discussed below).\nWe hope to surmount this problem, by organising\n\u2018shared task\u2019 events to which NLG researchers can\nsubmit systems based on a supplied data set of in-\nputs and (human-written) text outputs. We will\nthen carry out our evaluation experiments on the\nsubmitted systems. We hope that such shared-task\nevents will also make it easier for new researchers\nto get involved in NLG, by providing data sets and\nan evaluation framework.\n2 Comparative Evaluations in NLG\nThere is a long history of shared task initiatives\nin NLP, of which the best known is perhaps MUC\n(Hirschman, 1998); others include TREC, PARSE-\nVAL, SENSEVAL, and the range of shared tasks or-\nganised by CoNLL. Such exercises are now com-\nmon in most areas of NLP, and have had a major\nimpact on many areas, including machine transla-\ntion and information extraction (see discussion of\nhistory of shared-task initiatives and their impact\nin Belz and Kilgarriff (2006)).\nOne of the best-known comparative studies\nof evaluation techniques was by Papineni et al.\n(2002) who proposed the BLEUmetric for machine\ntranslation and showed that BLEU correlated well\nwith human judgements when comparing several\nmachine translation systems. Several other studies\nof this type have been carried out in the MT and\nSummarisation communities.\nThe first comparison of NLG evaluation tech-\nniques which we are aware of is by Bangalore et al.\n(2000). The authors manually created several\nvariants of sentences from the Wall Street Jour-\nnal, and evaluated these sentences using both hu-\nman judgements and several corpus-based metrics.\nThey used linear regression to suggest a combina-\ntion of the corpus-based metrics which they be-\nlieve is a better predictor of human judgements\nthan any of the individual metrics.\nIn our work (Belz and Reiter, 2006), we used\nseveral different evaluation techniques (human\nand corpus-based) to evaluate the output of five\nNLG systems which generated wind descriptions\nfor weather forecasts. We then analysed how well\nthe corpus-based evaluations correlated with the\nhuman-based evaluations. Amongst other things,\nwe concluded that BLEU-type metrics work rea-\nsonably well when comparing statistical NLG sys-\ntems, but less well when comparing statistical NLG\nsystems to knowledge-based NLG systems.\nWe worked in this domain because of the avail-\nability of the SumTime corpus (Sripada et al.,\n2003), which contains both numerical weather\nprediction data (i.e., inputs to NLG) and human\nwritten forecast texts (i.e., target outputs from\nNLG). We are not aware of any other NLG-related\ncorpora which contain a large number of texts and\ncorresponding input data sets, and are freely avail-\nable to the research community.\n3 Our Proposal\nWe intend to apply for funding for a three-year\nproject to create more shared input\/output data sets\n(we are focusing on data-to-text tasks for the rea-\nsons discussed in Belz and Kilgarriff (2006)), or-\nganise shared task workshops, and create and test\na range of methods for evaluating submitted sys-\ntems.\n3.1 Step 1: Create data sets\nWe intend to create input\/output data sets that con-\ntain the following types of representations:\n\u2022 raw non-linguistic input data;\n\u2022 structured content representations, roughly\ncorresponding to document plans (Reiter and\nDale, 2000);\n\u2022 semantic-level representations, roughly cor-\nresponding to text specifications (Reiter and\nDale, 2000);\n\u2022 actual human-authored corpus texts.\nThe presence of intermediate representations in\nour data sets means that researchers who are just\ninterested in document planning, microplanning,\nor surface realisation do not need to build com-\nplete NLG systems in order to participate.\nWe will create the semantic-level representa-\ntions by parsing the corpus texts, probably us-\ning a LinGO parser1. We will create the content\nrepresentations using application-specific analysis\ntools, similar to a tool we have already created for\nSumTime wind statements. The actual data sets\nwe currently intend to create are as follows (see\nalso summary in Table 1).\nSumTime weather statements: These are brief\nstatements which describe predicted precipitation\nand cloud over a forecast period. We will extract\nthe texts (and the corresponding input data) from\nthe existing SumTime corpus.\nStatistics summaries: We will ask people (prob-\nably students) to write paragraph-length textual\nsummaries of statistical data. The actual data will\ncome from opinion polls or national statistics of-\nfices. The corpus will also include data about the\nauthors (e.g., age, sex, domain expertise).\nNurses\u2019 reports: As part of a new project at Ab-\nerdeen, Babytalk2, we will be acquiring a corpus\nof texts written by nurses to summarise the status\nof a baby in a neonatal intensive care unit, along\nwith the raw data this is based on (sensor read-\nings, records of actions taken such as giving med-\nication).\n3.2 Step 2: Organise workshops\nThe second step is to organise workshops. We\nintend to use a fairly standard organisation (Belz\nand Kilgarriff, 2006). We will release the data\nsets (but not the reference texts), give people six\nmonths to develop systems, and invite people who\nsubmit systems to a workshop. Participants can\nsubmit either complete data-to-text NLG systems,\nor components which just do document planning,\nmicroplanning, or realisation.\nWe are planning to increase the number and\ncomplexity of tasks from one round to the next,\nas this has been useful in other NLP evaluations\n(Belz and Kilgarriff, 2006); for example, we will\nadd surface realisation as a separate task in round\n2 and layout\/structuring task in round 3.\nWe will carry out all evaluation activities (see\nbelow) ourselves, workshop participants will not\nbe involved in this.\n3.3 Step 3: Evaluation\nThe final step is to evaluate the systems and com-\nponents submitted to the workshop. As the main\n1http:\/\/lingo.stanford.edu\/\n2http:\/\/www.csd.abdn.ac.uk\/research\/babytalk\/\nCorpus num texts num ref (*) text size main NLG challenges\nWeather statements 3000 300 1-2 sentences content det, lex choice, aggregation\nStatistical summaries 1000 100 paragraph above plus surface realisation\nNurses\u2019 reports 200 50 several paras above plus text structuring\/layout\n(*) In addition to the main corpus, we will also gather texts which will be used as reference texts for\ncorpus-based evaluations; \u2018num ref\u2019 is the number of such texts. These texts will not be released.\nTable 1: Planned GENEVAL data sets.\npurpose of this whole exercise is to see how well\ndifferent evaluation techniques correlate with each\nother, we plan to carry out a range of different\nevaluations, including the following.\nCorpus-based evaluations: We will develop\nnew, linguistically grounded evaluation metrics,\nand compare these to existing metrics including\nBLEU, NIST, and string-edit distance. We will also\ninvestigate how sensitive different metrics are to\nsize and make-up of the reference corpus.\nHuman-based preference judgements: We will\ninvestigate different experimental designs and\nmethods for overcoming respondent bias (e.g.\nwhat is known as \u2018central tendency bias\u2019, where\nsome respondents avoid judgements at either end\nof a scale). As we showed previously (Belz and\nReiter, 2006) that there are significant inter-subject\ndifferences in ratings, one thing we want to deter-\nmine is how many subjects are needed to get reli-\nable and reproducible results.\nTask performance. This depends on the do-\nmain, but e.g. in the nurse-report domain we\ncould use the methodology of (Law et al., 2005),\nwho showed medical professionals the texts, asked\nthem to make a treatment decision, and then rated\nthe correctness of the suggested treatments.\nAs well as recommendations about the appro-\npriateness of existing evaluation techniques, we\nhope the above experiments will allow us to sug-\ngest new evaluation techniques for NLG.\n4 Next Steps\nAt this point, we encourage NLG researchers to\ngive us their views regarding our plans for the or-\nganisation of GENEVAL, the data and evaluation\nmethods we are planning to use, to suggest addi-\ntional data sets or evaluation techniques, and espe-\ncially to let us know whether they would be inter-\nested in participating.\nIf our proposal is successful, we hope that the\nproject will start in summer 2007, with the first\ndata set released in late 2007 and the first work-\nshop in summer 2008. ELRA\/ELDA have also al-\nready agreed to help us with this work, contribut-\ning human and data resources.\nReferences\nSrinavas Bangalore, Owen Rambow, and Steve Whit-\ntaker. 2000. Evaluation metrics for generation. In\nProceedings of INLG-2000, pages 1\u20138.\nAnja Belz and Adam Kilgarriff. 2006. Shared-task\nevaluations in HLT: Lessons for NLG. In Proceed-\nings of INLG-2006.\nAnja Belz and Ehud Reiter. 2006. Comparing auto-\nmatic and human evaluation of NLG systems. In\nProceedings of EACL-2006, pages 313\u2013320.\nLynette Hirschman. 1998. The evolution of evaluation:\nLessons from the Message Understanding Confer-\nences. Computer Speech and Language, 12:283\u2013\n285.\nAnna Law, Yvonne Freer, Jim Hunter, Robert Logie,\nNeil McIntosh, and John Quinn. 2005. Generat-\ning textual summaries of graphical time series data\nto support medical decision making in the neonatal\nintensive care unit. Journal of Clinical Monitoring\nand Computing, 19:183\u2013194.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic\nevaluation of machine translation. In Proceedings\nof ACL-2002, pages 311\u2013318.\nEhud Reiter and Robert Dale. 2000. Building Natural\nLanguage Generation Systems. Cambridge Univer-\nsity Press.\nEhud Reiter and Somayajulu Sripada. 2002. Should\ncorpora texts be gold standards for NLG? In Pro-\nceedings of INLG-2002, pages 97\u2013104.\nEhud Reiter, Roma Robertson, and Liesl Osman. 2003.\nLessons from a failure: Generating tailored smoking\ncessation letters. Artificial Intelligence, 144:41\u201358.\nSomayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin\nYu. 2003. Exploiting a parallel text-data corpus. In\nProceedings of Corpus Linguistics 2003, pages 734\u2013\n743.\n"}