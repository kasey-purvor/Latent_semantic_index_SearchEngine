{"doi":"10.1017\/S0261444802001751","coreId":"71411","oai":"oai:eprints.lancs.ac.uk:1020","identifiers":["oai:eprints.lancs.ac.uk:1020","10.1017\/S0261444802001751"],"title":"State of the art review : language testing and assessment (part two).","authors":["Alderson, J. Charles","Banerjee, Jayanti"],"enrichments":{"references":[{"id":16366102,"title":"(2000).Assessing the communication skills of veterinary students: whose criteria? In","authors":[],"date":null,"doi":null,"raw":"DOUGLAS,D .& M YERS,R. (2000).Assessing the communication skills of veterinary students: whose criteria? In A. J. Kunnan (Ed.), Fairness and validation in language assessment (Studies in Language Testing Series, Vol. 9, pp. 60\u201381). Cambridge: UCLES\/Cambridge University Press.","cites":null},{"id":16366175,"title":"(2000).Authenticity in language testing: some outstanding questions.Language Testing,17(1),43\u201364.","authors":[],"date":null,"doi":"10.1177\/026553220001700102","raw":"LEWKOWICZ, J.A. (2000).Authenticity in language testing: some outstanding questions.Language Testing,17(1),43\u201364.","cites":null},{"id":16366303,"title":"(2000).Validation of holistic scoring for ESL writing assessment: how raters evaluate compositions. In","authors":[],"date":null,"doi":null,"raw":"SAKYI,A.A. (2000).Validation of holistic scoring for ESL writing assessment: how raters evaluate compositions. In A. J. Kunnan (Ed.), Fairness and validation in language assessment (Studies in Language Testing Series, Vol. 9, pp. 129\u201352). Cambridge: University of Cambridge Local Examinations Syndicate and Cambridge University Press.","cites":null},{"id":16366091,"title":"(2001).Speaking as a psycholinguistic process:the machine within. Paper presented at the LTRC\/AAAL Symposium, St Louis.","authors":[],"date":null,"doi":null,"raw":"DE BOT,K. (2001).Speaking as a psycholinguistic process:the machine within. Paper presented at the LTRC\/AAAL Symposium, St Louis.","cites":null},{"id":16366441,"title":"(2001).Task difficulty in testing spoken language:a socio-cognitive perspective. Paper presented at the Language Testing Research Colloquium,St Louis.","authors":[],"date":"2000","doi":null,"raw":"WEIR,C.,O\u2019SULLIVAN,B .& F RENCH,A. (2001).Task difficulty in testing spoken language:a socio-cognitive perspective. Paper presented at the Language Testing Research Colloquium,St Louis. WELLING-SLOOTMAEKERS,M. (1999).Language examinations in Dutch secondary schools from 2000 onwards. Levende Talen, 542,488\u201390.","cites":null},{"id":16365966,"title":"(2001).The impact of handwriting on the scoring of essays. Paper presented at the Association of Language Testers","authors":[],"date":null,"doi":null,"raw":"Language Testing,12(1),1\u201315. BROWN,A. (2001).The impact of handwriting on the scoring of essays. Paper presented at the Association of Language Testers in Europe (ALTE) Conference,Barcelona.","cites":null},{"id":16365897,"title":"(2001).Writing evaluation: what can analytic versus holistic essay scoring tell us?","authors":[],"date":"1991","doi":"10.1016\/s0346-251x(01)00025-2","raw":"BACHA, N. (2001).Writing evaluation: what can analytic versus holistic essay scoring tell us? System,29(3),371\u201384. BACHMAN,L.F. (1990).Fundamental considerations in language testing.Oxford:Oxford University Press. BACHMAN,L.F. (1991).What does language testing have to offer? TESOL Quarterly,25(4),671\u2013704.","cites":null},{"id":16366389,"title":"(2001b).Speaking as a cognitive tool. Paper presented at the LTRC\/AAAL Symposium,St Louis.","authors":[],"date":null,"doi":null,"raw":"SWAIN,M. (2001b).Speaking as a cognitive tool. Paper presented at the LTRC\/AAAL Symposium,St Louis.","cites":null},{"id":16366453,"title":"14) Amsterdam: John Benjamins Publishing Company, p.205\u201338. YOUNG,R.(1995).Conversational styles in language proficiency interviews.Language Learning,45(1),3\u201342.","authors":[],"date":null,"doi":"10.1111\/j.1467-1770.1995.tb00961.x","raw":"14) Amsterdam: John Benjamins Publishing Company, p.205\u201338. YOUNG,R.(1995).Conversational styles in language proficiency interviews.Language Learning,45(1),3\u201342.","cites":null},{"id":16366448,"title":"A comparison of assessment tasks used to measure FL reading comprehension.","authors":[],"date":"1993","doi":"10.2307\/329673","raw":"WOLF, D. F. (1993a). A comparison of assessment tasks used to measure FL reading comprehension. Modern Language Journal, 77(4),473\u201389. WOLF,D.F. (1993b).Issues in reading comprehension assessment: implications for the development of research instruments and classroom tests.Foreign Language Annals,26(3),322\u201331.","cites":null},{"id":16366380,"title":"A computer-assisted, computer-adaptive oral proficiency assessment instrument prototype. Poster presented at The Language Testing Research Colloquium,St Louis.","authors":[],"date":"2001","doi":null,"raw":"STAUFFER,S .&  K ENYON, D. M. (2001). A computer-assisted, computer-adaptive oral proficiency assessment instrument prototype. Poster presented at The Language Testing Research Colloquium,St Louis.","cites":null},{"id":16366204,"title":"A cross-cultural study of second language narrative discourse on an oral proficiency test.","authors":[],"date":"1998","doi":null,"raw":"MORTON, J. (1998). A cross-cultural study of second language narrative discourse on an oral proficiency test. Prospect, 13(2), 20\u201335.","cites":null},{"id":16366271,"title":"A framework for second language vocabulary assessment.Language Testing,18(1),1\u201332.","authors":[],"date":"2001","doi":"10.1177\/026553220101800101","raw":"READ,J .&  C HAPELLE, C. A. (2001). A framework for second language vocabulary assessment.Language Testing,18(1),1\u201332.","cites":null},{"id":16366397,"title":"A study of interrater reliability of the ACTFL Oral Proficiency Interview in five European languages:data from ESL,French,German,Russian and Spanish. Foreign Language Annals,28(3),407\u201322.","authors":[],"date":"1995","doi":"10.1111\/j.1944-9720.1995.tb00808.x","raw":"THOMPSON, I. (1995). A study of interrater reliability of the ACTFL Oral Proficiency Interview in five European languages:data from ESL,French,German,Russian and Spanish. Foreign Language Annals,28(3),407\u201322.","cites":null},{"id":16366200,"title":"A study of the decision-making behaviour of composition markers.","authors":[],"date":"1996","doi":null,"raw":"MILANOVIC, M., SAVILLE,N .&  S HEN, S. (1996). A study of the decision-making behaviour of composition markers. In M.Milanovic & N.Saville (Eds.),Performance testing,cognition and assessment.(92\u2013114) Cambridge:Cambridge University Press.","cites":null},{"id":16366141,"title":"A study of the effects of variation of short-term memory load, reading response length, and processing hierarchy on TOEFL listening comprehension item performance","authors":[],"date":"1991","doi":null,"raw":"HENNING, G. (1991). A study of the effects of variation of short-term memory load, reading response length, and processing hierarchy on TOEFL listening comprehension item performance (TOEFL Research Report 33). Princeton, New Jersey: Educational Testing Service.","cites":null},{"id":16366170,"title":"A vocabulary-size test of controlled productive ability.Language Testing,16(1),33\u201351.","authors":[],"date":"1999","doi":"10.1177\/026553229901600103","raw":"LAUFER,B . ,&  N ATION, P. (1999). A vocabulary-size test of controlled productive ability.Language Testing,16(1),33\u201351.","cites":null},{"id":16366295,"title":"Accommodative questions in oral proficiency interviews.Language Testing,9(2),173\u201387.","authors":[],"date":"1992","doi":"10.1177\/026553229200900205","raw":"ROSS, S. (1992). Accommodative questions in oral proficiency interviews.Language Testing,9(2),173\u201387.","cites":null},{"id":16365804,"title":"ACTFL Proficiency Guidelines. New York: American Council on the Teaching of Foreign Languages.","authors":[],"date":"1986","doi":"10.1111\/j.1540-4781.1967.tb06732.x","raw":"ACTFL (1986). ACTFL Proficiency Guidelines. New York: American Council on the Teaching of Foreign Languages.","cites":null},{"id":16366193,"title":"An alternative to multiple choice vocabulary tests.Language","authors":[],"date":"1987","doi":"10.1177\/026553228700400202","raw":"MEARA,P .&  B UXTON, B. (1987). An alternative to multiple choice vocabulary tests.Language Testing,4(2),142 \u201354.","cites":null},{"id":16365892,"title":"An exploratory study into the construct validity of a reading comprehension test: triangulation of data sources.","authors":[],"date":"1991","doi":"10.1177\/026553229100800104","raw":"ANDERSON,N .,B ACHMAN,L.,P ERKINS,K.& C OHEN,A. (1991). An exploratory study into the construct validity of a reading comprehension test: triangulation of data sources. Language Testing,8(1),41\u201366.","cites":null},{"id":16366167,"title":"An introduction to structural equation modelling for language assessment research.","authors":[],"date":"1998","doi":"10.1177\/026553229801500302","raw":"KUNNAN, A. J. (1998). An introduction to structural equation modelling for language assessment research. Language Testing, 15(3),295\u2013352.","cites":null},{"id":16366446,"title":"An investigation of planning time and proficiency level on oral test discourse.","authors":[],"date":"1997","doi":"10.1177\/026553229701400105","raw":"WIGGLESWORTH, G. (1997). An investigation of planning time and proficiency level on oral test discourse. Language Testing, 14(1),85\u2013106.","cites":null},{"id":16366195,"title":"An investigation of speaking test reliability with particular reference to examiner attitude to the speaking test format and candidate\/examiner discourse produced.In R.Tulloh (Ed.),IELTS Research Reports","authors":[],"date":"1999","doi":null,"raw":"MERRYLEES,B .&  M CDOWELL, C. (1999). An investigation of speaking test reliability with particular reference to examiner attitude to the speaking test format and candidate\/examiner discourse produced.In R.Tulloh (Ed.),IELTS Research Reports 1999 (Volume 2, 1\u201335). Canberra: IELTS Australia Pty Limited.","cites":null},{"id":16366255,"title":"analysis of the relationships between test takers\u2019 cognitive and metacognitive strategy use and second language test performance.","authors":[],"date":"1997","doi":"10.1111\/0023-8333.91997009","raw":"PURPURA, J. E. (1997).An analysis of the relationships between test takers\u2019 cognitive and metacognitive strategy use and second language test performance. Language Learning, 42(2), 289\u2013325.","cites":null},{"id":16365917,"title":"and ability analysis as a basis for examining content and construct comparability in two EFL proficiency test batteries.Language Testing,5(2),128\u201359.","authors":[],"date":"1988","doi":"10.1177\/026553228800500203","raw":"BACHMAN,L .F . ,K UNNAN,A . ,V ANNIARAJAN,S .&  L YNCH,B . (1988).Task and ability analysis as a basis for examining content and construct comparability in two EFL proficiency test batteries.Language Testing,5(2),128\u201359.","cites":null},{"id":16366209,"title":"and learning vocabulary.New York: Heinle and Heinle.","authors":[],"date":"1990","doi":null,"raw":"NATION,I.S.P. (1990).Teaching and learning vocabulary.New York: Heinle and Heinle.","cites":null},{"id":16365941,"title":"and validating the 2000 Word Level and University Word Level vocabulary tests.","authors":[],"date":"1999","doi":"10.1177\/026553229901600202","raw":"BEGLAR,D .& HUNT,A. (1999).Revising and validating the 2000 Word Level and University Word Level vocabulary tests.","cites":null},{"id":16366198,"title":"and washback in language testing.","authors":[],"date":"1996","doi":"10.1177\/026553229601300302","raw":"MESSICK, S. (1996).Validity and washback in language testing. Language Testing,13(3),241\u201356.","cites":null},{"id":16366138,"title":"Answering questions in LPIs: a case study, In","authors":[],"date":"1998","doi":"10.1075\/sibil.14.08he","raw":"HE, A.W. (1998). Answering questions in LPIs: a case study, In Young, R & He, A.W. (Eds.), Talking and testing: discourse approaches to the assessment of oral proficiency, Studies in Bilingualism (Vol.14) Amsterdam:John Benjamins Publishing Company,p.10\u201316.","cites":null},{"id":16366017,"title":"Applications in automated essay scoring and feedback. Paper presented at the Association of Language Testers","authors":[],"date":"2001","doi":null,"raw":"BURSTEIN,J .&  L EACOCK, C. (2001). Applications in automated essay scoring and feedback. Paper presented at the Association of Language Testers in Europe (ALTE) Conference,Barcelona.","cites":null},{"id":16365859,"title":"Applied linguistics and language testing: a case study.","authors":[],"date":"1992","doi":"10.1093\/applin\/13.2.149","raw":"ALDERSON,J .C .&  C LAPHAM, C. (1992). Applied linguistics and language testing: a case study. Applied Linguistics, 13(2), 149\u201367.","cites":null},{"id":16366161,"title":"Are instruments measuring aspects of language acquisition valid? Toepaste Taalwetenschap in Artikelen,56(1),35\u201345.","authors":[],"date":"1997","doi":null,"raw":"KLEIN GUNNEWIEK, L. (1997). Are instruments measuring aspects of language acquisition valid? Toepaste Taalwetenschap in Artikelen,56(1),35\u201345.","cites":null},{"id":16366218,"title":"Assessing achievement in distance learning.Prospect,6(2),58\u201366.","authors":[],"date":"1991","doi":null,"raw":"O\u2019LOUGHLIN, K. (1991). Assessing achievement in distance learning.Prospect,6(2),58\u201366.","cites":null},{"id":16366009,"title":"Assessing listening. Cambridge:","authors":[],"date":"2001","doi":"10.1017\/cbo9780511732959","raw":"BUCK, G. (2001). Assessing listening. Cambridge: Cambridge University Press.","cites":null},{"id":16365851,"title":"Assessing Reading. Cambridge:","authors":[],"date":"2000","doi":"10.1017\/cbo9780511732935","raw":"ALDERSON, J. C. (2000). Assessing Reading. Cambridge: Cambridge University Press.","cites":null},{"id":16366168,"title":"Assessing the assessments: the OPI and the SOPI.Foreign Language Annals,30(4),503\u201312. LAUFER,B. (1992).How much lexis is necessary for reading comprehension? In","authors":[],"date":"1997","doi":"10.1111\/j.1944-9720.1997.tb00857.x","raw":"KUO,J .&  J IANG, X. (1997). Assessing the assessments: the OPI and the SOPI.Foreign Language Annals,30(4),503\u201312. LAUFER,B. (1992).How much lexis is necessary for reading comprehension? In P. J. L.Arnaud & H. Bejoint (Eds.), Vocabulary and applied linguistics (pp.126\u201332).London:Macmillan. LAUFER,B.(1997).The lexical plight in second language reading: words you don\u2019t know,words you think you know,and words you can\u2019t guess.In J.Coady & T.Huckin (Eds.),Second language vocabulary acquisition (pp. 20\u201334). Cambridge: Cambridge University Press.","cites":null},{"id":16366164,"title":"Assessing writing abilities. Annual Review of Applied Linguistics,18,219\u201340. KUNNAN,A.J. (1992).An investigation of a criterion-referenced test using G-theory and factor and cluster analyses. Language Testing,9(1),30\u201349.","authors":[],"date":"1998","doi":"10.1177\/026553229200900104","raw":"KROLL, B. (1998). Assessing writing abilities. Annual Review of Applied Linguistics,18,219\u201340. KUNNAN,A.J. (1992).An investigation of a criterion-referenced test using G-theory and factor and cluster analyses. Language Testing,9(1),30\u201349.","cites":null},{"id":16366051,"title":"Assessment and testing. Annual Review of Applied Linguistics,20,147\u201361.","authors":[],"date":"2000","doi":"10.1017\/s0267190500200093","raw":"CLAPHAM, C. (2000). Assessment and testing. Annual Review of Applied Linguistics,20,147\u201361.","cites":null},{"id":16366106,"title":"assessment of an L2 listening comprehension construct: a tentative model for test specification and development.","authors":[],"date":"1993","doi":"10.2307\/328942","raw":"DUNKEL,P . ,H ENNING,G .&  C HAUDRON, C. (1993).The assessment of an L2 listening comprehension construct: a tentative model for test specification and development. Modern Language Journal,77(2),180\u201391.","cites":null},{"id":16366340,"title":"assessment of writing ability: expert readers versus lay readers. Language Testing,14(2),157\u201384.","authors":[],"date":"1997","doi":"10.1177\/026553229701400203","raw":"SCHOONEN,R . ,V ERGEER,M .&  E ITING, M. (1997).The assessment of writing ability: expert readers versus lay readers. Language Testing,14(2),157\u201384.","cites":null},{"id":16366414,"title":"assessment:what goes on in raters\u2019 minds? In L. Hamp-Lyons (Ed.), Assessing second language writing in academic contexts (pp. 111-126). Norwood, NJ:Ablex Publishing Co-orporation.","authors":[],"date":"1991","doi":null,"raw":"TESOL Quarterly,23(3),489\u2013508. VAUGHAN,C. (1991).Holistic assessment:what goes on in raters\u2019 minds? In L. Hamp-Lyons (Ed.), Assessing second language writing in academic contexts (pp. 111-126). Norwood, NJ:Ablex Publishing Co-orporation.","cites":null},{"id":16365896,"title":"assessments of foreign students\u2019","authors":[],"date":"1993","doi":"10.1177\/003368829302400104","raw":"Unpublished PhD,La Trobe University,Melbourne,Australia. ASTIKA, G. G. (1993).Analytical assessments of foreign students\u2019 writing.RELC Journal,24(1),61\u201372.","cites":null},{"id":16366120,"title":"Assesssment in English for Academic Purposes: putting content validity in its place.","authors":[],"date":"1999","doi":"10.1093\/applin\/20.2.221","raw":"FULCHER, G. (1999a). Assesssment in English for Academic Purposes: putting content validity in its place. Applied Linguistics,20(2),221\u201336.","cites":null},{"id":16366225,"title":"ATALLER,R. (2001).Spanish speaking test for elementary students: SOPI. Poster presented at the Language Testing Research Colloquium,St Louis.","authors":[],"date":null,"doi":null,"raw":"OSA-MELERO,L.& B ATALLER,R. (2001).Spanish speaking test for elementary students: SOPI. Poster presented at the Language Testing Research Colloquium,St Louis.","cites":null},{"id":16366223,"title":"Authentic assessment for English language learners.New York:Addison-Wesley.","authors":[],"date":"1996","doi":null,"raw":"O\u2019MALLEY,J .M .&  P IERCE, L.V. (1996). Authentic assessment for English language learners.New York:Addison-Wesley.","cites":null},{"id":16365895,"title":"Authenticity and validity in language testing: investigating the reading components of IELTS and TOEFL.","authors":[],"date":"1997","doi":null,"raw":"ANH,V.T. P. (1997). Authenticity and validity in language testing: investigating the reading components of IELTS and TOEFL.","cites":null},{"id":16365910,"title":"AVIDSON,F .,R YAN,K.& CHOI,I.-C.(1995).An investigation into the comparability of two tests of English as a foreign language. (Studies in Language Testing Series,Vol. 1). Cambridge: University of Cambridge Local Examinations Syndicate\/","authors":[],"date":null,"doi":null,"raw":"BACHMAN,L.F .,D AVIDSON,F .,R YAN,K.& CHOI,I.-C.(1995).An investigation into the comparability of two tests of English as a foreign language. (Studies in Language Testing Series,Vol. 1). Cambridge: University of Cambridge Local Examinations Syndicate\/ Cambridge University Press.","cites":null},{"id":16366021,"title":"bases of communicative approaches to second language teaching and testing. Applied Linguistics,1(1),1\u201347.","authors":[],"date":"1980","doi":"10.1093\/applin\/1.1.1","raw":"CANALE,M .&  S WAIN, M. (1980).Theoretical bases of communicative approaches to second language teaching and testing. Applied Linguistics,1(1),1\u201347.","cites":null},{"id":16366234,"title":"behavioural anchoring analysis of three ESL reading comprehension tests.","authors":[],"date":"1988","doi":"10.2307\/3587259","raw":"PERKINS,K.& B RUTTEN,S.R. (1988a).A behavioural anchoring analysis of three ESL reading comprehension tests. TESOL Quarterly,22(4),607\u201322.","cites":null},{"id":16365944,"title":"BERNHARDT,E.B.(1991).A psycholinguistic perspective on second language literacy. In","authors":[],"date":"1999","doi":null,"raw":"Language Testing,16(2),131\u201362. BERNHARDT,E.B.(1991).A psycholinguistic perspective on second language literacy. In J. H. Hulstijn & J. F. Matter (Eds.), Reading in two languages (Vol. 7, pp. 31\u201344).Amsterdam: Free University Press. BERNHARDT,E.B. (1999).If reading is reader-based,can there be a computer-adaptive test of reading? In M.Chalhoub-Deville (Ed.), Issues in computer-adaptive testing of reading proficiency. (Studies in Language Testing Series,Volume 10) Cambridge: UCLES-Cambridge University Press.","cites":null},{"id":16366401,"title":"Bottom-up or top-down processing as a discriminator of L2 listening performance. Applied Linguistics,19(4),432-451.","authors":[],"date":"1998","doi":"10.1093\/applin\/19.4.432","raw":"TSUI,A .B .M .&  F ULLILOVE, J. (1998). Bottom-up or top-down processing as a discriminator of L2 listening performance. Applied Linguistics,19(4),432-451.","cites":null},{"id":16366096,"title":"BRISAY,M.(1994).Problems in developing an alternative to the TOEFL.TESL Canada Journal,12(1),47\u201357.","authors":[],"date":null,"doi":null,"raw":"DES BRISAY,M.(1994).Problems in developing an alternative to the TOEFL.TESL Canada Journal,12(1),47\u201357.","cites":null},{"id":16366128,"title":"C-Test: state of the art (trans. from German).Zeitschrift fur Fremdsprachenforschung,6(2),37\u201360.","authors":[],"date":"1995","doi":null,"raw":"GROTJAHN, R. (1995).The C-Test: state of the art (trans. from German).Zeitschrift fur Fremdsprachenforschung,6(2),37\u201360.","cites":null},{"id":16366083,"title":"Cambridge-TOEFL comparability study:a example of the cross-national comparison of language tests.AILA Review,7,24\u201345.","authors":[],"date":"1990","doi":null,"raw":"DAVIDSON,F .& B ACHMAN,L.F. (1990).The Cambridge-TOEFL comparability study:a example of the cross-national comparison of language tests.AILA Review,7,24\u201345.","cites":null},{"id":16366427,"title":"Can a language interview be used to measure interactional skill? CALS Working Papers in TEFL,2,1\u201328.","authors":[],"date":"1999","doi":null,"raw":"WALSH, P. (1999). Can a language interview be used to measure interactional skill? CALS Working Papers in TEFL,2,1\u201328.","cites":null},{"id":16366159,"title":"CARPENTER,H.(2001).Effects of examinee control on examinee attitudes and performance on a computerized oral proficiency test. Paper presented at the Language Testing Research Colloquium,St Louis.","authors":[],"date":null,"doi":null,"raw":"KENYON,D .M.,MALABONGA,V.& CARPENTER,H.(2001).Effects of examinee control on examinee attitudes and performance on a computerized oral proficiency test. Paper presented at the Language Testing Research Colloquium,St Louis.","cites":null},{"id":16366040,"title":"CHAPELLE,C.A.(1994).Are C-tests valid measures for L2 vocabulary research? Second Language Research,10,157\u201387. CHAPELLE,C.A. (1998).Construct definition and validity inquiry in SLA research. In","authors":[],"date":null,"doi":"10.1017\/cbo9781139524711.004","raw":"CHAPELLE,C.A.(1994).Are C-tests valid measures for L2 vocabulary research? Second Language Research,10,157\u201387. CHAPELLE,C.A. (1998).Construct definition and validity inquiry in SLA research. In L. F. Bachman & A. D. Cohen (Eds.), Interfaces between second language acquisition and language testing research (32\u201370).Cambridge:Cambridge University Press.","cites":null},{"id":16366182,"title":"characteristics and rater bias: implications for training.","authors":[],"date":"1995","doi":"10.1177\/026553229501200104","raw":"LUMLEY,T.& MCNAMARA,T.F. (1995).Rater characteristics and rater bias: implications for training. Language Testing, 12(1), 54\u201371.","cites":null},{"id":16365867,"title":"Cognition and reading: cognitive levels as embodied in test questions. Reading in a Foreign Language,5(2),253\u201370.","authors":[],"date":"1989","doi":null,"raw":"ALDERSON,J .C .&  L UKMANI,Y. (1989). Cognition and reading: cognitive levels as embodied in test questions. Reading in a Foreign Language,5(2),253\u201370.","cites":null},{"id":16366464,"title":"College students\u2019 attitudes towards written versus oral tests of English as a Foreign Language.Language Testing,5(1),100\u201314. n Language testing and assessment","authors":[],"date":"1988","doi":"10.1177\/026553228800500107","raw":"ZEIDNER,M .&  B ENSOUSSAN, M. (1988). College students\u2019 attitudes towards written versus oral tests of English as a Foreign Language.Language Testing,5(1),100\u201314. n Language testing and assessment (Part 2)","cites":null},{"id":16366419,"title":"Coming to grips with lexical richness in spontaneous speech data.Language Testing,17(1),65\u201383.","authors":[],"date":"2000","doi":"10.1177\/026553220001700103","raw":"VERMEER,A. (2000). Coming to grips with lexical richness in spontaneous speech data.Language Testing,17(1),65\u201383.","cites":null},{"id":16366310,"title":"communicative approach to testing written English in non-native speakers. Rassegna Italiana di Linguistica Applicata,23(2),67\u201391.","authors":[],"date":"1991","doi":null,"raw":"SALVI, R. (1991).A communicative approach to testing written English in non-native speakers. Rassegna Italiana di Linguistica Applicata,23(2),67\u201391.","cites":null},{"id":16366148,"title":"communicative competence.In J.B.Pride &","authors":[],"date":"1972","doi":null,"raw":"HYMES,D. (1972).On communicative competence.In J.B.Pride & J. Holmes (Eds.), Sociolinguistics: Selected readings (267\u201393). Harmondsworth,Middlesex:Penguin.","cites":null},{"id":16366203,"title":"Communicative language testing: revolution or evolution? in Alderson,","authors":[],"date":"1979","doi":null,"raw":"MORROW, K. (1979). Communicative language testing: revolution or evolution? in Alderson, J.C & Hughes,A. (Eds.) Issues in Language Testing,ELT Documents 111,London:The British Council.","cites":null},{"id":16366206,"title":"Communicative syllabus design. Cambridge:","authors":[],"date":"1978","doi":"10.2307\/3586758","raw":"MUNBY, J. (1978). Communicative syllabus design. Cambridge: Cambridge University Press.","cites":null},{"id":16366139,"title":"comparison of listening and speaking tests for student placement. Edinburgh Working Papers in Applied Linguistics,6,27\u201340.","authors":[],"date":"1995","doi":null,"raw":"HELLER,A.,LYNCH,T.& WRIGHT,L. (1995).A comparison of listening and speaking tests for student placement. Edinburgh Working Papers in Applied Linguistics,6,27\u201340.","cites":null},{"id":16366322,"title":"comparison of two methods for detecting differential item functioning in an ESL placement test.","authors":[],"date":"1991","doi":"10.1177\/026553229100800201","raw":"SASAKI, M. (1991).A comparison of two methods for detecting differential item functioning in an ESL placement test. Language Testing,8(2),95\u2013111.","cites":null},{"id":16365879,"title":"Comprehension and text genre: an analysis of n Language testing and assessment","authors":[],"date":"1988","doi":"10.2307\/328239","raw":"ALLEN,E .D . ,B ERNHARDT,E .B . ,B ERRY,M .T .&  D EMEL, M. (1988). Comprehension and text genre: an analysis of n Language testing and assessment (Part 2) 105http:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9 secondary school foreign language readers. Modern Language Journal,72,163\u201372.","cites":null},{"id":16366121,"title":"Computerising an English language placement test.ELT Journal,53(4),289\u201399.","authors":[],"date":"1999","doi":"10.1093\/elt\/53.4.289","raw":"FULCHER, G. (1999b). Computerising an English language placement test.ELT Journal,53(4),289\u201399.","cites":null},{"id":16366160,"title":"Confirmation sequences as interactional resources in Korean language proficiency interviews,In","authors":[],"date":"1998","doi":"10.1075\/sibil.14.17kim","raw":"KIM,K .&  S UH, K. (1998). Confirmation sequences as interactional resources in Korean language proficiency interviews,In Young, R & He, A.W. (Eds.), Talking and testing: discourse approaches to the assessment of oral proficiency, Studies in Bilingualism (Vol.14) Amsterdam:John Benjamins Publishing Company,p.297\u2013332.","cites":null},{"id":16366404,"title":"Constructing rating scales for second language tests.","authors":[],"date":"1995","doi":"10.1093\/elt\/49.1.3","raw":"UPSHUR,J .&  T URNER, C. E. (1995). Constructing rating scales for second language tests. English Language Teaching Journal, 49(1),3\u201312.","cites":null},{"id":16365903,"title":"Content analysis and statistical modeling of EFL proficiency tests. Paper presented at the 11th Annual Language Testing Research Colloquium,San Antonio,Texas.","authors":[],"date":"1989","doi":null,"raw":"BACHMAN,L .F . ,D AVIDSON,F . ,L YNCH,B .&  R YAN, K. (1989). Content analysis and statistical modeling of EFL proficiency tests. Paper presented at the 11th Annual Language Testing Research Colloquium,San Antonio,Texas.","cites":null},{"id":16366230,"title":"Demystifying the TOEFL Reading Test.","authors":[],"date":"1992","doi":"10.2307\/3586868","raw":"PEIRCE, B. N. (1992). Demystifying the TOEFL Reading Test. TESOL Quarterly,26(4),665\u201389.","cites":null},{"id":16366027,"title":"Deriving oral assessment scales across different tests and rater groups.","authors":[],"date":"1995","doi":"10.1177\/026553229501200102","raw":"CHALHOUB-DEVILLE, M. (1995). Deriving oral assessment scales across different tests and rater groups. Language Testing, 12(1), 16\u201333.","cites":null},{"id":16366215,"title":"descriptors for language proficiency scales.Language","authors":[],"date":"1998","doi":"10.1177\/026553229801500204","raw":"NORTH,B .& S CHNEIDER,G. (1998).Scaling descriptors for language proficiency scales.Language Testing,15 (2),217\u201362.","cites":null},{"id":16366212,"title":"Designing second language performance assessments (Technical Report 18).Hawai\u2019i:University of","authors":[],"date":"1998","doi":null,"raw":"NORRIS,J . ,B ROWN,J .D . ,H UDSON,T .& Y OSHIOKA, J. (1998). Designing second language performance assessments (Technical Report 18).Hawai\u2019i:University of Hawai\u2019i Press.","cites":null},{"id":16366337,"title":"Developing and exploring the behaviour of two new versions of the Vocabulary Levels Test.Language Testing,18(1),55\u201388.","authors":[],"date":"2001","doi":"10.1177\/026553220101800103","raw":"SCHMITT,N . ,S CHMITT,D .&  C LAPHAM, C. (2001). Developing and exploring the behaviour of two new versions of the Vocabulary Levels Test.Language Testing,18(1),55\u201388.","cites":null},{"id":16366222,"title":"Developing homogeneous TOEFL scales by multidimensional scaling. Language Testing,7(1),1\u201312.","authors":[],"date":"1990","doi":"10.1177\/026553229000700102","raw":"OLTMAN,P .K .&  S TRICKER, L. J. (1990). Developing homogeneous TOEFL scales by multidimensional scaling. Language Testing,7(1),1\u201312.","cites":null},{"id":16366104,"title":"Developing listening prototypes using a corpus of spoken academic English. Paper presented at the Language Testing Research Colloquium,St.Louis.","authors":[],"date":"2001","doi":null,"raw":"DOUGLAS,D .&  N ISSAN, S. (2001). Developing listening prototypes using a corpus of spoken academic English. Paper presented at the Language Testing Research Colloquium,St.Louis.","cites":null},{"id":16366293,"title":"Developing practical speaking tests for the foreign language classroom: a small group approach. Foreign Language Annals,25(6),487\u201396.","authors":[],"date":"1992","doi":"10.1111\/j.1944-9720.1992.tb01132.x","raw":"ROBINSON, R. E. (1992). Developing practical speaking tests for the foreign language classroom: a small group approach. Foreign Language Annals,25(6),487\u201396.","cites":null},{"id":16366171,"title":"Developing small-scale standardised tests using an integrated approach.","authors":[],"date":"1991","doi":null,"raw":"LAURIER, M., & DES BRISAY, M. (1991). Developing small-scale standardised tests using an integrated approach. Bulletin of the CAAL,13(1),57\u201372. LAZARATON,A. (1992).The structural organisation of a language interview: a conversation-analytic perspective. System, 20(3), 373\u201386.","cites":null},{"id":16366370,"title":"development and validation of a simulated oral proficiency interview.","authors":[],"date":"1992","doi":"10.2307\/329767","raw":"STANSFIELD,C .W .&  K ENYON, D. M. (1992).The development and validation of a simulated oral proficiency interview. Modern Language Journal,76(2),129\u201341.","cites":null},{"id":16366326,"title":"Development of an analytic rating scale for Japanese L1 writing.","authors":[],"date":"1999","doi":"10.1177\/026553229901600403","raw":"SASAKI,M .&  H IROSE, K. (1999). Development of an analytic rating scale for Japanese L1 writing. Language Testing, 16(4), 457\u201378.","cites":null},{"id":16366142,"title":"Dimensionality and construct validity of language tests.Language Testing,9(1),1\u201311. Language testing and assessment","authors":[],"date":"1992","doi":"10.1177\/026553229200900102","raw":"HENNING, G. (1992a). Dimensionality and construct validity of language tests.Language Testing,9(1),1\u201311. Language testing and assessment (Part 2) n 108http:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9 HENNING, G. (1992b).The ACTFL Oral Proficiency Interview: validity evidence.System,20(3),365\u201372.","cites":null},{"id":16365953,"title":"dimensionality of a placement test from several analytical perspectives. Language Testing,12(1),72\u201398.","authors":[],"date":"1995","doi":"10.1177\/026553229501200105","raw":"BLAIS,J . - G .&  L AURIER, M. D. (1995).The dimensionality of a placement test from several analytical perspectives. Language Testing,12(1),72\u201398.","cites":null},{"id":16366192,"title":"dimensions of lexical competence.","authors":[],"date":"1996","doi":null,"raw":"MEARA, P. (1996).The dimensions of lexical competence. In G. Brown,K.Malmkjaer & J.Williams (Eds.),Performance and competence in second language acquisition. (35\u201353) Cambridge: Cambridge University Press.","cites":null},{"id":16366353,"title":"Discourse analysis in language testing. Annual Review of Applied Linguistics,11,115\u201331.","authors":[],"date":"1990","doi":"10.1017\/s0267190500001999","raw":"SHOHAMY, E. (1990a). Discourse analysis in language testing. Annual Review of Applied Linguistics,11,115\u201331.","cites":null},{"id":16366300,"title":"discourse of accommodation in oral proficiency interviews.","authors":[],"date":"1992","doi":"10.1017\/s0272263100010809","raw":"ROSS,S .&  B ERWICK, R. (1992).The discourse of accommodation in oral proficiency interviews. Studies in Second Language Acquisition,14,159\u201376.","cites":null},{"id":16366298,"title":"Divergent frame interpretations in oral proficiency interview interaction, In","authors":[],"date":"1998","doi":"10.1075\/sibil.14.18ros","raw":"ROSS, S. (1998). Divergent frame interpretations in oral proficiency interview interaction, In Young, R & He,A.W. (Eds.), Talking and testing: discourse approaches to the assessment of oral proficiency, Studies in Bilingualism (Vol. 14) Amsterdam: John Benjamins Publishing Company,p.333\u201353.","cites":null},{"id":16366228,"title":"Do different speech interactions yield different kinds of language? In","authors":[],"date":"1997","doi":null,"raw":"PAVLOU, P. (1997). Do different speech interactions yield different kinds of language? In A. Huhta,V. Kohonen, L. KurkiSuonio & S.Luoma (Eds.),Current developments and alternatives Language testing and assessment (Part 2) n 110http:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9 in language assessment. (pp. 185\u2013201) Jyv\u00e4skyla: University of Jyv\u00e4skyla.","cites":null},{"id":16365991,"title":"Do English and ESL faculties rate writing samples differently?","authors":[],"date":"1991","doi":"10.2307\/3587078","raw":"BROWN, J. D. (1991). Do English and ESL faculties rate writing samples differently? TESOL Quarterly,25(4),587\u2013603.","cites":null},{"id":16366156,"title":"Do\u2019s and don\u2019ts: recommendations for oral examiners of foreign languages.NovELTy,3(3),21\u201335.","authors":[],"date":"1996","doi":null,"raw":"KATONA, L. (1996). Do\u2019s and don\u2019ts: recommendations for oral examiners of foreign languages.NovELTy,3(3),21\u201335.","cites":null},{"id":16366114,"title":"Does the text matter in a multiple-choice test of comprehension? The case for the construct validity of TOEFL\u2019s minitalks.","authors":[],"date":"1999","doi":"10.1177\/026553229901600102","raw":"FREEDLE,R .&  K OSTIN, I. (1999). Does the text matter in a multiple-choice test of comprehension? The case for the construct validity of TOEFL\u2019s minitalks. Language Testing, 16(1), 2\u201332.","cites":null},{"id":16365914,"title":"E IGNOR,D.(1997).Recent advances in quantitative test analysis.","authors":[],"date":null,"doi":null,"raw":"BACHMAN,L.F .& E IGNOR,D.(1997).Recent advances in quantitative test analysis. In C. M. Clapham & D. Corson (Eds.), Language testing and assessment (Volume 7, pp. 227\u201342). Dordrecht,The Netherlands:Kluwer Academic Publishing.","cites":null},{"id":16366233,"title":"effect of passage topical structure types on ESL reading comprehension difficulty.","authors":[],"date":"1992","doi":"10.1177\/026553229200900204","raw":"PERKINS, K. (1992).The effect of passage topical structure types on ESL reading comprehension difficulty. Language Testing, 9(2),163\u201373.","cites":null},{"id":16366153,"title":"effect of prior knowledge on EAP listening-test performance.","authors":[],"date":"1995","doi":"10.1177\/026553229501200106","raw":"JENSEN,C .& H ANSEN,C. (1995).The effect of prior knowledge on EAP listening-test performance. Language Testing, 12(1), 99\u2013119.","cites":null},{"id":16366350,"title":"effect of question preview in listening comprehension tests.Language Testing,14(2),185\u2013213.","authors":[],"date":"1997","doi":"10.1177\/026553229701400204","raw":"SHERMAN, J. (1997).The effect of question preview in listening comprehension tests.Language Testing,14(2),185\u2013213.","cites":null},{"id":16366132,"title":"effects of note-taking on listening comprehension in the Test of English as a Foreign Language.Language Testing,11(1),29\u201347.","authors":[],"date":"1994","doi":"10.1177\/026553229401100104","raw":"HALE,G .A.& C OURTNEY,R. (1994).The effects of note-taking on listening comprehension in the Test of English as a Foreign Language.Language Testing,11(1),29\u201347.","cites":null},{"id":16366430,"title":"Effects of training on raters of ESL","authors":[],"date":"1994","doi":"10.1177\/026553229401100206","raw":"WEIGLE, S. C. (1994). Effects of training on raters of ESL compositions.Language Testing,11(2),197\u2013223.","cites":null},{"id":16366285,"title":"Einheitliche Pr\u00fcfungsanforderungen in der Abiturpr\u00fcfung Englisch? Eine Betrachtung nach einer","authors":[],"date":"1991","doi":null,"raw":"REINHARD, D. (1991). Einheitliche Pr\u00fcfungsanforderungen in der Abiturpr\u00fcfung Englisch? Eine Betrachtung nach einer Vergleichskorretur.Die Neueren Sprachen,90(6),624 \u201335.","cites":null},{"id":16366451,"title":"ELTOVICH,B.(1994).Learning to rate essays:A study of scorer cognition.Paper presented at the Annual Meeting of the American Educational Research Association,New Orleans,LA.","authors":[],"date":null,"doi":null,"raw":"WOLFE,E.& F ELTOVICH,B.(1994).Learning to rate essays:A study of scorer cognition.Paper presented at the Annual Meeting of the American Educational Research Association,New Orleans,LA.","cites":null},{"id":16365869,"title":"English language education in Hungary, Part II: Examining Hungarian learners\u2019achievements in English.Budapest:The British Council.","authors":[],"date":"2000","doi":null,"raw":"ALDERSON,J .C . ,N AGY,E .&  \u00d6 VEGES, E. (Eds.) (2000). English language education in Hungary, Part II: Examining Hungarian learners\u2019achievements in English.Budapest:The British Council.","cites":null},{"id":16366118,"title":"English language placement test:issues in reliability and validity.Language Testing,14(2),113\u201339.","authors":[],"date":"1997","doi":"10.1177\/026553229701400201","raw":"FULCHER,G. (1997a).An English language placement test:issues in reliability and validity.Language Testing,14(2),113\u201339.","cites":null},{"id":16366395,"title":"ESL writing assessment: subject-matter knowledge and its impact on performance. English for Specific Purposes,9(2),123\u201343.","authors":[],"date":"1990","doi":"10.1016\/0889-4906(90)90003-u","raw":"TEDICK, D. J. (1990). ESL writing assessment: subject-matter knowledge and its impact on performance. English for Specific Purposes,9(2),123\u201343.","cites":null},{"id":16366424,"title":"Evaluating a placement test.Language Testing,11(3),321\u201344.","authors":[],"date":"1994","doi":"10.1177\/026553229401100305","raw":"WALL,D . ,C LAPHAM,C .&  A LDERSON, J. C. (1994). Evaluating a placement test.Language Testing,11(3),321\u201344.","cites":null},{"id":16366211,"title":"Evaluating English oral skills through the technique of writing as if speaking.System,19(3),203\u201316.","authors":[],"date":"1991","doi":"10.1016\/0346-251x(91)90045-q","raw":"NORRIS, C. B. (1991). Evaluating English oral skills through the technique of writing as if speaking.System,19(3),203\u201316.","cites":null},{"id":16365927,"title":"evaluation of communicative language proficiency:a critique of the ACTFL oral interview.Modern Language Journal,70,380\u201390.","authors":[],"date":"1986","doi":"10.1111\/j.1540-4781.1986.tb05294.x","raw":"BACHMAN,L .F .&  S AVIGNON, S. (1986).The evaluation of communicative language proficiency:a critique of the ACTFL oral interview.Modern Language Journal,70,380\u201390.","cites":null},{"id":16366095,"title":"Examination of the relationships among TSE,","authors":[],"date":"1992","doi":"10.1177\/026553229200900203","raw":"DEMAURO, G. (1992). Examination of the relationships among TSE, TWE and TOEFL scores. Language Testing, 9(2), 149\u201361.","cites":null},{"id":16366387,"title":"Examining dialogue: another approach to content specification and to validating inferences drawn from test scores.Language Testing,18(3),275\u2013302.","authors":[],"date":"2001","doi":"10.1177\/026553220101800302","raw":"SWAIN, M. (2001a). Examining dialogue: another approach to content specification and to validating inferences drawn from test scores.Language Testing,18(3),275\u2013302.","cites":null},{"id":16365938,"title":"Examining the Yes\/No vocabulary test: some methodological issues in theory and practice. Language Testing,18(3),235\u201374.","authors":[],"date":"2001","doi":"10.1177\/026553220101800301","raw":"BEECKMANS,R . ,E YCKMANS,J . ,J ANSSENS,V .,DUFRANNE,M .& VAN DEVELDE, H. (2001). Examining the Yes\/No vocabulary test: some methodological issues in theory and practice. Language Testing,18(3),235\u201374.","cites":null},{"id":16366343,"title":"Examining validity in a performance test: the listening summary translation exam (LSTE).Language Testing,13,83\u2013109.","authors":[],"date":"1996","doi":"10.1177\/026553229601300106","raw":"SCOTT,M .L . ,S TANSFIELD,C .W .&  K ENYON, D. M. (1996). Examining validity in a performance test: the listening summary translation exam (LSTE).Language Testing,13,83\u2013109.","cites":null},{"id":16366077,"title":"Expertise in evaluating second-language compositions.Language Testing,7(1),31\u201351.","authors":[],"date":"1990","doi":"10.1177\/026553229000700104","raw":"CUMMING, A. (1990). Expertise in evaluating second-language compositions.Language Testing,7(1),31\u201351.","cites":null},{"id":16366444,"title":"Exploring bias analysis as a tool for improving rater consistency in assessing oral interaction. Language Testing,10(3),305\u201335.","authors":[],"date":"1993","doi":"10.1177\/026553229301000306","raw":"WIGGLESWORTH, G. (1993). Exploring bias analysis as a tool for improving rater consistency in assessing oral interaction. Language Testing,10(3),305\u201335.","cites":null},{"id":16366226,"title":"Exploring gender and oral proficiency interview performance.System,28(3),373\u201386. O\u2019SULLIVAN,B. (2000b).Towards a model of performance in oral language testing.","authors":[],"date":"2000","doi":"10.1016\/s0346-251x(00)00018-x","raw":"O\u2019SULLIVAN, B. (2000a). Exploring gender and oral proficiency interview performance.System,28(3),373\u201386. O\u2019SULLIVAN,B. (2000b).Towards a model of performance in oral language testing. Unpublished PhD dissertation, University of Reading,Reading.","cites":null},{"id":16366115,"title":"F REEMAN,D.E.(1992).Portfolio assessment for bilingual learners.Bilingual Basics,8.","authors":[],"date":null,"doi":null,"raw":"FREEMAN,Y .S.& F REEMAN,D.E.(1992).Portfolio assessment for bilingual learners.Bilingual Basics,8.","cites":null},{"id":16366248,"title":"factors in the assessment of oral interaction:gender and status.In S.Anivan (Ed.),Current developments in language testing","authors":[],"date":"1991","doi":null,"raw":"PORTER, D. (1991b).Affective factors in the assessment of oral interaction:gender and status.In S.Anivan (Ed.),Current developments in language testing (Vol. 25, pp. 92\u2013102). Singapore: SEAMEO Regional Language Centre.Anthology Series.","cites":null},{"id":16366411,"title":"fainting in coils: oral proficiency interviews as conversation.","authors":[],"date":"1989","doi":"10.2307\/3586922","raw":"VAN LIER,L. (1989).Reeling,writhing,drawling,stretching,and fainting in coils: oral proficiency interviews as conversation.","cites":null},{"id":16366391,"title":"fairness:a DIF analysis of an L2 vocabulary test.Language Testing,17(3),323\u201340.","authors":[],"date":"2000","doi":"10.1177\/026553220001700303","raw":"TAKALA,S .& K AFTANDJIEVA,F. (2000).Test fairness:a DIF analysis of an L2 vocabulary test.Language Testing,17(3),323\u201340.","cites":null},{"id":16366037,"title":"Field independence: a source of language test variance? Language Testing,5(1),62\u201382.","authors":[],"date":"1988","doi":"10.1177\/026553228800500105","raw":"CHAPELLE, C. (1988). Field independence: a source of language test variance? Language Testing,5(1),62\u201382.","cites":null},{"id":16366276,"title":"Fossilisation or evolution: the case of grammar testing. In","authors":[],"date":"2001","doi":null,"raw":"REA-DICKINS, P. (2001). Fossilisation or evolution: the case of grammar testing. In C. Elder, A. Brown, E. Grove, K. Hill, N. Iwashita T. Lumley. T. F. McNamara & K. O\u2019Loughlin (Eds.), Experimenting with uncertainty: essays in honour of Alan Davies (Studies in Language Testing Series,Vol.11,pp.22\u201332). Cambridge: University of Cambridge Local Examinations Syndicate and Cambridge University Press.","cites":null},{"id":16366201,"title":"Framing the language proficiency interview as a speech event:native and non-native speakers\u2019questions,In Young,R & He,A.W.(Eds.),Talking and testing: discourse approaches to the assessment of oral proficiency,","authors":[],"date":"1998","doi":"10.1075\/sibil.14.09mod","raw":"MODER,C . L .&  H ALLECK, G.B. (1998). Framing the language proficiency interview as a speech event:native and non-native speakers\u2019questions,In Young,R & He,A.W.(Eds.),Talking and testing: discourse approaches to the assessment of oral proficiency, Studies in Bilingualism (Vol. 14) Amsterdam: John Benjamins Publishing Company,p.117\u201346.","cites":null},{"id":16366093,"title":"G LAS,C.A.W.(1989).Validation of listening comprehension tests using item response theory.","authors":[],"date":null,"doi":"10.1177\/026553228700400204","raw":"DE JONG,J .& G LAS,C.A.W.(1989).Validation of listening comprehension tests using item response theory. Language Testing, 4(2),170\u201394.","cites":null},{"id":16366046,"title":"grammatical and textual features in L2 writing samples: the case of French as a foreign language.The Modern Language Journal,83(2),219\u201332. CHILD,J.R.(1987).Language proficiency levels and the typology of texts. In","authors":[],"date":"1999","doi":"10.1111\/0026-7902.00017","raw":"CHIANG, S.Y. (1999).Assessing grammatical and textual features in L2 writing samples: the case of French as a foreign language.The Modern Language Journal,83(2),219\u201332. CHILD,J.R.(1987).Language proficiency levels and the typology of texts. In H. Byrnes & M. Canale (Eds.), Defining and developing proficiency: Guidelines, implementations and concepts (pp.97\u2013106).Lincolnwood,IL:National Textbook Co.","cites":null},{"id":16366173,"title":"hierarchies of reading skills and text types.Modern Language Journal,72,173\u201387.","authors":[],"date":"1988","doi":"10.2307\/328240","raw":"LEE,J.F .& M USUMECI,D. (1988).On hierarchies of reading skills and text types.Modern Language Journal,72,173\u201387.","cites":null},{"id":16366435,"title":"Hypothesis testing in construct validation. In","authors":[],"date":"1995","doi":null,"raw":"WEIGLE,S .C .&  L YNCH, B. (1995). Hypothesis testing in construct validation. In A. Cumming & R. Berwick (Eds.), Language testing and assessment (Part 2) n 112http:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9 Validation in language testing (pp. 58-71). Clevedon, Avon: Multilingual Matters Ltd.","cites":null},{"id":16366437,"title":"Identifying the language problems of overseas students in tertiary education in the UK. Unpublished PhD dissertation,University of London,London.","authors":[],"date":"1983","doi":null,"raw":"WEIR, C. J. (1983). Identifying the language problems of overseas students in tertiary education in the UK. Unpublished PhD dissertation,University of London,London.","cites":null},{"id":16366220,"title":"impact of gender in the IELTS oral interview. In","authors":[],"date":"2000","doi":null,"raw":"O\u2019LOUGHLIN,K. (2000).The impact of gender in the IELTS oral interview. In R. Tulloh (Ed.), IELTS Research Reports 2000 (Volume 3,1\u201328).Canberra:IELTS Australia Pty Limited.","cites":null},{"id":16365987,"title":"Improving ESL placement test using two perspectives.TESOL Quarterly,23(1),65\u201383.","authors":[],"date":"1989","doi":"10.2307\/3587508","raw":"BROWN, J. D. (1989). Improving ESL placement test using two perspectives.TESOL Quarterly,23(1),65\u201383.","cites":null},{"id":16366124,"title":"influence of proficiency, language background, and topic on the production of grammatical form and error on the Test of Written English. In","authors":[],"date":"1997","doi":null,"raw":"GINTHER,A.& GRANT, L. (1997).The influence of proficiency, language background, and topic on the production of grammatical form and error on the Test of Written English. In A. Huhta, V. Kohonen, L. Kurki-Suonio & S. Luoma (Eds.), Current developments and alternatives in language assessment (pp. 385\u201397).Jyv\u00e4skyla:University of Jyv\u00e4skyla.","cites":null},{"id":16366140,"title":"influence of test and sample dimensionality on latent trait person ability and item difficulty calibrations.Language Testing,5(1),83\u201399. HENNING,G.","authors":[],"date":"1988","doi":"10.1177\/026553228800500106","raw":"HENNING, G. (1988).The influence of test and sample dimensionality on latent trait person ability and item difficulty calibrations.Language Testing,5(1),83\u201399. HENNING,G. (1989).Meanings and implications of the principle of local independence.Language Testing,6(1),95\u2013108.","cites":null},{"id":16366068,"title":"Interactive evaluation of listening comprehension: how the context may help.","authors":[],"date":"1998","doi":"10.1076\/call.11.1.35.5727","raw":"CONIAM, D. (1998). Interactive evaluation of listening comprehension: how the context may help. Computer Assisted Language Learning,11(1),35\u201353.","cites":null},{"id":16365899,"title":"Interfaces between second language acquisition and language testing research. Cambridge:","authors":[],"date":"1998","doi":"10.1017\/cbo9781139524711","raw":"BACHMAN,L .F .&  C OHEN,A. D. (1998). Interfaces between second language acquisition and language testing research. Cambridge: Cambridge University Press.","cites":null},{"id":16366172,"title":"Interlocutor support in oral proficiency interviews:the case of CASE.Language Testing,13(2),151\u201372.","authors":[],"date":"1996","doi":"10.1177\/026553229601300202","raw":"LAZARATON,A. (1996). Interlocutor support in oral proficiency interviews:the case of CASE.Language Testing,13(2),151\u201372.","cites":null},{"id":16366197,"title":"interplay of evidence and consequences in the validation of performance assessments.","authors":[],"date":"1989","doi":"10.3102\/0013189x023002013","raw":"MESSICK, S. (1989). Validity. In R. L. Linn (Ed.), Educational measurement.Third edition.(13\u2013103) New York:Macmillan. MESSICK,S. (1994).The interplay of evidence and consequences in the validation of performance assessments. Educational Researcher,23(2),13\u201323.","cites":null},{"id":16365968,"title":"Interviewer style and candidate performance in the IELTS oral interview.","authors":[],"date":"1998","doi":null,"raw":"BROWN,A .&  H ILL, K. (1998). Interviewer style and candidate performance in the IELTS oral interview. In S.Wood (Ed.), IELTS Research Reports 1998 (Volume 1, pp. 1\u201319). Sydney: ELICOS Association Ltd.","cites":null},{"id":16365984,"title":"Interviewer variability in specific-purpose language performance tests. In","authors":[],"date":"1997","doi":null,"raw":"BROWN,A.& LUMLEY,T. (1997). Interviewer variability in specific-purpose language performance tests. In A. Huhta, V. Kohonen,L.Kurki-Suonio & S.Luoma (Eds.),Current developments and alternatives in language assessment (137\u201350).Jyv\u00e4skyla: Centre for Applied Language Studies,University of Jyv\u00e4skyla.","cites":null},{"id":16366174,"title":"Investigating authenticity in language testing.","authors":[],"date":"1997","doi":"10.1177\/026553220001700102","raw":"LEWKOWICZ, J. A. (1997). Investigating authenticity in language testing. Unpublished PhD dissertation, Lancaster University, Lancaster.","cites":null},{"id":16366383,"title":"Investigating construct validity through test-taker introspection. Unpublished PhD dissertation,","authors":[],"date":"1994","doi":null,"raw":"STOREY, P. (1994). Investigating construct validity through test-taker introspection. Unpublished PhD dissertation, University of Reading,Reading. STOREY,P. (1997).Examining the test-taking process:a cognitive perspective on the discourse cloze test.Language Testing,14(2), 214\u201331.","cites":null},{"id":16365981,"title":"Investigating raters\u2019 orientations in specific-purpose taskbased oral assessment. Paper presented at the Language Testing Research Colloquium,St Louis.","authors":[],"date":"2001","doi":null,"raw":"BROWN, A., IWASHITA,N . ,M CNAMARA,T .F .&  O \u2019 H AGAN,S . (2001). Investigating raters\u2019 orientations in specific-purpose taskbased oral assessment. Paper presented at the Language Testing Research Colloquium,St Louis.","cites":null},{"id":16366258,"title":"Investigating the effects of strategy use and second language test performance with high- and lowability test takers: a structural equation modelling approach. Language Testing,15(3),333\u201379.","authors":[],"date":"1998","doi":"10.1177\/026553229801500303","raw":"PURPURA, J. E. (1998). Investigating the effects of strategy use and second language test performance with high- and lowability test takers: a structural equation modelling approach. Language Testing,15(3),333\u201379.","cites":null},{"id":16366237,"title":"investigation of patterns of discontinuous learning: implications for ESL measurement. Language Testing,13(1),63\u201382.","authors":[],"date":"1996","doi":"10.1177\/026553229601300105","raw":"PERKINS,K.& G ASS,S.M. (1996).An investigation of patterns of discontinuous learning: implications for ESL measurement. Language Testing,13(1),63\u201382.","cites":null},{"id":16366081,"title":"investigation of the construct validity of the ACTFL Proficiency guidelines and oral interview procedure.","authors":[],"date":"1990","doi":"10.1111\/j.1944-9720.1990.tb00330.x","raw":"DANDONOLI,P .&  H ENNING, G. (1990).An investigation of the construct validity of the ACTFL Proficiency guidelines and oral interview procedure. Foreign Language Annals, 23(1), 11\u201322. DANON-BOILEAU,L. (1997).Peut-on \u00e9valuer une acquisition du langage? Les Langues Modernes,2,15\u201323.","cites":null},{"id":16366176,"title":"is a test testing? An investigation of the agreement between students\u2019test-taking processes and test constructors\u2019presumptions. Unpublished M.A.,Lancaster University,Lancaster.","authors":[],"date":"1992","doi":null,"raw":"LI,W. (1992).What is a test testing? An investigation of the agreement between students\u2019test-taking processes and test constructors\u2019presumptions. Unpublished M.A.,Lancaster University,Lancaster.","cites":null},{"id":16366150,"title":"Is C-testing superior to cloze? Language Testing,12(2),194\u2013216.","authors":[],"date":"1995","doi":"10.1177\/026553229501200204","raw":"JAFARPUR, A. (1995). Is C-testing superior to cloze? Language Testing,12(2),194\u2013216.","cites":null},{"id":16366183,"title":"Is speaking performance assessment based mainly on grammar? Paper presented at the Language Testing Research Colloquium,St Louis.","authors":[],"date":"2001","doi":null,"raw":"LUMLEY,T .& QIAN, D. (2001). Is speaking performance assessment based mainly on grammar? Paper presented at the Language Testing Research Colloquium,St Louis.","cites":null},{"id":16366074,"title":"Issues in the evaluation of academic listening tests. Language Testing Update,","authors":[],"date":"1998","doi":null,"raw":"COOMBE,C . ,K INNEY,J .&  C ANNING, C. (1998). Issues in the evaluation of academic listening tests. Language Testing Update, 24,32\u201345.","cites":null},{"id":16366238,"title":"item difficulty in a reading comprehension test with an artificial neural network.Language Testing,12(1),34\u201353.","authors":[],"date":"1995","doi":"10.1177\/026553229501200103","raw":"PERKINS,K.,GUPTA,L.& T AMMANA,R. (1995).Predicting item difficulty in a reading comprehension test with an artificial neural network.Language Testing,12(1),34\u201353.","cites":null},{"id":16366235,"title":"item discriminability study of textually explicit, textually implicit, and scriptally implicit questions.RELC Journal,19(2),1\u201311.","authors":[],"date":"1988","doi":"10.1177\/003368828801900201","raw":"PERKINS,K.& B RUTTEN,S.R. (1988b).An item discriminability study of textually explicit, textually implicit, and scriptally implicit questions.RELC Journal,19(2),1\u201311.","cites":null},{"id":16366143,"title":"Item response theory and the assumption of unidimensionality for language tests.Language Testing,2(2),141\u201354.","authors":[],"date":"1985","doi":"10.1177\/026553228500200203","raw":"HENNING,G . ,H UDSON,T .& TURNER, J. (1985). Item response theory and the assumption of unidimensionality for language tests.Language Testing,2(2),141\u201354.","cites":null},{"id":16366187,"title":"Item Response Theory and the validation of an ESP test for health professionals.","authors":[],"date":"1990","doi":"10.1177\/026553229000700105","raw":"MCNAMARA,T. (1990). Item Response Theory and the validation of an ESP test for health professionals. Language Testing, 7(1),52\u201375. MCNAMARA,T.F. (1991).Test dimensionality:IRT analysis of an ESP listening test.Language Testing,8(2),139\u201359.","cites":null},{"id":16366378,"title":"job-relevant listening summary translation exam in Minnan. In","authors":[],"date":"2000","doi":null,"raw":"STANSFIELD,C .W . ,W U,W .M.&  VAN DER HEIDE, M. (2000).A job-relevant listening summary translation exam in Minnan. In A. J. Kunnan (Ed.), Fairness and validation in language assessment (Studies in Language Testing Series,Vol. 9, pp. 177\u2013200). Cambridge: University of Cambridge Local Examinations Syndicate and Cambridge University Press.","cites":null},{"id":16365948,"title":"K AMIL,M.L.(1995).Interpreting relationships between L1 and L2 reading: consolidating the linguistic threshold and the linguistic interdependence hypotheses.","authors":[],"date":null,"doi":"10.1093\/applin\/16.1.15","raw":"BERNHARDT,E.B .& K AMIL,M.L.(1995).Interpreting relationships between L1 and L2 reading: consolidating the linguistic threshold and the linguistic interdependence hypotheses.","cites":null},{"id":16366155,"title":"KAGA,M.(1991).Dictation as a measure of Japanese proficiency.","authors":[],"date":null,"doi":"10.1177\/026553229100800202","raw":"KAGA,M.(1991).Dictation as a measure of Japanese proficiency. Language Testing,8(2),112\u201324.","cites":null},{"id":16366158,"title":"KEMPE,V.& MACWHINNEY,B.(1996).The crosslinguistic assessment of foreign language vocabulary learning.","authors":[],"date":null,"doi":"10.1017\/s0142716400007621","raw":"KEMPE,V.& MACWHINNEY,B.(1996).The crosslinguistic assessment of foreign language vocabulary learning. Applied Psycholinguistics,17(2),149\u201383.","cites":null},{"id":16366416,"title":"knowledge of monolingual and bilingual children.","authors":[],"date":"1993","doi":"10.1093\/applin\/14.4.344","raw":"VERHALLEN,M.& S CHOONEN,R. (1993).Lexical knowledge of monolingual and bilingual children. Applied Linguistics, 14, 344\u201363.","cites":null},{"id":16365970,"title":"Language background and item difficulty: the development of a computer-adaptive test of Japanese.System,24(2),199\u2013206.","authors":[],"date":"1996","doi":"10.1016\/0346-251x(96)00004-8","raw":"BROWN,A .&  I WASHITA, N. (1996). Language background and item difficulty: the development of a computer-adaptive test of Japanese.System,24(2),199\u2013206.","cites":null},{"id":16366087,"title":"Language Teaching and Linguistics Abstracts, 11, 145\u201359 and 215\u201331. DAVIES,A. (1988).Operationalising uncertainty in language testing:an argument in favour of content validity.Language","authors":[],"date":null,"doi":"10.1177\/026553228800500103","raw":"Language Teaching and Linguistics Abstracts, 11, 145\u201359 and 215\u201331. DAVIES,A. (1988).Operationalising uncertainty in language testing:an argument in favour of content validity.Language Testing, 5(1),32\u201348.","cites":null},{"id":16365863,"title":"Language test construction and evaluation. Cambridge:","authors":[],"date":"1995","doi":null,"raw":"ALDERSON,J .C . ,C LAPHAM,C .&  W ALL, D. (1995). Language test construction and evaluation. Cambridge: Cambridge University Press.","cites":null},{"id":16365856,"title":"Language testing and assessment (Part 1).Language Teaching,34(4),213\u201336.","authors":[],"date":"2001","doi":"10.1017\/s0261444800014464","raw":"ALDERSON,J .C .&  B ANERJEE, J. (2001). Language testing and assessment (Part 1).Language Teaching,34(4),213\u201336.","cites":null},{"id":16365923,"title":"Language testing in practice.Oxford:Oxford","authors":[],"date":"1996","doi":null,"raw":"BACHMAN,L .F .&  P ALMER, A. S. (1996). Language testing in practice.Oxford:Oxford University Press.","cites":null},{"id":16366355,"title":"Language testing priorities: a different perspective.Foreign Language Annals,23(5),385\u201394.","authors":[],"date":"1990","doi":"10.1111\/j.1944-9720.1990.tb00392.x","raw":"SHOHAMY, E. (1990b). Language testing priorities: a different perspective.Foreign Language Annals,23(5),385\u201394.","cites":null},{"id":16366085,"title":"Language testing: survey articles 1 and 2.","authors":[],"date":"1978","doi":null,"raw":"DAVIES, A. (1978). Language testing: survey articles 1 and 2.","cites":null},{"id":16366422,"title":"Large-scale oral testing.","authors":[],"date":"1990","doi":"10.1093\/applin\/11.2.200","raw":"WALKER, C. (1990). Large-scale oral testing. Applied Linguistics, 11(2),200-219.","cites":null},{"id":16365931,"title":"latent variable aproach to listening and reading: testing factorial invariance across two groups of children in the Korean\/English Two-Way Immersion Program.Language Testing,15(3),380\u2013414.","authors":[],"date":"1998","doi":"10.1177\/026553229801500304","raw":"BAE,J.& B ACHMAN,L. (1998).A latent variable aproach to listening and reading: testing factorial invariance across two groups of children in the Korean\/English Two-Way Immersion Program.Language Testing,15(3),380\u2013414.","cites":null},{"id":16366260,"title":"Learner strategy use and performance on language tests:A structural equation modelling approach.(Studies in","authors":[],"date":"1999","doi":null,"raw":"PURPURA, J. E. (1999). Learner strategy use and performance on language tests:A structural equation modelling approach.(Studies in Language Testing Series, Vol. 8). Cambridge: University of Cambridge Local Examinations Syndicate\/ Cambridge University Press.","cites":null},{"id":16366459,"title":"Let them eat cake!\u201d or how to avoid losing your head in cross-cultural conversations, In","authors":[],"date":"1998","doi":"10.1075\/sibil.14.19you","raw":"YOUNG,R .&  H ALLECK, G. B. (1998). \u201cLet them eat cake!\u201d or how to avoid losing your head in cross-cultural conversations, In Young, R & He, A.W. (Eds.), Talking and testing: Discourse approaches to the assessment of oral proficiency Studies in Bilingualism (Vol. 14) Amsterdam: John Benjamins Publishing Company,p.355\u201382.","cites":null},{"id":16366219,"title":"Lexical density in candidate output on direct and semi-direct versions of an oral proficiency test. Language Testing,12(2),217\u201337.","authors":[],"date":"1995","doi":"10.1177\/026553229501200205","raw":"O\u2019LOUGHLIN, K. (1995). Lexical density in candidate output on direct and semi-direct versions of an oral proficiency test. Language Testing,12(2),217\u201337.","cites":null},{"id":16366178,"title":"LISKIN-GASPARRO,J.(2001).Speaking as proficiency. Paper presented at the LTRC\/AAAL Symposium,St Louis.","authors":[],"date":null,"doi":null,"raw":"LISKIN-GASPARRO,J.(2001).Speaking as proficiency. Paper presented at the LTRC\/AAAL Symposium,St Louis.","cites":null},{"id":16365961,"title":"listening abilities. Annual Review of Applied Linguistics,18,171\u201391.","authors":[],"date":"1998","doi":"10.1017\/s0267190500003536","raw":"BRINDLEY, G. (1998).Assessing listening abilities. Annual Review of Applied Linguistics,18,171\u201391.","cites":null},{"id":16366001,"title":"Listening comprehension: construct validity and trait characteristics.Language Learning,42(3),313\u201357. BUCK,G. (1994).The appropriacy of psychometric measurement models for testing second language listening comprehension.","authors":[],"date":"1992","doi":"10.1111\/j.1467-1770.1992.tb01339.x","raw":"BUCK, G. (1992b). Listening comprehension: construct validity and trait characteristics.Language Learning,42(3),313\u201357. BUCK,G. (1994).The appropriacy of psychometric measurement models for testing second language listening comprehension.","cites":null},{"id":16366373,"title":"Listening Summary Translation Exam (LSTE)","authors":[],"date":"1997","doi":"10.1177\/026553229601300106","raw":"STANSFIELD,C .W . ,W U,W .M .&  L IU, C. C. (1997). Listening Summary Translation Exam (LSTE) in Taiwanese, aka Minnan (Final Project Report. ERIC Document Reproduction Service, ED 413 788). N. Bethesda, MD: Second Language Testing,Inc.","cites":null},{"id":16366385,"title":"Literacy in theory and practice. Cambridge:","authors":[],"date":"1984","doi":"10.1017\/s0142716400007402","raw":"STREET, B.V. (1984). Literacy in theory and practice. Cambridge: Cambridge University Press.","cites":null},{"id":16366189,"title":"LUMLEY,T.(1997).The effect of interlocutor and assessment mode variables in overseas assessments of speaking skills in occupational settings.Language Testing,14(2),","authors":[],"date":null,"doi":"10.1177\/026553229701400202","raw":"MCNAMARA,T .F .& LUMLEY,T.(1997).The effect of interlocutor and assessment mode variables in overseas assessments of speaking skills in occupational settings.Language Testing,14(2), 140\u201356.","cites":null},{"id":16366089,"title":"Maintaining American face in the Korean oral exam: reflections on the power of cross-cultural context, In","authors":[],"date":"1998","doi":"10.1075\/sibil.14.16dav","raw":"DAVIES, C. E. (1998). Maintaining American face in the Korean oral exam: reflections on the power of cross-cultural context, In Young, R & He, A.W. (Eds.), Talking and testing: discourse approaches to the assessment of oral proficiency, Studies in Bilingualism (Vol.14) Amsterdam:John Benjamins Publishing Company,p.271\u201396.","cites":null},{"id":16366157,"title":"Meaning negotiation in the Hungarian oral proficiency examination of English, In Young, R & He,A.W. (Eds.),Talking and testing:discourse approaches to the assessment of oral proficiency,","authors":[],"date":"1998","doi":"10.1075\/sibil.14.14kat","raw":"KATONA, L. (1998). Meaning negotiation in the Hungarian oral proficiency examination of English, In Young, R & He,A.W. (Eds.),Talking and testing:discourse approaches to the assessment of oral proficiency, Studies in Bilingualism (Vol. 14) Amsterdam: John Benjamins Publishing Company,p.239\u201367.","cites":null},{"id":16366365,"title":"Measured words.","authors":[],"date":"1995","doi":"10.1177\/026553229601300108","raw":"SPOLSKY, B. (1995). Measured words. Oxford: Oxford University Press.","cites":null},{"id":16366109,"title":"Miscommunication in language proficiency interviews of first-year German students:a comparison with natural conversation, In","authors":[],"date":"1998","doi":"10.1075\/sibil.14.10egb","raw":"EGBERT, M. M. (1998). Miscommunication in language proficiency interviews of first-year German students:a comparison with natural conversation, In Young, R & He, A.W. (Eds.), Talking and testing: discourse approaches to the assessment of oral proficiency, Studies in Bilingualism (Vol. 14) Amsterdam: John Benjamins Publishing Company,p.147\u201369.","cites":null},{"id":16366188,"title":"Modelling performance: opening Pandora\u2019s Box.Applied Linguistics,16(2),159\u201375.","authors":[],"date":"1995","doi":"10.1093\/applin\/16.2.159","raw":"MCNAMARA, T. F. (1995). Modelling performance: opening Pandora\u2019s Box.Applied Linguistics,16(2),159\u201375.","cites":null},{"id":16366165,"title":"Modelling relationships among some test-taker characteristics and performance on EFL tests: an approach to construct validation.","authors":[],"date":"1994","doi":"10.1177\/026553229401100301","raw":"KUNNAN, A. J. (1994). Modelling relationships among some test-taker characteristics and performance on EFL tests: an approach to construct validation. Language Testing, 11(3), 225\u201352.","cites":null},{"id":16366030,"title":"models, assessment frameworks and test construction.","authors":[],"date":"1997","doi":"10.1177\/026553229701400102","raw":"CHALHOUB-DEVILLE, M. (1997).Theoretical models, assessment frameworks and test construction. Language Testing, 14(1), 3\u201322.","cites":null},{"id":16366024,"title":"Modified oral proficiency interview: Its purpose, development and description. In","authors":[],"date":"1997","doi":null,"raw":"CASCALLAR, M. I. (1997). Modified oral proficiency interview: Its purpose, development and description. In A. Huhta, V. Kohonen, L. Kurki-Suonio & S. Luoma (Eds.), Current developments and alternatives in language assessment (pp.485\u201394). Jyv\u00e4skyla:University of Jyv\u00e4skyla.","cites":null},{"id":16366098,"title":"Modified scoring, traditional item analysis and Sato\u2019s caution index used to investigate the reading recall protocol.","authors":[],"date":"1993","doi":"10.1177\/026553229301000202","raw":"DEVILLE,C .&  C HALHOUB-DEVILLE, M. (1993). Modified scoring, traditional item analysis and Sato\u2019s caution index used to investigate the reading recall protocol. Language Testing, 10(2), 117\u201332.","cites":null},{"id":16366100,"title":"n Language testing and assessment (Part 2) 107http:\/\/journals.cambridge.org Downloaded:","authors":[],"date":"2009","doi":null,"raw":"n Language testing and assessment (Part 2) 107http:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9 DOUGLAS, D. (1994). Quantity and quality in speaking test performance.Language Testing,11(2),125\u201344.","cites":null},{"id":16366329,"title":"n Language testing and assessment (Part 2) 111http:\/\/journals.cambridge.org Downloaded: 26","authors":[],"date":"2009","doi":null,"raw":"n Language testing and assessment (Part 2) 111http:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9 SAVILLE,N .&  H ARGREAVES, P. (1999).Assessing speaking in the revised FCE.English Language Teaching Journal,53(1),42\u201351.","cites":null},{"id":16366190,"title":"Network structures and vocabulary acquisition in a foreign language. In","authors":[],"date":"1992","doi":"10.1075\/lllt.24","raw":"MEARA, P. (1992). Network structures and vocabulary acquisition in a foreign language. In P. J. L. Arnaud & H. Bejoint (Eds.), Vocabulary and applied linguistics (pp. 62\u201370). London: Macmillan.","cites":null},{"id":16365950,"title":"New exams in secondary education, new question types.An investigation into the reliability of the evaluation of open-ended questions in foreign-language exams.Levende Talen,537,173\u201381.","authors":[],"date":"1999","doi":null,"raw":"BHGEL,K .&  L EIJN, M. (1999). New exams in secondary education, new question types.An investigation into the reliability of the evaluation of open-ended questions in foreign-language exams.Levende Talen,537,173\u201381.","cites":null},{"id":16365808,"title":"New procedures for validating proficiency tests of ESP? Theory and practice.","authors":[],"date":"1988","doi":"10.1177\/026553228800500207","raw":"ALDERSON, J. C. (1988). New procedures for validating proficiency tests of ESP? Theory and practice. Language Testing, 5(2),220\u201332.","cites":null},{"id":16366216,"title":"NURWENI,A.& READ,J.(1999).The English vocabulary knowledge of Indonesian university students. English for Specific Purposes,18,161\u201375.","authors":[],"date":null,"doi":"10.1016\/s0889-4906(98)00005-2","raw":"NURWENI,A.& READ,J.(1999).The English vocabulary knowledge of Indonesian university students. English for Specific Purposes,18,161\u201375.","cites":null},{"id":16365884,"title":"of English as a Foreign Language and First Certificate of English tests as predictors of academic success for undergraduate students at the University of Bahrain.System,27(3),389\u201399.","authors":[],"date":"1999","doi":"10.1016\/s0346-251x(99)00033-0","raw":"AL-MUSAWI,N.M.& A L-ANSARI,S.H. (1999).Test of English as a Foreign Language and First Certificate of English tests as predictors of academic success for undergraduate students at the University of Bahrain.System,27(3),389\u201399.","cites":null},{"id":16366207,"title":"of oral skills: a comparison of scores obtained through audio recordings to those obtained through face-to-face evaluation.","authors":[],"date":"1993","doi":"10.1177\/003368829302400102","raw":"NAMBIAR,M .K .&  G OON, C. (1993).Assessment of oral skills: a comparison of scores obtained through audio recordings to those obtained through face-to-face evaluation. RELC Journal,24(1),15\u201331.","cites":null},{"id":16366011,"title":"of the rule-space procedure to language testing: examining attributes of a free response listening test.Language Testing,15(2),119\u201357.","authors":[],"date":"1998","doi":"10.1177\/026553229801500201","raw":"BUCK,G .&  T ATSUOKA, K. (1998).Application of the rule-space procedure to language testing: examining attributes of a free response listening test.Language Testing,15(2),119\u201357.","cites":null},{"id":16366362,"title":"Oral Examinations: an historical note.","authors":[],"date":"1990","doi":"10.1177\/026553229000700203","raw":"SPOLSKY, B. (1990). Oral Examinations: an historical note. Language Testing,7(2),158\u201373.","cites":null},{"id":16366134,"title":"Oral Proficiency Interview:discrete point test or a measure of communicative language ability? Foreign Language Annals,25(3),227\u201331.","authors":[],"date":"1992","doi":"10.1111\/j.1944-9720.1992.tb00532.x","raw":"HALLECK,G.B. (1992).The Oral Proficiency Interview:discrete point test or a measure of communicative language ability? Foreign Language Annals,25(3),227\u201331.","cites":null},{"id":16366177,"title":"Oral tests in Swedish schools: a five-year experiment.System,20(3),279\u201392.","authors":[],"date":"1992","doi":"10.1016\/0346-251x(92)90040-a","raw":"LINDBLAD,T. (1992). Oral tests in Swedish schools: a five-year experiment.System,20(3),279\u201392.","cites":null},{"id":16366111,"title":"Overcoming gender bias in oral testing: the effect of introducing candidates.System,22(3),341\u201348.","authors":[],"date":"1994","doi":"10.1016\/0346-251x(94)90019-1","raw":"FERGUSON, B. (1994). Overcoming gender bias in oral testing: the effect of introducing candidates.System,22(3),341\u201348.","cites":null},{"id":16366180,"title":"Perceptions of language-trained raters and occupational experts in a test of occupational English language proficiency. English for Specific Purposes,","authors":[],"date":"1998","doi":"10.1016\/s0889-4906(97)00016-1","raw":"LUMLEY, T. (1998). Perceptions of language-trained raters and occupational experts in a test of occupational English language proficiency. English for Specific Purposes, 17(4), 347\u201367.","cites":null},{"id":16366247,"title":"PORTER,D.(1991a).Affective factors in language testing.In","authors":[],"date":null,"doi":null,"raw":"PORTER,D.(1991a).Affective factors in language testing.In J.C. Alderson & B. North (Eds.), Language testing in the 1990s: the communicative legacy (pp.32\u201340).London:Macmillan (Modern English Publications in association with the British Council).","cites":null},{"id":16366242,"title":"Portfolio assessment for language minority students.Washington D.C.:National Clearinghouse for Bilingual Education.","authors":[],"date":"1992","doi":null,"raw":"PIERCE,L .V .&  O \u2019 M ALLEY, J. M. (1992). Portfolio assessment for language minority students.Washington D.C.:National Clearinghouse for Bilingual Education.","cites":null},{"id":16366282,"title":"Probing above the ceiling in oral interviews: what\u2019s up there? In","authors":[],"date":"1997","doi":null,"raw":"REED,D .J .&  H ALLECK, G. B. (1997). Probing above the ceiling in oral interviews: what\u2019s up there? In A. Huhta,V. Kohonen, L. Kurki-Suonio & S. Luoma (Eds.), Current developments and alternatives in language assessment. (pp. 225\u201338) Jyv\u00e4skyla: University of Jyv\u00e4skyla.","cites":null},{"id":16366126,"title":"Prochievement REM? testing of speaking.Foreign Language Annals,22(5),487\u201396.","authors":[],"date":"1989","doi":"10.1111\/j.1944-9720.1989.tb02772.x","raw":"GONZALEZ PINO, B. (1989). Prochievement REM? testing of speaking.Foreign Language Annals,22(5),487\u201396.","cites":null},{"id":16365934,"title":"Qualitative approaches to test validation.In C.Clapham & D.Corson (Eds.),Language testing and assessment","authors":[],"date":"1997","doi":null,"raw":"BANERJEE,J .&  L UOMA, S. (1997). Qualitative approaches to test validation.In C.Clapham & D.Corson (Eds.),Language testing and assessment (Vol. 7, pp. 275\u201387). Dordrecht, The Netherlands:Kluwer Academic Publishers. BARNWELL,D.(1989).\u2018Naive\u2019native speakers and judgements of oral proficiency in Spanish.Language Testing,6(2),152\u201363.","cites":null},{"id":16366408,"title":"questions:answers in the foreign language? Toegepaste Taalwetenschap in","authors":[],"date":"1998","doi":"10.1075\/ttwia.58.19elm","raw":"VAN ELMPT,M.& LOONEN,P. (1998).Open questions:answers in the foreign language? Toegepaste Taalwetenschap in Artikelen,58, 149-\u201354.","cites":null},{"id":16366154,"title":"Re-analyzing the OPI: how much does it look like natural conversation? In","authors":[],"date":"1998","doi":"10.1075\/sibil.14.04joh","raw":"JOHNSON,M  &  T YLER,A. (1998). Re-analyzing the OPI: how much does it look like natural conversation? In Young, R & He, A.W. (Eds.), Talking and testing: discourse approaches to the assessment of oral proficiency, Studies in Bilingualism (Vol. 14) Amsterdam:John Benjamins Publishing Company,p.27\u201351.","cites":null},{"id":16365958,"title":"reactions to a placement test. Language Testing,7(1),13\u201330.","authors":[],"date":"1990","doi":"10.1177\/026553229000700103","raw":"BRADSHAW, J. (1990).Test-takers\u2019 reactions to a placement test. Language Testing,7(1),13\u201330.","cites":null},{"id":16366231,"title":"reading comprehension in LSP:does topic familiarity affect assessed difficulty and actual performance? Reading in a Foreign Language,","authors":[],"date":"1990","doi":null,"raw":"PERETZ,A .S .&  S HOHAM, M. (1990).Testing reading comprehension in LSP:does topic familiarity affect assessed difficulty and actual performance? Reading in a Foreign Language, 7(1), 447\u201355.","cites":null},{"id":16365812,"title":"reading comprehension skills (Part One).Reading in a Foreign Language,6(2),425\u201338.","authors":[],"date":"1990","doi":null,"raw":"ALDERSON, J. C. (1990a).Testing reading comprehension skills (Part One).Reading in a Foreign Language,6(2),425\u201338.","cites":null},{"id":16365817,"title":"reading comprehension skills (Part Two).Reading in a Foreign Language,7(1),465\u2013503.","authors":[],"date":"1990","doi":null,"raw":"ALDERSON, J. C. (1990b).Testing reading comprehension skills (Part Two).Reading in a Foreign Language,7(1),465\u2013503.","cites":null},{"id":16366308,"title":"Reading expository prose at the post-secondary level: the influence of textual variables on L2 reading comprehension (a genre-based approach).Reading in a Foreign Language,8(1),645\u201362.","authors":[],"date":"1991","doi":null,"raw":"SALAGER-MEYER, F. (1991). Reading expository prose at the post-secondary level: the influence of textual variables on L2 reading comprehension (a genre-based approach).Reading in a Foreign Language,8(1),645\u201362.","cites":null},{"id":16366061,"title":"Reading in English and Spanish: evidence from adult ESL students.Language Learning,29,121\u201350.","authors":[],"date":"1979","doi":"10.1111\/j.1467-1770.1979.tb01055.x","raw":"CLARKE, M. (1979). Reading in English and Spanish: evidence from adult ESL students.Language Learning,29,121\u201350.","cites":null},{"id":16366108,"title":"Reading proficiency assessment and the ILR\/ACTFL text typology: a reevaluation. The Modern Language Journal,80(3),350\u201361.","authors":[],"date":"1996","doi":"10.2307\/329441","raw":"EDWARDS,A. L. (1996). Reading proficiency assessment and the ILR\/ACTFL text typology: a reevaluation. The Modern Language Journal,80(3),350\u201361.","cites":null},{"id":16366263,"title":"Reflections on research and assessment in written composition.","authors":[],"date":"1992","doi":null,"raw":"PURVES,A. C. (1992). Reflections on research and assessment in written composition. Research in the Teaching of English, 26(1), 108\u201322. RAFFALDINI,T. (1988).The use of situation tests as measures of communicative ability. Studies in Second Language Acquisition, 10(2),197\u2013216.","cites":null},{"id":16365821,"title":"relationship between grammar and reading in an English for academic purposes test battery.","authors":[],"date":"1993","doi":null,"raw":"ALDERSON, J. C. (1993).The relationship between grammar and reading in an English for academic purposes test battery. In D.Douglas & C.Chapelle (Eds.),A new decade of language testing research: Selected papers from the 1990 Language Testing Research Colloquium (203\u201319).Alexandria,Va:TESOL.","cites":null},{"id":16366334,"title":"relationship between TOEFL vocabulary items and meaning, association, collocation and wordclass knowledge.Language Testing,16,189\u2013216.","authors":[],"date":"1999","doi":"10.1177\/026553229901600204","raw":"SCHMITT, N. (1999).The relationship between TOEFL vocabulary items and meaning, association, collocation and wordclass knowledge.Language Testing,16,189\u2013216.","cites":null},{"id":16366147,"title":"Relationships among IRT item discrimination amd item fit indices in criterion-referenced language testing.Language Testing,8(2),160\u201381. HUDSON,T. (1993).Surrogate indices for item information functions in criterion-referenced lanuage testing.","authors":[],"date":"1991","doi":"10.1177\/026553229100800205","raw":"HUDSON,T. (1991). Relationships among IRT item discrimination amd item fit indices in criterion-referenced language testing.Language Testing,8(2),160\u201381. HUDSON,T. (1993).Surrogate indices for item information functions in criterion-referenced lanuage testing. Language Testing, 10(2),171\u201391.","cites":null},{"id":16365994,"title":"relative importance of persons, items, subtests and languages to TOEFL test variance. Language Testing,16(2),217\u201338. Language testing and assessment","authors":[],"date":"1999","doi":"10.1177\/026553229901600205","raw":"BROWN, J. D. (1999).The relative importance of persons, items, subtests and languages to TOEFL test variance. Language Testing,16(2),217\u201338. Language testing and assessment (Part 2) n 106http:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9 BUCK,G. (1990).The testing of second language listening comprehension. Unpublished PhD dissertation, Lancaster University, Lancaster.","cites":null},{"id":16366112,"title":"Relaxing in pairs.","authors":[],"date":"1999","doi":"10.1093\/elt\/53.1.36","raw":"FOOT, M. (1999). Relaxing in pairs. English Language Teaching Journal,53(1),36\u201341.","cites":null},{"id":16366243,"title":"Research and development \u2013 a complex relationship.Language Testing Update,24,46\u201359.","authors":[],"date":"1998","doi":null,"raw":"POLLARD, J. (1998). Research and development \u2013 a complex relationship.Language Testing Update,24,46\u201359.","cites":null},{"id":16366076,"title":"Research Report 1(i) The ELTS validation project report. London\/ Cambridge: The British Council\/ University of Cambridge Local Examinations Syndicate.","authors":[],"date":"1988","doi":null,"raw":"CRIPER,C .&  D AVIES,A. (1988). Research Report 1(i) The ELTS validation project report. London\/ Cambridge: The British Council\/ University of Cambridge Local Examinations Syndicate.","cites":null},{"id":16366287,"title":"REVES,T.& LEVINE,A.(1992).From needs analysis to criterionreferenced testing.System,20(2),201\u201310.","authors":[],"date":null,"doi":"10.1016\/0346-251x(92)90025-x","raw":"REVES,T.& LEVINE,A.(1992).From needs analysis to criterionreferenced testing.System,20(2),201\u201310.","cites":null},{"id":16366393,"title":"Revising instruments for rating speaking: combining qualitative and quantitative insights. Paper presented at the Language Testing Research Colloquium, St Louis.","authors":[],"date":"2001","doi":null,"raw":"TAYLOR,L .&  J ONES, N. (2001). Revising instruments for rating speaking: combining qualitative and quantitative insights. Paper presented at the Language Testing Research Colloquium, St Louis.","cites":null},{"id":16366305,"title":"Revising the revised format of the ACTFL Oral Proficiency Interview.","authors":[],"date":"2000","doi":"10.1177\/026553220001700301","raw":"SALABERRY, R. (2000). Revising the revised format of the ACTFL Oral Proficiency Interview. Language Testing, 17(3), 289\u2013310.","cites":null},{"id":16366281,"title":"Revisiting raters and ratings in oral language assessment. In","authors":[],"date":"2001","doi":null,"raw":"REED,D .J .&  C OHEN,A. D. (2001). Revisiting raters and ratings in oral language assessment. In C. Elder,A. Brown, E. Grove, K.Hill,N.Iwashita,T.Lumley,T.F.McNamara & K.O\u2019Loughlin (Eds.), Experimenting with uncertainty: essays in honour of Alan Davies (Studies in Language Testing Series, Volume 11, pp.82\u201396).Cambridge:UCLES\/Cambridge University Press.","cites":null},{"id":16366129,"title":"role of video media in listening assessment.System,25(3),333\u201345.","authors":[],"date":"1997","doi":"10.1016\/s0346-251x(97)00026-2","raw":"GRUBA, P. (1997).The role of video media in listening assessment.System,25(3),333\u201345.","cites":null},{"id":16366136,"title":"ROLL,B.(1997).TOEFL 2000:writing:composition, community and assessment.","authors":[],"date":null,"doi":null,"raw":"HAMP-LYONS,L.& K ROLL,B.(1997).TOEFL 2000:writing:composition, community and assessment. Princeton, NJ: Educational Testing Service.","cites":null},{"id":16366194,"title":"S CHICK,L.(2000).Ratings,raters and test performance:an exploratory study.In A.J.Kunnan (Ed.),Fairness and validation in language assessment","authors":[],"date":"1990","doi":null,"raw":"MEIRON,B .& S CHICK,L.(2000).Ratings,raters and test performance:an exploratory study.In A.J.Kunnan (Ed.),Fairness and validation in language assessment (Studies in Language Testing Series, Volume 9, pp. 153\u201376). Cambridge: UCLES\/ Cambridge University Press. MEREDITH,R.A. (1990).The oral proficiency interview in real life: sharpening the scale.Modern Language Journal,74(3),288\u201396.","cites":null},{"id":16366123,"title":"score comparisons across language proficiency test batteries justified? An IELTS-TOEFL comparability study. Edinburgh Working Papers in","authors":[],"date":"1994","doi":null,"raw":"Language and Education,9(3),179\u201393. GERANPAYEH,A. (1994).Are score comparisons across language proficiency test batteries justified? An IELTS-TOEFL comparability study. Edinburgh Working Papers in Applied Linguistics, 5, 50\u201365. GHONSOOLY,B. (1993).Development and validation of a translation test. Edinburgh Working Papers in Applied Linguistics, 4, 54\u201362. GINTHER, A. (forthcoming). Context and content visuals and performance on listening comprehension stimuli. Language Testing.","cites":null},{"id":16366059,"title":"Screen-to-screen\u2019 testing: an exploratory study of oral proficiency interviewing using video teleconferencing.System,20(3),293\u2013304.","authors":[],"date":"1992","doi":"10.1016\/0346-251x(92)90041-z","raw":"CLARK,J .L .D .&  H OOSHMAND, D. (1992). \u2018Screen-to-screen\u2019 testing: an exploratory study of oral proficiency interviewing using video teleconferencing.System,20(3),293\u2013304.","cites":null},{"id":16366360,"title":"SHOHAMY,E.& INBAR,O.(1991).Validation of listening comprehension tests: the effect of text and question type. Language Testing,8(1),23\u201340. SPENCE-BROWN,R (2001).The eye of the beholder:authenticity in an embedded assessment task.","authors":[],"date":null,"doi":"10.1177\/026553229100800103","raw":"SHOHAMY,E.& INBAR,O.(1991).Validation of listening comprehension tests: the effect of text and question type. Language Testing,8(1),23\u201340. SPENCE-BROWN,R (2001).The eye of the beholder:authenticity in an embedded assessment task. Language Testing, 18 (4), 463\u201381.","cites":null},{"id":16366162,"title":"Simulating conversations in oral-proficiency assessment: a conversation analysis of role plays and nonscripted interview in language exams.","authors":[],"date":"1999","doi":"10.1177\/026553229901600203","raw":"KORMOS, J. (1999). Simulating conversations in oral-proficiency assessment: a conversation analysis of role plays and nonscripted interview in language exams. Language Testing, 16(2), 163\u201388.","cites":null},{"id":16365898,"title":"Speaking as a realization of communicative competence. Paper presented at the LTRC\/AAAL Symposium, St Louis.","authors":[],"date":"2001","doi":null,"raw":"BACHMAN, L. F. (2001). Speaking as a realization of communicative competence. Paper presented at the LTRC\/AAAL Symposium, St Louis.","cites":null},{"id":16366345,"title":"Speaking as performance within a discourse domain. Paper presented at the LTRC\/AAAL Symposium, St Louis.","authors":[],"date":"2001","doi":null,"raw":"SELINKER, L. (2001). Speaking as performance within a discourse domain. Paper presented at the LTRC\/AAAL Symposium, St Louis.","cites":null},{"id":16366071,"title":"Speaking as register. Paper presented at the LTRC\/AAAL Symposium,St Louis.","authors":[],"date":"2001","doi":null,"raw":"CONRAD, S. (2001). Speaking as register. Paper presented at the LTRC\/AAAL Symposium,St Louis.","cites":null},{"id":16366110,"title":"Standards for the development and use of tests: the standards for educational and psychological testing.","authors":[],"date":"1999","doi":"10.1027\/\/1015-5759.17.3.157","raw":"EIGNOR, D. (1999). Standards for the development and use of tests: the standards for educational and psychological testing. European Journal of Psychological Assessment, 17(3), pp. 157\u201363.","cites":null},{"id":16366131,"title":"Student major field and text content: interactive effects on reading comprehension in the Test of English as a Foreign Language.Language Testing,5(1),46\u201361.","authors":[],"date":"1988","doi":"10.1177\/026553228800500104","raw":"HALE, G.A. (1988). Student major field and text content: interactive effects on reading comprehension in the Test of English as a Foreign Language.Language Testing,5(1),46\u201361.","cites":null},{"id":16366125,"title":"students\u2019oral proficiency in an outcome-based curriculum: student performance and teacher intuitions.","authors":[],"date":"1998","doi":"10.2307\/328680","raw":"GLISAN,E.W .& F OLTZ,D.A. (1998).Assessing students\u2019oral proficiency in an outcome-based curriculum: student performance and teacher intuitions. The Modern Language Journal, 83(1),1\u201318.","cites":null},{"id":16366014,"title":"subskills of reading:rule-space analysis of a multiple-choice test of second language reading comprehension.","authors":[],"date":"1997","doi":"10.1111\/0023-8333.00016","raw":"BUCK,G . ,T ATSUOKA,K .&  K OSTIN, I. (1997).The subskills of reading:rule-space analysis of a multiple-choice test of second language reading comprehension. Language Learning, 47(3), 423\u201366.","cites":null},{"id":16366405,"title":"Systematic effects in the rating of second-language speaking ability: test method and learner discourse.Language Testing,16(1),82\u2013111.","authors":[],"date":"1999","doi":"10.1177\/026553229901600105","raw":"UPSHUR,J .&  T URNER, C. E. (1999). Systematic effects in the rating of second-language speaking ability: test method and learner discourse.Language Testing,16(1),82\u2013111.","cites":null},{"id":16366461,"title":"Talking and testing: Discourse approaches to the assessment of oral proficiency (Studies in Bilingualism,Vol.14).Amsterdam:John Benjamins.","authors":[],"date":"1998","doi":"10.1075\/sibil.14","raw":"YOUNG,R .&  H E, A. W. (1998). Talking and testing: Discourse approaches to the assessment of oral proficiency (Studies in Bilingualism,Vol.14).Amsterdam:John Benjamins.","cites":null},{"id":16366117,"title":"tasks: issues in task design and the group oral.Language Testing,13(1),23\u201351. FULCHER,G. (1996b).Does thick description lead to smart tests? A data-based approach to rating scale construction. Language Testing,13(2),208\u201338.","authors":[],"date":"1996","doi":"10.1177\/026553229601300205","raw":"FULCHER, G. (1996a).Testing tasks: issues in task design and the group oral.Language Testing,13(1),23\u201351. FULCHER,G. (1996b).Does thick description lead to smart tests? A data-based approach to rating scale construction. Language Testing,13(2),208\u201338.","cites":null},{"id":16365956,"title":"Taxonomy of educational objectives. Handbook 1:Cognitive domain.New York:Longman.","authors":[],"date":"1956","doi":null,"raw":"BLOOM,B .S . ,E NGLEHART, M., FURST,E .J . ,H ILL,W .H .& KRATHWOHL, D. R. (1956). Taxonomy of educational objectives. Handbook 1:Cognitive domain.New York:Longman.","cites":null},{"id":16366406,"title":"Teaching Spanish to Hispanic bilinguals: a look at oral proficiency testing and the proficiency movement.Hispania,72(2),392\u2013401.","authors":[],"date":"1989","doi":"10.2307\/343163","raw":"VALDES, G. (1989). Teaching Spanish to Hispanic bilinguals: a look at oral proficiency testing and the proficiency movement.Hispania,72(2),392\u2013401.","cites":null},{"id":16366145,"title":"test at the gate:models of literacy in reading assessment.TESOL Quarterly,26(3),433\u201361.","authors":[],"date":"1992","doi":"10.2307\/3587173","raw":"HILL,C.& P ARRY,K. (1992).The test at the gate:models of literacy in reading assessment.TESOL Quarterly,26(3),433\u201361.","cites":null},{"id":16366166,"title":"Test taker characteristics and test performance: A structural modelling approach.","authors":[],"date":"1995","doi":"10.1177\/026553229401100301","raw":"KUNNAN,A. J. (1995). Test taker characteristics and test performance: A structural modelling approach. (Studies in Language Testing Series, Vol. 2). Cambridge: University of Cambridge Local Examinations Syndicate\/ Cambridge University Press.","cites":null},{"id":16366348,"title":"test validity.Review of Research","authors":[],"date":"1993","doi":"10.2307\/1167347","raw":"SHEPARD,L. (1993).Evaluating test validity.Review of Research in Education,19,405\u201350.","cites":null},{"id":16366152,"title":"test-takers\u2019 choice: an investigation of the effect of topic on language-test performance.Language Testing,16(4),426\u201356.","authors":[],"date":"1999","doi":"10.1177\/026553229901600402","raw":"JENNINGS, M., FOX,J . ,G RAVES,B .&  S HOHAMY, E. (1999).The test-takers\u2019 choice: an investigation of the effect of topic on language-test performance.Language Testing,16(4),426\u201356.","cites":null},{"id":16366399,"title":"test-trait fallacy.","authors":[],"date":"1979","doi":"10.1037\/0003-066x.34.5.402","raw":"TRYON,W.W. (1979).The test-trait fallacy. American Psychologist, 334,402\u20136.","cites":null},{"id":16366319,"title":"Testing meaning construction: can we do it fairly? Language Testing,6(1),77\u201394.","authors":[],"date":"1989","doi":"10.1177\/026553228900600107","raw":"SARIG, G. (1989). Testing meaning construction: can we do it fairly? Language Testing,6(1),77\u201394.","cites":null},{"id":16366273,"title":"testing of grammar.In C.Clapham & D. Corson (Eds.), Language testing and assessment","authors":[],"date":"1997","doi":"10.1007\/978-1-4020-4489-2_9","raw":"REA-DICKINS,P. (1997).The testing of grammar.In C.Clapham & D. Corson (Eds.), Language testing and assessment (Vol. 7, pp. 87\u201397). Dordrecht, The Netherlands: Kluwer Academic Publishers.","cites":null},{"id":16366119,"title":"testing of L2 speaking.In C.Clapham &","authors":[],"date":"1997","doi":null,"raw":"FULCHER,G. (1997b).The testing of L2 speaking.In C.Clapham & D. Corson (Eds.), Language testing and assessment (Vol. 7, pp. 75\u201385). Dordrecht, The Netherlands: Kluwer Academic Publishers.","cites":null},{"id":16366006,"title":"testing of listening in a second language.In C.Clapham & D.Corson (Eds.),Language testing and assessment","authors":[],"date":"1997","doi":null,"raw":"Language Testing,11(3),145\u201370. BUCK,G. (1997).The testing of listening in a second language.In C.Clapham & D.Corson (Eds.),Language testing and assessment (Volume 7, pp. 65\u201374). Dordrecht,The Netherlands: Kluwer Academic Publishers.","cites":null},{"id":16366035,"title":"The \u2018free\u2019 conversation and the assessment of oral proficiency.","authors":[],"date":"1995","doi":"10.1080\/09571739585200031","raw":"CHAMBERS,F .&  R ICHARDS, B. (1995). The \u2018free\u2019 conversation and the assessment of oral proficiency. Language Learning Journal,11,6\u201310.","cites":null},{"id":16366151,"title":"the C-test be improved with classical item analysis?","authors":[],"date":"1999","doi":"10.1016\/s0346-251x(98)00043-8","raw":"JAFARPUR,A. (1999a).Can the C-test be improved with classical item analysis? System,27(1),76\u201389. JAFARPUR,A. (1999b).What\u2019s magical about the rule-of-two for constructing C-tests? RELC Journal,30(2),86\u2013100.","cites":null},{"id":16366116,"title":"The construct validation of rating scales for oral tests in English as a foreign language.","authors":[],"date":"1993","doi":null,"raw":"FULCHER, G. (1993). The construct validation of rating scales for oral tests in English as a foreign language. Unpublished PhD dissertation,Lancaster University,Lancaster.","cites":null},{"id":16366214,"title":"The development of a common framework scale of language proficiency based on a theory of measurement.","authors":[],"date":"1995","doi":"10.1016\/0346-251x(95)00032-f","raw":"NORTH, B. (1995). The development of a common framework scale of language proficiency based on a theory of measurement. Unpublished PhD dissertation, Thames Valley University, London.","cites":null},{"id":16366268,"title":"The development of a new measure of L2 vocabulary knowledge.Language Testing,10(3),354\u201371.","authors":[],"date":"1993","doi":"10.1177\/026553229301000308","raw":"READ, J. (1993). The development of a new measure of L2 vocabulary knowledge.Language Testing,10(3),354\u201371.","cites":null},{"id":16366122,"title":"The development of a scoring scheme for content in transactional writing: some indicators of audience awareness.","authors":[],"date":"1995","doi":"10.1080\/09500789509541412","raw":"GARRETT,P . ,G RIFFITHS,Y .,JAMES,C .&  S CHOLFIELD, P. (1995). The development of a scoring scheme for content in transactional writing: some indicators of audience awareness.","cites":null},{"id":16366049,"title":"The development of IELTS: A study of the effect of background knowledge on reading comprehension. (Studies in Language Testing Series,Vol. 4). Cambridge:","authors":[],"date":"1996","doi":"10.1177\/026553229801500206","raw":"CLAPHAM, C. (1996). The development of IELTS: A study of the effect of background knowledge on reading comprehension. (Studies in Language Testing Series,Vol. 4). Cambridge: Cambridge University Press.","cites":null},{"id":16366137,"title":"The development of second language proficiency, Cambridge:","authors":[],"date":"1990","doi":"10.1177\/026765839100700304","raw":"HARLEY,B . ,A LLEN,P . ,C UMMINS,J .&  S WAIN, M. (1990)  The development of second language proficiency, Cambridge: Cambridge University Press HARLOW,L .L .&  C AMINERO, R. (1990). Oral testing of beginning language students at large universities: is it worth the trouble?  Foreign Language Annals,23(6),489\u2013501.","cites":null},{"id":16366133,"title":"The direct testing of oral skills in university foreign language teaching.IRAL,31(1),23\u201338. HALL,E. (1991).Variations in composing behaviours of academic ESL writers in test and non-test situations.","authors":[],"date":"1993","doi":null,"raw":"HALL, C. (1993). The direct testing of oral skills in university foreign language teaching.IRAL,31(1),23\u201338. HALL,E. (1991).Variations in composing behaviours of academic ESL writers in test and non-test situations. TESL Canada, 8(2),9\u201333.","cites":null},{"id":16365875,"title":"The effect of students\u2019 academic discipline on their performance on ESP reading tests.Language Testing,2(2),192\u2013204.","authors":[],"date":"1985","doi":"10.1177\/026553228500200207","raw":"ALDERSON,J .C .&  U RQUHART, A. H. (1985). The effect of students\u2019 academic discipline on their performance on ESP reading tests.Language Testing,2(2),192\u2013204.","cites":null},{"id":16365889,"title":"The evaluation of Spanish-speaking bilinguals\u2019 oral proficiency according to ACTFL guidelines (trans. from Spanish).Hispania,80(2),328\u201341.","authors":[],"date":"1997","doi":null,"raw":"ALONSO, E. (1997). The evaluation of Spanish-speaking bilinguals\u2019 oral proficiency according to ACTFL guidelines (trans. from Spanish).Hispania,80(2),328\u201341.","cites":null},{"id":16366149,"title":"The paired learner interview: a preliminary investigation applying Vygotskian insights.","authors":[],"date":"1998","doi":"10.1080\/07908319808666542","raw":"IKEDA, K. (1998). The paired learner interview: a preliminary investigation applying Vygotskian insights. Language, Culture and Curriculum,11(1),71\u201396. JAFARPUR,A. (1987).The short-context technique:an alternative for testing reading comprehension. Language Testing, 4(2), 195\u2013220.","cites":null},{"id":16366113,"title":"The prediction of TOEFL reading item difficulty: implications for construct validity. Language Testing,10(2),133\u201370.","authors":[],"date":"1993","doi":"10.1177\/026553229301000203","raw":"FREEDLE,R .&  K OSTIN, I. (1993). The prediction of TOEFL reading item difficulty: implications for construct validity. Language Testing,10(2),133\u201370.","cites":null},{"id":16366181,"title":"The process of the assessment of writing performance:the rater\u2019s perspective. Unpublished PhD dissertation,The","authors":[],"date":"2000","doi":null,"raw":"LUMLEY,T. (2000). The process of the assessment of writing performance:the rater\u2019s perspective. Unpublished PhD dissertation,The University of Melbourne,Melbourne. LUMLEY, T. (forthcoming). Assessment criteria in a large scale writing test: what do they really mean to the raters? Language Testing.","cites":null},{"id":16366278,"title":"The relationship between criterion-based levels of oral proficiency and norm-referenced scores of general proficiency in English as a second language.System,20(3),","authors":[],"date":"1992","doi":"10.1016\/0346-251x(92)90044-4","raw":"REED, D. J. (1992). The relationship between criterion-based levels of oral proficiency and norm-referenced scores of general proficiency in English as a second language.System,20(3), 329\u201345.","cites":null},{"id":16366331,"title":"The reliability ritual.Language Testing,8(2),125\u201338.","authors":[],"date":"1991","doi":"10.1177\/026553229100800203","raw":"SCHILS,E .D .J . ,VAN DER POEL,M .G .M .&  W ELTENS, B. (1991). The reliability ritual.Language Testing,8(2),125\u201338.","cites":null},{"id":16366252,"title":"the revised Test of Spoken English against a criterion of communicative success.","authors":[],"date":"1999","doi":"10.1191\/026553299673108653","raw":"POWERS,D .E.,SCHEDL,M.A.,WILSON LEUNG,S.& B UTLER,F .A. (1999).Validating the revised Test of Spoken English against a criterion of communicative success. Language Testing, 16(4), 399\u2013425.","cites":null},{"id":16366456,"title":"The role of speaking in discursive practice. Paper presented at the LTRC\/AAAL Symposium,St Louis.","authors":[],"date":"2001","doi":null,"raw":"YOUNG, R. (2001). The role of speaking in discursive practice. Paper presented at the LTRC\/AAAL Symposium,St Louis.","cites":null},{"id":16365964,"title":"The role of test-taker feedback in the test development process: test-takers\u2019 reactions to a tape-mediated test of proficiency in spoken Japanese.","authors":[],"date":"1993","doi":"10.1177\/026553229301000305","raw":"BROWN, A. (1993). The role of test-taker feedback in the test development process: test-takers\u2019 reactions to a tape-mediated test of proficiency in spoken Japanese. Language Testing, 10(3), 277\u2013303. BROWN,A. (1995).The effect of rater variables in the development of an occupation-specific language performance test.","cites":null},{"id":16366064,"title":"The short circuit hypothesis of ESL reading \u2013 or when language competence interferes with reading performance.In P.L.Carrell,J.Devine,& D.Eskey (Eds.), Interactive approaches to second language reading","authors":[],"date":"1988","doi":"10.1017\/cbo9781139524513.013","raw":"CLARKE, M. (1988). The short circuit hypothesis of ESL reading \u2013 or when language competence interferes with reading performance.In P.L.Carrell,J.Devine,& D.Eskey (Eds.), Interactive approaches to second language reading (pp. 114\u201324). Cambridge:Cambridge University Press.","cites":null},{"id":16366367,"title":"The speaking construct in historical perspective. Paper presented at the LTRC\/AAAL Symposium,St Louis.","authors":[],"date":"2001","doi":null,"raw":"SPOLSKY, B. (2001). The speaking construct in historical perspective. Paper presented at the LTRC\/AAAL Symposium,St Louis.","cites":null},{"id":16365997,"title":"The testing of listening comprehension: an introspective study.Language Testing,8(1),67\u201391. BUCK,G. (1992a).Translation as a language testing process:Does it work? Language Testing,9(2),123\u201348.","authors":[],"date":"1991","doi":"10.1177\/026553229100800105","raw":"BUCK, G. (1991). The testing of listening comprehension: an introspective study.Language Testing,8(1),67\u201391. BUCK,G. (1992a).Translation as a language testing process:Does it work? Language Testing,9(2),123\u201348.","cites":null},{"id":16366079,"title":"The testing of writing in a second language.In C.Clapham & D.Corson (Eds.),Language testing and assessment (Volume 7,pp.51\u201363).Dordrecht,The Netherlands:","authors":[],"date":"1997","doi":null,"raw":"CUMMING, A. (1997). The testing of writing in a second language.In C.Clapham & D.Corson (Eds.),Language testing and assessment (Volume 7,pp.51\u201363).Dordrecht,The Netherlands: Kluwer Academic Publishers.","cites":null},{"id":16366357,"title":"The validity of direct versus semi-direct oral tests.Language Testing,11 (2) 99\u2013124 SHOHAMY,E.,GORDON,C.& K RAEMER,R.(1992).The effect of raters\u2019 background and training on the reliability of direct writing tests.Modern Language Journal,76(1),27\u201333.","authors":[],"date":"1994","doi":"10.1177\/026553229401100202","raw":"SHOHAMY, E. (1994). The validity of direct versus semi-direct oral tests.Language Testing,11 (2) 99\u2013124 SHOHAMY,E.,GORDON,C.& K RAEMER,R.(1992).The effect of raters\u2019 background and training on the reliability of direct writing tests.Modern Language Journal,76(1),27\u201333.","cites":null},{"id":16366266,"title":"TOEFL test of written English:causes for concern.TESOL Quarterly,24(3),427\u201342.","authors":[],"date":"1990","doi":"10.2307\/3587228","raw":"RAIMES,A. (1990).The TOEFL test of written English:causes for concern.TESOL Quarterly,24(3),427\u201342.","cites":null},{"id":16366290,"title":"Type-Token and TypeType measures of vocabulary diversity and lexical style: an annotated bibliography. Reading: University of Reading.","authors":[],"date":"1997","doi":null,"raw":"RICHARDS,B .&  M ALVERN, D. (1997). Type-Token and TypeType measures of vocabulary diversity and lexical style: an annotated bibliography. Reading: University of Reading. http:\/\/www.rdg.ac.uk\/~ehscrichb\/home1.html RILEY,G .L .&  L EE, J. F. (1996). A comparison of recall and summary protocols as measures of second language reading comprehension.Language Testing,13(2),173\u201389.","cites":null},{"id":16366163,"title":"Understanding TOEFL\u2019s","authors":[],"date":"1991","doi":"10.1177\/003368829102200102","raw":"KROLL, B. (1991). Understanding TOEFL\u2019s Test of Written English.RELC Journal,22(1),20\u201333.","cites":null},{"id":16366186,"title":"University of Edinburgh Test of English at Matriculation: validation report. Edinburgh Working Papers in Applied Linguistics,5,66\u201377.","authors":[],"date":"1994","doi":null,"raw":"LYNCH,T. (1994).The University of Edinburgh Test of English at Matriculation: validation report. Edinburgh Working Papers in Applied Linguistics,5,66\u201377. MARISI,P.M. (1994).Questions of regionalism in native-speaker OPI performance: the French-Canadian experience. Foreign Language Annals,27(4),505\u201321.","cites":null},{"id":16365907,"title":"use of test method characteristics in the content analysis and design of EFL proficiency tests.Language Testing,13(2),125\u201350.","authors":[],"date":"1996","doi":"10.1177\/026553229601300201","raw":"BACHMAN,L .F . ,D AVIDSON,F .&  M ILANOVIC, M. (1996).The use of test method characteristics in the content analysis and design of EFL proficiency tests.Language Testing,13(2),125\u201350.","cites":null},{"id":16366432,"title":"Using FACETS to model rater training effects.Language Testing,15(2),263\u201387.","authors":[],"date":"1998","doi":"10.1177\/026553229801500205","raw":"WEIGLE, S. C. (1998). Using FACETS to model rater training effects.Language Testing,15(2),263\u201387.","cites":null},{"id":16366185,"title":"Using G-theory and Many-facet Rasch measurement in the development of performance assessments of the ESL speaking skills of immigrants.Language Testing,15(2),158\u201380.","authors":[],"date":"1998","doi":"10.1177\/026553229801500202","raw":"LYNCH,B .&  M CNAMARA, T. F. (1998). Using G-theory and Many-facet Rasch measurement in the development of performance assessments of the ESL speaking skills of immigrants.Language Testing,15(2),158\u201380.","cites":null},{"id":16366169,"title":"Validating a computer adaptive test of vocabulary size and strength. Paper presented at the Language Testing Research Colloquium,St.Louis.","authors":[],"date":"2001","doi":null,"raw":"LAUFER,B . ,E LDER,C . ,&  H ILL, K. (2001). Validating a computer adaptive test of vocabulary size and strength. Paper presented at the Language Testing Research Colloquium,St.Louis.","cites":null},{"id":16366044,"title":"Validity in language assessment. Annual Review of Applied Linguistics,19,254\u201372.","authors":[],"date":"1999","doi":"10.1017\/s0267190599190135","raw":"CHAPELLE, C. (1999). Validity in language assessment. Annual Review of Applied Linguistics,19,254\u201372.","cites":null},{"id":16366127,"title":"Verbal protocol analysis in language testing research: A handbook.","authors":[],"date":"1998","doi":"10.1177\/026553229901600405","raw":"GREEN, A. (1998). Verbal protocol analysis in language testing research: A handbook. (Studies in Language Testing Series, Vol. 5). Cambridge: University of Cambridge Local Examinations Syndicate\/ Cambridge University Press.","cites":null},{"id":16366184,"title":"What does your language test measure? Unpublished PhD dissertation,","authors":[],"date":"2001","doi":null,"raw":"LUOMA, S. (2001). What does your language test measure? Unpublished PhD dissertation, University of Jyv\u00e4skyla, Jyv\u00e4skyla. LYNCH,B. (1988).Person dimensionality in language test validation.Language Testing,5(2),206\u201319.","cites":null},{"id":16366146,"title":"Who should be the judge? The use of nonnative speakers as raters on a test of English as an international language. In","authors":[],"date":"1997","doi":null,"raw":"HILL, K. (1997). Who should be the judge? The use of nonnative speakers as raters on a test of English as an international language. In A. Huhta, V. Kohonen, L. Kurki-Suonio & S. Luoma (Eds.), Current developments and alternatives in language assessment (pp.275\u201390).Jyv\u00e4skyla:University of Jyv\u00e4skyla.","cites":null},{"id":16366144,"title":"writing portfolios: issues in the validity and meaning of scores.","authors":[],"date":"1993","doi":"10.1207\/s15326977ea0103_2","raw":"HERMAN,J .,G EARHART,M.& B AKER,E. (1993).Assessing writing portfolios: issues in the validity and meaning of scores. Educational Assessment,1(3),201\u201324.","cites":null},{"id":16366135,"title":"writing profiles: an investigation of the transferability of a multipletrait scoring instrument across ESL writing assessment contexts.Language Learning,41(3),337\u201373.","authors":[],"date":"1991","doi":"10.1111\/j.1467-1770.1991.tb00610.x","raw":"HAMP-LYONS,L.& HENNING,G. (1991).Communicative writing profiles: an investigation of the transferability of a multipletrait scoring instrument across ESL writing assessment contexts.Language Learning,41(3),337\u201373.","cites":null},{"id":16365920,"title":"YNCH,B.K.& M ASON,M.(1995).Investigating variability in tasks and rater judgments in a performance test of foreign language speaking.Language Testing,12(2),238\u201357.","authors":[],"date":null,"doi":"10.1177\/026553229501200206","raw":"BACHMAN,L.F .,L YNCH,B.K.& M ASON,M.(1995).Investigating variability in tasks and rater judgments in a performance test of foreign language speaking.Language Testing,12(2),238\u201357.","cites":null},{"id":16366179,"title":"you don\u2019t know can\u2019t help you. An exploratory study of background knowledge and second n Language testing and assessment (Part 2) 109http:\/\/journals.cambridge.org Downloaded:","authors":[],"date":"1990","doi":null,"raw":"LONG, D. R. (1990).What you don\u2019t know can\u2019t help you. An exploratory study of background knowledge and second n Language testing and assessment (Part 2) 109http:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9 language listening comprehension. Studies in Second Language Learning,REM Check 12,65\u201380. LUMLEY,T. (1993).The notion of subskills in reading comprehension tests: an EAP example. Language Testing, 10(3), 211\u201334.","cites":null}],"documentType":{"type":0.7777777778}},"contributors":[],"datePublished":"2002-04-17","abstract":"In Part 1 of this two-part review article (Alderson & Banerjee, 2001), we first addressed issues of washback, ethics, politics and standards. After a discussion of trends in testing on a national level and in testing for specific purposes, we surveyed developments in computer-based testing and then finally examined self-assessment, alternative assessment and the assessment of young learners. In this second part, we begin by discussing recent theories of construct validity and the theories of language use that help define the constructs that we wish to measure through language tests. The main sections of the second part concentrate on summarising recent research into the constructs themselves, in turn addressing reading, listening, grammatical and lexical abilities, speaking and writing. Finally we discuss a number of outstanding issues in the field","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/71411.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/1020\/1\/displayFulltext.pdf","pdfHashValue":"c2eba0f878a51a580e9099cce550b06a3b93351a","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:1020<\/identifier><datestamp>\n      2018-01-24T00:00:57Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D50:5031<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        State of the art review : language testing and assessment (part two).<\/dc:title><dc:creator>\n        Alderson, J. Charles<\/dc:creator><dc:creator>\n        Banerjee, Jayanti<\/dc:creator><dc:subject>\n        P Philology. Linguistics<\/dc:subject><dc:description>\n        In Part 1 of this two-part review article (Alderson & Banerjee, 2001), we first addressed issues of washback, ethics, politics and standards. After a discussion of trends in testing on a national level and in testing for specific purposes, we surveyed developments in computer-based testing and then finally examined self-assessment, alternative assessment and the assessment of young learners. In this second part, we begin by discussing recent theories of construct validity and the theories of language use that help define the constructs that we wish to measure through language tests. The main sections of the second part concentrate on summarising recent research into the constructs themselves, in turn addressing reading, listening, grammatical and lexical abilities, speaking and writing. Finally we discuss a number of outstanding issues in the field.<\/dc:description><dc:date>\n        2002-04-17<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1017\/S0261444802001751<\/dc:relation><dc:identifier>\n        Alderson, J. Charles and Banerjee, Jayanti (2002) State of the art review : language testing and assessment (part two). Language Teaching, 35 (2). pp. 79-113. ISSN 0261-4448<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/1020\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1017\/S0261444802001751","http:\/\/eprints.lancs.ac.uk\/1020\/"],"year":2002,"topics":["P Philology. Linguistics"],"subject":["Journal Article","PeerReviewed"],"fullText":"http:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nprovides an account of the development of changing\nviews in language testing on validity, and language\ntesters have come to accept that there is no one \nsingle answer to the question \u2018What does our test\nmeasure?\u2019 or \u2018Does this test measure what it is sup-\nposed to measure?\u2019. Messick argues that the question\nshould be rephrased along the lines of: \u2018What is the\nevidence that supports particular interpretations and\nuses of scores on this test?\u2019.Validity is not a character-\nistic of a test, but a feature of the inferences made on\nthe basis of test scores and the uses to which a test is\nput. One validates not a test, but \u2018a principle for mak-\ning inferences\u2019 (Cronbach & Meehl, 1955:297).This\nconcern with score interpretations and uses necessar-\nily raises the issue of test consequences, and in educa-\ntional measurement, as well as in language testing\nspecifically, debates continue about the legitimacy of\nincorporating test consequences into validation.\nWhether test developers can be held responsible for\ntest use and misuse is controversial, especially in light\nof what we increasingly know about issues like test\nwashback (see Part One of this review).A new term\nhas been invented \u2013 \u2018consequential validity\u2019 \u2013 but it\nis far from clear whether this is a legitimate area of\nconcern or a political posture.\nMessick (1989:20) presents what he calls a \u2018pro-\ngressive matrix\u2019 (see Figure 1), where the columns\nrepresent the outcomes of testing and the rows rep-\nresent the types of arguments that should be used to\njustify testing outcomes. Each of the cells contains\n\u2018construct validity\u2019, but new facets are added as one\ngoes through the matrix from top left to bottom\nright.\nAs a result of this unified perspective, validation is\nnow seen as ongoing, as the continuous monitoring\nand updating of relevant information, indeed as a\nprocess that is never complete.\nBachman and Palmer (1996) build on this new\nunified perspective by articulating a theory of test\nusefulness, which they see as the most important \ncriterion by which tests should be judged. In so\ndoing, they explicitly incorporate Messick\u2019s unified\nIn Part 1 of this two-part review article (Alderson &\nBanerjee, 2001), we first addressed issues of wash-\nback, ethics, politics and standards.After a discussion\nof trends in testing on a national level and in testing\nfor specific purposes, we surveyed developments in\ncomputer-based testing and then finally examined\nself-assessment, alternative assessment and the assess-\nment of young learners.\nIn this second part, we begin by discussing recent\ntheories of construct validity and the theories of \nlanguage use that help define the constructs that \nwe wish to measure through language tests. The \nmain sections of the second part concentrate on \nsummarising recent research into the constructs\nthemselves, in turn addressing reading, listening,\ngrammatical and lexical abilities, speaking and \nwriting. Finally we discuss a number of outstanding\nissues in the field.\nConstruct validity and theories of\nlanguage use\nTraditionally, testers have distinguished different\ntypes of validity: content, predictive, concurrent,\nconstruct and even face validity. In a number of \npublications, Messick has challenged this view (for\nexample 1989, 1994, 1996), and argued that con-\nstruct validity is a multifaceted but unified and over-\narching concept, which can be researched from a\nnumber of different perspectives. Chapelle (1999)\nLang.Teach. 35, 79\u2013113. DOI: 10.1017\/S0261444802001751 Printed in the United Kingdom \u00aa 2002 Cambridge University Press 79\nJ Charles Alderson is Professor of Linguistics and\nEnglish Language Education at Lancaster University.\nHe holds an MA in German and French from Oxford\nUniversity and a PhD in Applied Linguistics from\nEdinburgh University. He is co-editor of the journal\nLanguage Testing (Edward Arnold), and co-editor of\nthe Cambridge Language Assessment Series\n(C.U.P.), and has published many books and articles on\nlanguage testing, reading in a foreign language, and\nevaluation of language education.\nJayanti Banerjee is a PhD student in the\nDepartment of Linguistics and Modern English\nLanguage at Lancaster University. She has been\ninvolved in a number of test development and research\nprojects and has taught on introductory testing courses.\nShe has also been involved in teaching English for\nAcademic Purposes (EAP) at Lancaster University. Her\nresearch interests include the teaching and assessment of\nEAP as well as qualitative research methods. She is par-\nticularly interested in issues related to the interpretation\nand use of test scores.\nState-of-the-Art Review\nLanguage testing and assessment (Part 2)\nJ Charles Alderson and Jayanti Banerjee Lancaster University, UK\nFigure 1. Messick\u2019s progressive matrix (cited in Chapelle, 1999:\n259).\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nview of construct validity, but also add dimensions\nthat affect test development in the real world. Test\nusefulness, in their view, consists of six major compo-\nnents, or what they call \u2018critical qualities\u2019 of language\ntests, namely construct validity, reliability, conse-\nquences, interactiveness, authenticity and practicality.\nInterestingly, Shohamy (1990b) had already argued\nthat a test validation research agenda should be defined\nin terms of utility (to what extent a test serves the\npractical information needs of a given audience), fea-\nsibility (ease of administration in different contexts)\nand fairness (whether tests are based on material\nwhich test takers are expected to know). We will\naddress the issue of authenticity and interactiveness\nbelow, but the appearance of practicality as a princi-\npled consideration in assessing the quality of a test\nshould be noted.What is less clear in the Bachman\nand Palmer account of test usefulness is how these\nvarious qualities should be measured and weighted\nin relation to each other.\nWhat is particularly useful about the reconceptu-\nalisation of construct validity is that it places the test\u2019s\nconstruct in the centre of focus, and readjusts tradi-\ntional concerns with test reliability. An emphasis on\nthe centrality of constructs \u2013 what we are trying to\nmeasure \u2013 requires testers to consider what is known\nabout language knowledge and ability, and ability to\nuse the language. Language testing involves not only\nthe psychometric and technical skills required to\nconstruct and analyse a test but also knowledge\nabout language: testers need to be applied linguists,\naware of the latest and most accepted theories of lan-\nguage description, of language acquisition and lan-\nguage use.They also need to know how these can be\noperationalised: how they can be turned into ways of\neliciting a person\u2019s language and language use.\nLanguage testing is not confined to a knowledge of\nhow to write test items that will discriminate\nbetween the \u2018strong\u2019 and the \u2018weak\u2019. Central to test-\ning is an understanding of what language is, and what\nit takes to learn and use language, which then\nbecomes the basis for establishing ways of assessing\npeople\u2019s abilities.A new series of books on language\ntesting, the Cambridge Language Assessment\nSeries (Cambridge University Press, edited by\nAlderson and Bachman), has the intention of com-\nbining insights from applied linguistic enquiry with\ninsights gained from language assessment, in order to\nshow how these insights can be incorporated into\nassessment tools and procedures, for the benefit of\nthe test developer and the classroom teacher.\nAlderson and Clapham (1992) report an attempt\nto find a model of language proficiency on which\nthe revised ELTS test \u2013 the IELTS test \u2013 could be\nbased.The authors did not find any consensus among\nthe applied linguists they surveyed, and report a deci-\nsion to be eclectic in calling upon theory in order to\ndevelop the IELTS (International English Language\nTesting System) specifications. If that survey were to\nbe repeated in the early 21st century we believe\nthere would be much more agreement, at least\namong language testers, as to what the most appro-\npriate model should be. Bachman (1991) puts for-\nward the view that a significant advance in language\ntesting is the development of a theory that considers\nlanguage ability to be multi-componential, and\nwhich acknowledges the influence of the test\nmethod and test taker characteristics on test perfor-\nmance. He describes what he calls an interactional\nmodel of language test performance that includes\ntwo major components, language ability and test\nmethod, where language ability consists of language\nknowledge and metacognitive strategies and test\nmethod includes characteristics of the environment,\nrubric, input, expected response and the relationship\nbetween input and expected response. This has\nbecome known as the Bachman model, as described\nin Bachman (1990) and Bachman and Palmer (1996)\nand it has become an influential point of reference,\nbeing increasingly incorporated into views of the\nconstructs of reading, listening, vocabulary and so on.\nThe model is a development of applied linguistic\nthinking by Hymes (1972) and Canale and Swain\n(1980), and by research, e.g., by Bachman and Palmer\n(1996) and by the Canadian Immersion studies\n(Harley et al., 1990) and it has developed as it has\nbeen scrutinised and tested. It remains very useful as\nthe basis for test construction, and for its account of\ntest method facets and task characteristics.\nChalhoub-Deville (1997) disagrees with this\nassessment of the usefulness of the Bachman model.\nShe reviews several theoretical models of language\nproficiency but considers that there is a degree of\nlack of congruence between theoretical models on\nthe one hand and operational assessment frame-\nworks, which necessarily define a construct in partic-\nular contexts, on the other. She argues that although\ntheoretical models are useful, there is an urgent need\nfor an empirically based, contextualised approach to\nthe development of assessment frameworks.\nNevertheless, we believe that one significant con-\ntribution of the Bachman model is that it not only\nbrings testing closer to applied linguistic theory, but\nalso to task research in second language acquisition\n(SLA), one of whose aims is to untangle the various\ncritical features of language learning tasks. The\nBachman and Palmer model of the characteristics of\ntest tasks shows how much more advanced testing\ntheory and thinking has become over the years, as\ntesters have agonised over their test methods.Yet SLA\nresearchers and other applied linguists often use\ntechniques for data elicitation that have long been\nquestioned in testing. One example is the use of\ncloze tests to measure gain in immersion studies;\nanother is the use of picture descriptions in studies of\noral performance and task design. Klein Gunnewiek\n(1997) critically examines the validity of instruments\nused in SLA to measure aspects of language acquisi-\nLanguage testing and assessment (Part 2)\nn\n80\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nHe draws up a possible research agenda that would\nflow from the inclusion of a social perspective \u2013 and in-\ndeed such a research agenda has already borne fruit in\nseveral studies of the nature of the interaction in oral\ntests (Porter, 1991a, 1991b;O\u2019Sullivan,2000a, 2000b).\nAs a result, we now have a somewhat better\nunderstanding of the complexity of oral perfor-\nmance, as reflected in the original McNamara model\n(1995:173), seen in Figure 2 below.\nWe can confidently predict that the implications\nof this model of the social dimensions of language\nproficiency will be a fruitful area for research for\nsome time to come.\nValidation research\nRecent testing literature has shown a continued\ninterest in studies which compare performances on\ndifferent tests, in particular the major influential tests\nof proficiency in English as a Foreign Language.\n(Unfortunately, there is much less research using \ntests of languages other than English.) Perhaps the\nbest known study, the Cambridge-TOEFL study\n(Bachman et al., 1988; Davidson & Bachman, 1990;\nKunnan, 1995; Bachman et al., 1995; Bachman et al.,\n1996), was expected to reveal differences between\nthe Cambridge suite of exams and the TOEFL.\nInterestingly, however, the study showed more simi-\nlarities than differences. The study also revealed\nproblems of low reliability and a lack of parallelism in\nsome of the tests studied, and this research led to sig-\nnificant improvements in those tests and in develop-\nment and validation procedures. This comparability\nstudy was also a useful testing ground for analyses\nbased on the Bachman model, and the various tests\nexamined were subject to scrutiny using that model\u2019s\nframework of content characteristics and test\nmethod facets.\nIn a similar vein, comparisons of the English\nProficiency Test Battery (EPTB), the English\nLanguage Battery (ELBA) and ELTS, as part of the\nELTS Validation Study (Criper & Davies, 1988),\nproved useful for the resulting insights into English\nfor Specific Purposes (ESP) testing, and the some-\nn\nLanguage testing and assessment (Part 2)\n81\ntion. Such critical reviews emphasise the need for\napplied linguists and SLA researchers to familiarise\nthemselves with the testing literature, lest they over-\nlook potential weaknesses in their methodologies.\nPerkins and Gass (1996) argue that, since pro-\nficiency is multidimensional, it does not always de-\nvelop at the same rate in all domains. Therefore,\nmodels that posit a single continuum of proficiency\nare theoretically flawed.They report research which\ntested the hypothesis that there is no linear relation-\nship between increasing competence in different \nlinguistic domains and growth in linguistic pro-\nficiency. They conclude with a discussion of the\nimplications of discontinuous learning patterns for\nthe measurement of language proficiency develop-\nment, and put forward some assessment models that\ncan accommodate discontinuous patterns of growth\nin language. In similar vein, Danon-Boileau (1997)\nargues that, since language development is complex,\nassessment of language acquisition needs to consider\ndifferent aspects of that process: not only proficiency\nat one point in time, or even how far students have\nprogressed, but also what they are capable of learn-\ning, in the light of their progress and achievement.\nWe have earlier claimed that there is no longer an\nissue about which model to use to underpin test\nspecifications and as the basis for testing research.\nIndeed, we have argued (see Part One) that the\nCouncil of Europe\u2019s Common European Framework\nwill be influential in the years to come in language\neducation generally, and one aspect of its useful-\nness will be its exposition of a model of language,\nlanguage use and language learning often explicitly\nbased on the Bachman model. For example, the\nDIALANG project referred to in Part One based the\nspecifications of its diagnostic tests on the Common\nEuropean Framework.At present, probably the most\nfamiliar aspects of the Framework are the various\nscales, developed by North and Schneider (1998) and\nothers, because they have obvious value in measuring\nand assessing learning and achievement.\nHowever,McNamara (1995;McNamara & Lumley,\n1997) challenges the Bachman model. McNamara\nargues that the model ignores the social dimension of\nlanguage proficiency, since the model is, in his opin-\nion, based on psychological rather than social psy-\nchological or social theories of language use. He\nurges language testers to acknowledge the social\nnature of language performance and to examine\nmuch more carefully its interactional \u2013 i.e., social \u2013\naspects. He points out that in oral tests, for example, a\ncandidate\u2019s performance may be affected by how the\ninterlocutor performs, or by the person with whom\nthe candidate is paired.The rater\u2019s perception of the\nperformance, and that person\u2019s use (or misuse) of \nrating scales, are other potential influences on the test\nscore, and he asks the important question:\u2018Whose per-\nformance are we assessing?\u2019This he calls the \u2018Pandora\u2019s\nBox\u2019of language testing (McNamara,1995).\nFigure 2. \u2018Proficiency\u2019 and its relation to perfor-\nmance (McNamara, 1995)\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nwhat surprising stability of constructs over very \ndifferent tests. Many validation studies have been\nconducted on TOEFL (some recent examples are\nFreedle & Kostin, 1993, 1999 and Brown, 1999), the\nTest of Spoken English (TSE) (for example, Powers et\nal., 1999) and comparisons have been made between\nTOEFL, TSE and the Test of Written English \n(TWE) (for example, DeMauro, 1992), comparisons\nof IELTS and TOEFL (Geranpayeh, 1994;Anh, 1997;\nAl-Musawi & Al-Ansari, 1999)), and of TOEFL \nwith newly developed tests (Des Brisay, 1994).\nAlderson (1988) claims to have developed new\nprocedures for validating ESP tests, using the exam-\nple of the IELTS test development project. Fulcher\n(1999a) criticises traditional approaches to the use of\ncontent validity in testing English for Academic\nPurposes (EAP), in light of the new Messick frame-\nwork and recent research into content specificity.\nAlthough testing researchers remain interested in\nthe validity of large-scale international proficiency\ntests, the literature contains quite a few accounts \nof smaller scale test development and validation.\nEvaluative studies of placement tests from a variety \nof perspectives are reported by Brown (1989),\nBradshaw (1990),Wall et al. (1994), Blais and Laurier\n(1995), Heller et al. (1995), and Fulcher (1997a,\n1999b). Lynch (1994) reports on the validation of\nThe University of Edinburgh Test of English at\nMatriculation (comparing the new test to the estab-\nlished English Proficiency Test Battery). Laurier and\nDes Brisay (1991) show how different statistical and\njudgemental approaches can be integrated in small-\nscale test development. Ghonsooly (1993) describes\nhow an objective translation test was developed and\nvalidated, and Zeidner and Bensoussan (1988) and\nBrown (1993) describe the value and use of test taker\nattitudes and feedback on tests in development or\nrevision. O\u2019Loughlin (1991) describes the develop-\nment of assessment procedures in a distance learning\nprogramme, and Pollard (1998) describes the history\nof development of a \u2018computer-resourced\u2019 English\nproficiency test, arguing that there should be more\npublications of \u2018research-in-development\u2019, to show\nhow test development and testing research can go\nhand-in-hand.\nA recent doctoral dissertation (Luoma, 2001)\nlooked at theories of test development and construct\nvalidation in order to explore how the two can be\nrelated, but the research revealed the lack of pub-\nlished studies that could throw light on problems in\ntest development (the one exception was IELTS).\nMost published studies of language test development\nare somewhat censored accounts, which stress the\npositive features of the tests rather than addressing\nproblems in development or construct definition or\nacknowledging the limitations of the published\nresults. It is very much to be hoped that accounts will\nbe published in future by test developers (along the\nlines of Peirce, 1992 or Alderson et al., 2000),\ndescribing critically how their language tests were\ndeveloped, and how the constructs were identified,\noperationalised, tested and revised. Such accounts\nwould represent a valuable contribution to applied\nlinguistics by helping researchers, not only test devel-\nopers, to understand the constructs and the issues\ninvolved in their operationalisation.\nA potentially valuable contribution to our knowl-\nedge of what constitutes and influences test perfor-\nmance and the measurement of language proficiency\nis the work being carried out by the the University\nof Cambridge Local Examinations Syndicate\n(UCLES), by, for example, the administration of\nquestionnaires which elicit information on cultural\nbackground, previous instruction, strategy, cognitive\nstyle and motivation (Kunnan, 1994, 1995; Purpura,\n1997, 1998, 1999). Such research, provided it is \npublished \u2018warts and all\u2019, has enormous potential. In\nthis respect, the new Cambridge Studies in\nLanguage Testing series (Cambridge University\nPress, edited by Milanovic and Weir) is a valuable\naddition to the language testing literature and to our\nunderstanding of test constructs and research meth-\nods, complementing the Research Reports from ETS.\nIt is hoped that other testing agencies and examina-\ntion boards will also publish details of their research,\nand that reports of the development of national\nexams, for example, will contribute to our under-\nstanding of test performance and learner characteris-\ntics, not just for English, but also for other languages.\nLanguage testers continue to use statistical means\nof test validation, and recent literature has reported\nthe use of a variety of techniques, reviewed by\nBachman and Eignor (1997). Perhaps the best known\ninnovation and most frequently reported method \nof test analysis has been the application of Item\nResponse Theory (IRT). Studies early in the period\nof this review include de Jong and Glas (1989), who\nreport the use of the Rasch model to analyse items in\na listening comprehension test, and McNamara\n(1990, 1991) who uses IRT to validate an ESP listen-\ning test. Hudson (1991) explores the relative merits\nof one- and two-parameter IRT models and tradi-\ntional bi-serial correlations as measures of item \ndiscrimination in criterion-referenced testing.\nAlthough he shows close relationships between the\nthree measures, he argues that, wherever possible, it is\nmost appropriate to use the two-parameter model,\nsince it explicitly takes account of item discrimina-\ntion. He later (Hudson, 1993) develops three differ-\nent indices of item discrimination which can be used\nin criterion-referenced testing situations where IRT\nmodels are not appropriate.\nOne claimed drawback of the use of IRT models\nis that they require that the tests on which they are\nused be unidimensional. Henning et al. (1985) and\nHenning (1988) study the effects of violating the\nassumption of unidimensionality and show that \ndistorted estimates of person ability result from such\nLanguage testing and assessment (Part 2)\nn\n82\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nviolations. Buck (1994) questions the value in lan-\nguage testing of the notion of unidimensionality,\narguing that language proficiency is necessarily a\nmultidimensional construct. However, others, for\nexample Henning (1992a), distinguish between psy-\nchological unidimensionality, which most language\ntests cannot meet, and psychometric unidimension-\nality, which they very often can. McNamara and\nLumley (1997) explore the use of multi-faceted\nRasch measurement to examine the effect of such\nfacets of spoken language assessment as interlocutor\nvariables, rapport between candidate and interlocu-\ntor, and the quality of audiotape recordings of per-\nformances. Since the results revealed the effect of\ninterlocutor variability and audiotape quality, the\nauthors conclude that multi-faceted Rasch measure-\nment is a valuable additional tool in test validation.\nOther studies investigate the value of multidimen-\nsional scaling in developing diagnostic interpreta-\ntions of TOEFL subscores (Oltman & Stricker,\n1990), and of a new technique known as \u2018Rule-\nSpace Methodology\u2019 to explore the cognitive and\nlinguistic attributes that underlie test performance\non a listening test (Buck & Tatsuoka, 1998) and a\nreading test (Buck et al., 1997). The authors argue\nthat the Rule-Space Methodology can explain \nperformance on complex verbal tasks and provide\ndiagnostic scores to test takers. Kunnan (1998) and\nPurpura (1999) provide introductions to structural\nequation modelling in language testing research, and\na number of articles in a special issue of the journal\nLanguage Testing demonstrate the value of such\napproaches in test validation (Bae & Bachman, 1998;\nPurpura, 1998).\nA relatively common theme in the exploration of\nstatistical techniques is the comparison of different\ntechniques to achieve the same ends. Bachman et al.\n(1995) investigate the use of generalisability theory\nand multi-faceted Rasch measurement to estimate\nthe relative contribution of variation in test tasks and\nrater judgements to variation in test scores on a \nperformance test of Spanish speaking ability among\nundergraduate students intending to study abroad.\nSimilarly, Lynch and McNamara (1998) investigate\ngeneralisability theory (using GENOVA) and multi-\nfaceted Rasch measurement (using FACETS) to\nanalyse performance test data and they compare their\nrelative advantages and roles. Kunnan (1992) com-\npares three procedures (G-theory, factor and cluster\nanalyses) to investigate the dependability and validity\nof a criterion-referenced test and shows how the dif-\nferent methods reveal different aspects of the test\u2019s\nusefulness. Lynch (1988) investigates differential item\nfunctioning (DIF \u2013 a form of item bias) as a result of\nperson dimensionality and Sasaki (1991) compares\ntwo methods for identifying differential item func-\ntioning when IRT models are inappropriate.\nHenning (1989) presents several different methods\nfor testing for local independence of test items.\nIn general, the conclusions of such studies are that\ndifferent methods have different advantages and dis-\nadvantages, and users of such statistical techniques\nneed to be aware of these.\nHowever, quantitative methods are not the only\ntechniques used in validation studies, and language\ntesting has diversified in the methods used to explore\ntest validity, as Banerjee and Luoma (1997) have\nshown. Qualitative research techniques like intro-\nspection and retrospection by test takers (e.g.,\nAlderson, 1990b; Storey, 1994; Storey, 1997; Green,\n1998) are now widely used in test validation.\nDiscourse analysis of student output, in oral as well as\nwritten performance, has also proved to be useful\n(Shohamy, 1990a; Lazaraton, 1992; Ross, 1992; Ross\n& Berwick, 1992; Young, 1995; Lazaraton, 1996;\nYoung & He, 1998). Increasingly, there is also trian-\ngulation of research methods across the so-called\nqualitative\/quantitative divide (see Anderson et al.,\n1991, for an early example of such triangulation) in\norder better to understand what constructs are\nindeed being measured.\nWe address the use of such qualitative techniques\nas well as triangulations of different methods in the\nfollowing sections, which deal with recent research\ninto the various constructs of language ability.\nAssessing reading\nHow the ability to read text in a foreign language\nmight best be assessed has long interested language\ntesting researchers. There is a vast literature and\nresearch tradition in the field of reading in one\u2019s first\nlanguage (L1), and this has had an influence on both\nthe theory of reading in a foreign language and on\nresearch into foreign language testing. Indeed, the\nissue of whether reading in a foreign language is a\nlanguage problem or a reading problem (Alderson,\n1984) is still a current research topic (Bernhardt,\n1991; Bernhardt & Kamil, 1995; Bernhardt, 1999).\nTheorists and practitioners in the 1980s argued\nabout whether L1 reading skills transferred to read-\ning in a foreign language. One practical implication\nof transfer might be that one should first teach stu-\ndents to read accurately and appropriately in the first\nlanguage before expecting them to do so in a foreign\nlanguage.A second implication for assessment might\nbe that tests of reading may actually be tests of one\u2019s\nlinguistic abilities and proficiency.\nHowever, consensus has slowly emerged that the\n\u2018short-circuit hypothesis\u2019 (Clarke, 1979, 1988) is\nessentially correct. This hypothesis posits that one\u2019s\nfirst language reading skills can only transfer to the\nforeign language once one has reached a threshold\nlevel of competence in that language.Whilst this may\nseem perfectly obvious at one level \u2013 how can you\npossibly read in a foreign language without knowing\nanything of that language? \u2013 the real question is at\nwhat point the short circuit ceases to operate. The\nn\nLanguage testing and assessment (Part 2)\n83\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nissue is not quite so simple as deciding that a given\nlevel of language proficiency is the threshold level.\nReading is an interaction between a reader with all\nthat the reader brings with him\/her \u2013 background\nknowledge, affect, reading purpose, intelligence, first\nlanguage abilities and more \u2013 and the text, whose\ncharacteristics include topic, genre, structure, lan-\nguage (organisation, syntax, vocabulary, cohesion)\nand so on.Thus a reader\u2019s first language reading skills\nmay \u2018transfer\u2019 at a lower level of foreign language\nproficiency, on a text on a familiar topic, written in\neasy language, with a clear structure, than they would\non a less familiar topic, with much less clearly struc-\ntured organisation,with difficult language.\nOne of the goals of testing research is to explore\nthe nature of difficulty \u2013 of tests, of test tasks and\nitems \u2013 and the causes of such difficulty.As indicated\nabove, difficulty is relative to readers, and thus the\nreader\u2019s ability and other characteristics have to be\ntaken into account. Perkins and Brutten (1988a)\nreport what they call a behavioural anchoring analy-\nsis of three foreign language reading tests. For each\ntest, they identified items that clearly discriminated\nbetween different levels of reading ability, and\nanalysed the items in terms of their relation to the\nstructure of the texts, the reader\u2019s background\nknowledge and the cognitive processes supposedly\nrequired to answer the questions. They claim that\nhigher-level students could comprehend micro-\npropositions and questions whose sources of infor-\nmation were implicit, whereas lower-level students\ncould not. Both students with higher reading ability\nand those with lower reading ability showed com-\npetence with linguistic structures that related parts \nof the text, regardless of their language proficiency.\nThis again raises the question of the relationship\nbetween reading ability and linguistic proficiency.\nPerkins et al. (1995) showed that an artificial neur-\nal network was an interesting technique for investi-\ngating empirically what might bring about item level\ndifficulty, and Buck et al. (1997) have developed \nthe Rule-Space Methodology to explore causes of \nitem difficulty.Whilst to date these techniques have \nmerely identified variables in item design \u2013 relation-\nship between words in the items and words in the\ntext, wording of distractors, and so on \u2013 future\nresearch might well be able to throw light on the\nconstructs that underlie reading tests, and thus\nenhance our understanding of what reading ability in\na foreign language consists of (but see Hill & Parry,\n1992, for a sceptical approach to the validity of any\ntraditional test of reading).\nAlderson (2000) suggests that views of difficulty in\nreading in a foreign language could be explored by\nlooking at how test developers have specified their\ntests at different levels of proficiency, and refers to the\nCambridge suite of tests, the Council of Europe\nFramework, and the frameworks used in national\nassessments of foreign language ability. One such\ncharacterisation is contained in the ACTFL\n(American Council on the Teaching of Foreign\nLanguages) Guidelines for Reading (Child, 1987).\nACTFL divides reading proficiency into three areas \u2013\ncontent, function and accuracy. Two parallel hier-\narchies are posited, one of text types and the other \nof reading skills, which are cross-sectioned to define\ndevelopmental levels (here developmental level is\nheld to indicate relative difficulty and ease). The\nACTFL Guidelines have been very influential in the\nUSA in foreign language education and assessment,\nbut have only rarely been subject to empirical scru-\ntiny. Lee and Musumeci (1988) examined students\ncompleting tests at five different levels of ACTFL dif-\nficulty, where questions were based on four different\ntypes of reading skill.They found no evidence for the\nproposed hierarchy of reading skills and text types:\nthe performances of readers from all levels were\nremarkably similar, and distinct from the hypothe-\nsised model. Further confirmation of this finding was\nmade by Allen et al. (1988). Debate continues, how-\never, as Edwards (1996) criticises the design of pre-\nvious studies, and claims to have shown that when\nsuitably trained raters select an adequate sample of\npassages at each level, and a variety of test methods\nare employed, then the ACTFL text hierarchy may\nindeed provide a sound basis for the development of\nforeign language reading tests. Shohamy (1990b)\ncriticises the ACTFL Guidelines for being simplistic,\nunidimensional and inadequate for the description of\ncontext-sensitive, unpredictable language use. She\nargues that it is important that the construction of\nlanguage tests be based on a more expanded and\nelaborated view of language.\nA common belief in foreign language reading \nis that there is a hierarchy of skills, as posited by\nACTFL, and asserted by Benjamin Bloom\u2019s\nTaxonomy of Educational Objectives (Bloom et al.,\n1956).This is an example of theory in first language\neducation being transferred, often uncritically, to for-\neign language education. This hierarchy has often\nbeen characterised as consisting of \u2018higher-order\u2019 and\n\u2018lower-order\u2019 skills, where \u2018understanding explicitly\nstated facts\u2019 is regarded as \u2018lower-order\u2019 and \u2018appreci-\nating the style of a text\u2019 or \u2018distinguishing between\nmain ideas and supporting detail\u2019 is held to be \n\u2018higher-order\u2019. In foreign language reading assess-\nment, it has often been held that it is important to\ntest more than \u2018mere\u2019 lower-order skills, and the\ninference from such beliefs has been that somehow\nhigher-order skills are not only more valuable, but\nalso more difficult for foreign language readers.\nAlderson and Lukmani (1989) and Alderson (1990a)\ncritically examine this assumption, and show that,\nfirstly, expert judges do not agree on whether test\nquestions are assessing higher- or lower-order skills,\nand secondly, that even for those items where experts\nagree on the level of skill being tested, there is no\ncorrelation between level of skill and item difficulty.\nLanguage testing and assessment (Part 2)\nn\n84\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nAlderson concludes that item difficulty does not\nnecessarily relate to \u2018level\u2019 of skill, and that no impli-\ncational scale exists such that students have to acquire\nlower-order skills before they can acquire higher-\norder skills.\nThis conclusion has proved controversial and\nLumley (1993) shows that, once teachers have been\ntrained to agree on a definition of skills, and provided\nany disagreements are discussed at length, substantial\nagreements on matching subskills to individual test\nitems can be reached. Alderson (1991a) argues that \nall that this research and similar findings by Bachman\net al. (1989) and Bachman et al. (1995) prove is that\nraters can be trained to agree. That does not, he\nclaims, mean that individual skills can be tested sepa-\nrately by individual test items.Alderson (1990b) and\nLi (1992) show that students completing tests pur-\nporting to assess individual sub-skills in individual\nitems can get answers correct for the \u2018wrong\u2019\nreason \u2013 i.e., without displaying the skill intended \u2013\nand they can get an item wrong for the right\nreason \u2013 that is, showing evidence of the skill in\nquestion. Reves and Levine (1992), after examining a\nmastery reading test, argue that \u2018enabling reading\nskills\u2019 are subsumed within the overall mastery of\nreading comprehension and therefore need not be\nspecified in the objectives of reading tests. Alderson\n(2000) concludes that individuals responding to test\nitems do so in a complex and interacting variety of\ndifferent ways, that experts judging test items are not\nwell placed to predict how learners, whose language\nproficiency is quite different from that of the experts,\nmight actually respond to test items, and that there-\nfore generalisations about what skills reading test\nitems might be testing are fatally flawed. This is\nsomething of a problem for test developers and\nresearchers.\nAnderson et al. (1991) offer an interesting\nmethodological perspective on this issue, by ex-\nploring the use of think-aloud protocols, content\nanalyses and empirical item performances in order \nto triangulate data on construct validity. Findings \non the nature of what is being tested in a reading test\nremain inconclusive, but such triangulated method-\nologies will be imperative for future such studies\n(and see above on construct validation).\nOne perennial area of concern in reading tests is\nthe effect of readers\u2019 background knowledge and the\ntext topic on any measure of reading ability. Perkins\nand Brutten (1988b) examine different types of read-\ning questions: textually explicit (which can be\nanswered directly from the text), textually implicit\n(which require inferences) and \u2018scriptally implicit\u2019\n(which can only be answered with background\nknowledge).They show significant differences in dif-\nficulty and discriminability and conclude that testing\nresearchers (and by implication test developers) must\ncontrol for background knowledge in reading \ntests. Hale (1988) examines this by assuming that a\nstudent\u2019s academic discipline (a crude measure of\nbackground knowledge) will interact with test con-\ntent in determining performance, and he shows that\nstudents in humanities\/social sciences and in the bio-\nlogical\/physical sciences perform better on passages\nrelated to their disciplines than on other passages.\nHowever, although significant, the differences were\nsmall and had no practical effect in terms of the\nTOEFL scale \u2013 perhaps, he concludes, because\nTOEFL passages are taken from general readings\nrather than from specialised textbooks. Peretz and\nShoham (1990) show that, although students rate\ntexts related to their fields of study as more compre-\nhensible than texts related to other topics, their sub-\njective evaluations of difficulty are not a reliable\npredictor of their actual performance on reading\ntests. Clapham (1996) also looked at students\u2019 ratings\nof their familiarity with, and their background\nknowledge of, the content of specific texts in the\nIELTS test battery. In addition she compared the stu-\ndents\u2019 academic discipline with their performance on\ntests based on texts within those disciplines. She con-\nfirmed earlier results by Alderson and Urquhart\n(1985) that showed that students do not necessarily\nperform better on tests in their subject area. Some\ntexts appear to be too specific for given fields and\nothers appear to be so general that they can be\nanswered correctly by students outside the discipline.\nClapham argues that EAP testing, based on the\nassumption that students will be advantaged by \ntaking tests in their subject area where they have\nbackground knowledge, is not necessarily justified\nand she later concludes (Clapham, 2000) that sub-\nject-specific reading tests should be replaced by tests\nof academic aptitude and grammatical knowledge.\nInterestingly, Alderson (1993) also concludes, on\nthe basis of a study of pilot versions of the IELTS\ntest, that it is difficult to distinguish between tests of\nacademic reading and contextualised tests of gram-\nmatical ability. Furthermore, with clear implications\nfor the short-circuit hypothesis mentioned above,\nClapham (1996) shows that students scoring less than\n60% on a test of grammatical knowledge appear to\nbe unable to apply their background knowledge to\nunderstanding a text, whereas students scoring above\n80% have sufficient linguistic proficiency to be able\nto overcome deficits in background knowledge\nwhen understanding texts. The suggestion is that\nthere must be two thresholds, and that only students\nbetween, in this case, scores of 60% and 80%, are able\nto use their background knowledge to compensate\nfor lack of linguistic proficiency.Again, these findings\nhave clear implications for what it is that reading tests\nmeasure.\nA rather different tradition of research into reading\ntests is one that looks at the nature of the text and its\nimpact on test difficulty. Perkins (1992), for example,\nstudies the effect of passage structure on test per-\nformance, and concludes that when questions are\nn\nLanguage testing and assessment (Part 2)\n85\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nderived from sentences where given information\nprecedes new information, and where relevant infor-\nmation occurs in subject position, they are easier\nthan questions derived from sentences with other\nkinds of topical structure. However, Salager-Meyer\n(1991) shows that matters are not so simple. She\ninvestigated the effect of text structure across differ-\nent levels of language competence and topical\nknowledge, and different degrees of passage familiar-\nity. Students\u2019 familiarity with passages affected test\nperformance more than text structure, and where\nstudents were less familiar with passages, changes in\ntext structure affected only weaker students, not\nstronger students. Where passages are completely\nunfamiliar, neither strong nor weak students are\naffected by high degrees of text structuring. Thus,\ntext structure as a variable in test difficulty must be\ninvestigated in relation to its interaction with other\ntext and reader characteristics, and not in isolation.\nFinally, one ongoing tradition in testing research is\nthe investigation of test method effects, and this is\noften conducted in the area of reading tests. Recent\nresearch has both continued the investigation of tra-\nditional methods like multiple-choice, short-answer\nquestions, gap-filling and C-tests, but has also gone\nbeyond these (see, for example, Jafarpur, 1987, 1995,\n1999a, 1999b).Chapelle (1988) reports research indi-\ncating that field independence may be a variable\nresponsible for systematic error in test scores and\nshows that there are different relationships between\nmeasures of field independence and cloze, multiple\nchoice and dictation tests. Grotjahn (1995) criticises\nstandard multiple-choice methods and proposes pos-\nsible alternatives, like the cloze procedure, C-tests\nand immediate recall. However,Wolf (1993a, 1993b)\nclaims that immediate recall may only assess the\nretrieval of low-level detail. Wolf concludes that\nlearners\u2019 ability to demonstrate their comprehension\ndepends on the task, and the language of the test\nquestions. She claims that selected response (mul-\ntiple-choice) and constructed response (cloze, short-\nanswer) questions measure different abilities (as also\nclaimed by Grotjahn, 1995), but that both may\nencourage bottom-up \u2018low-level\u2019 processing. She also\nsuggests that questions in the first language rather\nthan the target language may be more appropriate for\nmeasuring comprehension rather than production (see\nalso the debate in the Netherlands reported in Part\nOne about the use of questions in Dutch to measure\nreading ability in English \u2013 Welling-Slootmaekers,\n1999, van Elmpt & Loonen, 1998 and Bhgel & Leijn,\n1999). Translation has also occasionally been re-\nsearched as a test method for assessing reading ability\n(Buck, 1992a), showing surprisingly good validity\nindices, but the call for more research into this test\nmethod has not yet been taken up by the testing\ncommunity.\nRecall protocols are increasingly being used as a\nmeasure of foreign language comprehension. Deville\nand Chalhoub-Deville (1993) caution against uncrit-\nical use of such techniques, and show that only when\nrecall scoring procedures are subjected to item and\nreliability analyses can they be considered an alterna-\ntive to other measures of comprehension. Riley and\nLee (1996) compare recall and summary protocols as\nmethods of testing understanding and conclude that\nthere are significant qualitative differences in the two\nmethods.The summaries contained more main ideas\nthan the recall protocols and the recalls contained a\nhigher percentage of details than main ideas.\nDifferent methods would appear to be appropriate\nfor testing different aspects of understanding.\nTesting research will doubtless continue to address\nthe issue of test method, sometimes repeating the\nresearch and debates of the 1970s and 1980s into the\nuse of cloze and C-test procedures, and sometimes\nmaking claims for other testing methods. The con-\nsensus, nevertheless, is that it is essential to use more\nthan one test method when attempting to measure a\nconstruct like reading comprehension.\nQuite what the construct of reading comprehen-\nsion is has been addressed in the above debates.\nHowever, one area that has received relatively little\nattention is: how do we know when somebody has\ncomprehended a text? This question is implicit in the\ndiscussion of multiple-choice questions or recall pro-\ntocols: is the ability to cope with such test methods\nequivalent to understanding a text? Research by\nSarig (1989) addresses head-on the problem of vari-\nable text meaning, pointing out that different readers\nmay indeed construct different meanings of a text\nand yet be \u2018correct\u2019. This is partly accounted for by\nschema theory, but still presents problems for decid-\ning when a reader has \u2018correctly interpreted a text\u2019.\nShe offers a methodology for arriving at a consensus\nview of text meaning, by analysing model answers to\nquestions from samples of readers from diverse back-\ngrounds and levels of expertise. She recommends\nthat the Meaning Consensus Criterion Answer be\nused as a basis for item scoring (and arguably also for\nscoring summary and recall protocols).\nHill and Parry (1992), however, offer a much more\nradical perspective. Following Street (1984), they\ncontrast two models of literacy \u2013 the autonomous\nand the pragmatic \u2013 and claim that traditional tests of\nreading assume that texts \u2018have meaning\u2019, and view\ntext, reader and the skill of reading itself as\nautonomous entities. They offer an alternative view\nof literacy, namely that it is socially constructed.They\nsay that the skill of reading goes beyond decoding\nmeaning to the socially conditioned negotiation of\nmeaning, where readers are seen as having social, not\njust individual, identities.They claim that reading and\nwriting are inseparable, and that their view of literacy\nrequires an alternative approach to the assessment of\nliteracy, one which includes a social dimension.The\nimplications of this have yet to be worked out in any\ndetail, and that will doubtless be the focus of work in\nLanguage testing and assessment (Part 2)\nn\n86\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nthe 21st century. Alderson (2000) gives examples of\nalternative assessment procedures which could be\nsubject to detailed validation studies.\nAssessing listening\nIn comparison to the other language skills, the\nassessment of listening has received little attention,\npossibly reflecting (as Brindley, 1998, argues) the\ndifficulties involved in identifying relevant features of\nwhat is essentially an invisible cognitive operation.\nRecent discussions of the construct of listening and\nhow the listening trait might be isolated and mea-\nsured (Buck, 1990, 1991, 1992b, 1994; Dunkel et al.,\n1993) suggest that there is a separate listening trait\nbut that it is not necessarily operationalised by oral\ninput alone.\nBuck has extended this claim (see Buck, 1997,\n2001), explaining that, while listening comprehen-\nsion might primarily be viewed as a process of con-\nstructing meaning from auditory input, that process\ninvolves more than the auditory signal alone.\nListening comprehension is seen as an inferential\nprocess involving the interaction between both lin-\nguistic and non-linguistic knowledge. Buck (2001)\nexplains that listening comprehension involves\nknowledge of discrete elements of language such as\nphonology, vocabulary and syntax but it goes beyond\nthis because listening also involves interpretation.\nListening must be done automatically in real time\n(listeners rarely get a second chance to hear exactly\nthe same text), involves background knowledge and\nlistener-specific variables (such as purpose for listen-\ning) and is a very individual process, implying that\nthe more complex a text is the more varied the pos-\nsible interpretations. It also has unique characteristics\nsuch as the variable nature of the acoustic input.\nListening input is characterised by features such as\nelision and the placement of stress and intonation.\nIdeas are not necessarily expressed in a linear gram-\nmatical manner and often contain redundancy and\nhesitation.\nAll these features raise the question of what is the\nbest approach to assessing listening. Recent research\ninto test methods has included research into the use\nof dictation (see Kaga, 1991 and Coniam, 1998) and\nsummary translation (see Stansfield et al., 1990, 1997;\nScott et al., 1996). While dictation has often been\nused as a measure of language proficiency in French\nand English as second languages, it has been argued\nthat it is not as effective a measure when the target\nlanguage has a very close relationship between its\npronunciation and orthography. Kaga (1991) consid-\ners the use of \u2018graduated dictation\u2019 (a form of modi-\nfied dictation) to assess the listening comprehension\nof adult learners of Japanese in a university context.\nHer results indicate that the \u2018graduated dictation\u2019 is\nan effective measure of language proficiency even in\nthe case of Japanese where, arguably, the pronuncia-\ntion and orthography in the target language are\nclosely related. Coniam (1998) also discusses the use\nof dictation to assess listening comprehension, in his\ncase a computer-based listening test \u2013 the \u2018Text\nDictation\u2019. He argues that this type of test is more\nappropriate as a test of listening than short fragments\nwhere the required responses are in the form of\ntrue\/false questions, gap-fill etc., because the text is\nmore coherent and provides more context. His\nresults indicate that the Text Dictation procedure \ndiscriminates well between students of different pro-\nficiency levels.\nSummary translation tests, such as the Listening\nSummary Translation Exam (LSTE) developed by\nStansfield et al. (1990, 1997, 2000) and Scott et al.\n(1996), first provide an instructional phase in which\nthe test takers are taught the informational and lin-\nguistic characteristics of a good summary.Test takers\nare then presented with three summary translation\ntasks.The input consists of conversations in the target\nlanguage (Spanish or Taiwanese). These vary in\nlength from one to three minutes. In each case the\ntest takers hear the input twice and are permitted to\ntake notes. They then have to write a summary of\nwhat they have heard, in English. Interestingly, this\ntest method not only assesses listening, but also \nwriting and the developers report that listening \nperformance in the target language has an inverse\nrelationship with writing performance in English. It\nis also clear from Kaga\u2019s and Coniam\u2019s research that\nthe target of the assessment is general language pro-\nficiency rather than the isolation of a specific listen-\ning trait.This confirms Buck\u2019s (1994) suggestion that\nthere are two types of listening test, the first being\norally presented tests of general language compre-\nhension and the second tests of the listening trait\nproper. Indeed, one of the challenges of assessing \nlistening is that it is well nigh impossible to construct\na \u2018pure\u2019 test of listening that does not require the use\nof another language skill. In addition to listening \nto aural input, test takers are likely to have to read\nwritten task instructions and\/or questions.They also\nhave to provide either oral or written responses to\nthe questions. Consequently, what might be intended\nas a listening test could also be assessing another \nlanguage skill.\nTo complicate matters further, other test factors\nsuch as the test method and test taker characteristics\nsuch as memory capacity (Henning, 1991) could also\ncontribute to the test score. Admittedly though,\nresearch into the effects of these factors has been\nsomewhat inconclusive. Hale and Courtney (1994)\nexamine the effect of note-taking on test taker per-\nformance on the listening section of the TOEFL.\nThey report that allowing test takers to make notes\nhad little effect on their test scores while actively\nurging them to take notes significantly impaired\ntheir performance. This finding perhaps says more\nabout the students\u2019 note-taking experiences and\nn\nLanguage testing and assessment (Part 2)\n87\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nhabits than about the value of note-taking in the\ncontext of the TOEFL listening test.\nSherman (1997) considers the effect of question\npreview. Subjects took listening tests in four different\nversions: questions before the listening exercise, sand-\nwiched between two hearings of the listening text,\nafter the text, or no questions at all. She found that\nthe test takers had a strong affective preference for\npreviewed questions but previewing did not neces-\nsarily result in more correct answers. In fact, the \nversion that produced significantly more correct\nanswers was the one in which the test takers heard\nthe passage twice (with the questions presented\nbetween the two hearings). It is not clear, in this case,\nwhether the enhanced scores were due to the oppor-\ntunity to preview the questions or the fact that the\ntext was played twice, or indeed a combination of\nthe two.\nIn an effort to disentangle method from trait,Yi\u2019an\n(1998) employed an immediate retrospective verbal\nreport procedure to investigate the effect of a mul-\ntiple-choice format on listening test performance.\nHer results, apart from providing evidence of \nthe explanatory power of qualitative approaches in\nassessment research (see also Banerjee & Luoma,\n1997), show how test takers activate both their \nlinguistic and non-linguistic knowledge in order to\nprocess input.Yi\u2019an argues that language proficiency\nand background knowledge interact and that non-\nlinguistic knowledge can either compensate for \ndeficiencies in linguistic knowledge or can facilitate\nlinguistic processing.The former is more likely in the\ncase of less able listeners who are only partially \nsuccessful in their linguistic processing. More com-\npetent and advanced listeners are more likely to use\ntheir non-linguistic knowledge to facilitate linguistic\nprocessing. However, the use of non-linguistic\nknowledge to compensate for linguistic deficiencies\ndoes not guarantee success in the item.Yi\u2019an\u2019s results\nalso indicate that the multiple-choice format dis-\nadvantages less able listeners and allows uninformed\nguessing. It also results in test takers getting an item\ncorrect for the wrong reasons.\nOther research into the testing of listening has\nlooked at text and task characteristics that affect diffi-\nculty (see Buck, 2001: 149-151, for a comprehensive\nsummary). Task characteristics that affect listening\ntest difficulty include those related to the informa-\ntion that needs to be processed, what the test taker is\nrequired to do with the information and how quick-\nly a response is required.Text characteristics that can\ninfluence test difficulty include the phonological\nqualities of the text and the vocabulary, grammar and\ndiscourse features. Apart from purely linguistic \ncharacteristics, text difficulty is also affected by \nthe degree of explicitness in the presentation of the\nideas, the order of presentation of the ideas and the\namount of redundancy.\nShohamy and Inbar (1991) investigated the effect\nof both texts and question types on test takers\u2019 scores,\nusing three texts with various oral features. Their\nresults indicate that texts located at different points\non the oral\/written continuum result in different test\nscores, the texts with more oral features being easier.\nThey also report that, regardless of the topic, text\ntype or the level of the test takers\u2019 language profi-\nciency, questions that refer to local cues are easier\nthan those that refer to global cues. Freedle and\nKostin (1999) examined 337 TOEFL multiple-\nchoice listening items in order to identify character-\nistics that contribute to item difficulty.Their findings\nindicate that the topic and rhetorical structure of the\ninput text affect item difficulty but that the two most\nimportant determinants of item difficulty are the\nlocation of the information necessary for the answer\nand the degree of lexical overlap between the text\nand the correct answer.When the necessary informa-\ntion comes at the beginning of the listening text the\nitem is always easier (regardless of the rhetorical\nstructure of the text) than if it comes later. In ad-\ndition, when words used in the listening passage are\nrepeated in the correct option, the item is easier than\nwhen words found in the listening passage are used\nin the distractors. In fact, lexical overlap between the\npassage and item distractors is the best predictor of\ndifficult items.\nLong (1990) and Jensen and Hansen (1995) look\nat the effect of background\/prior knowledge on lis-\ntening test performance. Jensen and Hansen postu-\nlate that listening proficiency level will affect the\nextent to which prior knowledge of a topic can be\naccessed and used, hypothesising that test takers will\nneed a high proficiency level in order to activate\ntheir prior knowledge. Their findings, however, do\nnot support this hypothesis. Instead, they conclude\nthat the benefit of prior knowledge is more likely to\nmanifest itself if the input text is technical in nature.\nLong (1990) used two Spanish listening texts, one\nabout a well-known pop group and the other about\na gold rush in Ecuador. She reports that the better\nthe Spanish proficiency of the test takers the better\ntheir score on both texts. However, for the text about\nthe pop group, there was no significant difference in\nscores between more and less proficient test takers.\nHer interpretation of this result is that background\nknowledge of a topic can compensate for linguistic\ndeficiencies. However, she warns that schematic\nknowledge can also have the reverse effect. Some test\ntakers actually performed badly on the gold rush text\nbecause they had applied their knowledge of a differ-\nent gold rush.\nLong does not attempt to explain this finding\n(beyond calling for further study of the interaction\nbetween schematic knowledge, language level and\ntext variables). However, Tsui and Fullilove (1998)\nsuggest that language proficiency level and text\nschema can interact with text processing as a dis-\ncriminator of test performance. They identify two\nLanguage testing and assessment (Part 2)\nn\n88\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\ntypes of text, one in which the schema activated by\nthe first part of the text is not congruent with the\nsubsequent linguistic input (\u2018non-matching\u2019) and the\nother in which the schema is uniform throughout\n(\u2018matching\u2019). They argue that \u2018non-matching\u2019 texts\ndemand that test takers process the incoming lin-\nguistic cues quickly and accurately, adjusting their\nschema when necessary. On the other hand, test \ntakers were able to rely on top-down processing to\nunderstand \u2018matching\u2019 texts. Their findings indicate\nthat, regardless of question type, the more proficient\ntest takers performed better on \u2018non-matching\u2019 texts\nthan did the less proficient. They conclude that \nbottom-up processing is more important than top-\ndown processing in distinguishing between different\nproficiency levels.\nWith the increasing use of technology (such as\nmulti-media and computers) in testing, researchers\nhave begun to address the issue of how visual infor-\nmation affects listening comprehension and test per-\nformance. Gruba (1997) discusses the role of video\nmedia in listening assessment, considering how the\nprovision of visual information influences the defi-\nnition and purpose of the assessment instrument.\nGinther (forthcoming) in a study of the listening \nsection of the computer-based TOEFL (CBT), has\nestablished that listening texts tend to be easier if\naccompanied by visual support that complements\nthe content, although the converse is true of visual\nsupport that provides context. The latter seems to\nhave the effect of slightly distracting the listener from\nthe text, an effect that is more pronounced with low\nproficiency test takers. Though her results are indi-\ncative rather than conclusive, this finding has led\nGinther to suggest that high proficiency candidates\nare better able to overcome the presence of context\nvisuals.\nThe increasing use of computer technology is also\nmanifest in the use of corpora to develop listening\nprototypes, bringing with it new concerns. Douglas\nand Nissan (2001) explain how a corpus of North\nAmerican academic discourse is providing the basis\nfor the development of prototype test tasks as part of\na major revision of the TOEFL. Far from providing\nthe revision project team with recorded data that\ncould be directly incorporated into test materials,\ninspection of the corpus foregrounded a number of\nconcerns related to the use of authentic materials.\nThese include considerations such as whether \nspeakers refer to visual material that has not been\nrecorded in the corpus text, whether the excerpt\nmeets fairness and sensitivity guidelines or requires\nculture-specific knowledge.The researchers empha-\nsise that not all authentic texts drawn from corpora\nare suitable for listening tests, since input texts need\nto be clearly recorded and the input needs to be\ndelivered at an appropriate speed.\nA crucial issue is how best to operationalise the\nconstruct of listening for a particular testing context.\nDunkel et al. (1993) propose a model for test specifi-\ncation and development that specifies the person,\ncompetence, text and item domains and compo-\nnents. Coombe et al. (1998) attempt to provide cri-\nteria by which English for Academic Purposes\npractitioners can evaluate the listening tests they cur-\nrently use, and they provide micro-skill taxonomies\ndistinguishing general and academic listening. They\nalso highlight the significance of factors such as \ncultural and background knowledge and discuss the\nimplications of using test methods that mimic \nreal-life authentic communicative situations rather\nthan \u2018indirect\u2019, discrete-point testing. Buck (2001)\nencourages us to think of the construct both in\nterms of the underlying competences and the nature\nof the tasks that listeners have to perform in the real\nworld. Based on his own research (see Buck, 1994),\nhe warns that items typically require a variety of\nskills for successful performance and that these can\ndiffer between test takers. He argues, therefore, that it\nis difficult to target particular constructs with any\nsingle task and that it is important to have a range of\ntask types to reflect the construct.\nClearly there is a need for test developers to con-\nsider their own testing situation and to establish\nwhich construct and what operationalisation of that\nconstruct is best for them.The foregoing discussion,\nlike other overviews of listening assessment (Buck,\n1997, Brindley, 1998), draws particular attention to\nthe challenges of assessing listening, in particular the\nlimits of our understanding of the nature of the con-\nstruct. Indeed, as Alderson and Bachman comment\nin Buck (2001):\nThe assessment of listening abilities is one of the least under-\nstood, least developed and yet one of the most important areas of\nlanguage testing and assessment. (2001: x \u2013 series editors\u2019 preface)\nAssessing grammar and vocabulary\nGrammar and vocabulary have enjoyed rather differ-\nent fortunes recently.The direct testing of grammar\nhas largely fallen out of favour, with little research\ntaking place (exceptions are Brown & Iwashita, 1996\nand Chung, 1997), while research into the assessment\nof vocabulary has flourished. Rea-Dickins (1997,\n2001) attributes the decline in the direct testing of\ngrammar to the interest in communicative language\nteaching, which has led to a diminished role for\ngrammar in teaching and consequently testing. She\nalso suggests that changes in the characterisation of\nlanguage proficiency for testing purposes have con-\ntributed to the shift of focus away from grammar.\nInstead, assessment tasks have been developed that\nreflect the target language use situation; such was the\ncase of the English as a Second Language Placement\nExamination (ESLPE) which, when it was revised,\nwas designed to assess the test takers\u2019 ability to\nunderstand and use language in academic contexts\n(see Weigle & Lynch, 1995). Furthermore, even in\nn\nLanguage testing and assessment (Part 2)\n89\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\ncases where a conscious effort has been made to\ndevelop a grammar test, for example when the\nIELTS test was being developed, a grammar test has\nnot been found to contribute much additional infor-\nmation about the test takers\u2019 language proficiency.\nIndeed, Alderson (1993) found that the proposed\nIELTS grammar sub-test correlated to some degree\nwith all of the other sub-tests and correlated particu-\nlarly highly with the reading sub-test.The grammar\ntest was therefore dropped, to save test taker time.\nThis overlap could be explained by the fact that\ngrammar is assessed implicitly in any assessment of\nlanguage skills e.g., when raters assign a mark for\naccuracy when assessing writing. Moreover, as Rea-\nDickins (2001) explains, grammar testing has evolved\nsince its introduction in the 1960s. It now encom-\npasses the understanding of cohesion and rhetorical\norganisation as well as the accuracy and appropriacy\nof language for the task. Task types also vary more\nwidely and include gap-filling and matching exer-\ncises, modified cloze and guided summary tasks.As a\nconsequence, both the focus and the method of\nassessment are very similar to those used in the\nassessment of reading, which suggests that Alderson\u2019s\n(1993) findings are not, in retrospect, surprising.\nCertainly (as was the case with the IELTS test), if\ndropping the grammar sub-test does not impact sig-\nnificantly on the reliability of the overall test, it\nwould make sense to drop it. Eliminating one sub-\ntest would also make sense from the point of view of\npracticality, because it would shorten the time taken\nto administer the full test battery. And, as Rea-\nDickins (2001) has demonstrated, the lack of explicit\ngrammar testing does not imply that grammar will\nnot be tested. Grammatical accuracy is usually one of\nthe criteria used in the assessment of speaking and\nwriting, and grammatical knowledge is also required\nin order successfully to complete reading items that\nare intended to measure test takers\u2019 grasp of details, of\ncohesion and of rhetorical organisation.\nThe assessment of vocabulary has been a more\nactive field recently than the assessment of grammar.\nFor example, the DIALANG diagnostic testing \nsystem, mentioned in Part One, uses a vocabulary\nsize test as part of its procedure for estimating test\ntakers\u2019 proficiency level in order to identify the\nappropriate level of test to administer to a test taker,\nand in addition offers a separate module testing vari-\nous aspects of vocabulary knowledge.\nAn active area of research has been the develop-\nment of vocabulary size tests. These tests are\npremised on the belief that learners need a certain\namount of vocabulary in order to be able to operate\nindependently in a particular context.Two different\nkinds of vocabulary size test have been developed.\nThe Vocabulary Levels Test, first developed by Nation\n(1990), requires test takers to match a word with its\ndefinition, presented in multiple-choice format in\nthe form of a synonym or a short phrase.Words are\nranked into five levels according to their frequency\nof occurrence and each test contains 36 words at\neach level. The other type of vocabulary size test\nemploys a different approach. Sometimes called the\nYes\/No vocabulary test, it requires test takers simply\nto say which of the words in a list they know. The\nwords are sampled according to their frequency of\noccurrence and a certain proportion of the items are\nnot real words in the target language.These pseudo-\nwords are used to identify when a test taker might be\nover-reporting their vocabulary knowledge (see\nMeara & Buxton, 1987 and Meara, 1996).Versions of\nthis test have been written for learners of different\ntarget languages such as French learners of Dutch\n(Beeckmans et al., 2001) and learners of Russian and\nGerman (Kempe & MacWhinney, 1996), and\nDIALANG has developed Yes\/No tests in 14\nEuropean languages.\nSince these two kinds of test were first developed,\nthey have been the subject of numerous modifications\nand validation. Knowing that Nation\u2019s Vocabulary\nLevels tests (Nation, 1990) had not undergone thor-\nough statistical analysis, Beglar and Hunt (1999)\nthought it unlikely that the different forms of these\ntests would be equivalent.Therefore, they took four\nforms of each of the 2000 Word Level and University\nWord Level tests, combining them to create a 72-\nitem 2000 Word Level test and a 72-item University\nWord Level test which they then administered to\n496 Japanese students. On the basis of their results,\nthey produced and validated revised versions of these\ntwo tests. Schmitt et al. (2001) also sought to validate\nthe four forms of the Vocabulary Levels Test (three of\nwhich had been written by the main author). In a\nvariation on Beglar and Hunt\u2019s (1999) study, they\nattempted to gather as diverse a sample of test takers\nas possible. The four forms were combined into \ntwo versions and these were administered to 801 \nstudents. The authors conclude that both versions \nprovide valid results and produce similar, if not\nequivalent, scores.\nWhen examining the Yes\/No vocabulary test,\nBeeckmans et al. (2001) were motivated by a con-\ncern that many test takers selected a high number of\npseudo-words and that these high \u2018false alarm\u2019 rates\nwere not restricted to weak students. There also\nseemed to be an inverse relationship between identi-\nfication of \u2018real\u2019 words and rejection of pseudo-\nwords. This is counter-intuitive, since one would\nexpect a test taker who is able to identify \u2018real\u2019 words\nconsistently also to be able to reject most or all \npseudo-words. They administered three forms of a\nYes\/No test of Dutch vocabulary (the forms differed\nin item order only) to 488 test takers.Their findings\nled them to conclude that the Yes\/No format is\ninsufficiently reliable and that the correction pro-\ncedures cannot cope with the presence of bias in \nthe test-taking population.\nWhat is not clear from this research, however, is\nLanguage testing and assessment (Part 2)\nn\n90\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nwhat counts as a word and what vocabulary size is\nenough.Although nobody would claim that vocabu-\nlary size is the key to learners\u2019 language needs (they\nalso need other skills such as a grasp of the structure\nof the language), there is general agreement that\nthere is a threshold below which learners are likely\nto struggle to decode the input they receive.\nHowever, there is little agreement on the nature of\nthis threshold. For instance, Nation (1990) and\nLaufer (1992, 1997) argue that learners at university\nlevel need to know at least 3000 word families.Yet\nNurweni and Read (1999) estimate that university\nlevel students only know about 1200 word families.\nApart from concerns about how to measure\nvocabulary size and how much vocabulary is\nenough, researchers have also investigated the depth\nof learners\u2019 word knowledge i.e., how well words are\nknown. Traditionally, this has been studied through\nindividual interviews where learners provide expla-\nnations of words which are then rated by the\nresearcher. One example of this approach is a study\nby Verhallen and Schoonen (1993) of both Dutch\nmonolingual and bilingual Turkish immigrant chil-\ndren in the Netherlands. They elicited as many\naspects as possible of the meaning of six Dutch words\nand report that the monolingual children were able\nto provide more extensive and varied meanings than\nthe bilingual children. However, such a methodology\nis time-consuming and restricts researchers to small\nsample sizes. Results are also susceptible to bias as a\nresult of the interview process and so other\napproaches to gathering such information have been\nattempted. Read (1993) reports on one alternative\napproach. He devised a written test in which test\nitems comprise a target word plus eight others.The\ntest takers\u2019 task is to identify which of the eight\nwords are semantically related to the target word\n(four in each item). This approach is, however, sus-\nceptible to guessing and Read suggests that a better\nalternative might be to require test takers to supply\nthe alternatives.\nThis challenge has been taken up by Laufer et al.\n(2001), who address three concerns. The first is\nwhether it is enough merely to establish how many\nwords are known. The second is how to accurately\n(and practically) measure depth of vocabulary\nknowledge. The third concern is how to measure\nboth receptive and productive dimensions of vocab-\nulary knowledge. Laufer et al. designed a test of\nvocabulary size and strength. Size was operationalised\nas the number of words known from various word\nfrequency levels. Strength was defined according to a\nhierarchy of depth of word-knowledge beginning\nfrom the easiest \u2013 receptive recognition (test takers\nidentify words they know) \u2013 and proceeding to the\nmost difficult \u2013 productive recall (test takers have to\nproduce target words).They piloted the test on 200\nadult test takers, paying attention to two questions\nthat they considered key to establishing the validity\nof the procedure.The first had to do with whether\nthe assumed hierarchy of depth of word-knowledge\nis valid, i.e., does recall presuppose recognition and\ndoes production presuppose reception? Second, how\nmany items does a test taker need to answer correctly\nat any level before they can be considered to have\nadequate vocabulary knowledge at that level? They\nconsider the results so far to be extremely promising\nand hope eventually to deliver the test in computer-\nadaptive form.\nBut such studies involve the isolation of vocabu-\nlary knowledge in order to measure it in detail.There\nhave also been efforts to assess vocabulary knowledge\nmore globally, including the attempts of Laufer and\nNation (1999) to develop and validate a vocabulary-\nsize test of controlled productive ability. They took\nvocabulary from the five frequency levels identified\nby Nation (1990) and constructed completion item\ntypes in which a truncated form of the word is pre-\nsented in a short sentence.Test takers are expected to\nbe able to use the context of the sentence to com-\nplete the word.The format bears some resemblance\nto the C-test (see Grotjahn, 1995) but has two key\ndifferences. The words are presented in single sen-\ntences rather than paragraphs and instead of always\npresenting the first half of the word being tested,\nLaufer and Nation include the minimal number of\nletters required to disambiguate the cue.They argue\nthat this productive procedure allows researchers to\nlook more effectively at breadth of vocabulary\nknowledge and is a useful complement to receptive\nmeasures of vocabulary size and strength.\nOther approaches to measuring vocabulary glob-\nally have involved calculating the lexical richness of\ntest takers\u2019 production (primarily writing), using\ncomputerised analyses, typically of type\/token ratios\n(see Richards & Malvern, 1997 for an annotated bib-\nliography of research in this area). However,Vermeer\n(2000), in a study of the spontaneous speech data of\nfirst and second language children learning Dutch,\nargues that these measures have limitations. His data\nshows that at early stages of vocabulary acquisition\nmeasures of lexical richness seem satisfactory.\nHowever, as learners progress beyond 3000 words,\nthe methods currently available produce lower esti-\nmations of vocabulary growth. He suggests, there-\nfore, that researchers should look not only at the\ndistribution of and relation between types and tokens\nused but also at the level of difficulty of the words\nused.\nResearch has also been conducted into the vocab-\nulary sections in test batteries. For instance, Schmitt\n(1999) carried out an exploratory study of a small\nnumber of TOEFL vocabulary items, administering\nthem to 30 pre-university students and then ques-\ntioning the students about their knowledge of the\ntarget words\u2019 associations, grammatical properties,\ncollocations and meaning senses. His results suggest\nthat the items are not particularly robust as measures\nn\nLanguage testing and assessment (Part 2)\n91\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nof association, word class and collocational knowl-\nedge of the target words. On the basis of this\nresearch, Schmitt argues for more investigation of\nwhat vocabulary items in test batteries are really\nmeasuring. Takala and Kaftandjieva (2000) adopt a\ndifferent focus, looking at whether gender bias could\naccount for differential item functioning (DIF) in the\nvocabulary section of the Finnish Foreign Language\nCertificate Examination.They report that the test as\na whole is not gender-biased but that particular\nitems do indicate DIF in favour of either males or\nfemales. They argue that these results have implica-\ntions for item-banking and call for more research,\nparticularly into whether it is practically (as opposed\nto theoretically) possible for a test containing DIF\nitems to be bias-free.\nHowever, all the work we have reported focuses\non vocabulary knowledge (whether it be size,\nstrength or depth), which is, as Chapelle (1994, 1998)\nhas argued, a rather narrow remit. She calls for a\nbroader construct, proposing a model that takes into\naccount the context of vocabulary use, vocabulary\nknowledge and metacognitive strategies for vocabu-\nlary use. Thus, in addition to refining our under-\nstanding of vocabulary knowledge and the\ninstruments we use to measure it (see Meara, 1992\nand 1996 for research into psycholinguistic measures\nof vocabulary) it is necessary to explore ways of\nassessing vocabulary under contextual constraints\nthat, Read and Chapelle (2001:1) argue,\u2018are relevant\nto the inferences to be made about lexical ability\u2019.\nAssessing speaking\nThe testing of speaking has a long history (Spolsky,\n1990, 1995, 2001) but it was not until the 1980s that\nthe direct testing of L2 oral proficiency became\ncommonplace, due, in no small measure, to the inter-\nest at the time in communicative language teaching.\nOral interviews, of the sort developed by the \nForeign Service Institute (FSI) and associated US\nGovernment agencies (and now known as OPIs \u2013\nOral Proficiency Interviews) were long hailed as\nvalid direct tests of speaking ability. Recently, how-\never, there has been a spate of criticisms of oral inter-\nviews, which have in their turn generated a number\nof research studies. Discourse, conversation and con-\ntent analyses show clearly that the oral proficiency\ninterview is only one of the many possible genres of\noral test tasks, and the language elicited by OPIs is\nnot the same as that elicited by other types of task,\nwhich involve different sorts of power relations and\nsocial interaction among interactants.\nSome researchers have attempted to reassure scep-\ntics about the capacity of oral tests to sample sufficient\nlanguage for accurate judgements of proficiency\n(Hall, 1993) and research has continued into indirect\nmeasures of speaking (e.g., Norris, 1991). In addi-\ntion, a number of studies document the development\nof large-scale oral testing systems in school and uni-\nversity settings (Gonzalez Pino, 1989; Harlow &\nCaminero, 1990; Walker, 1990; Lindblad, 1992;\nRobinson, 1992).An influential set of guidelines for\nthe assessment of oral language proficiency was \npublished in 1986 by the American Council on the\nTeaching of Foreign Languages (the ACTFL guide-\nlines \u2013 ACTFL, 1986). This was followed by the\nintroduction of the widely influential ACTFL Oral\nProficiency Interview (ACTFL OPI).\nThis increase in the testing of speaking was\naccompanied by a corresponding expansion of\nresearch into how speaking might best be assessed.\nThe first major subject of this research was, not \nsurprisingly, the ACTFL OPI and there have been a\nnumber of studies investigating the construct validity\nof the test (e.g., Raffaldini, 1988; Valdes, 1989;\nDandonoli & Henning, 1990; Henning, 1992b;\nAlonso, 1997); the validity of the scores and rating\nscale (e.g., Meredith, 1990; Halleck, 1992; Huebner\n& Jensen, 1992; Reed, 1992; Marisi, 1994; Glisan &\nFoltz, 1998); and rater behaviour and performance\n(Barnwell, 1989; Thompson, 1995). Conclusions\nhave varied, with some researchers arguing for the\nusefulness and validity of the OPI and its accompa-\nnying rating scale and others criticising it for its lack\nof theoretical and empirical support (e.g., Bachman\n& Savignon, 1986; Shohamy, 1990b; Salaberry, 2000).\nFulcher (1997b: 75) argues that speaking tests are\nparticularly problematic from the point of view of\nreliability, validity, practicality and generalisability.\nIndeed, underlying the debate about the ACTFL\nOPI are precisely these concerns.\nQuestions about the nature of oral proficiency,\nabout the best way of eliciting it, and about the eval-\nuation of oral performances have motivated much\nresearch in this area during the last decade.The most\nrecent manifestation of this interest has been a joint\nsymposium between the Language Testing Research\nColloquium (LTRC) and the American Association\nof Applied Linguistics (AAAL) held in February\n2001 which was devoted to the definition and assess-\nment of speaking ability.The LTRC\/AAAL sympo-\nsium encompassed a range of perspectives on\nspeaking, looking at the mechanical aspects of speak-\ning (de Bot, 2001), the sociolinguistic and strategic\nfeatures of speaking ability (Bachman, 2001; Conrad,\n2001; Selinker, 2001; Swain, 2001b;Young, 2001) and\nthe implications for task design of the context-\ndependent nature of speaking performance (Liskin-\nGasparro, 2001).\nThe most common mode of delivery of oral \nproficiency tests is the face-to-face oral proficiency\ninterview (such as the ACTFL OPI). Until the 1990s\nthis took the form of a one-to-one interaction\nbetween a test taker and an interlocutor\/examiner.\nHowever, this format has been criticised because the\nasymmetrical relationship between the participants\nresults in reduced or no opportunities for genuine\nLanguage testing and assessment (Part 2)\nn\n92\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nconversational interaction to occur. Discussions of\nthe nature of oral proficiency in the early 1990s (van\nLier, 1989 and Lazaraton, 1992) focused on the \nrelationship between OPIs and non-test discourse.\nQuestioning the popular belief that an oral profi-\nciency interview (OPI) is a \u2018structured conversational\nexchange\u2019, van Lier asked two crucial questions: first,\nhow similar is test taker performance in an OPI to\nnon-interview discourse (conversation), and second,\nshould OPIs strive to approximate conversations?\nHis view was that OPIs frequently do not result in\ndiscourse resembling conversational exchange (a\nview shared by Lazaraton, 1992, Chambers &\nRichards, 1995 and Kormos, 1999) and that\nresearchers need to think carefully about whether\noral proficiency is best displayed through conversa-\ntion. Johnson and Tyler (1998) take a single OPI that\nis used to train OPI testers and analyse it for features\nof naturally occurring conversation.They report that\nthis OPI lacks features typical of normal conversa-\ntion. For instance, the turn-taking is more structured\nand predictable with longer turns always being taken\nby the test taker. Features of topic nomination and\nnegotiation differ as well and the test taker has no\ncontrol over the selection of the next speaker. Finally,\nthe tester tends not to react to\/add to the test taker\u2019s\ncontributions, and this contributes to the lack of\ncommunicative involvement observed. Egbert\n(1998), in her study of German OPIs, also suggests\nthat repair is managed differently. The repair strat-\negies expected are more cumbersome and formal\nthan would occur in normal German native-speaker\nconversation. However, Moder and Halleck (1998)\nchallenge the relevance of this, asking whether the\nlack of resemblance between an OPI and naturally\noccurring conversation is important. They suggest\ninstead that it should be viewed as a type of in-\nterview, arguing that this is an equally relevant com-\nmunicative speech event.\nResearchers have sought to understand the nature\nof the OPI as a communicative speech event from a\nnumber of different perspectives. Picking up on\nresearch into the linguistic features of the question-\nanswer pair (Lazaraton, 1992; Ross & Berwick,\n1992), He (1998) looks at answers in the question-\nanswer pair. She focuses specifically on a failing \nperformance, looking for evidence in the test taker\u2019s\nanswers that were construed as indicating a limited\nlanguage proficiency. She identifies a number of fea-\ntures including an unwillingness to elaborate, pauses\nfollowing questions, and wrong and undecipherable\nresponses.Yoshida-Morise (1998) investigates the use\nof communication strategies by Japanese learners of\nEnglish and the relationship between the strategies\nused and the learners\u2019 level of proficiency. She reports\nthat the number and nature of communication\nstrategies used varies according to the proficiency of\nthe learner. Davies (1998), Kim and Suh (1998), Ross\n(1998), and Young and Halleck (1998) look at the\nOPI as a cross-cultural encounter. They discuss the\nconstruction of identity and maintenance of face, the\neffect of cultural assumptions on test taker behaviour\nand how this might in turn be interpreted by the\ninterlocutor\/examiner.\nResearchers have also been concerned about the\neffect of the interlocutor on the test \u2018experience\u2019\nsince any variation in the way tasks are presented to\ntest takers might impact on their subsequent perfor-\nmance (Ross, 1992; Katona, 1996; Lazaraton, 1996;\nBrown & Hill, 1998; O\u2019Loughlin, 2000), as might the\nfailure of an interlocutor to exploit the full range \nof a test taker\u2019s ability (Reed & Halleck, 1997).\nAddressing the concern that the interlocutor might\nhave an effect on the amount of language that candi-\ndates actually produce, a study by Merrylees and\nMcDowell (1999) found that the interviewer typi-\ncally speaks far more than the test taker.Taking the\nview that the interview format obscures differences\nin the conversational competence of the candidates,\nKormos (1999) found that the conversational inter-\naction was more symmetrical in a guided role-play\nactivity.Yet even the guided role-play is dependent\non the enthusiasm with which the interlocutor\nembraces the spirit of the role-play, as research by\nBrown and Lumley (1997) suggests. As a result of\ntheir work on the Occupational English Test (OET)\nthey report that the more the interlocutor identifies\nwith the role presented (rather than with the test\ntaker), i.e., the more genuinely s\/he plays the part\nrequired by the role play, the more challenging the\ninteraction becomes for the test taker. Katona (1998)\nhas studied meaning negotiation, considering the\neffect of familiarity with the interlocutor on the way\nin which meaning is negotiated between the partici-\npants. She concludes that both the frequency and\ntype of negotiation differ according to whether the\ninterlocutor is known or unfamiliar to the test taker:\nif the interlocutor is unknown to the test taker, this is\nmore likely to result in misunderstandings and the\ndiscourse is more artificial and formal.\nOther variations on the format of the OPI include\nthe assessment of pair and group activities, as well as\nthe inclusion of more than one examiner. For exam-\nple, the oral components of the Cambridge main\nsuite of tests have gradually been revised to adopt a\npaired format with two examiners (Saville &\nHargreaves, 1999). It is argued that the paired format\nallows for a variety of patterns of interaction (i.e.,\nexaminer-examinee(s), examinee(s)-examiner, and\nexaminee-examinee) and that assessment is fairer\nwith two examiners (typically a holistic judgement\nfrom the interlocutor\/examiner and an analytic score\nfrom the silent assessor). Ikeda (1998) experimented\nwith a paired learner interview in which learners\ntake both the role of the interviewer and intervie-\nwee, and he claims that such a format is effective in\nreducing communicative stress for the participants \nas well as in eliciting authentic participation.\nn\nLanguage testing and assessment (Part 2)\n93\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nNevertheless, there are concerns about the paired\nformat, particularly with respect to the relationship\nbetween the test takers (Foot, 1999) and the effect of\ntest taker characteristics on test performance (e.g.,\nYoung, 1995; Berry, 1997; Morton, 1998). Research\nconducted outside the field of testing also has impli-\ncations for oral assessment. Swain (2001a: 275),\nreporting on a study of 13-14 year olds in French\nimmersion classes, argues that \u2018in a group, perfor-\nmance is jointly constructed and distributed across\nthe participants. Dialogues construct cognitive and\nstrategic processes which in turn construct student\nperformance\u2019. This raises the question as to whose\nperformance is being assessed in group oral tests, and\nsuggests that it may not be fair to assign scores to\nindividuals in group assessment.\nThere has been considerable interest in developing\nOPIs that are delivered by other modes such as tape-\nmediated tests (also called \u2018simulated oral proficiency\ntests\u2019 \u2013 SOPIs) in language laboratories (Osa-Melero\n& Bataller, 2001); via video teleconferencing (Clark\n& Hooshmand, 1992); over the telephone, as in the\nPhonePass test (www.ordinate.com) and the FBI\u2019s\nmodified oral proficiency test (MOPI \u2013 Cascallar,\n1997); as well as by computer (Kenyon et al., 2001;\nStauffer & Kenyon, 2001; Strong-Krause, 2001).The\nuse of technology is attractive for the flexibility it\naffords in the testing process.Tape-mediated tests, for\ninstance, make it possible to test large numbers of\nstudents at the same time, while telephone and\nvideo-teleconferencing enable testing to take place\neven when the test taker and the assessor(s) are in\ntwo or more locations. Computer-based tests, particu-\nlarly those that are computer-adaptive, offer possi-\nbly the best opportunity for a speaking test to be\ntruly sensitive to a test taker\u2019s performance at each\nstage of the test. Furthermore, both tape-mediated\nand computer-based tests, by removing the human\ninterlocutor, offer the guarantee that each test taker\nwill receive the same test, i.e., the task instructions\nand support will be identical in every administration\n(e.g., Stansfield & Kenyon, 1992).\nThese innovations have not been unproblematic,\nhowever, since there are inevitably questions about\nthe effect of the mode of test delivery on test taker\nperformance. In addition, researchers have sought to\nexplore the comparability of scores achieved and \nthe language generated in SOPIs in contrast with\nOPIs. Research into the tape-mediated SOPI (see\nStansfield & Kenyon, 1992; Shohamy, 1994;\nO\u2019Loughlin, 1995; Kuo & Jiang, 1997) has shown\nthat, while test takers\u2019 scores for direct and semi-\ndirect tests (OPI and SOPI) are comparable, the\nnumber and types of functions and topics covered by\nthe elicitation tasks in the two modes vary. In addi-\ntion, the language samples obtained differ in terms of\nthe communicative strategies displayed and in dis-\ncourse features such as lexical density. The different\ninteractions do appear to yield different language and\ntherefore reveal different aspects of the test takers\u2019\noral proficiency. Consequently, the OPI and SOPI\nare not considered to be easily interchangeable and\ntest developers are encouraged to select the mode of\ndelivery according to their specific testing needs.\nMode of delivery aside, however, researchers have\nbeen concerned with what test takers are asked to do\nin the oral proficiency test and the effect that this has\non their performance. Part of this concern has to do\nwith the plausibility of the task set (Chambers &\nRichards, 1995) but also with the capacity of the\ntask(s) to reflect underlying speaking ability (Fulcher,\n1996a and Pavlou, 1997).Yet another aspect of con-\ncern is task difficulty. In her work on semi-direct\ntests, Wigglesworth (1997) manipulated planning\ntime in order to investigate differences in the com-\nplexity and accuracy of the elicited discourse. She\nconcludes that the influence of planning time\ndepends on the proficiency level of the examinee\nsuch that, when presented with a cognitively chal-\nlenging task, high-proficiency candidates are more\nlikely to produce more accurate answers when given\nmore planning time. Low-proficiency candidates do\nnot appear to benefit similarly.Whether test develop-\ners are able correctly to judge the cognitive challenge\nof the tasks they set has been challenged by Norris \net al. (1998), who argue that the factors currently\nbelieved to affect task difficulty are hypothetical\nrather than empirically derived.Weir et al. (2001) are\ncurrently trying to fill this gap in research by devel-\noping an analytic framework of the variables within a\ntask that contribute to task difficulty. They believe\nthat, while the effect of interlocutor-related variables\nwill be less easily predicted, the framework will offer\na means of estimating the effect of psycholinguistic\naspects of task difficulty.\nIt is perhaps less clear how the effect of task diffi-\nculty manifests itself in test scores. Douglas (1994)\nhypothesises that similar scores represent qualitatively\ndifferent performances. Subsequent investigations by\nPavlou (1997) and Meiron and Schick (2000) have\nborne out this hypothesis, finding that even where\ntest takers receive ratings that are not significantly\ndifferent, the underlying performances are different\nin their discourse.This may not in itself be problem-\natic unless the qualitative differences in the perfor-\nmances are deemed important indicators of oral\nproficiency. If qualitatively different performances are\nassigned the same score then there might well be a\nproblem with the rating criteria used or with how\nthey are interpreted and implemented by raters. A\nrecent study by Lumley and Qian (2001) of the fea-\ntures of test taker performance that account for the\nratings assigned on two language tests indicates that\nperceptions of grammatical accuracy seem to have\nthe strongest influence on scores.\nThe assessment criteria written into oral pro-\nficiency scales, as well as raters\u2019 interpretations of\nthem, are long-standing concerns for test developers\nLanguage testing and assessment (Part 2)\nn\n94\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nand researchers since the validity of interpretations of\nability depends on the criteria used to rate perfor-\nmance. Part of the problem with rating criteria has\nbeen that they have tended to be a priori construc-\ntions developed by proclaimed experts.This view, as\nwell as the practice of using generic scales (i.e., scales\nthat are intended to be used with any task, rather\nthan scales specific to particular tasks), has been com-\nprehensively challenged in recent years (Fulcher,\n1993; Chalhoub-Deville, 1995; North, 1995; Upshur\n& Turner, 1995; Fulcher, 1996b; Upshur & Turner,\n1999; Walsh, 1999; Taylor & Jones, 2001). Recently,\nrating and reporting scales have often been empiri-\ncally derived either partially or wholly from a sample\nof actual task performances (Fulcher, 1993; Upshur\n& Turner, 1995, 1999; Douglas & Myers, 2000;\nBrown et al., 2001;Taylor & Jones, 2001).\nOnce the criteria have been developed, it is\nimportant to develop suitable training procedures \nfor raters (for example,Wigglesworth, 1993) and rig-\norous re-accreditation procedures (e.g., Lumley &\nMcNamara, 1995). However, training is not enough\nand the rating process also needs to be monitored.\nThere has as a result been systematic investigation of\nthe factors that could influence the ratings assigned,\nalbeit with mixed conclusions. While neither the\nsubject expertise of the rater (Lumley, 1998) nor the\ngender of the test taker in relation to the interlocutor\n(O\u2019Loughlin, 2000) seems to impact on ratings, the\ngender and professional standing of the test taker\ncould influence the rating assigned (Ferguson, 1994).\nAlso, although overall scores might not be affected by\nvariables such as the occupational and linguistic\nbackground of raters, ratings on individual criteria\nhave been found to vary significantly according to\nsuch variables (Brown, 1995). Comparing the rating\nof audio-recordings of speaking performances with\nratings of live performances, it has been found that\nraters underestimate the scores of more proficient\ncandidates when they only have access to audio data\n(Nambiar & Goon, 1993). Moreover, while poorly\nrecorded performances tend to be judged more\nharshly, performances in which the interlocutor is\ndeemed to be less than competent are judged more\nleniently (McNamara & Lumley, 1997). These and\nother findings (summarised in Reed & Cohen, 2001)\nhave implications for rater training as well as for rater\nselection and the development of assessment proce-\ndures.\nDespite the research activity which the field has\nseen, the continuing volume of work in this area\nindicates that the speaking construct is still not fully\nunderstood.As Upshur and Turner argue:\nthere is no theory of method to explain how particular aspects of\nmethod affect discourse and how these discourse differences are\nthen reflected in test scores \u2026 Nor is there a developed explana-\ntion of how rater and examinee characteristics interact with one\nanother and with discourse characteristics to yield ratings, or\nhow tasks relate to well functioning rating scales. (1999: 106)\nMore insights are constantly being drawn from areas\nsuch as applied linguistics, discourse analysis and \nsecond language acquisition (see Bachman & Cohen,\n1998) but there is clearly scope for more cross-\ndisciplinary research.\nAssessing writing\nAs both speaking and writing tests are examples of\nwhat has come to be known as performance testing,\nthe testing of writing ability has faced some of the\nsame problems as the testing of speaking \u2013 what cri-\nteria to use for the assessment of performance, how\nto ensure reliability of subjective marking, what sort\nof tasks will elicit the sort of language required.\nHowever, in the assessment of second and foreign\nlanguage writing ability, one answer to the problem\nof the subjectivity of essay marking was to seek to\ntest the ability by indirect means.Thus, until the late\n1970s, and even beyond into the 1980s, it was regard-\ned as acceptable to argue that an estimate of one\u2019s\nability to write extended prose could be gained from\nindirect, usually multiple-choice, tests of grammar,\ncohesion and coherence, and error detection. The\ncurrent practice of requiring extended writing in\norder to judge writing proficiency began in the late\n1970s, reflecting the now dominant view that \nwriting ability extends beyond vocabulary and gram-\nmar to include aspects of text discourse. This move\nfrom indirect testing of writing to direct testing \nwas encouraged by the communicative movement,\nresulting in writing tasks being increasingly realistic\nand communicative \u2013 of the sort that a test taker\nmight be expected to do in real life (e.g., letters,\nmemo, academic essays). This approach to writing\nassessment also required scoring to take account not\nonly of the specific characteristics of the test taker\u2019s\nvocabulary and grammar, but also the discourse\nstructure of the writing. The research that resulted\n(documented in surveys by Cumming, 1997 and\nKroll, 1998) has been, as in the case of speaking, pri-\nmarily concerned with the \u2018what\u2019 and the \u2018how\u2019 of\ntesting the skill, with questions related to the number\nand nature of composition tasks, the discourse of the\nwriting that test takers produce, and the design and\napplication of scoring procedures. (See Purves, 1992\nfor a discussion of these concerns in the light of a\ncomparative study of achievement in written com-\nposition initiated by the International Association for\nthe Evaluation of Educational Achievement). It is the\ndesign and application of scoring procedures that has\nproved the most problematic and it is this issue that\nwe address first.\nConcerns about appropriate scoring procedures\nfor the productive skills (both speaking and writing)\nhave been repeatedly raised over the last decade by\nresearchers working on both small- and large-scale\nexamination projects. Reinhard (1991) looked at the\nassessment of a single examination paper by a cross-\nn\nLanguage testing and assessment (Part 2)\n95\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nsection of English teachers in Lower Saxony, finding\nthat, despite the provision of standardised assessment\nrequirements for all final examination papers in the\ncountry, the paper was rated with grades ranging\nfrom \u2018excellent\u2019 to \u2018unsatisfactory\u2019. He uses his find-\nings to question the practice of double-marking but\nthese findings also throw into question the \u2018standards\u2019\nof assessment provided by the central education\nauthority as well as the training provided to teachers\nin using these criteria.\nAs with assessment criteria for speaking, there has\nbeen much discussion of the form and content of\nwriting scales. In particular, researchers have been\nconcerned with the design and validation of scoring\nschemes (e.g., Garrett et al., 1995;Wu, 1995; Chiang,\n1999), looking particularly at the relevance of scor-\ning criteria for the assessment context. For instance,\nGarrett et al. (1995) discuss the importance of audi-\nence awareness as a criterion in contexts where the\nwriter\u2019s judgement of their reader influences\nwhether and how some information is included.Wu\n(1995) argues for the use of a discourse analysis per-\nspective in a range of assessment contexts. Sasaki and\nHirose (1999) document their dissatisfaction with\nthe lack of a standard scale for rating Japanese L1\nwriting and the use in that context of rating scales\noriginally devised for the assessment of English as a\nforeign language (EFL).They describe the develop-\nment of an analytic rating scale for Japanese universi-\nty-level L1 writing that more accurately reflects the\ncriteria of importance in the assessment of Japanese\nL1 writing.To do so they first identified key criteria\nthat were thought to influence Japanese L1 writing\nassessment and then asked teachers to rank these\nassessment criteria according to their importance in\njudging the quality of writing. Sasaki and Hirose\u2019s\nanalysis of this ranking exercise resulted in a 6-cate-\ngory scale that, interestingly, bore little resemblance\nto the categories in the EFL scale. Indeed, the valida-\ntion exercise revealed that the new scale focused\nraters\u2019 attention on features of the text not empha-\nsised in the EFL scale. Sasaki and Hirose argue that\nthis finding reflects the particular criteria of rele-\nvance in the assessment of Japanese L1 writing and\nwarn against the unreflective use of rating scales not\noriginally designed for the context in which they are\nused.\nThis important point seems to run somewhat\ncontrary to the work by Hamp-Lyons and Henning\n(1991) who argue that not everyone has the\nresources to design and validate an instrument of\ntheir own and who investigate the validity of using a\nmultiple-trait scoring procedure to obtain commu-\nnicative writing profiles of adult non-native English\nspeakers in different assessment contexts from that\nfor which the rating scale was originally designed.\nThey applied the New Profile Scale (NPS) to essays\ntaken from contexts in which the essays were of dif-\nferent timed lengths, for different rhetorical purposes\nand written by students of different levels of educa-\ntional preparation, and found that the scale was high-\nly reliable. They concede, however, that it is less\ninformative at the level of the subscales, concluding\nthat the use of the NPS in new assessment contexts\nwould serve an educational rather than a statistical\npurpose.\nThere has also been research into the relative mer-\nits of analytic and holistic scoring schemes (e.g.,\nBacha, 2001). The latter, though considered easy to\napply, are generally deemed to result in less informa-\ntion about an individual performance and to have a\nlimited capacity to provide diagnostic feedback,\nwhile the former, though they provide more infor-\nmation about individual performances, can be time-\nconsuming to use.An interesting question is whether\nthe assessment criteria provided are used at all. In his\ninvestigation of raters\u2019 use of a holistic scoring scale,\nSakyi (2000) reports that not all raters focus on the\nscoring guide and he identifies four distinct rating\nstyles. Some raters focus on errors in the text, others\non the essay topic and presentation of ideas and yet\nothers simply assign a score depending on their per-\nsonal reaction to the text. Where raters consciously\nfollowed the scoring guide, they tended to depend\non one or two particular features to distinguish\nbetween different levels of ability. Summarising his\nfindings, Sakyi suggests that, in addition to features of\nthe text (content and language), the other factors\ninfluencing the score awarded are the raters\u2019 personal\nbiases and\/or their expectations and personal moni-\ntoring factors.\nIndeed, Salvi argues that reliable assessment has\noften seemed to be simply \u201cexpert \u2018guess-work\u2019\u201d\n(1991:67). Consequently, researchers have recently\ntried to understand better the rating process, as well\nas the nature of the expertise involved, by studying\nthe thinking processes used by raters to arrive at\njudgements and the assessment criteria that most\ninfluence the final score (e.g.,Vaughan, 1991;Astika,\n1993;Huot, 1993).\nThe behaviour that Sakyi described could be\nattributed to the relative expertise of the raters, their\nbackground and\/or the training they received, all of\nwhich have continued to be the subject of investiga-\ntion in the last decade (e.g., Cumming, 1990; Brown,\n1991; Shohamy et al., 1992; Schoonen et al., 1997).\nCumming (1990) describes the decision-making of\nexperienced and inexperienced raters during the\nassessment process, revealing 28 common decision-\nmaking behaviours, many of which differed signifi-\ncantly in use between the two groups. Brown (1991)\nreports a similar finding in a study of the rating\nbehaviour of subject specialists and ESL specialists.\nWhile there were no statistically significant mean\ndifferences in the ratings given by the two groups of\nraters, a feature analysis showed that they may have\narrived at their scores from different perspectives.\nSchoonen et al. (1997) report on the reliability of\nLanguage testing and assessment (Part 2)\nn\n96\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nexpert and lay assessors, finding that although both\ngroups of raters were reliable in their assessments of\nthe content of writing assignments, expert raters\ntended to be more reliable in their ratings of usage.\nThis presents an interesting contrast with Shohamy et\nal.\u2019s (1992) study in which they report that raters can\nassess reliably regardless of their expertise and previ-\nous training, arguing that on this basis responsible\nnon-teachers could be employed to assess writing\nsamples. It must be noted, however, that this research\nis based on a single writing assessment with a special-\nly designed rating scale. Schoonen et al.\u2019s report, on\nthe other hand, is a result of the analysis of three\nstudies in which raters assessed three kinds of writing\nassignments. They make the point that the differ-\nences in reliability between expert and lay assessors\nwas partly dependent on the type of writing task\nbeing assessed.The more structured the writing task\nand the scoring criteria, the more reliable the lay\nreaders were in their ratings of usage. Schoonen et al.\nconclude, therefore, that the effect of expertise is\ndependent on the writing task to be assessed and the\nscoring criteria provided. They also argue that the\neffect of expertise is most clearly felt in judgements\nof language usage.\nTraining is generally assumed to have a positive\neffect on reliability, and Weigle (1994) argues that\ntraining can clarify the intended scoring criteria for\nraters, modify their expectations of student writing\nand provide a reference group of other raters against\nwhom raters compare themselves. However, Weigle\n(1998) shows that these positive effects are more mani-\nfest in increased intra-rater reliability than in inter-rater\nreliability. The latter is harder to achieve and, despite\ntraining, inexperienced raters tended to be more severe\nin their assessments than experienced raters.\nResearch from the mid-1980s has shown that\nnon-native speakers tend to judge performances\nmore harshly than native speakers (Hill, 1997). More\nrecent research by Brown (1995), albeit in the testing\nof speaking, indicates that, when both native and\nnon-native speakers are trained, their judgements are\ncomparable. If anything, the non-native speakers are\nmore in agreement than native speakers. Further-\nmore, it might sometimes be logistically more practi-\ncal or theoretically useful to use non-native speakers\nas assessors as in the case of work by Hill (1997) in an\nEnglish proficiency test for Indonesia (EPTI). Hill\u2019s\nresearch differs from earlier research in that rather\nthan using a native-speaker ideal, the local non-\nnative variety was the criterion. As a consequence,\nHill reports an interesting twist to previous findings,\ndemonstrating that the non-native raters in this case\nwere less harsh than their native speaker counter-\nparts. This suggests that the crux of the matter \nlies not with the type of rater and the training they\nhave received but with the criterion measure applied\nand the interaction between the raters and that mea-\nsure.\nThis is the focus of research by Lumley (2000 and\nforthcoming). He traces the decision-making process\nof four experienced, trained and reliable raters as\nthey rate 48 written scripts from a language profi-\nciency test. His data show that the raters arrive at the\nassigned score by a similar process. However, it is less\nclear how they reconcile what appears to be a ten-\nsion between the scoring criteria and the quality of\nthe scripts. Indeed, the raters\u2019 task when assigning a\nscore is to reconcile their impression of text quality\nwith both specific features of the text and the word-\ning of the scoring criteria. The complexity of the\ntask is further exacerbated by the fact that scoring\ncriteria cannot be exhaustive in their description of a\nperformance on a particular aspect at any single\nlevel. His conclusion is that \u2018ratings and scales repre-\nsent \u2026 a set of negotiated principles which the raters\nuse as a basis for reliable action, rather than a valid\ndescription of language performance\u2019 (Lumley, forth-\ncoming).\nThis problem might be alleviated or eliminated if\ncurrent work into computer scoring comes to\nfruition. Burstein and Leacock (2001) provide a\nprogress report on the development of an \u2018e-rater\u2019,\ni.e., a computer-based scoring system for essays.This\nprovides a holistic score that has been found to have\nhigh levels of agreement with human raters. The\nfocus of ongoing research is on the provision of\ndetailed feedback that will support text revision.\nHowever, developing a computer-based system to\nprovide this feedback is complex.Work has begun at\nthe level of discourse structure and grammatical\naccuracy and, to develop this system, models have\nbeen built from samples of essays in which the\nhuman reader has annotated the discourse structures\nof the essays. Grammatical errors are identified based\non unexpected sequences of words and\/or part-of\nspeech tags. Testing of this procedure is promising,\nindicating that the computer-based system is able\nautomatically to identify features in new essays that\nsignal the discourse structure. Of course, the useful-\nness of the feedback provided depends on the sys-\ntem\u2019s ability to identify the precise error based on the\nunexpected sequences. Other questions also persist\nsuch as the capacity of the system to identify socio-\nlinguistic features of note.\nIn the meantime, work continues to identify fur-\nther the factors that influence human ratings. One\nsimple example is that of the effect of handwriting\nlegibility on test scores. It has long been claimed that\nthe quality of handwriting affects the final score\nawarded (Hamp-Lyons & Kroll, 1997). It is certainly\ntrue that raters comment on legibility (Vaughan,\n1991; Huot, 1993; Wolfe & Feltovich, 1994;\nMilanovic et al., 1996) but the effect on scores has\nnot been empirically established. Brown (2001)\ndescribes a controlled experiment in which 40 hand-\nwritten essays were typed and the resulting 80 essays\nwere rated using IELTS bandscales.The handwritten\nn\nLanguage testing and assessment (Part 2)\n97\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nessays were also judged for legibility by independent\nraters. The results indicate that the handwritten\nscripts were consistently marked higher than the\ntyped versions. These score differences were most\nmarked for the least legible scripts, indicating that\npoor handwriting, in fact, advantages students.\nThough research interest in rater behaviour and its\nconsequences for test scores has been dominant\nrecently, there has also been some interest in the\ndesign of test tasks and the factors affecting test taker\nperformance. Research into test tasks has been pri-\nmarily motivated by discomfort with the lack of fit\nbetween the demands of writing under test condi-\ntions and real-life writing demands. Cho\u2019s (2001)\nresearch addresses the criticism typically levelled\nagainst essay writing under timed conditions, i.e.,\nthat this does not reflect actual\/normal practice\nbecause revision and the opportunity to reflect on\none\u2019s writing is an essential aspect of the skill.\nHowever, though there have been concerted argu-\nments for the portfolio approach (e.g., Freeman &\nFreeman, 1992; Pierce & O\u2019Malley, 1992; Herman et\nal., 1993; O\u2019Malley & Pierce, 1996), the timed essay\ncontinues to predominate for logistical reasons. Cho\ndeveloped a workshop essay that is practical to\nadminister but still incorporates activities to facilitate\nreflection and revision. She argues that this approach\nis more valid and also more accurately reflects the\nwriting abilities of learners.\nAnother criticism of timed impromptu writing\ntests is the lack of topic choice offered to test takers\nand the possible lack of comparability of topics across\ndifferent test versions (Raimes, 1990). To boost test\nuser confidence in the writing tasks on the Test of\nWritten English (TWE), Kroll (1991) describes in\nsome detail the topic development process and the\nprocedures for reading and scoring the TWE.\nHowever, the stability of scores across test versions is\nnot necessarily confirmation of topic comparability.\nTest scores are the product of the whole measure-\nment process \u2013 the prompt, the raters, and the scor-\ning criteria and procedures. The prompts in\nparticular may generate a different range of language\nstructures \u2013 thus, essentially, testing different things\n(see Ginther & Grant, 1997). This might be further\ncomplicated by the interaction between topic and\ntest taker characteristics such as language proficiency,\nlanguage background and background knowledge\n(Tedick, 1990 and Ginther & Grant, 1997). And the\nissues become even more complex when the\nprompts being investigated are part of an integrated\ntest and involve a video recording and a reading pas-\nsage (as is the case in the Carleton Academic English\nTest,CAEL \u2013 see Jennings et al., 1999).\nHall (1991) studied the composing behaviours of\nwriters composing in different contexts. Having\nobserved a group of writers in both test and non-test\nsituations, he argues that the two writing contexts\nhad a discernible effect on the writing process. He\nreports variations in the complexity of the texts gen-\nerated under the two conditions, the allocation of\ntime to various composing activities, the writers\u2019\npausing behaviours and in the alterations they made\nwhile writing. Picking up on the reasonable implica-\ntion that assessment practices need to distinguish\nbetween writing problems and language problems,\nCumming (1990) establishes that raters do, in fact,\ntreat test takers\u2019 language proficiency and writing\nskills separately. However, with the move towards\ncomputer-based testing and the option currently\noffered to candidates for the TOEFL-CBT of com-\nposing their essay on-screen, research needs to\naddress the effect of mode. As part of the on-going\ndevelopment of a computer-based IELTS test, work\nhas begun to investigate how test takers write when\nthey handwrite and when they compose on-screen,\nsince writing by hand may result in different com-\nposing strategies and behaviours from writing on-\nscreen and in a different writing product.\nChallenges for the future\nFinally in this two-part review, we address a number\nof issues that are currently preoccupying the field,\nand which are likely to be the subject of debate, and\nhopefully research, for some time to come.These are:\nthe nature, role and effect of authenticity in language\ntests; how to design language tests; the tenability of\nthe traditional distinction between reliability and\nvalidity; and the validation of language tests. We\nbelieve that these present the same sort of challenge\nto language testers as does the Pandora\u2019s Box\nMcNamara identified in 1995, and which we have\ndescribed at the beginning of this second part,\nnamely the social nature of language communication\nand the implications for test design.\nAuthenticity\nSince the advent of communicative language testing\nin the 1970s, authenticity has been a concern in lan-\nguage testing and it has often been argued that if we\nwish to predict a candidate\u2019s ability to communicate\nin the real world, then texts and tasks should be as\nsimilar to that real world as possible. Bachman (1991)\nmakes a distinction between \u2018situational authenticity\u2019\nand \u2018interactional authenticity\u2019. Situational authen-\nticity, glossed as \u2018life-likeness\u2019, is held to involve some\ndegree of replication, in a test, of actual speech events\nin language use situations. In contrast, interactional\nauthenticity is \u2018a function of the extent and type of\ninvolvement of test takers\u2019 language ability in accom-\nplishing a test task\u2019 (op.cit: 91). Later, Bachman and\nPalmer (1996) consider authenticity to be a critical\nquality of language tests, alongside validity, reliability,\nconsequences, interactiveness and practicality. They\nseparate the notion of authenticity from that of\ninteractiveness and define authenticity as \u2018the degree\nLanguage testing and assessment (Part 2)\nn\n98\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nof correspondence of the characteristics of a given\nlanguage test task to the features of a TLU [target\nlanguage use]task\u2019 (1996:23).\nNevertheless, Bachman (1990) acknowledges the\ncomplexity of the issue and argues that authenticity\nis not an all-or-nothing affair: a test task could be\nhigh on situational authenticity and low on interac-\ntional authenticity, or vice versa. In other words,\n\u2018tasks would not necessarily be either authentic or\nnon-authentic but would lie on a continuum which\nwould be determined by the extent to which the\nassessment task related to the context in which it\nwould be normally performed in real life\u2019 (cited in\nLewkowicz, 2000: 48).As early as 1981,Alderson had\nreported discussions of the idea that since tests are\nauthentic speech events in themselves, they are not\nthe same sort of event as other communicative \nsituations and thus disauthenticate any attempt to\nreplicate other real-world settings. Candidates in \nlanguage tests are not concerned to communicate\ninformation, but are eager to display their language\nproficiency. Although more recent discussions have\nbecome more focused, they have been speculative or\ntheoretical, rather than being informed by empirical\nresearch findings. For example, Bachman and Palmer\n(1996) claim that authenticity has a strong effect on\ncandidates\u2019 test performance, but they do not provide\nsupporting evidence for this.\nHowever, Lewkowicz (1997, 2000) challenges this\nbelief, and reports a number of studies of authentici-\nty which result in some interesting findings. Firstly,\nshe found that students taking a number of different\nlanguage tests preferred more familiar, albeit less\nauthentic tests, like the TOEFL to less familiar tests\nthat related more closely to their proven needs for\nthe use of the foreign language (in academic con-\ntexts). Test authenticity was not an issue for the \nstudents: they were more concerned with the diffi-\nculty and familiarity of the tasks. Furthermore, there\nwas no evidence of any effect of authenticity on test\nperformance. In a second study, teacher judges could\nnot distinguish between authentic and inauthentic\/\nmodified oral and written texts. Thirdly, a study of\nexam board editing committees producing tests\nclaimed to be authentic to target language use situa-\ntions revealed that they frequently edited texts and tasks,\nand rarely appealed to the criterion of authenticity\nwhen deciding whether to change texts and tasks.A\nfourth study examined the effect of providing source\ntexts for students to base their writing on: it is often\nclaimed that the integration of reading and writing\ntasks makes writing tasks more authentic in terms of\ntarget language use needs. However, students given\nthe source texts did not produce better writing, and\nin fact some students were disadvantaged by copying\nlong chunks from the source texts.\nSpence-Brown (2001) describes theoretical and\npractical issues surrounding the use of authentic data\nin class-based assessment in a university-level\nJapanese course in Australia. Students were required\nto interview native speakers of Japanese outside the\nclassroom, and were assessed on the basis of their\ntape-recorded interviews, and their written reports.\nSpence-Brown describes the various approaches\nused by different students to the task, which included\nrehearsing the interview, editing the results, and\nengaging in spontaneous, but flawed, discourse.\nDescribing a number of task management strategies\nengaged in by students, she argues that the very act\nof assessment changes the nature of a potentially\nauthentic task and thus compromises authenticity.\nShe concludes that authenticity must be related to\nthe implementation of an activity, not to its design.\nClearly, more empirical research is needed before\nthe nature and value of \u2018authenticity\u2019 can be resolved,\nand we predict that the next decade will see much\nmore clarification of this area, hopefully based on\nfocused research studies.\nHow are we to design our tests?\nThe call for authenticity in test development, espe-\ncially in text selection and task design, has a long his-\ntory, but owes its origins to what Chapelle (1998),\nfollowing Messick (1989), calls the behaviourist per-\nspective on construct definition. (She defines a con-\nstruct as a \u2018meaningful interpretation of observed\nbehaviour\u2019 \u2013 ie test responses, 1998: 33). The scores\nfrom tests are interpreted by behaviourists as \u2018derived\nfrom responses made to carefully defined stimuli for\nthe purpose of predicting responses made to similar\nnaturally occurring stimuli found in vocational,\nacademic, and other settings\u2019 (Tryon, 1979:402, cited \nin Chapelle, 1998). \u2018Use of contextual factors to\nexplain performance consistency reflects a behav-\niourist approach to construct definition\u2019 (Chapelle,\n1998:39). In this approach to test design, one seeks\nvalidity and generalisability of score interpretation by\nrecreating what Bachman and Palmer (1996) call the\n\u2018target language use (TLU) situation\u2019. Needs analysis\nin the Munby tradition (Munby, 1978;Weir, 1983) or\ntask analysis in more recent writings (Bachman and\nPalmer, 1996) is the basis for test specifications: the\ntest designer analyses what future test takers have to\ndo in the real world and seeks to simulate that as\nclosely as possible in their test (bearing in mind that a\ntest can never be a replica of the real world). The\nHymesian features of context (setting, participants,\nends, art characteristics, instrumentality, communica-\ntive key, norms and genre) are identified in the TLU\nand replicated in the test.Thus, the onus is on the test\ndeveloper to show that test performance is a good\nsample of the behaviour that would occur in the real\nsetting. However, since authenticity is a problematic\nconcept, as we have seen, the important question is:\nto what extent can we indeed use TLU characteris-\ntics in a test situation? The behaviourist perspective\noffers little guidance.\nn\nLanguage testing and assessment (Part 2)\n99\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nAn alternative perspective, and one which has a\nlong tradition in educational measurement generally,\nis trait theory. Trait theorists attribute test scores to\ncharacteristics of test takers, rather than to character-\nistics of the context or test setting, and thus they\ndefine their test constructs in terms of the knowl-\nedge and internal processes of the test taker.A trait is\n\u2018a relatively stable characteristic of a person \u2013 an\nattribute, enduring process, or disposition \u2013 which is\nconsistently manifested to some degree when rele-\nvant, despite considerable variation in the range of\nsettings and circumstances\u2019 (Messick, 1989:15, cited\nin Chapelle, 1998). Thus a test developer would\nattempt to tap traits, which are hypothesised as being\nindependent of the settings in which they are\nobserved. This is similar to what Morrow (1979)\ncalled \u2018enabling skills\u2019: abilities which underly perfor-\nmances across a range of contexts, and which are thus\nposited to be stable and generalisable. From this per-\nspective, one defines the traits one wishes to tap,\nincorporates this into one\u2019s test construct, and opera-\ntionalises it in whatever way is appropriate to the\nconstruct, in terms of knowledge, skills and ability.\nThe problem with this approach is that it is too\nsimplistic: we know from applied linguistics and \nsecond language acquisition research that language\nbehaviour is not independent of the settings in\nwhich it occurs. The influence of context, however\ndefined, has long been recognised as important in\nlanguage performance, and the assessment of lan-\nguage ability.Thus a trait perspective alone must be\ninadequate.\nThe difference between these two approaches is\nillustrated in Alderson (2000), where Chapter 4 deals\nwith defining the construct of reading ability, but\nChapter 5 develops a framework, based on Bachman\n(1990) and Bachman and Palmer (1996), for reading\ntest design which relies upon an analysis of TLU \nsituations.\nChapelle (1998) suggests that both approaches are\ninadequate and proposes that a third perspective, an\n\u2018interactionalist\u2019 perspective, based on recent think-\ning in applied linguistics more generally, is more\nappropriate. In this perspective, characteristics of the\nlearner and characteristics of the TLU are defined\nand incorporated into test specifications. Inter-\nactionalist perspectives are \u2018intermediate views,\nattributing some behavioural consistencies to traits,\nsome to situational factors, and some to interactions\nbetween them, in various and arguable proportions\u2019\n(Messick, 1989: 15, cited in Chapelle, 1998). Trait\ncomponents, in other words, can \u2018no longer be\ndefined in context-independent, absolute terms, and\ncontextual features cannot be defined without refer-\nence to their impact on underlying characteristics\u2019\n(Chapelle, 1998: 43). Performance is a sign of under-\nlying traits in interaction with relevant contextual\nfeatures. It is therefore context-bound.\nWhilst this third perspective seems a useful com-\npromise between two rather extreme positions (and\nis illustrated in Chapter 6 of Alderson, 2000), the\ndevil in applying it to test design lies in the detail,\nand the key to practical implementation is what\nMessick, cited above, called the \u2018various and arguable\nproportions\u2019. What we simply do not know at \npresent is what these proportions are, how trait and\ncontext interact under what circumstances and thus\nhow best to combine the two perspectives in test\ndesign.Whilst applied linguistic and testing research\nmight eventually throw light on some of these com-\nplexities, in the meantime tests have to be developed,\neven though we acknowledge our ignorance of the\nspecifics of the relevant interactions (this is, in part,\nwhat McNamara called Pandora\u2019s Box).\nMoreover, as we will argue in the next section,\nsecond language acquisition research shows variabili-\nty in learners\u2019 interlanguage, and in an individual\u2019s\nperformance across a range of contexts. And \ncrucially, testing research as well as theory, has shown\n(see, for example,Alderson, 1990b) that different test\ntakers\u2019 responses to the same item can be due to dif-\nferent causes and processes. Thus, to take a simple\nexample, one test taker may get an item correct\nbecause he knows the meaning of a particular word,\nwhere another test taker gets it right because she has\nsuccessfully guessed the meaning of the word from\ncontext. Similarly, a learner may get an item right\ndespite not having the ability supposedly being \ntested (for example, by luck, using test-taking \nstrategies, background knowledge, inferencing, asso-\nciations, and so on), and another learner may get the\nitem wrong despite \u2018having\u2019 the ability supposedly\nbeing measured (by bad luck, being distracted by an\nunknown word, not paying attention to key features\nof context, not understanding a particular gram-\nmatical structure, and so on).\nRecent language testing research has attempted to\nuncover the processes and strategies that learners\nengage in when responding to test items, and the\nmost clear message to emerge from this research is\nthat how individuals approach test items varies enor-\nmously.What an item may be testing for one individ-\nual is not necessarily the same as what it might test\nfor another individual.This, unfortunately, is the log-\nical conclusion of an interactionalist perspective, one\nwhich Messick also recognised: \u2018The notion that a\ntest score reflects a single uniform construct inter-\npretation...becomes illusory. Indeed, that a test\u2019s\nconstruct interpretation might need to vary from\none type of person to another (or from one setting\nor occasion to another) is a major current conun-\ndrum in educational and psychological measure-\nment\u2019 (Messick, 1989: 55, cited in Chapelle, 1998).\nThus strategies, and presumably traits, can vary across\npersons and tasks, even when the same scores are\nachieved.The same test score may represent different\nabilities, or different combinations of abilities, or dif-\nferent interactions between traits and contexts, and it\nLanguage testing and assessment (Part 2)\nn\n100\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nis currently impossible to say exactly what a score\nmight mean.This we might term The Black Hole of\nlanguage testing.\nAs we have seen in the previous sections examin-\ning language constructs, there is much debate in the\ntesting of reading and listening about the interaction\nbetween reader, task and text. In the testing of speak-\ning, the interaction between participants (pairs in\nsome tasks, interlocutor and test taker in others), and\nwith the task and the person doing the assessment, is\nacknowledged to be complex, made more difficult\nstill by the interplay of variables like gender, status,\ncultural background, peer familiarity, the linguistic\nproficiency level of the test taker and of any partner.\nIn the assessment of writing, the impact of task\nrubric, input, scoring criteria and rater on the result-\ning score needs much further research, and in the\ntesting of grammatical and lexical abilities, the varia-\ntion in performance depending upon presence or\nabsence of context (however defined) is still little\nunderstood. Understanding the nature of the tasks\nwe present to test takers and how these tasks interact\nwith various features, including the characteristics of\ndifferent test takers within the testing context,presents\nthe most important challenge for language testers for\nthe next few years. It is a conundrum that we have\nacknowledged for years but have not yet really come\nto grips with. In Bachman\u2019s words, \u2018proponents of\ntask-based approaches have missed the essential point\nthat it is not either tasks or constructs, but both that need\nto be specified in test development and understood \nin the way we use and interpret test scores\u2019 (Lyle\nBachman,personal communication,2001).\nReliability vs validity\nDavies (1978), in the first survey of language testing\nfor this journal, argued that if we maximise reliability,\nit may be at the expense of validity, and if we maxi-\nmise validity, it is likely to be at the expense of \nreliability. It is often said that the two concepts are\ncomplementary, since a test needs to be reliable to be\nvalid, although the reverse is not necessarily true.We\nhave already discussed recent views of validity at the\nbeginning of this article. However,Alderson (1991b)\nproblematises the distinction between reliability and\nvalidity. Although the difference between the two is\nin theory clear, problems arise, he claims, when con-\nsidering how reliability is measured. Swain (1993)\nalso argues that since SLA research establishes that\ninterlanguage is variable, the notion of internal con-\nsistency as a desirable feature of language tests is\nhighly questionable. Indeed she claims that high\ninternal consistency indicates low validity.\nAlderson (1991b) argues that, although test-retest\nreliability is the easiest measure of reliability to con-\nceptualise, there are problems with the concept. In\ntheory, if a person takes the same test on a second\noccasion, and the test is reliable, the score should\nremain constant. But the score might have changed\nbecause candidates have learned from the first\nadministration or because their ability has changed in\nsome way. In which case, a somewhat lower test-\nretest correlation might be expected, and this would\nbe a valid indication of the change in ability.\nAlderson claims that it is not clear that it would rep-\nresent lack of reliability.\nAnother way of measuring reliability is the use of\nparallel forms of the test. But parallel forms of a test\nare often validated by correlations (concurrent vali-\ndity), and so high correlations between parallel forms\nwould be a measure of validity, not reliability.\nAlderson goes on to question the use of Cronbach\u2019s\nalpha or either of the Kuder-Richardson formulae to\nmeasure reliability, or, rather, item homogeneity.\nSuch formulae test the hypothesis that all the items\nare a random sample from the same domain.\nHowever, he argues that most language tests are not\nhomogeneous and are not intended to be: different\ntest methods are used, for good reasons, tests are\nbased on a number of different text types, and tests of\nlinguistic features (grammar, vocabulary) deliberately\nvary in their content. Since SLA acquisition research\nshows that learners vary in their performance on dif-\nferent tasks, and that this variation can be systematic,\nrather than random, systematic variability might be\nthe rule, not the exception. Thus, low item homo-\ngeneity coefficients might be expected, rather than\nthe reverse. Alderson therefore concludes that a low\nCronbach alpha might be a measure of the validity of\nthe test and a high reliability coefficient could sug-\ngest that the test did not include items that were suf-\nficiently heterogeneous. In a similar vein, Schils et al.\n(1991) present empirical evidence that shows that\nthe use of Cronbach alpha has limited usefulness as a\nreliability measure, especially when calculated post\nhoc, since it depends on the heterogeneity of the can-\ndidate sample and the range of item difficulties.\nIn attempting to validate the model of commu-\nnicative proficiency posited by Canale-Swain and\nBachman, Swain (1993) carried out a number of fac-\ntor analyses. However, factor analysis requires the\nreliabilities of the constituent tests to be high. Swain\nfailed to achieve adequate levels of reliability (item\nhomogeneity) and she comments: \u2018we succeeded in\ngetting a rather low estimate of internal consistency\nby averaging again and again \u2013 in effect, by lengthen-\ning the test and making it more and more complex.\nThe cost is that information on how learners\u2019 perfor-\nmance varies from task to task has been lost\u2019\n(1993:199). She concludes: \u2018if variation in interlan-\nguage is systematic, what does this imply about the\nappropriateness of a search for internal test consis-\ntency?\u2019 (op. cit. 204).\nIn short, how we conceptualise and operationalise\nreliability is problematic, especially in the light of\nwhat is known about variation in language perfor-\nmance.\nn\nLanguage testing and assessment (Part 2)\n101\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nHowever, it may be that there is a way forward.\nFirstly, many would now argue that, given Messick\u2019s\nunitary view of validity, reliability has been merged,\nconceptually, into a unified view of validity. In effect,\nthis means that we need not agonise, as Alderson\n(1991b) does, over whether what we call reliability is\n\u2018actually\u2019 validity. What matters is how we identify\nvariability in test scores, and to what we can attribute\nsuch variation.Variation\/ variability that is relevant\nto our constructs is evidence of the validity of our\ninterpretation of test scores, whereas construct-irrel-\nevant variance is to be avoided or reduced. It is more\nimportant to understand whether such variation is\ndue to error \u2013 traditionally identified as sources of\nlack of reliability \u2013 or to constructs that should not\nbe being measured, like test-wiseness or particular\ntest method effects, than to label this variation as reli-\nability or validity.Thus, making a distinction between\n\u2018reliability\u2019 and \u2018validity\u2019 is irrelevant in this unified\nview of validity.What matters is explaining sources of\nvariability.\nAnd thus, secondly, the focus in discussions of \nreliability is beginning to shift from trying to esti-\nmate a global reliability, as criticised by Alderson, to\nidentifying and estimating the effects of multiple\nsources of measurement error. One of the biggest\nproblems with classical approaches to reliability, as\noutlined above, is that they cannot identify different,\nconcurrent, sources of error and their interactions.\nRecent work (for example, Bachman et al., 1995 and\nLynch and McNamara, 1998), utilising generalisa-\nbility theory and multi-faceted item response theory,\nseeks to identify, and thus eventually to reduce or\neliminate, particular sources of error.We are likely to\nsee more fine-grained explorations of sources of\nerror in the future.\nValidation: how to?\nAnd so we come back full circle to where we started\nthe second part of this review: validity and validation.\nThe Messickian unified notion of construct validity\nhas led to an acceptance that there is no one best way\nto validate the inferences to be made from test scores\nfor particular purposes. Rather, there are a variety of\ndifferent perspectives from which evidence for valid-\nity can be accumulated, and thus in a sense, validation\nis never complete: more evidence can always be\ngathered for or against a particular interpretation of\ntest scores. Unfortunately, this can be frustrating for\ntest developers, who want \u2013 or should want \u2013 to\nknow how best to validate their tests, and when they\ncan safely claim that they know what are valid and\nwhat are invalid inferences that can be drawn from\nthe scores on the tests they produce. Shepard (1993)\nexpresses a similar concern: \u2018if construct validity is\nseen as an exhaustive process that can be accom-\nplished over a 50-year period, test developers may be\ninclined to think that any validity information is\ngood enough in the short run\u2019 (1993: 444, cited in\nChapelle, 1998).To many, the theoretical expositions\nof construct validity and validation are too abstract\nand remote from the reality of test development and\nuse. Alderson et al. (1995) report on a survey of the\nvalidation practice of UK EFL examination boards,\nand it is evident from that survey that very little\ninformation on validity was routinely gathered by\nthose boards, and even more rarely was it reported to\ntest users. Indeed, some boards even questioned the\nneed for such evidence.\nHowever, attention has turned in recent years and\nmonths to the relationship between test develop-\nment and test validation. Luoma (2001) discusses this\nrelationship at length, develops a framework within\nwhich to discuss and describe test development pro-\ncedures, and then relates this framework to a frame-\nwork for test validation derived from Messick. Her\ncharacterisation of the test development process in\nrelation to test validation is given in Figure 3 below,\nand aspects of test validation are described in her\nTable 6 (see p. 104).\nLuoma seeks to explore how test developers do\nactually validate their tests by reference to three case\nstudies from the published literature. Interestingly,\neven these case studies only provide a limited insight\ninto the actual processes of test validation through\ntest development, and the language testing literature\nas a whole lacks detailed accounts of how evidence\nfor test validity has been gathered in specific cases. If\ntheoretical accounts of how validation should pro-\nceed are to be of use in the real world of test devel-\nopment, then much more attention will need to be\npaid to developing both descriptions of actual valida-\ntion studies and guidelines for validation in specific\ncircumstances.\nEven as we write these concluding paragraphs,\ndebate is raging on the language testers\u2019 bulletin\nboard, LTEST-L, about validation. Some contribu-\ntors maintain that most tests go into use with very\nlittle serious validation evidence (G. Buck, Director\nof the Test Development and Standards Division at\nthe Defense Language Institute, Foreign Language\nCenter, Monterey, USA). Bachman responded that\n\u201cthe fact that tests are used with no attention to vali-\ndation does not condone this practice\u201d and argued\nthat \u201cthe strength of the validation argument and\n(the) amount of evidence that needs to be brought to\nsupport that argument needs to be considered with\nrespect to the seriousness, or impact of the decisions\nto be made. However, even in relatively low-stakes\ntests, such as classroom assessment, I would argue that\nsome validation argument needs to be formulated,\nand some evidence needs to be provided in support\nof this argument.\u2026What counts as \u2018reasonable\u2019 will\ndepend on the potential impact of the intended use,\nand which stake-holders the test developer\/user needs\nto convince.\u201d In similar vein, Spolsky suggested that\nthere ought to be an assessment of the risks involved in\nLanguage testing and assessment (Part 2)\nn\n102\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nusing any test for making decisions (Spolsky, personal\ncommunication, 13 November, 2001).\nOther participants recommended articles like\nChapelle (1999) and case studies like Chapelle\n(1994) and Wall et al. (1994) as providing examples of\nhow validation might proceed. Chapelle presents an\ninteresting rhetorical approach to forming a valida-\ntion argument. She draws up a table with arguments\nfor and against the validity of inferences from a par-\nticular test. Davidson (personal communication, 13\nNovember, 2001) argues that professional testing\ncompanies often only present the arguments in\nfavour of their tests.\nIn the electronic discussion, Douglas stated that\n\u201cvalidation is indeed an act of faith \u2013 we know that\nwe can never prove a test to be valid for any\npurpose \u2013 all we can do is provide evidence that our\ncolleagues and our target audiences will find con-\nvincing \u2013 we know when we have enough validation\nto actually use a test when we\u2019ve met the (dynamic)\nstandards\/practice established by the profession\u201d\n(personal communication, 13 November, 2001).\nEmphasising that \u201ctest development is a complex\ntask and demands methods that match the complexi-\nty\u201d, Kunnan nevertheless argues that language testers\ndo know how to assemble validity evidence, in the\nfollowing ways:\n1. Content-related evidence collected from expert judgments\nthrough checklists for what they believe a test is measuring in\nterms of content relevance, representativeness, operations and\nconditions, and choice of language variety.\n2. Construct-related evidence collected from exploratory factor\nanalyses used to check for internal structure of test perfor-\nmance data; and\/or check for convergent and discriminant\nvalidity; and from test takers through verbal protocol reports,\nif possible.\n3. Criterion-related evidence collected from correlations of test\nperformance data with job performance\/teacher estimates of\nacademic work, if possible at the same time the test is given\nn\nLanguage testing and assessment (Part 2)\n103\nFigure 3. The Test Development Process\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 2)\nn\n104\nGoals Means\nTo measure the right thing Define skills to be assessed in detail\nDefine task characteristics and task rubrics\nCheck acceptability and appropriacy through peer and test \npolicy board comments\nAnalyse tasks from the perspective of task demands to make \ncloser description of skills\nRefine tasks through peer comments\nUse empirical information from trialling to select best tasks\nUse empirical information from trialling as criterion when \ntest forms are constructed\nTo measure consistently Use empirical item information from trialling to select best \ntasks\nCheck that all new test forms follow content and statistical \ncriteria\nMonitor standardisation of administration including the \nadministration of interactive speaking tests\nMonitor standardisation of rating when human rating is used\nMonitor measurement properties of actual tests and make \nrevisions in methods of construction and\/or analysis as \nnecessary\nTo measure economically Analyse possible overlap through eg. factor analysis\nRemove all overlapping test sections that you can provided \nthat you can deliver the scores that users need and provided \nthat measurement properties do not suffer\nFit as many items in test time as possible but monitor \nspeededness\nTo provide comparable scores across Follow standardised administration procedures\nadministrations Monitor reliability\nUse well-documented methods for score conversion and test \nform equation\nTo provide positive impact and avoid Predict possible consequences and analyse realised \nnegative consequences consequences\nEnsure that negative consequences cannot be traced to test \ninvalidity\nConsult and observe learners, teachers,materials writers,\ncurriculum designers and researchers as sources of data on \npossible washback\nTo provide accountable professional Document all procedures carefully\nservice Provide advice for score interpretation\nReport measurement properties of reported scores\nLuoma\u2019s (2001) Table 6.\nGoals for test development and means for reaching them\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\n(concurrent type); and from correlations of test performance\ndata with job performance\/teacher estimates of academic\nwork, at a later time (predictive type).\n4. Reliability evidence collected from test-retest and parallel\nform analyses, inter-rater analyses, and internal consistency\nanalyses.\n5. Absence of bias evidence collected from DIF analyses for\ninterested test taker subgroups (examples, gender, race\/eth-\nnicity, age) and from analyses of standard setting (cut scores), if\ncut scores are used; and evidence of test access information in\nterms of opportunity to learn and information in terms of\naccommodations for disabled test takers.\n(Kunnan, personal communication, 14 November, 2001)\nNevertheless, several contributors to the debate\nacknowledged that there are no simple answers to\nthe practical question: how much evidence is\nenough? Nor is guidance currently available on what\nto do when the various sources of evidence contra-\ndict each other or do not provide clear-cut support\nfor the validity argument. Others pointed out that\nthe problem is that the purpose of the test, the stakes\ninvolved, the testing expertise available, the nature of\nthe educational institution(s) involved, the available\nresources, and many other factors, will all affect deci-\nsions about the adequacy of the validity evidence.\nBuck argued that what is needed is \u201ccriteria to\nenable (test developers) to determine when the evi-\ndence they have collected is enough to support the\nvalidity of the test use\u201d. Eignor agrees, citing Dwyer\nand Fremer as calling for \u201cthe articulation of procedures\nor programs for conducting validation research that\nclearly support the transition of the current conceptions\nof validity from theory to practice\u201d (Eignor, 1999).\nGiven the increased concern we have seen in this\nreview with the consequences of test use, the ethics\nand politics of testing, and the reconceptualisation of\nthe very nature of validity, it is likely that this debate\nwill go on for some time, but, despite the context-\nbound nature of many validation studies, it will\nhopefully be possible eventually to offer test devel-\nopers and users more concrete guidance on how to\ninterpret and evaluate conflicting sources of validity\nand contrasting arguments for and against the validi-\nty of particular test uses.\nEnvoi\nIn this review, we have sought to provide an\noverview of recent developments and thinking in\nlanguage testing.Whilst new concerns have come to\nthe fore, have proved fruitful in terms of the research\nthey have generated, and have widened and broad-\nened the debate about language testing in general,\nnevertheless old concerns continue. Even though a\nunified view of validity is widely accepted, how per-\ntinent aspects of validity and reliability are to be\ninvestigated and established is still problematic, and\nthis is likely to continue so for some time to come.\nInsights into the constructs we measure as language\ntesters have certainly been enhanced by a greater\nunderstanding of the nature of language, how it is\nused and how it is learned, but dilemmas faced by\nany attempt to measure language proficiency remain.\nTo use Davies\u2019 classic phrase, testing is about \u2018opera-\ntionalising uncertainty\u2019 (Davies, 1988).Which is what\nis exciting about the current state of the art in lan-\nguage testing \u2013 realising that we know less than we\nthink, whether it is washback, politics and innova-\ntion, ethical behaviour, what exactly we are testing,\nor how to know what we are testing.The challenge\nfor the next decade will be to enhance our under-\nstanding of these issues.\nReferences\nACTFL (1986). ACTFL Proficiency Guidelines. New York:\nAmerican Council on the Teaching of Foreign Languages.\nALDERSON, J. C. (1981). Report of the discussion on commu-\nnicative language testing. In J. C.Alderson & A. Hughes (Eds.),\nIssues in Language Testing (ELT Documents,Vol. 111). London:\nThe British Council.\nALDERSON, J. C. (1984). Reading in a foreign language: a reading\nproblem or a language problem? In J. C. Alderson & A. H.\nUrquhart (Eds.), Reading in a foreign language (pp. 1\u201324).\nLondon:Longman.\nALDERSON, J. C. (1988). New procedures for validating pro-\nficiency tests of ESP? Theory and practice. Language Testing,\n5(2), 220\u201332.\nALDERSON, J. C. (1990a). Testing reading comprehension skills\n(Part One).Reading in a Foreign Language, 6(2), 425\u201338.\nALDERSON, J. C. (1990b). Testing reading comprehension skills\n(Part Two).Reading in a Foreign Language, 7(1), 465\u2013503.\nALDERSON, J. C. (1991a). Letter. Reading in a Foreign Language,\n7(2), 599\u2013603.\nALDERSON, J. C. (1991b). Dis-sporting life. Response to Alastair\nPollitt\u2019s paper: \u2018Giving students a sporting chance: Assessment\nby counting and judging\u2019. In J. C.Alderson & B. North (Eds.),\nLanguage testing in the 1990s:The communicative legacy (60\u201370).\nLondon: Macmillan (Modern English Publications in associa-\ntion with the British Council).\nALDERSON, J. C. (1993).The relationship between grammar and\nreading in an English for academic purposes test battery. In \nD. Douglas & C. Chapelle (Eds.), A new decade of language test-\ning research: Selected papers from the 1990 Language Testing\nResearch Colloquium (203\u201319).Alexandria,Va: TESOL.\nALDERSON, J. C. (2000). Assessing Reading. Cambridge:\nCambridge University Press.\nALDERSON, J. C. & BANERJEE, J. (2001). Language testing and\nassessment (Part 1).Language Teaching, 34(4), 213\u201336.\nALDERSON, J. C. & CLAPHAM, C. (1992). Applied linguistics \nand language testing: a case study. Applied Linguistics, 13(2),\n149\u201367.\nALDERSON, J. C., CLAPHAM, C. & WALL, D. (1995). Language test\nconstruction and evaluation. Cambridge: Cambridge University\nPress.\nALDERSON, J. C. & LUKMANI,Y. (1989). Cognition and reading:\ncognitive levels as embodied in test questions. Reading in a\nForeign Language, 5(2), 253\u201370.\nALDERSON, J. C., NAGY, E. & \u00d6VEGES, E. (Eds.) (2000). English\nlanguage education in Hungary, Part II: Examining Hungarian\nlearners\u2019 achievements in English. Budapest:The British Council.\nALDERSON, J. C. & URQUHART, A. H. (1985). The effect of \nstudents\u2019 academic discipline on their performance on ESP\nreading tests.Language Testing, 2(2), 192\u2013204.\nALLEN, E. D., BERNHARDT, E. B., BERRY, M. T. & DEMEL,\nM. (1988). Comprehension and text genre: an analysis of \nn\nLanguage testing and assessment (Part 2)\n105\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nsecondary school foreign language readers. Modern Language\nJournal, 72, 163\u201372.\nAL-MUSAWI, N. M. & AL-ANSARI, S. H. (1999).Test of English as\na Foreign Language and First Certificate of English tests as\npredictors of academic success for undergraduate students at\nthe University of Bahrain.System,27(3), 389\u201399.\nALONSO, E. (1997). The evaluation of Spanish-speaking bilin-\nguals\u2019 oral proficiency according to ACTFL guidelines (trans.\nfrom Spanish).Hispania, 80(2), 328\u201341.\nANDERSON, N., BACHMAN, L., PERKINS, K. & COHEN,A. (1991).\nAn exploratory study into the construct validity of a reading\ncomprehension test: triangulation of data sources. Language\nTesting, 8(1), 41\u201366.\nANH, V. T. P. (1997). Authenticity and validity in language testing:\ninvestigating the reading components of IELTS and TOEFL.\nUnpublished PhD,La Trobe University,Melbourne,Australia.\nASTIKA, G. G. (1993). Analytical assessments of foreign students\u2019\nwriting.RELC Journal, 24(1), 61\u201372.\nBACHA, N. (2001).Writing evaluation: what can analytic versus\nholistic essay scoring tell us? System,29(3), 371\u201384.\nBACHMAN, L. F. (1990). Fundamental considerations in language test-\ning.Oxford:Oxford University Press.\nBACHMAN, L. F. (1991).What does language testing have to offer?\nTESOL Quarterly, 25(4), 671\u2013704.\nBACHMAN, L. F. (2001). Speaking as a realization of communicative\ncompetence. Paper presented at the LTRC\/AAAL Symposium,\nSt Louis.\nBACHMAN, L. F. & COHEN, A. D. (1998). Interfaces between second\nlanguage acquisition and language testing research. Cambridge:\nCambridge University Press.\nBACHMAN, L. F., DAVIDSON, F., LYNCH, B. & RYAN, K. (1989).\nContent analysis and statistical modeling of EFL proficiency tests.\nPaper presented at the 11th Annual Language Testing\nResearch Colloquium,San Antonio,Texas.\nBACHMAN, L. F., DAVIDSON, F. & MILANOVIC, M. (1996).The use\nof test method characteristics in the content analysis and design\nof EFL proficiency tests.Language Testing,13(2), 125\u201350.\nBACHMAN,L. F.,DAVIDSON,F.,RYAN,K.& CHOI, I.-C. (1995).An\ninvestigation into the comparability of two tests of English as a \nforeign language. (Studies in Language Testing Series, Vol. 1).\nCambridge: University of Cambridge Local Examinations\nSyndicate\/ Cambridge University Press.\nBACHMAN, L. F. & EIGNOR, D. (1997). Recent advances in quan-\ntitative test analysis. In C. M. Clapham & D. Corson (Eds.),\nLanguage testing and assessment (Volume 7, pp. 227\u201342).\nDordrecht,The Netherlands:Kluwer Academic Publishing.\nBACHMAN, L. F., KUNNAN, A., VANNIARAJAN, S. & LYNCH, B.\n(1988).Task and ability analysis as a basis for examining con-\ntent and construct comparability in two EFL proficiency test\nbatteries.Language Testing, 5(2), 128\u201359.\nBACHMAN, L. F., LYNCH, B. K. & MASON, M. (1995). Investigating\nvariability in tasks and rater judgments in a performance test\nof foreign language speaking.Language Testing, 12(2), 238\u201357.\nBACHMAN, L. F. & PALMER, A. S. (1996). Language testing in\npractice.Oxford:Oxford University Press.\nBACHMAN, L. F. & SAVIGNON, S. (1986).The evaluation of com-\nmunicative language proficiency: a critique of the ACTFL oral\ninterview.Modern Language Journal, 70, 380\u201390.\nBAE, J.& BACHMAN,L. (1998).A latent variable aproach to listen-\ning and reading: testing factorial invariance across two groups\nof children in the Korean\/English Two-Way Immersion\nProgram.Language Testing, 15(3), 380\u2013414.\nBANERJEE, J. & LUOMA, S. (1997). Qualitative approaches to test\nvalidation. In C. Clapham & D. Corson (Eds.), Language testing\nand assessment (Vol. 7, pp. 275\u201387). Dordrecht, The\nNetherlands:Kluwer Academic Publishers.\nBARNWELL, D. (1989). \u2018Naive\u2019 native speakers and judgements of\noral proficiency in Spanish.Language Testing, 6(2), 152\u201363.\nBEECKMANS, R., EYCKMANS, J., JANSSENS,V., DUFRANNE, M. &\nVAN DE VELDE, H. (2001). Examining the Yes\/No vocabulary\ntest: some methodological issues in theory and practice.\nLanguage Testing, 18(3), 235\u201374.\nBEGLAR, D. & HUNT,A. (1999). Revising and validating the 2000\nWord Level and University Word Level vocabulary tests.\nLanguage Testing, 16(2), 131\u201362.\nBERNHARDT, E. B. (1991).A psycholinguistic perspective on sec-\nond language literacy. In J. H. Hulstijn & J. F. Matter (Eds.),\nReading in two languages (Vol. 7, pp. 31\u201344). Amsterdam: Free\nUniversity Press.\nBERNHARDT, E. B. (1999). If reading is reader-based, can there be\na computer-adaptive test of reading? In M. Chalhoub-Deville\n(Ed.), Issues in computer-adaptive testing of reading proficiency.\n(Studies in Language Testing Series,Volume 10) Cambridge:\nUCLES-Cambridge University Press.\nBERNHARDT, E. B. & KAMIL, M. L. (1995). Interpreting relation-\nships between L1 and L2 reading: consolidating the linguistic\nthreshold and the linguistic interdependence hypotheses.\nApplied Linguistics, 16(1), 15\u201334.\nBERRY,V. (1997). Ethical considerations when assessing oral pro-\nficiency in pairs. In A. Huhta,V. Kohonen, L. Kurki-Suonio &\nS. Luoma (Eds.), Current developments and alternatives in language\nassessment (pp. 107\u201323). Jyv\u00e4skyla: Centre for Applied\nLanguage Studies,University of Jyv\u00e4skyla.\nBHGEL, K. & LEIJN, M. (1999). New exams in secondary educa-\ntion, new question types. An investigation into the reliability\nof the evaluation of open-ended questions in foreign-lan-\nguage exams.Levende Talen, 537, 173\u201381.\nBLAIS, J.-G. & LAURIER, M. D. (1995). The dimensionality of a\nplacement test from several analytical perspectives. Language\nTesting, 12(1), 72\u201398.\nBLOOM, B. S., ENGLEHART, M., FURST, E. J., HILL, W. H. &\nKRATHWOHL, D. R. (1956). Taxonomy of educational objectives.\nHandbook 1:Cognitive domain.New York:Longman.\nBRADSHAW, J. (1990). Test-takers\u2019 reactions to a placement test.\nLanguage Testing, 7(1), 13\u201330.\nBRINDLEY, G. (1998). Assessing listening abilities. Annual Review\nof Applied Linguistics, 18, 171\u201391.\nBROWN, A. (1993). The role of test-taker feedback in the test\ndevelopment process: test-takers\u2019 reactions to a tape-mediated\ntest of proficiency in spoken Japanese. Language Testing, 10(3),\n277\u2013303.\nBROWN, A. (1995).The effect of rater variables in the develop-\nment of an occupation-specific language performance test.\nLanguage Testing, 12(1), 1\u201315.\nBROWN,A. (2001). The impact of handwriting on the scoring of essays.\nPaper presented at the Association of Language Testers in\nEurope (ALTE) Conference,Barcelona.\nBROWN, A. & HILL, K. (1998). Interviewer style and candidate\nperformance in the IELTS oral interview. In S. Wood (Ed.),\nIELTS Research Reports 1998 (Volume 1, pp. 1\u201319). Sydney:\nELICOS Association Ltd.\nBROWN, A. & IWASHITA, N. (1996). Language background and\nitem difficulty: the development of a computer-adaptive test\nof Japanese.System,24(2), 199\u2013206.\nBROWN, A., IWASHITA, N., MCNAMARA, T. F. & O\u2019HAGAN, S.\n(2001). Investigating raters\u2019 orientations in specific-purpose task-\nbased oral assessment. Paper presented at the Language Testing\nResearch Colloquium,St Louis.\nBROWN, A. & LUMLEY,T. (1997). Interviewer variability in spe-\ncific-purpose language performance tests. In A. Huhta, V.\nKohonen, L. Kurki-Suonio & S. Luoma (Eds.), Current develop-\nments and alternatives in language assessment (137\u201350). Jyv\u00e4skyla:\nCentre for Applied Language Studies,University of Jyv\u00e4skyla.\nBROWN, J. D. (1989). Improving ESL placement test using two\nperspectives.TESOL Quarterly, 23(1), 65\u201383.\nBROWN, J. D. (1991). Do English and ESL faculties rate writing\nsamples differently? TESOL Quarterly, 25(4), 587\u2013603.\nBROWN, J. D. (1999).The relative importance of persons, items,\nsubtests and languages to TOEFL test variance. Language\nTesting, 16(2), 217\u201338.\nLanguage testing and assessment (Part 2)\nn\n106\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nBUCK, G. (1990). The testing of second language listening comprehen-\nsion. Unpublished PhD dissertation, Lancaster University,\nLancaster.\nBUCK, G. (1991). The testing of listening comprehension: an\nintrospective study.Language Testing, 8(1), 67\u201391.\nBUCK, G. (1992a).Translation as a language testing process: Does\nit work? Language Testing, 9(2), 123\u201348.\nBUCK, G. (1992b). Listening comprehension: construct validity\nand trait characteristics.Language Learning, 42(3), 313\u201357.\nBUCK, G. (1994).The appropriacy of psychometric measurement\nmodels for testing second language listening comprehension.\nLanguage Testing, 11(3), 145\u201370.\nBUCK, G. (1997).The testing of listening in a second language. In\nC. Clapham & D. Corson (Eds.), Language testing and assessment\n(Volume 7, pp. 65\u201374). Dordrecht, The Netherlands: Kluwer\nAcademic Publishers.\nBUCK, G. (2001). Assessing listening. Cambridge: Cambridge\nUniversity Press.\nBUCK, G. & TATSUOKA, K. (1998). Application of the rule-space\nprocedure to language testing: examining attributes of a free\nresponse listening test.Language Testing, 15(2), 119\u201357.\nBUCK, G., TATSUOKA, K. & KOSTIN, I. (1997). The subskills of\nreading: rule-space analysis of a multiple-choice test of second\nlanguage reading comprehension. Language Learning, 47(3),\n423\u201366.\nBURSTEIN, J. & LEACOCK, C. (2001). Applications in automated\nessay scoring and feedback. Paper presented at the Association of\nLanguage Testers in Europe (ALTE) Conference,Barcelona.\nCANALE, M. & SWAIN, M. (1980).Theoretical bases of commu-\nnicative approaches to second language teaching and testing.\nApplied Linguistics, 1(1), 1\u201347.\nCASCALLAR, M. I. (1997). Modified oral proficiency interview:\nIts purpose, development and description. In A. Huhta,\nV. Kohonen, L. Kurki-Suonio & S. Luoma (Eds.), Current\ndevelopments and alternatives in language assessment (pp. 485\u201394).\nJyv\u00e4skyla:University of Jyv\u00e4skyla.\nCHALHOUB-DEVILLE, M. (1995). Deriving oral assessment scales\nacross different tests and rater groups. Language Testing, 12(1),\n16\u201333.\nCHALHOUB-DEVILLE, M. (1997).Theoretical models, assessment\nframeworks and test construction. Language Testing, 14(1),\n3\u201322.\nCHAMBERS, F. & RICHARDS, B. (1995). The \u2018free\u2019 conversation\nand the assessment of oral proficiency. Language Learning\nJournal, 11, 6\u201310.\nCHAPELLE, C. (1988). Field independence: a source of language\ntest variance? Language Testing, 5(1), 62\u201382.\nCHAPELLE, C.A. (1994).Are C-tests valid measures for L2 vocab-\nulary research? Second Language Research, 10, 157\u201387.\nCHAPELLE, C.A. (1998). Construct definition and validity inquiry\nin SLA research. In L. F. Bachman & A. D. Cohen (Eds.),\nInterfaces between second language acquisition and language testing\nresearch (32\u201370).Cambridge:Cambridge University Press.\nCHAPELLE, C. (1999). Validity in language assessment. Annual\nReview of Applied Linguistics, 19, 254\u201372.\nCHIANG, S.Y. (1999). Assessing grammatical and textual features\nin L2 writing samples: the case of French as a foreign lan-\nguage.The Modern Language Journal, 83(2), 219\u201332.\nCHILD, J. R. (1987). Language proficiency levels and the typology\nof texts. In H. Byrnes & M. Canale (Eds.), Defining and devel-\noping proficiency: Guidelines, implementations and concepts\n(pp. 97\u2013106). Lincolnwood, IL:National Textbook Co.\nCHO,Y. (2001). Examining a process-oriented writing assessment for\nlarge scale assessment. Paper presented at the Language Testing\nResearch Colloquium,St. Louis.\nCHUNG, J.-M. (1997).A comparison of two multiple-choice test\nformats for assessing English structure competence. Foreign\nLanguage Annals, 30(1), 111\u201323.\nCLAPHAM, C. (1996). The development of IELTS: A study of the\neffect of background knowledge on reading comprehension. (Studies\nin Language Testing Series, Vol. 4). Cambridge: Cambridge\nUniversity Press.\nCLAPHAM, C. (2000). Assessment and testing. Annual Review of\nApplied Linguistics, 20, 147\u201361.\nCLARK, J. L. D. & HOOSHMAND, D. (1992). \u2018Screen-to-screen\u2019\ntesting: an exploratory study of oral proficiency interviewing\nusing video teleconferencing.System,20(3), 293\u2013304.\nCLARKE, M. (1979). Reading in English and Spanish: evidence\nfrom adult ESL students.Language Learning, 29, 121\u201350.\nCLARKE, M. (1988). The short circuit hypothesis of ESL\nreading \u2013 or when language competence interferes with read-\ning performance. In P. L. Carrell, J. Devine, & D. Eskey (Eds.),\nInteractive approaches to second language reading (pp. 114\u201324).\nCambridge:Cambridge University Press.\nCONIAM, D. (1998). Interactive evaluation of listening compre-\nhension: how the context may help. Computer Assisted\nLanguage Learning, 11(1), 35\u201353.\nCONRAD, S. (2001). Speaking as register. Paper presented at the\nLTRC\/AAAL Symposium,St Louis.\nCOOMBE, C., KINNEY, J. & CANNING, C. (1998). Issues in the\nevaluation of academic listening tests. Language Testing Update,\n24, 32\u201345.\nCRIPER, C. & DAVIES, A. (1988). Research Report 1(i) The ELTS\nvalidation project report. London\/ Cambridge: The British\nCouncil\/ University of Cambridge Local Examinations\nSyndicate.\nCRONBACH, L. J. & MEEHL, P. E. (1955). Construct validity in\npsychological tests.Psychological Bulletin, 52, 281\u2013302.\nCUMMING, A. (1990). Expertise in evaluating second-language\ncompositions.Language Testing, 7(1), 31\u201351.\nCUMMING, A. (1997). The testing of writing in a second lan-\nguage. In C. Clapham & D. Corson (Eds.), Language testing and\nassessment (Volume 7, pp. 51\u201363). Dordrecht,The Netherlands:\nKluwer Academic Publishers.\nDANDONOLI, P. & HENNING, G. (1990). An investigation of the\nconstruct validity of the ACTFL Proficiency guidelines and\noral interview procedure. Foreign Language Annals, 23(1),\n11\u201322.\nDANON-BOILEAU, L. (1997). Peut-on \u00e9valuer une acquisition du\nlangage? Les Langues Modernes, 2, 15\u201323.\nDAVIDSON, F. & BACHMAN, L. F. (1990).The Cambridge-TOEFL\ncomparability study: a example of the cross-national compari-\nson of language tests.AILA Review, 7, 24\u201345.\nDAVIES, A. (1978). Language testing: survey articles 1 and 2.\nLanguage Teaching and Linguistics Abstracts, 11, 145\u201359 and\n215\u201331.\nDAVIES,A. (1988). Operationalising uncertainty in language test-\ning: an argument in favour of content validity. Language Testing,\n5(1), 32\u201348.\nDAVIES, C. E. (1998). Maintaining American face in the Korean\noral exam: reflections on the power of cross-cultural context,\nIn Young, R & He, A.W. (Eds.), Talking and testing: discourse\napproaches to the assessment of oral proficiency, Studies in\nBilingualism (Vol. 14) Amsterdam: John Benjamins Publishing\nCompany, p. 271\u201396.\nDE BOT, K. (2001). Speaking as a psycholinguistic process: the machine\nwithin. Paper presented at the LTRC\/AAAL Symposium,\nSt Louis.\nDE JONG, J. & GLAS, C.A.W. (1989).Validation of listening com-\nprehension tests using item response theory. Language Testing,\n4(2), 170\u201394.\nDEMAURO, G. (1992). Examination of the relationships among\nTSE, TWE and TOEFL scores. Language Testing, 9(2),\n149\u201361.\nDES BRISAY, M. (1994). Problems in developing an alternative to\nthe TOEFL.TESL Canada Journal, 12(1), 47\u201357.\nDEVILLE, C. & CHALHOUB-DEVILLE, M. (1993). Modified scor-\ning, traditional item analysis and Sato\u2019s caution index used to\ninvestigate the reading recall protocol. Language Testing, 10(2),\n117\u201332.\nn\nLanguage testing and assessment (Part 2)\n107\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nDOUGLAS, D. (1994). Quantity and quality in speaking test per-\nformance.Language Testing, 11(2), 125\u201344.\nDOUGLAS, D. & MYERS, R. (2000).Assessing the communication\nskills of veterinary students: whose criteria? In A. J. Kunnan\n(Ed.), Fairness and validation in language assessment (Studies \nin Language Testing Series, Vol. 9, pp. 60\u201381). Cambridge:\nUCLES\/Cambridge University Press.\nDOUGLAS, D. & NISSAN, S. (2001). Developing listening prototypes\nusing a corpus of spoken academic English. Paper presented at the\nLanguage Testing Research Colloquium,St. Louis.\nDUNKEL, P., HENNING, G. & CHAUDRON, C. (1993).The assess-\nment of an L2 listening comprehension construct: a tentative\nmodel for test specification and development. Modern\nLanguage Journal, 77(2), 180\u201391.\nEDWARDS,A. L. (1996). Reading proficiency assessment and the\nILR\/ACTFL text typology: a reevaluation. The Modern\nLanguage Journal, 80(3), 350\u201361.\nEGBERT, M. M. (1998). Miscommunication in language profi-\nciency interviews of first-year German students: a comparison\nwith natural conversation, In Young, R & He, A.W. (Eds.),\nTalking and testing: discourse approaches to the assessment of oral\nproficiency, Studies in Bilingualism (Vol. 14) Amsterdam: John\nBenjamins Publishing Company, p. 147\u201369.\nEIGNOR, D. (1999). Standards for the development and use \nof tests: the standards for educational and psychological test-\ning. European Journal of Psychological Assessment, 17(3), pp.\n157\u201363.\nFERGUSON, B. (1994). Overcoming gender bias in oral testing:\nthe effect of introducing candidates.System,22(3), 341\u201348.\nFOOT, M. (1999). Relaxing in pairs. English Language Teaching\nJournal, 53(1), 36\u201341.\nFREEDLE, R. & KOSTIN, I. (1993). The prediction of TOEFL\nreading item difficulty: implications for construct validity.\nLanguage Testing, 10(2), 133\u201370.\nFREEDLE, R. & KOSTIN, I. (1999). Does the text matter in a \nmultiple-choice test of comprehension? The case for the con-\nstruct validity of TOEFL\u2019s minitalks. Language Testing, 16(1),\n2\u201332.\nFREEMAN,Y. S. & FREEMAN, D. E. (1992). Portfolio assessment for\nbilingual learners.Bilingual Basics, 8.\nFULCHER, G. (1993). The construct validation of rating scales for oral\ntests in English as a foreign language. Unpublished PhD disserta-\ntion,Lancaster University, Lancaster.\nFULCHER, G. (1996a).Testing tasks: issues in task design and the\ngroup oral.Language Testing, 13(1), 23\u201351.\nFULCHER, G. (1996b). Does thick description lead to smart tests?\nA data-based approach to rating scale construction. Language\nTesting, 13(2), 208\u201338.\nFULCHER, G. (1997a).An English language placement test: issues\nin reliability and validity.Language Testing, 14(2), 113\u201339.\nFULCHER, G. (1997b).The testing of L2 speaking. In C. Clapham\n& D. Corson (Eds.), Language testing and assessment (Vol. 7,\npp. 75\u201385). Dordrecht, The Netherlands: Kluwer Academic\nPublishers.\nFULCHER, G. (1999a). Assesssment in English for Academic\nPurposes: putting content validity in its place. Applied\nLinguistics, 20(2), 221\u201336.\nFULCHER, G. (1999b). Computerising an English language\nplacement test.ELT Journal, 53(4), 289\u201399.\nGARRETT, P., GRIFFITHS,Y., JAMES, C. & SCHOLFIELD, P. (1995).\nThe development of a scoring scheme for content in transac-\ntional writing: some indicators of audience awareness.\nLanguage and Education, 9(3), 179\u201393.\nGERANPAYEH, A. (1994). Are score comparisons across language\nproficiency test batteries justified? An IELTS-TOEFL compa-\nrability study. Edinburgh Working Papers in Applied Linguistics, 5,\n50\u201365.\nGHONSOOLY, B. (1993). Development and validation of a transla-\ntion test. Edinburgh Working Papers in Applied Linguistics, 4,\n54\u201362.\nGINTHER, A. (forthcoming). Context and content visuals and\nperformance on listening comprehension stimuli. Language\nTesting.\nGINTHER, A. & GRANT, L. (1997).The influence of proficiency,\nlanguage background, and topic on the production of gram-\nmatical form and error on the Test of Written English. In A.\nHuhta, V. Kohonen, L. Kurki-Suonio & S. Luoma (Eds.),\nCurrent developments and alternatives in language assessment (pp.\n385\u201397). Jyv\u00e4skyla:University of Jyv\u00e4skyla.\nGLISAN, E.W. & FOLTZ, D.A. (1998).Assessing students\u2019 oral pro-\nficiency in an outcome-based curriculum: student perfor-\nmance and teacher intuitions. The Modern Language Journal,\n83(1), 1\u201318.\nGONZALEZ PINO, B. (1989). Prochievement REM? testing of\nspeaking.Foreign Language Annals, 22(5), 487\u201396.\nGREEN, A. (1998). Verbal protocol analysis in language testing\nresearch: A handbook. (Studies in Language Testing Series,\nVol. 5). Cambridge: University of Cambridge Local\nExaminations Syndicate\/ Cambridge University Press.\nGROTJAHN, R. (1995). The C-Test: state of the art (trans. from\nGerman).Zeitschrift fur Fremdsprachenforschung, 6(2), 37\u201360.\nGRUBA, P. (1997). The role of video media in listening assess-\nment.System,25(3), 333\u201345.\nHALE, G.A. (1988). Student major field and text content: inter-\nactive effects on reading comprehension in the Test of English\nas a Foreign Language.Language Testing, 5(1), 46\u201361.\nHALE, G.A. & COURTNEY, R. (1994).The effects of note-taking\non listening comprehension in the Test of English as a Foreign\nLanguage.Language Testing, 11(1), 29\u201347.\nHALL, C. (1993). The direct testing of oral skills in university \nforeign language teaching. IRAL,31(1), 23\u201338.\nHALL, E. (1991).Variations in composing behaviours of academic\nESL writers in test and non-test situations. TESL Canada,\n8(2), 9\u201333.\nHALLECK, G. B. (1992).The Oral Proficiency Interview: discrete\npoint test or a measure of communicative language ability?\nForeign Language Annals, 25(3), 227\u201331.\nHAMP-LYONS, L.& HENNING,G. (1991).Communicative writing\nprofiles: an investigation of the transferability of a multiple-\ntrait scoring instrument across ESL writing assessment con-\ntexts.Language Learning, 41(3), 337\u201373.\nHAMP-LYONS, L. & KROLL, B. (1997). TOEFL 2000: writing: com-\nposition, community and assessment. Princeton, NJ: Educational\nTesting Service.\nHARLEY, B., ALLEN, P., CUMMINS, J. & SWAIN, M. (1990) The\ndevelopment of second language proficiency, Cambridge: Cam-\nbridge University Press\nHARLOW, L. L. & CAMINERO, R. (1990). Oral testing of begin-\nning language students at large universities: is it worth the\ntrouble? Foreign Language Annals, 23(6), 489\u2013501.\nHE, A.W. (1998). Answering questions in LPIs: a case study, In\nYoung, R & He, A.W. (Eds.), Talking and testing: discourse\napproaches to the assessment of oral proficiency, Studies in\nBilingualism (Vol. 14) Amsterdam: John Benjamins Publishing\nCompany, p.10\u201316.\nHELLER,A., LYNCH,T.& WRIGHT,L. (1995).A comparison of lis-\ntening and speaking tests for student placement. Edinburgh\nWorking Papers in Applied Linguistics, 6, 27\u201340.\nHENNING, G. (1988). The influence of test and sample dimen-\nsionality on latent trait person ability and item difficulty cali-\nbrations.Language Testing, 5(1), 83\u201399.\nHENNING, G. (1989). Meanings and implications of the principle\nof local independence.Language Testing, 6(1), 95\u2013108.\nHENNING, G. (1991). A study of the effects of variation of short-term\nmemory load, reading response length, and processing hierarchy on\nTOEFL listening comprehension item performance (TOEFL\nResearch Report 33). Princeton, New Jersey: Educational\nTesting Service.\nHENNING, G. (1992a). Dimensionality and construct validity of\nlanguage tests.Language Testing, 9(1), 1\u201311.\nLanguage testing and assessment (Part 2)\nn\n108\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nHENNING, G. (1992b).The ACTFL Oral Proficiency Interview:\nvalidity evidence.System,20(3), 365\u201372.\nHENNING, G., HUDSON,T. & TURNER, J. (1985). Item response\ntheory and the assumption of unidimensionality for language\ntests.Language Testing, 2(2), 141\u201354.\nHERMAN, J., GEARHART, M. & BAKER, E. (1993).Assessing writ-\ning portfolios: issues in the validity and meaning of scores.\nEducational Assessment, 1(3), 201\u201324.\nHILL, C. & PARRY, K. (1992).The test at the gate: models of liter-\nacy in reading assessment.TESOL Quarterly, 26(3), 433\u201361.\nHILL, K. (1997). Who should be the judge? The use of non-\nnative speakers as raters on a test of English as an international\nlanguage. In A. Huhta, V. Kohonen, L. Kurki-Suonio & S.\nLuoma (Eds.), Current developments and alternatives in language\nassessment (pp. 275\u201390). Jyv\u00e4skyla:University of Jyv\u00e4skyla.\nHUDSON,T. (1991). Relationships among IRT item discrimina-\ntion amd item fit indices in criterion-referenced language\ntesting.Language Testing, 8(2), 160\u201381.\nHUDSON,T. (1993). Surrogate indices for item information func-\ntions in criterion-referenced lanuage testing. Language Testing,\n10(2), 171\u201391.\nHUEBNER, T. & JENSEN, A. (1992). A study of foreign language\nproficiency-based testing in secondary schools. Foreign\nLanguage Annals, 25(2), 105\u201315.\nHUOT, B. (1993).The influence of holistic scoring procedures on\nreading and rating student essays. In M.Williamson & B. Huot\n(Eds.), Validating holistic scoring for writing assessment: theoretical and\nempirical foundations (pp.206\u201336).Cresskill,NJ:Hampton Press.\nHYMES, D. (1972). On communicative competence. In J. B. Pride\n& J. Holmes (Eds.), Sociolinguistics: Selected readings (267\u201393).\nHarmondsworth,Middlesex:Penguin.\nIKEDA, K. (1998). The paired learner interview: a preliminary\ninvestigation applying Vygotskian insights. Language, Culture\nand Curriculum,11(1), 71\u201396.\nJAFARPUR,A. (1987).The short-context technique: an alternative\nfor testing reading comprehension. Language Testing, 4(2),\n195\u2013220.\nJAFARPUR, A. (1995). Is C-testing superior to cloze? Language\nTesting, 12(2), 194\u2013216.\nJAFARPUR,A. (1999a). Can the C-test be improved with classical\nitem analysis? System,27(1), 76\u201389.\nJAFARPUR,A. (1999b).What\u2019s magical about the rule-of-two for\nconstructing C-tests? RELC Journal, 30(2), 86\u2013100.\nJENNINGS, M., FOX, J., GRAVES, B. & SHOHAMY, E. (1999). The\ntest-takers\u2019 choice: an investigation of the effect of topic on\nlanguage-test performance.Language Testing, 16(4), 426\u201356.\nJENSEN, C. & HANSEN, C. (1995).The effect of prior knowledge\non EAP listening-test performance. Language Testing, 12(1),\n99\u2013119.\nJOHNSON, M & TYLER, A. (1998). Re-analyzing the OPI: how\nmuch does it look like natural conversation? In Young, R &\nHe, A.W. (Eds.), Talking and testing: discourse approaches to the\nassessment of oral proficiency, Studies in Bilingualism (Vol. 14)\nAmsterdam: John Benjamins Publishing Company, p. 27\u201351.\nKAGA, M. (1991). Dictation as a measure of Japanese proficiency.\nLanguage Testing, 8(2), 112\u201324.\nKATONA, L. (1996). Do\u2019s and don\u2019ts: recommendations for oral\nexaminers of foreign languages.NovELTy,3(3), 21\u201335.\nKATONA, L. (1998). Meaning negotiation in the Hungarian oral\nproficiency examination of English, In Young, R & He, A.W.\n(Eds.), Talking and testing: discourse approaches to the assessment of\noral proficiency, Studies in Bilingualism (Vol. 14) Amsterdam:\nJohn Benjamins Publishing Company, p. 239\u201367.\nKEMPE,V. & MACWHINNEY, B. (1996).The crosslinguistic assess-\nment of foreign language vocabulary learning. Applied\nPsycholinguistics, 17(2), 149\u201383.\nKENYON,D.M.,MALABONGA,V.& CARPENTER,H. (2001).Effects\nof examinee control on examinee attitudes and performance on a com-\nputerized oral proficiency test. Paper presented at the Language\nTesting Research Colloquium,St Louis.\nKIM, K. & SUH, K. (1998). Confirmation sequences as interac-\ntional resources in Korean language proficiency interviews, In\nYoung, R & He, A.W. (Eds.), Talking and testing: discourse\napproaches to the assessment of oral proficiency, Studies in\nBilingualism (Vol. 14) Amsterdam: John Benjamins Publishing\nCompany, p. 297\u2013332.\nKLEIN GUNNEWIEK, L. (1997). Are instruments measuring\naspects of language acquisition valid? Toepaste Taalwetenschap in\nArtikelen, 56(1), 35\u201345.\nKORMOS, J. (1999). Simulating conversations in oral-proficiency\nassessment: a conversation analysis of role plays and non-\nscripted interview in language exams. Language Testing, 16(2),\n163\u201388.\nKROLL, B. (1991). Understanding TOEFL\u2019s Test of Written\nEnglish.RELC Journal, 22(1), 20\u201333.\nKROLL, B. (1998). Assessing writing abilities. Annual Review of\nApplied Linguistics, 18, 219\u201340.\nKUNNAN,A. J. (1992).An investigation of a criterion-referenced\ntest using G-theory and factor and cluster analyses. Language\nTesting, 9(1), 30\u201349.\nKUNNAN, A. J. (1994). Modelling relationships among some \ntest-taker characteristics and performance on EFL tests: an\napproach to construct validation. Language Testing, 11(3),\n225\u201352.\nKUNNAN, A. J. (1995). Test taker characteristics and test performance:\nA structural modelling approach. (Studies in Language Testing\nSeries, Vol. 2). Cambridge: University of Cambridge Local\nExaminations Syndicate\/ Cambridge University Press.\nKUNNAN, A. J. (1998). An introduction to structural equation\nmodelling for language assessment research. Language Testing,\n15(3), 295\u2013352.\nKUO, J. & JIANG, X. (1997). Assessing the assessments: the OPI\nand the SOPI.Foreign Language Annals, 30(4), 503\u201312.\nLAUFER,B. (1992). How much lexis is necessary for reading com-\nprehension? In P. J. L. Arnaud & H. Bejoint (Eds.), Vocabulary\nand applied linguistics (pp.126\u201332).London:Macmillan.\nLAUFER, B. (1997).The lexical plight in second language reading:\nwords you don\u2019t know, words you think you know, and words\nyou can\u2019t guess. In J. Coady & T. Huckin (Eds.), Second language\nvocabulary acquisition (pp. 20\u201334). Cambridge: Cambridge\nUniversity Press.\nLAUFER, B., ELDER, C., & HILL, K. (2001). Validating a computer\nadaptive test of vocabulary size and strength. Paper presented at\nthe Language Testing Research Colloquium,St. Louis.\nLAUFER, B., & NATION, P. (1999). A vocabulary-size test of \ncontrolled productive ability.Language Testing, 16(1), 33\u201351.\nLAURIER, M., & DES BRISAY, M. (1991). Developing small-scale\nstandardised tests using an integrated approach. Bulletin of the\nCAAL,13(1), 57\u201372.\nLAZARATON,A. (1992).The structural organisation of a language\ninterview: a conversation-analytic perspective. System, 20(3),\n373\u201386.\nLAZARATON, A. (1996). Interlocutor support in oral proficiency\ninterviews: the case of CASE.Language Testing, 13(2), 151\u201372.\nLEE, J. F. & MUSUMECI, D. (1988). On hierarchies of reading skills\nand text types.Modern Language Journal, 72, 173\u201387.\nLEWKOWICZ, J. A. (1997). Investigating authenticity in language \ntesting. Unpublished PhD dissertation, Lancaster University,\nLancaster.\nLEWKOWICZ, J.A. (2000).Authenticity in language testing: some\noutstanding questions.Language Testing, 17(1), 43\u201364.\nLI,W. (1992). What is a test testing? An investigation of the agreement\nbetween students\u2019 test-taking processes and test constructors\u2019 presump-\ntions. Unpublished M.A., Lancaster University, Lancaster.\nLINDBLAD, T. (1992). Oral tests in Swedish schools: a five-year\nexperiment.System,20(3), 279\u201392.\nLISKIN-GASPARRO, J. (2001). Speaking as proficiency. Paper present-\ned at the LTRC\/AAAL Symposium,St Louis.\nLONG, D. R. (1990). What you don\u2019t know can\u2019t help you. An\nexploratory study of background knowledge and second \nn\nLanguage testing and assessment (Part 2)\n109\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nlanguage listening comprehension. Studies in Second Language\nLearning,REM Check 12, 65\u201380.\nLUMLEY, T. (1993). The notion of subskills in reading compre-\nhension tests: an EAP example. Language Testing, 10(3),\n211\u201334.\nLUMLEY, T. (1998). Perceptions of language-trained raters and\noccupational experts in a test of occupational English \nlanguage proficiency. English for Specific Purposes, 17(4),\n347\u201367.\nLUMLEY, T. (2000). The process of the assessment of writing perfor-\nmance: the rater\u2019s perspective. Unpublished PhD dissertation,The\nUniversity of Melbourne,Melbourne.\nLUMLEY, T. (forthcoming). Assessment criteria in a large scale\nwriting test: what do they really mean to the raters? Language\nTesting.\nLUMLEY,T. & MCNAMARA,T. F. (1995). Rater characteristics and\nrater bias: implications for training. Language Testing, 12(1),\n54\u201371.\nLUMLEY, T. & QIAN, D. (2001). Is speaking performance assessment\nbased mainly on grammar? Paper presented at the Language\nTesting Research Colloquium,St Louis.\nLUOMA, S. (2001). What does your language test measure?\nUnpublished PhD dissertation, University of Jyv\u00e4skyla,\nJyv\u00e4skyla.\nLYNCH, B. (1988). Person dimensionality in language test valida-\ntion.Language Testing, 5(2), 206\u201319.\nLYNCH, B. & MCNAMARA, T. F. (1998). Using G-theory and\nMany-facet Rasch measurement in the development of \nperformance assessments of the ESL speaking skills of \nimmigrants.Language Testing, 15(2), 158\u201380.\nLYNCH,T. (1994).The University of Edinburgh Test of English at\nMatriculation: validation report. Edinburgh Working Papers in\nApplied Linguistics, 5, 66\u201377.\nMARISI, P. M. (1994). Questions of regionalism in native-speaker\nOPI performance: the French-Canadian experience. Foreign\nLanguage Annals, 27(4), 505\u201321.\nMCNAMARA,T. (1990). Item Response Theory and the valida-\ntion of an ESP test for health professionals. Language Testing,\n7(1), 52\u201375.\nMCNAMARA,T. F. (1991).Test dimensionality: IRT analysis of an\nESP listening test.Language Testing, 8(2), 139\u201359.\nMCNAMARA, T. F. (1995). Modelling performance: opening\nPandora\u2019s Box.Applied Linguistics, 16(2), 159\u201375.\nMCNAMARA,T. F. & LUMLEY,T. (1997).The effect of interlocutor\nand assessment mode variables in overseas assessments of\nspeaking skills in occupational settings. Language Testing, 14(2),\n140\u201356.\nMEARA, P. (1992). Network structures and vocabulary acquisi-\ntion in a foreign language. In P. J. L. Arnaud & H. Bejoint\n(Eds.), Vocabulary and applied linguistics (pp. 62\u201370). London:\nMacmillan.\nMEARA, P. (1996).The dimensions of lexical competence. In G.\nBrown, K. Malmkjaer & J.Williams (Eds.), Performance and com-\npetence in second language acquisition. (35\u201353) Cambridge:\nCambridge University Press.\nMEARA, P. & BUXTON, B. (1987). An alternative to multiple\nchoice vocabulary tests.Language Testing, 4(2), 142 \u201354.\nMEIRON, B. & SCHICK, L. (2000). Ratings, raters and test perfor-\nmance: an exploratory study. In A. J. Kunnan (Ed.), Fairness and\nvalidation in language assessment (Studies in Language Testing\nSeries, Volume 9, pp. 153\u201376). Cambridge: UCLES\/\nCambridge University Press.\nMEREDITH,R.A. (1990).The oral proficiency interview in real life:\nsharpening the scale.Modern Language Journal,74(3),288\u201396.\nMERRYLEES, B. & MCDOWELL, C. (1999). An investigation of\nspeaking test reliability with particular reference to examiner\nattitude to the speaking test format and candidate\/examiner\ndiscourse produced. In R.Tulloh (Ed.), IELTS Research Reports\n1999 (Volume 2, 1\u201335). Canberra: IELTS Australia Pty\nLimited.\nMESSICK, S. (1989). Validity. In R. L. Linn (Ed.), Educational \nmeasurement.Third edition. (13\u2013103) New York:Macmillan.\nMESSICK, S. (1994).The interplay of evidence and consequences\nin the validation of performance assessments. Educational\nResearcher, 23(2), 13\u201323.\nMESSICK, S. (1996).Validity and washback in language testing.\nLanguage Testing, 13(3), 241\u201356.\nMILANOVIC, M., SAVILLE, N. & SHEN, S. (1996). A study of \nthe decision-making behaviour of composition markers. In\nM.Milanovic & N.Saville (Eds.),Performance testing, cognition and\nassessment. (92\u2013114) Cambridge:Cambridge University Press.\nMODER, C.L. & HALLECK, G.B. (1998). Framing the language\nproficiency interview as a speech event: native and non-native\nspeakers\u2019 questions, In Young, R & He,A.W. (Eds.), Talking and\ntesting: discourse approaches to the assessment of oral proficiency,\nStudies in Bilingualism (Vol. 14) Amsterdam: John Benjamins\nPublishing Company, p. 117\u201346.\nMORROW, K. (1979). Communicative language testing: revolu-\ntion or evolution? in Alderson, J.C & Hughes,A. (Eds.) Issues\nin Language Testing, ELT Documents 111, London:The British\nCouncil.\nMORTON, J. (1998). A cross-cultural study of second language\nnarrative discourse on an oral proficiency test. Prospect, 13(2),\n20\u201335.\nMUNBY, J. (1978). Communicative syllabus design. Cambridge:\nCambridge University Press.\nNAMBIAR, M. K. & GOON, C. (1993).Assessment of oral skills: a\ncomparison of scores obtained through audio recordings to\nthose obtained through face-to-face evaluation. RELC\nJournal, 24(1), 15\u201331.\nNATION, I. S. P. (1990). Teaching and learning vocabulary. New York:\nHeinle and Heinle.\nNORRIS, C. B. (1991). Evaluating English oral skills through the\ntechnique of writing as if speaking.System,19(3), 203\u201316.\nNORRIS, J., BROWN, J. D., HUDSON, T. & YOSHIOKA, J. (1998).\nDesigning second language performance assessments (Technical\nReport 18).Hawai\u2019i:University of Hawai\u2019i Press.\nNORTH, B. (1995). The development of a common framework scale of\nlanguage proficiency based on a theory of measurement.\nUnpublished PhD dissertation, Thames Valley University,\nLondon.\nNORTH, B. & SCHNEIDER, G. (1998). Scaling descriptors for lan-\nguage proficiency scales.Language Testing, 15 (2), 217\u201362.\nNURWENI,A. & READ, J. (1999).The English vocabulary knowl-\nedge of Indonesian university students. English for Specific\nPurposes, 18, 161\u201375.\nO\u2019LOUGHLIN, K. (1991). Assessing achievement in distance\nlearning.Prospect, 6(2), 58\u201366.\nO\u2019LOUGHLIN, K. (1995). Lexical density in candidate output on\ndirect and semi-direct versions of an oral proficiency test.\nLanguage Testing, 12(2), 217\u201337.\nO\u2019LOUGHLIN, K. (2000).The impact of gender in the IELTS oral\ninterview. In R. Tulloh (Ed.), IELTS Research Reports 2000\n(Volume 3, 1\u201328).Canberra: IELTS Australia Pty Limited.\nOLTMAN, P. K. & STRICKER, L. J. (1990). Developing homoge-\nneous TOEFL scales by multidimensional scaling. Language\nTesting, 7(1), 1\u201312.\nO\u2019MALLEY, J. M. & PIERCE, L.V. (1996). Authentic assessment for\nEnglish language learners.New York:Addison-Wesley.\nOSA-MELERO, L. & BATALLER, R. (2001). Spanish speaking test for\nelementary students: SOPI. Poster presented at the Language\nTesting Research Colloquium,St Louis.\nO\u2019SULLIVAN, B. (2000a). Exploring gender and oral proficiency\ninterview performance.System,28(3), 373\u201386.\nO\u2019SULLIVAN, B. (2000b). Towards a model of performance in oral lan-\nguage testing. Unpublished PhD dissertation, University of\nReading,Reading.\nPAVLOU, P. (1997). Do different speech interactions yield differ-\nent kinds of language? In A. Huhta, V. Kohonen, L. Kurki-\nSuonio & S. Luoma (Eds.), Current developments and alternatives\nLanguage testing and assessment (Part 2)\nn\n110\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nin language assessment. (pp. 185\u2013201) Jyv\u00e4skyla: University of\nJyv\u00e4skyla.\nPEIRCE, B. N. (1992). Demystifying the TOEFL Reading Test.\nTESOL Quarterly, 26(4), 665\u201389.\nPERETZ, A. S. & SHOHAM, M. (1990). Testing reading compre-\nhension in LSP: does topic familiarity affect assessed difficulty\nand actual performance? Reading in a Foreign Language, 7(1),\n447\u201355.\nPERKINS, K. (1992).The effect of passage topical structure types\non ESL reading comprehension difficulty. Language Testing,\n9(2), 163\u201373.\nPERKINS, K. & BRUTTEN, S. R. (1988a).A behavioural anchoring\nanalysis of three ESL reading comprehension tests. TESOL\nQuarterly, 22(4), 607\u201322.\nPERKINS, K. & BRUTTEN, S. R. (1988b).An item discriminability\nstudy of textually explicit, textually implicit, and scriptally\nimplicit questions.RELC Journal, 19(2), 1\u201311.\nPERKINS, K. & GASS, S. M. (1996).An investigation of patterns of\ndiscontinuous learning: implications for ESL measurement.\nLanguage Testing, 13(1), 63\u201382.\nPERKINS, K., GUPTA, L. & TAMMANA, R. (1995). Predicting item\ndifficulty in a reading comprehension test with an artificial\nneural network.Language Testing, 12(1), 34\u201353.\nPIERCE, L.V. & O\u2019MALLEY, J. M. (1992). Portfolio assessment for \nlanguage minority students.Washington D.C.: National Clearing-\nhouse for Bilingual Education.\nPOLLARD, J. (1998). Research and development \u2013 a complex\nrelationship.Language Testing Update, 24, 46\u201359.\nPORTER, D. (1991a).Affective factors in language testing. In J. C.\nAlderson & B. North (Eds.), Language testing in the 1990s: the\ncommunicative legacy (pp. 32\u201340). London: Macmillan (Modern\nEnglish Publications in association with the British Council).\nPORTER, D. (1991b). Affective factors in the assessment of oral\ninteraction: gender and status. In S.Anivan (Ed.), Current devel-\nopments in language testing (Vol. 25, pp. 92\u2013102). Singapore:\nSEAMEO Regional Language Centre.Anthology Series.\nPOWERS, D. E., SCHEDL, M.A.,WILSON LEUNG, S. & BUTLER, F.A.\n(1999).Validating the revised Test of Spoken English against a\ncriterion of communicative success. Language Testing, 16(4),\n399\u2013425.\nPURPURA, J. E. (1997). An analysis of the relationships between\ntest takers\u2019 cognitive and metacognitive strategy use and \nsecond language test performance. Language Learning, 42(2),\n289\u2013325.\nPURPURA, J. E. (1998). Investigating the effects of strategy use\nand second language test performance with high- and low-\nability test takers: a structural equation modelling approach.\nLanguage Testing, 15(3), 333\u201379.\nPURPURA, J. E. (1999). Learner strategy use and performance on \nlanguage tests:A structural equation modelling approach. (Studies in\nLanguage Testing Series, Vol. 8). Cambridge: University of\nCambridge Local Examinations Syndicate\/ Cambridge\nUniversity Press.\nPURVES,A. C. (1992). Reflections on research and assessment in\nwritten composition. Research in the Teaching of English, 26(1),\n108\u201322.\nRAFFALDINI,T. (1988).The use of situation tests as measures of\ncommunicative ability. Studies in Second Language Acquisition,\n10(2), 197\u2013216.\nRAIMES,A. (1990).The TOEFL test of written English: causes for\nconcern.TESOL Quarterly, 24(3), 427\u201342.\nREAD, J. (1993). The development of a new measure of L2\nvocabulary knowledge.Language Testing, 10(3), 354\u201371.\nREAD, J. & CHAPELLE, C. A. (2001). A framework for second \nlanguage vocabulary assessment.Language Testing, 18(1), 1\u201332.\nREA-DICKINS, P. (1997).The testing of grammar. In C. Clapham\n& D. Corson (Eds.), Language testing and assessment (Vol. 7,\npp. 87\u201397). Dordrecht, The Netherlands: Kluwer Academic\nPublishers.\nREA-DICKINS, P. (2001). Fossilisation or evolution: the case of\ngrammar testing. In C. Elder, A. Brown, E. Grove, K. Hill,\nN. Iwashita T. Lumley. T. F. McNamara & K. O\u2019Loughlin\n(Eds.), Experimenting with uncertainty: essays in honour of Alan\nDavies (Studies in Language Testing Series,Vol. 11, pp. 22\u201332).\nCambridge: University of Cambridge Local Examinations\nSyndicate and Cambridge University Press.\nREED, D. J. (1992). The relationship between criterion-based \nlevels of oral proficiency and norm-referenced scores of gen-\neral proficiency in English as a second language. System, 20(3),\n329\u201345.\nREED, D. J. & COHEN,A. D. (2001). Revisiting raters and ratings\nin oral language assessment. In C. Elder, A. Brown, E. Grove,\nK.Hill,N. Iwashita,T.Lumley,T.F.McNamara & K.O\u2019Loughlin\n(Eds.), Experimenting with uncertainty: essays in honour of Alan\nDavies (Studies in Language Testing Series, Volume 11,\npp.82\u201396).Cambridge:UCLES\/Cambridge University Press.\nREED, D. J. & HALLECK, G. B. (1997). Probing above the ceiling\nin oral interviews: what\u2019s up there? In A. Huhta,V. Kohonen,\nL. Kurki-Suonio & S. Luoma (Eds.), Current developments and\nalternatives in language assessment. (pp. 225\u201338) Jyv\u00e4skyla:\nUniversity of Jyv\u00e4skyla.\nREINHARD, D. (1991). Einheitliche Pr\u00fcfungsanforderungen in\nder Abiturpr\u00fcfung Englisch? Eine Betrachtung nach einer\nVergleichskorretur.Die Neueren Sprachen, 90(6), 624 \u201335.\nREVES,T. & LEVINE,A. (1992). From needs analysis to criterion-\nreferenced testing.System,20(2), 201\u201310.\nRICHARDS, B. & MALVERN, D. (1997). Type-Token and Type-\nType measures of vocabulary diversity and lexical style: an \nannotated bibliography. Reading: University of Reading.\nhttp:\/\/www.rdg.ac.uk\/~ehscrichb\/home1.html\nRILEY, G. L. & LEE, J. F. (1996). A comparison of recall and \nsummary protocols as measures of second language reading\ncomprehension.Language Testing, 13(2), 173\u201389.\nROBINSON, R. E. (1992). Developing practical speaking tests for\nthe foreign language classroom: a small group approach.\nForeign Language Annals, 25(6), 487\u201396.\nROSS, S. (1992). Accommodative questions in oral proficiency\ninterviews.Language Testing, 9(2), 173\u201387.\nROSS, S. (1998). Divergent frame interpretations in oral profi-\nciency interview interaction, In Young, R & He, A.W. (Eds.),\nTalking and testing: discourse approaches to the assessment of oral\nproficiency, Studies in Bilingualism (Vol. 14) Amsterdam: John\nBenjamins Publishing Company, p. 333\u201353.\nROSS, S. & BERWICK, R. (1992).The discourse of accommoda-\ntion in oral proficiency interviews. Studies in Second Language\nAcquisition, 14, 159\u201376.\nSAKYI,A.A. (2000).Validation of holistic scoring for ESL writing\nassessment: how raters evaluate compositions. In A. J. Kunnan\n(Ed.), Fairness and validation in language assessment (Studies in\nLanguage Testing Series, Vol. 9, pp. 129\u201352). Cambridge:\nUniversity of Cambridge Local Examinations Syndicate and\nCambridge University Press.\nSALABERRY, R. (2000). Revising the revised format of the\nACTFL Oral Proficiency Interview. Language Testing, 17(3),\n289\u2013310.\nSALAGER-MEYER, F. (1991). Reading expository prose at the\npost-secondary level: the influence of textual variables on L2\nreading comprehension (a genre-based approach). Reading in a\nForeign Language, 8(1), 645\u201362.\nSALVI, R. (1991). A communicative approach to testing written\nEnglish in non-native speakers. Rassegna Italiana di Linguistica\nApplicata, 23(2), 67\u201391.\nSARIG, G. (1989). Testing meaning construction: can we do it\nfairly? Language Testing, 6(1), 77\u201394.\nSASAKI, M. (1991). A comparison of two methods for detecting\ndifferential item functioning in an ESL placement test.\nLanguage Testing, 8(2), 95\u2013111.\nSASAKI, M. & HIROSE, K. (1999). Development of an analytic \nrating scale for Japanese L1 writing. Language Testing, 16(4),\n457\u201378.\nn\nLanguage testing and assessment (Part 2)\n111\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nSAVILLE, N. & HARGREAVES, P. (1999). Assessing speaking in the\nrevised FCE.English Language Teaching Journal, 53(1), 42\u201351.\nSCHILS, E. D. J.,VAN DER POEL, M. G. M. & WELTENS, B. (1991).\nThe reliability ritual.Language Testing, 8(2), 125\u201338.\nSCHMITT, N. (1999).The relationship between TOEFL vocabu-\nlary items and meaning, association, collocation and word-\nclass knowledge.Language Testing, 16, 189\u2013216.\nSCHMITT, N., SCHMITT, D. & CLAPHAM, C. (2001). Developing\nand exploring the behaviour of two new versions of the\nVocabulary Levels Test.Language Testing, 18(1), 55\u201388.\nSCHOONEN, R.,VERGEER, M. & EITING, M. (1997).The assess-\nment of writing ability: expert readers versus lay readers.\nLanguage Testing, 14(2), 157\u201384.\nSCOTT, M. L., STANSFIELD, C. W. & KENYON, D. M. (1996).\nExamining validity in a performance test: the listening sum-\nmary translation exam (LSTE).Language Testing, 13, 83\u2013109.\nSELINKER, L. (2001). Speaking as performance within a discourse\ndomain. Paper presented at the LTRC\/AAAL Symposium,\nSt Louis.\nSHEPARD, L. (1993). Evaluating test validity. Review of Research in\nEducation, 19, 405\u201350.\nSHERMAN, J. (1997).The effect of question preview in listening\ncomprehension tests.Language Testing, 14(2), 185\u2013213.\nSHOHAMY, E. (1990a). Discourse analysis in language testing.\nAnnual Review of Applied Linguistics, 11, 115\u201331.\nSHOHAMY, E. (1990b). Language testing priorities: a different\nperspective.Foreign Language Annals, 23(5), 385\u201394.\nSHOHAMY, E. (1994). The validity of direct versus semi-direct\noral tests.Language Testing, 11 (2) 99\u2013124\nSHOHAMY, E., GORDON, C. & KRAEMER, R. (1992).The effect of\nraters\u2019 background and training on the reliability of direct\nwriting tests.Modern Language Journal, 76(1), 27\u201333.\nSHOHAMY, E. & INBAR, O. (1991).Validation of listening compre-\nhension tests: the effect of text and question type. Language\nTesting, 8(1), 23\u201340.\nSPENCE-BROWN, R (2001).The eye of the beholder: authenticity\nin an embedded assessment task. Language Testing, 18 (4),\n463\u201381.\nSPOLSKY, B. (1990). Oral Examinations: an historical note.\nLanguage Testing, 7(2), 158\u201373.\nSPOLSKY, B. (1995). Measured words. Oxford: Oxford University\nPress.\nSPOLSKY, B. (2001). The speaking construct in historical perspective.\nPaper presented at the LTRC\/AAAL Symposium,St Louis.\nSTANSFIELD, C. W. & KENYON, D. M. (1992). The development\nand validation of a simulated oral proficiency interview.\nModern Language Journal, 76(2), 129\u201341.\nSTANSFIELD, C. W., SCOTT, M. L. & KENYON, D. M. (1990).\nListening summary translation exam (LSTE) \u2013 Spanish (Final\nProject Report. ERIC Document Reproduction Service, ED\n323 786).Washington DC:Centre for Applied Linguistics.\nSTANSFIELD, C. W., WU, W. M. & LIU, C. C. (1997). Listening\nSummary Translation Exam (LSTE) in Taiwanese, aka Minnan\n(Final Project Report. ERIC Document Reproduction\nService, ED 413 788). N. Bethesda, MD: Second Language\nTesting, Inc.\nSTANSFIELD, C.W.,WU,W. M. & VAN DER HEIDE, M. (2000). A\njob-relevant listening summary translation exam in Minnan.\nIn A. J. Kunnan (Ed.), Fairness and validation in language assess-\nment (Studies in Language Testing Series,Vol. 9, pp. 177\u2013200).\nCambridge: University of Cambridge Local Examinations\nSyndicate and Cambridge University Press.\nSTAUFFER, S. & KENYON, D. M. (2001). A computer-assisted, com-\nputer-adaptive oral proficiency assessment instrument prototype.\nPoster presented at The Language Testing Research\nColloquium,St Louis.\nSTOREY, P. (1994). Investigating construct validity through test-taker\nintrospection. Unpublished PhD dissertation, University of\nReading,Reading.\nSTOREY, P. (1997). Examining the test-taking process: a cognitive\nperspective on the discourse cloze test. Language Testing, 14(2),\n214\u201331.\nSTREET, B. V. (1984). Literacy in theory and practice. Cambridge:\nCambridge University Press.\nSTRONG-KRAUSE, D. (2001). A tree-based modeling approach to \nconstruct validation of a computer-delivered speaking test. Paper \npresented at the Language Testing Research Colloquium,\nSt Louis.\nSWAIN, M. (1993). Second-language testing and second-language\nacquisition: is there a conflict with traditional psychometrics?\nLanguage Testing, 10(2), 191\u2013207.\nSWAIN, M. (2001a). Examining dialogue: another approach to\ncontent specification and to validating inferences drawn from\ntest scores.Language Testing, 18(3), 275\u2013302.\nSWAIN, M. (2001b). Speaking as a cognitive tool. Paper presented at\nthe LTRC\/AAAL Symposium,St Louis.\nTAKALA, S. & KAFTANDJIEVA, F. (2000).Test fairness: a DIF analy-\nsis of an L2 vocabulary test.Language Testing, 17(3), 323\u201340.\nTAYLOR, L. & JONES, N. (2001). Revising instruments for rating\nspeaking: combining qualitative and quantitative insights. Paper \npresented at the Language Testing Research Colloquium,\nSt Louis.\nTEDICK, D. J. (1990). ESL writing assessment: subject-matter\nknowledge and its impact on performance. English for Specific\nPurposes, 9(2), 123\u201343.\nTHOMPSON, I. (1995). A study of interrater reliability of the\nACTFL Oral Proficiency Interview in five European lan-\nguages: data from ESL, French, German, Russian and Spanish.\nForeign Language Annals, 28(3), 407\u201322.\nTRYON,W.W. (1979).The test-trait fallacy. American Psychologist,\n334, 402\u20136.\nTSUI, A. B. M. & FULLILOVE, J. (1998). Bottom-up or top-down\nprocessing as a discriminator of L2 listening performance.\nApplied Linguistics, 19(4), 432-451.\nUPSHUR, J. & TURNER, C. E. (1995). Constructing rating scales\nfor second language tests. English Language Teaching Journal,\n49(1), 3\u201312.\nUPSHUR, J. & TURNER, C. E. (1999). Systematic effects in the \nrating of second-language speaking ability: test method and\nlearner discourse.Language Testing, 16(1), 82\u2013111.\nVALDES, G. (1989). Teaching Spanish to Hispanic bilinguals: a\nlook at oral proficiency testing and the proficiency move-\nment.Hispania, 72(2), 392\u2013401.\nVAN ELMPT, M. & LOONEN, P. (1998). Open questions: answers in\nthe foreign language? Toegepaste Taalwetenschap in Artikelen, 58,\n149-\u201354.\nVAN LIER, L. (1989). Reeling, writhing, drawling, stretching, and\nfainting in coils: oral proficiency interviews as conversation.\nTESOL Quarterly, 23(3), 489\u2013508.\nVAUGHAN, C. (1991). Holistic assessment: what goes on in raters\u2019\nminds? In L. Hamp-Lyons (Ed.), Assessing second language \nwriting in academic contexts (pp. 111-126). Norwood, NJ:Ablex\nPublishing Co-orporation.\nVERHALLEN, M. & SCHOONEN, R. (1993). Lexical knowledge of\nmonolingual and bilingual children. Applied Linguistics, 14,\n344\u201363.\nVERMEER, A. (2000). Coming to grips with lexical richness in\nspontaneous speech data.Language Testing, 17(1), 65\u201383.\nWALKER, C. (1990). Large-scale oral testing. Applied Linguistics,\n11(2), 200-219.\nWALL, D., CLAPHAM, C. & ALDERSON, J. C. (1994). Evaluating a\nplacement test.Language Testing, 11(3), 321\u201344.\nWALSH, P. (1999). Can a language interview be used to measure\ninteractional skill? CALS Working Papers in TEFL,2, 1\u201328.\nWEIGLE, S. C. (1994). Effects of training on raters of ESL com-\npositions.Language Testing, 11(2), 197\u2013223.\nWEIGLE, S. C. (1998). Using FACETS to model rater training\neffects.Language Testing, 15(2), 263\u201387.\nWEIGLE, S. C. & LYNCH, B. (1995). Hypothesis testing in con-\nstruct validation. In A. Cumming & R. Berwick (Eds.),\nLanguage testing and assessment (Part 2)\nn\n112\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nValidation in language testing (pp. 58-71). Clevedon, Avon:\nMultilingual Matters Ltd.\nWEIR, C. J. (1983). Identifying the language problems of overseas \nstudents in tertiary education in the UK. Unpublished PhD \ndissertation,University of London,London.\nWEIR, C., O\u2019SULLIVAN, B. & FRENCH,A. (2001). Task difficulty in\ntesting spoken language: a socio-cognitive perspective. Paper present-\ned at the Language Testing Research Colloquium,St Louis.\nWELLING-SLOOTMAEKERS, M. (1999). Language examinations in\nDutch secondary schools from 2000 onwards. Levende Talen,\n542, 488\u201390.\nWIGGLESWORTH, G. (1993). Exploring bias analysis as a tool for\nimproving rater consistency in assessing oral interaction.\nLanguage Testing, 10(3), 305\u201335.\nWIGGLESWORTH, G. (1997). An investigation of planning time\nand proficiency level on oral test discourse. Language Testing,\n14(1), 85\u2013106.\nWOLF, D. F. (1993a). A comparison of assessment tasks used to\nmeasure FL reading comprehension. Modern Language Journal,\n77(4), 473\u201389.\nWOLF, D. F. (1993b). Issues in reading comprehension assessment:\nimplications for the development of research instruments and\nclassroom tests.Foreign Language Annals, 26(3), 322\u201331.\nWOLFE, E. & FELTOVICH, B. (1994). Learning to rate essays:A study\nof scorer cognition. Paper presented at the Annual Meeting of the\nAmerican Educational Research Association,New Orleans,LA.\n(ERIC Document Reproduction Service No.ED 368 377).\nWU, S.-M. (1995). Evaluating narrative essays: a discourse analy-\nsis perspective.RELC Journal, 26(1), 1\u201326.\nYI\u2019AN,W. (1998).What do tests of listening comprehension test?\nA retrospection study of EFL test-takers performing a multi-\nple-choice task.Language Testing, 15(1), 21\u201344.\nYOSHIDA-MORISE,Y. (1998).The use of communication strate-\ngies in language proficiency interviews, In Young, R & He,\nA.W. (Eds.), Talking and testing: discourse approaches to the \nassessment of oral proficiency, Studies in Bilingualism (Vol.\n14) Amsterdam: John Benjamins Publishing Company,\np. 205\u201338.\nYOUNG, R. (1995). Conversational styles in language proficiency\ninterviews.Language Learning, 45(1), 3\u201342.\nYOUNG, R. (2001). The role of speaking in discursive practice. Paper\npresented at the LTRC\/AAAL Symposium,St Louis.\nYOUNG, R. & HALLECK, G. B. (1998). \u201cLet them eat cake!\u201d\nor how to avoid losing your head in cross-cultural con-\nversations, In Young, R & He, A.W. (Eds.), Talking and \ntesting: Discourse approaches to the assessment of oral proficiency\nStudies in Bilingualism (Vol. 14) Amsterdam: John Benjamins\nPublishing Company, p. 355\u201382.\nYOUNG, R. & HE, A. W. (1998). Talking and testing: Discourse\napproaches to the assessment of oral proficiency (Studies in\nBilingualism,Vol. 14).Amsterdam: John Benjamins.\nZEIDNER, M. & BENSOUSSAN, M. (1988). College students\u2019 atti-\ntudes towards written versus oral tests of English as a Foreign\nLanguage.Language Testing, 5(1), 100\u201314.\nn\nLanguage testing and assessment (Part 2)\n113\n"}