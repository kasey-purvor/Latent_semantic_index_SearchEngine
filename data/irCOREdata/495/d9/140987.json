{"doi":"10.1109\/TII.2009.2037494","coreId":"140987","oai":"oai:dspace.lib.cranfield.ac.uk:1826\/5062","identifiers":["oai:dspace.lib.cranfield.ac.uk:1826\/5062","10.1109\/TII.2009.2037494"],"title":"Bidirectional branch and bound for controlled variable selection. Part III: local\naverage loss minimization","authors":["Kariwala, Vinay","Cao, Yi"],"enrichments":{"references":[{"id":37923389,"title":"A branch and bound algorithm for feature subset selection.","authors":[],"date":"1977","doi":"10.1109\/tc.1977.1674939","raw":"P. Narendra and K. Fukunaga. A branch and bound algorithm for feature subset selection. IEEE Trans. Comput., C-26:917{922, 1977.","cites":null},{"id":37923394,"title":"A more ecient branch and bound algorithm for feature selection. Pattern Recognition, 26:883{889,","authors":[],"date":"1993","doi":"10.1016\/0031-3203(93)90054-z","raw":"B. Yu and B. Yuan. A more ecient branch and bound algorithm for feature selection. Pattern Recognition, 26:883{889, 1993. List of Figures 1 Solution trees for selecting 2 out of 6 elements. . . . . . . . . . . . . . . . . . . . . . . . . . 4 2 Random test 1: (a) computation time and (b) number of nodes evaluated against ny; Random test 2: (c) computation time and (d) number of nodes evaluated against ny. . . . . 17 3 Random test 3: (a) computation time and (b) number of nodes evaluated against nu; Random test 4: (c) computation time and (d) number of nodes evaluated against nu. . . . . 18 234 (a) Average losses of 10-best measurement combinations against the number of measurements, (b) Comparison of computation time, (c) Comparison of number of node evaluations, and (d) Ratios of computation time and number of node evaluations required by PB3 over DOWN algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 List of Tables 1 BAB programs for comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16","cites":null},{"id":37923366,"title":"An improved branch and bound algorithm for feature selection.","authors":[],"date":"2003","doi":"10.1016\/s0167-8655(03)00020-5","raw":"X.-W. Chen. An improved branch and bound algorithm for feature selection. Pattern Recognition Letters, 24:1925{1933, 2003.","cites":null},{"id":37923363,"title":"Bidirectional branch and bound for controlled variable selection: Part I. Principles and minimum singular value criterion.","authors":[],"date":"2008","doi":"10.1016\/j.compchemeng.2007.11.011","raw":"Y. Cao and V. Kariwala. Bidirectional branch and bound for controlled variable selection: Part I. Principles and minimum singular value criterion. Comput. Chem. Engng., 32(10):2306{2319, 2008.","cites":null},{"id":37923387,"title":"Bidirectional branch and bound for controlled variable selection: Part II. Exact local method for self-optimizing control.","authors":[],"date":"2009","doi":"10.1016\/j.compchemeng.2009.01.014","raw":"V. Kariwala and Y. Cao. Bidirectional branch and bound for controlled variable selection: Part II. Exact local method for self-optimizing control. Comput. Chem. Eng., 33(8):1402{1412, 2009.","cites":null},{"id":37923390,"title":"Dynamics and control of distillation columns - A tutorial introduction.","authors":[],"date":"1997","doi":"10.4173\/mic.1997.3.1","raw":"S. Skogestad. Dynamics and control of distillation columns - A tutorial introduction. Trans. IChemE Part A, 75:539{562, 1997.","cites":null},{"id":37923393,"title":"Fast branch & bound algorithms for optimal feature selection.","authors":[],"date":"2004","doi":"10.1109\/tpami.2004.28","raw":"P. Somol, P. Pudil, and J. Kittler. Fast branch & bound algorithms for optimal feature selection. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 900{912, 2004.","cites":null},{"id":37923364,"title":"Improved branch and bound method for control structure screening.","authors":[],"date":"2005","doi":"10.1016\/j.ces.2004.10.025","raw":"Y. Cao and P. Saha. Improved branch and bound method for control structure screening. Chem. Engg. Sci., 60(6):1555{1564, 2005.","cites":null},{"id":37923388,"title":"Local self-optimizing control with average loss minimization.","authors":[],"date":"2008","doi":"10.1021\/ie070897+","raw":"V. Kariwala, Y. Cao, and S. Janardhanan. Local self-optimizing control with average loss minimization. Ind. Eng. Chem. Res., 47(4):1150{1158, 2008.","cites":null},{"id":37923382,"title":"Matrix Computations. The Johns Hopkins","authors":[],"date":"1993","doi":null,"raw":"G. H. Golub and C. F. van Loan. Matrix Computations. The Johns Hopkins University Press, Baltimore, MD, 3rd edition, 1993.","cites":null},{"id":37923392,"title":"Multivariable Feedback Control: Analysis and Design.","authors":[],"date":"2005","doi":"10.1007\/978-1-4612-0313-1_9","raw":"S. Skogestad and I. Postlethwaite. Multivariable Feedback Control: Analysis and Design. John Wiley & Sons, Chichester, UK, 2nd edition, 2005.","cites":null},{"id":37923368,"title":"Nonlinear and Mixed-Integer Optimization.","authors":[],"date":"1995","doi":"10.1007\/0-306-48332-7_301","raw":"C. A. Floudas. Nonlinear and Mixed-Integer Optimization. Oxford University Press, Newyork, NY, USA, 1995.","cites":null},{"id":37923384,"title":"On a new approach for self-optimizing control structure design.","authors":[],"date":"2009","doi":"10.3182\/20090712-4-tr-2008.00126","raw":"S. Heldt. On a new approach for self-optimizing control structure design. In Proc. 7th Intl. Symposium on ADCHEM, Istanbul, Turkey, 2009.","cites":null},{"id":37923386,"title":"Optimal measurement combination for local self-optimizing control.","authors":[],"date":"2007","doi":"10.1021\/ie0610187","raw":"V. Kariwala. Optimal measurement combination for local self-optimizing control. Ind. Eng. Chem. Res., 46(11):3629{3634, 2007.","cites":null},{"id":37923362,"title":"Optimal measurement combinations as controlled variables.","authors":[],"date":"2009","doi":"10.1016\/j.jprocont.2008.01.002","raw":"V. Alstad, S. Skogestad, and E. S. Hori. Optimal measurement combinations as controlled variables. J. Proc. Control, 19(1):138{148, 2009.","cites":null},{"id":37923383,"title":"Optimal selection of controlled variables.","authors":[],"date":"2003","doi":"10.1021\/ie020833t","raw":"I. J. Halvorsen, S. Skogestad, J. C. Morud, and V. Alstad. Optimal selection of controlled variables. Ind. Eng. Chem. Res., 42(14):3273{3284, 2003.","cites":null},{"id":37923391,"title":"Plantwide control: The search for the self-optimizing control structure.","authors":[],"date":"2000","doi":"10.1016\/s0959-1524(00)00023-8","raw":"S. Skogestad. Plantwide control: The search for the self-optimizing control structure. J. Proc. Control, 10(5):487{507, 2000.","cites":null},{"id":37923385,"title":"Selection of controlled variables: Maximum gain rule and combination of measurements.","authors":[],"date":"2008","doi":"10.1021\/ie0711978","raw":"E. S. Hori and S. Skogestad. Selection of controlled variables: Maximum gain rule and combination of measurements. Ind. Eng. Chem. Res., 47(23):9465{9471, 2008. 22[10] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, UK, 1985.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-02-05T00:00:00Z","abstract":"The selection of controlled variables (CVs) from available measurements through\nexhaustive search is computationally forbidding for large-scale processes. We\nhave recently proposed novel bidirectional branch and bound (B-3) approaches for\nCV selection using the minimum singular value (MSV) rule and the local worst-\ncase loss criterion in the framework of self-optimizing control. However, the\nMSV rule is approximate and worst-case scenario may not occur frequently in\npractice. Thus, CV selection by minimizing local average loss can be deemed as\nmost reliable. In this work, the B-3 approach is extended to CV selection based\non local average loss metric. Lower bounds on local average loss and, fast\npruning and branching algorithms are derived for the efficient B-3 algorithm.\nRandom matrices and binary distillation column case study are used to\ndemonstrate the computational efficiency of the proposed method","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/140987.pdf","fullTextIdentifier":"http:\/\/dx.doi.org\/10.1109\/TII.2009.2037494","pdfHashValue":"07f268c99a0c24f2dec2c9a364d840ede12be87f","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\noai:dspace.lib.cranfield.ac.uk:1826\/5062<\/identifier><datestamp>2016-07-12T10:55:56Z<\/datestamp><setSpec>hdl_1826_19<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>Bidirectional branch and bound for controlled variable selection. Part III: local\naverage loss minimization<\/dc:title><dc:creator>Kariwala, Vinay<\/dc:creator><dc:creator>Cao, Yi<\/dc:creator><dc:subject>Branch and bound combinatorial optimization control structure design controlled variables self-optimizing control self-optimizing control algorithm combination<\/dc:subject><dc:description>The selection of controlled variables (CVs) from available measurements through\nexhaustive search is computationally forbidding for large-scale processes. We\nhave recently proposed novel bidirectional branch and bound (B-3) approaches for\nCV selection using the minimum singular value (MSV) rule and the local worst-\ncase loss criterion in the framework of self-optimizing control. However, the\nMSV rule is approximate and worst-case scenario may not occur frequently in\npractice. Thus, CV selection by minimizing local average loss can be deemed as\nmost reliable. In this work, the B-3 approach is extended to CV selection based\non local average loss metric. Lower bounds on local average loss and, fast\npruning and branching algorithms are derived for the efficient B-3 algorithm.\nRandom matrices and binary distillation column case study are used to\ndemonstrate the computational efficiency of the proposed method.<\/dc:description><dc:publisher>IEEE<\/dc:publisher><dc:date>2011-11-13T23:25:16Z<\/dc:date><dc:date>2011-11-13T23:25:16Z<\/dc:date><dc:date>2010-02-05T00:00:00Z<\/dc:date><dc:type>Article<\/dc:type><dc:identifier>Vinay Kariwala and Yi Cao, Bidirectional Branch and Bound for Controlled Variable Selection. Part III: Local Average Loss Minimization. IEEE Transactions On Industrial Informatics, Vol.6(1), 2010, p.54-61<\/dc:identifier><dc:identifier>1551-3203<\/dc:identifier><dc:identifier>http:\/\/dx.doi.org\/10.1109\/TII.2009.2037494<\/dc:identifier><dc:identifier>http:\/\/dspace.lib.cranfield.ac.uk\/handle\/1826\/5062<\/dc:identifier><dc:language>en_UK<\/dc:language><dc:rights>This is the author\u2019s version of a work that was accepted for publication in IEEE Transactions On Industrial Informatics. Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may have been made to this work since it was submitted for publication. A definitive version was subsequently published in IEEE Transactions On Industrial Informatics, Vol.6(1), 2010, p.54-61 DOI:10.1109\/TII.2009.2037494<\/dc:rights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:1551-3203","1551-3203"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":["Branch and bound combinatorial optimization control structure design controlled variables self-optimizing control self-optimizing control algorithm combination"],"subject":["Article"],"fullText":"Bidirectional Branch and Bound for Controlled Variable Selection\nPart III: Local Average Loss Minimization\nVinay Kariwala\u2020 and Yi Cao\u2021\u2217\n\u2020 Division of Chemical & Biomolecular Engineering,\nNanyang Technological University, Singapore 637459\n\u2021School of Engineering, Cranfield University, Cranfield, Bedford MK43 0AL, UK\nAbstract\nThe selection of controlled variables (CV) from available measurements through exhaustive search\nis computationally forbidding for large-scale problems. We have recently proposed novel bidirectional\nbranch and bound (B3) approaches for CV selection using the minimum singular value (MSV) rule and\nthe local worst-case loss criterion in the framework of self-optimizing control. However, the MSV rule is\napproximate and worst-case scenario may not occur frequently in practice. Thus, CV selection through\nminimization of local average loss can be deemed as most reliable. In this work, the B3 approach is\nextended to CV selection based on the recently developed local average loss metric. Lower bounds on\nlocal average loss and, fast pruning and branching algorithms are derived for the efficient B3 algorithm.\nRandom matrices and binary distillation column case study are used to demonstrate the computational\nefficiency of the proposed method.\nKeywords: Branch and bound, Control structure design, Controlled variables, Combinatorial opti-\nmization, Self-optimizing control.\n1 Introduction\nControl structure design deals with the selection of controlled and manipulated variables, and the pairings\ninterconnecting these variables. Among these tasks, the selection of controlled variables (CVs) from\navailable measurements can be deemed to be most important. Traditionally, CVs have been selected\n\u2217Corresponding Author: Tel: +44-1234-750111; Fax: +44-1234-754685; E-mail:y.cao@cranfield.ac.uk\n1\nbased on intuition and process knowledge. To systematically select CVs, Skogestad [16] introduced the\nconcept of self-optimizing control. In this approach, CVs are selected such that in presence of disturbances,\nthe loss incurred in implementing the operational policy by holding the selected CVs at constant setpoints\nis minimal, as compared to the use of an online optimizer.\nThe choice of CVs based on the general non-linear formulation of self-optimizing control requires solving\nlarge-dimensional non-convex optimization problems [16]. To quickly pre-screen alternatives, local methods\nhave been proposed including the minimum singular value (MSV) rule [17] and exact local methods with\nworst-case [7] and average loss minimization [13]. Though the local methods simplify loss evaluation for a\nsingle alternative, every feasible alternative still needs to be evaluated to find the optimal solution. As the\nnumber of alternatives grows rapidly with process dimensions, such an exhaustive search is computationally\nintractable for large-scale processes. Thus, an efficient method is needed to find a subset of available\nmeasurements, which can be used as CVs (Problem 1).\nInstead of selecting CVs as a subset of available measurements, it is possible to obtain lower losses using\nlinear combinations of available measurements as CVs [7]. Recently, explicit solutions to the problem of\nfinding locally optimal measurement combinations have been proposed [1, 8, 11, 13]. It is, however, noted\nin [1, 11, 13] that the use of combinations of a few measurements as CVs often provide similar loss as\nthe case where combinations of all available measurements are used. Though the former approach results\nin control structures with lower complexity, it gives rise to another combinatorial optimization problem\ninvolving the identification of the set of measurements, whose combinations can be used as CVs (Problem\n2).\nBoth Problems 1 and 2 can be seen as subset selection problems, for which only exhaustive search and\nbranch and bound (BAB) method guarantee globally optimal solution [4]. A BAB approach divides the\nproblem into several sub-problems (nodes). For minimization problems, lower bounds are computed on\nthe selection criteria for all solutions with target subset size, which can be reached from the node under\nconsideration. Subsequently, the current node is pruned (eliminated without further evaluation), if the\ncomputed lower bound is greater than an upper bound on the optimal solution (usually taken as the best\nknown solution). The pruning of nodes allows the BAB method to gain efficiency in comparison with\nexhaustive search. The traditional BAB methods for subset selection use downwards approach, where\npruning is performed on nodes with gradually decreasing subset size [3, 4, 14, 18, 19]. Recently, a novel\nbidirectional BAB (B3) approach [2] has been proposed for CV selection, where non-optimal nodes are\npruned in downwards as well as upwards (gradually increasing subset size) directions simultaneously, which\nsignificantly reduces the solution time.\n2\nThe bidirectional BAB (B3) approach has been applied to solve Problem 1 with MSV rule [2] and local\nworst-case loss [12] as selection criteria. A partially bidirectional BAB (PB3) method has also been\nproposed to solve Problem 2 through minimization of local worst-case loss [12]. The MSV rule, however,\nis approximate and can lead to non-optimal set of CVs [9]. Selection of CVs based on local worst-case loss\nminimization can also be conservative, as the worst-case may not occur frequently in practice [13]. Thus,\nCV selection through minimization of local average loss, which represents the expected loss incurred over\nthe long-term operation of the plant, can be deemed as most reliable.\nIn this paper, a B3 method for solving Problem 1 through minimization of local average loss is proposed.\nUpwards and downwards lower bounds on the local average loss and fast pruning algorithms are derived\nto obtain an efficient B3 algorithm. The downwards lower bound derived for Problem 1 can also be used\nfor pruning non-optimal nodes for Problem 2 with local average loss minimization. The upwards lower\nbound for Problem 2, however, only holds when the number of elements of the node under consideration\nexceeds a certain number. To realize the advantages of bidirectional BAB method to some extent, we\ndevelop a PB3 method for selection of measurements, whose combination can be used as CVs to minimize\nlocal average loss. Random matrices and binary distillation column case study are used to demonstrate\nthe computational efficiency of the proposed method.\nThe rest of this paper is organized as follows: a tutorial overview of the unidirectional and bidirectional\nBAB methods for subset selection problems is given in Section 2. The local method for self-optimizing\ncontrol is described and the CV selection problems through local average loss minimization are posed in\nSection 3. The B3 methods for solving these CV selection problems are presented in Section 4 and their\nnumerical efficiency is demonstrated in Section 5. The work is concluded in Section 6.\n2 BAB Methods for Subset selection\nLet Xm = {xi}, i = 1, 2, \u00b7 \u00b7 \u00b7 ,m, be an m-element set. The subset selection problem with selection\ncriterion T involves finding an n-element subset Xn \u2282 Xm such that\nT (X\u2217n) = min\nXn\u2282Xm\nT (Xn) (1)\nFor a subset selection problem, the total number of candidates is Cnm, which can be extremely large for\nlarge m and n rendering exhaustive search unviable. Nevertheless, BAB approach can find the globally\noptimal subset without exhaustive search.\n3\n1 2 3\n4432 3 4\n555\n666666\n554443\n6655554 6\n5\n6\nroot search\ndirection\n(a) downward search\nroot\n1 2 3 4 5\n665654654365432\nsearch\ndirection\n(b) upward search\nFigure 1: Solution trees for selecting 2 out of 6 elements.\n2.1 Unidirectional BAB approaches\nDownwards BAB method. BAB search is traditionally conducted downwards (gradually decreasing\nsubset size) [3, 4, 14, 18, 19]. A downwards solution tree for selecting 2 out of 6 elements is shown in\nFigure 1(a), where the root node is same as Xm. Other nodes represent subsets obtained by eliminating\none element from their parent sets. Labels at nodes denote the elements discarded there.\nTo describe the pruning principle, let B be an upper bound of the globally optimal criterion, i.e. B \u2265\nT (X\u2217n) and Tn(Xs) be a downwards lower bound over all n-element subsets of Xs, i.e.\nTn(Xs) \u2264 T (Xn) \u2200Xn \u2286 Xs (2)\nIf Tn(Xs) > B, it follows that\nT (Xn) > T (X\u2217n) \u2200Xn \u2286 Xs (3)\nHence, all n-element subsets of Xs cannot be optimal and can be pruned without further evaluation.\nUpwards BAB method. Subset selection can also be performed upwards (gradually increasing subset\nsize) [12]. An upwards solution tree for selecting 2 out of 6 elements is shown in Figure 1(b), where the\nroot node is an empty set. Other nodes represent supersets obtained by adding one element to their parent\nsets. Labels at nodes denote the elements added there.\nTo introduce the pruning principle, let the upwards lower bound of the selection criterion be defined as\nTn(Xt) \u2264 T (Xn) \u2200Xn \u2287 Xt (4)\nSimilar to downwards BAB, if Tn(Xt) > B,\nT (Xn) > T (X\u2217n) \u2200Xn \u2287 Xt (5)\n4\nHence, all n-element supersets of Xt cannot be optimal and can be pruned without further evaluation.\nRemark 1 (Monotonicity) When T is upwards (downwards) monotonic, T (Xs) with s < n (s > n), can\nbe used as the upwards (downwards) lower bound on T for pruning [2]. Although, monotonicity simplifies\nlower bound estimation, it is not a prerequisite for the use of BAB methods and the availability of any\nlower bound suffices.\n2.2 Bidirectional BAB approach\nThe upwards and downwards BAB approaches can be combined to form a more efficient bidirectional BAB\n(B3) approach. This approach is applicable to any subset selection problem, for which both upwards and\ndownwards lower bounds on the selection criterion are available [2].\nBidirectional pruning. In a B3 approach, the whole subset selection problem is divided into several\nsubproblems. A sub-problem is represented as the 2-tuple S = (Ff , Cc), where Ff is an f -element fixed\nset and Cc is a c-element candidate set. Here, f \u2264 n and n \u2264 f + c \u2264 m. The elements of Ff are included\nin all n-element subsets that can be obtained by solving S, while elements of Cc can be freely chosen\nto append Ff . In terms of fixed and candidate sets, downwards and upwards pruning can be performed\nif Tn(Ff \u222a Cc) > B and Tn(Ff ) > B, respectively. In B3 approach, these pruning conditions are used\ntogether (bidirectional pruning), where the subproblem S is pruned, if either downwards or upwards\npruning condition is met.\nThe use of bidirectional pruning significantly improves the efficiency as non-optimal subproblems can be\npruned at an early stage of the search. Further gain in efficiency is achieved by carrying out pruning on\nthe sub-problems of S, instead of on S directly. For xi \u2208 Cc, upward pruning is conducted by discarding xi\nfrom Cc, if Tn(Ff \u222a xi) > B. Similarly, if Tn(Ff \u222a (Cc\\xi)) > B, then downward pruning is performed by\nmoving xi from Cc to Ff . Here, an advantage of performing pruning on sub-problems is that the bounds\nTn(Ff \u222a xi) and Tn(Ff \u222a (Cc\\xi)) can be computed from Tn(Ff ) and Tn(Ff \u222a Cc), respectively, for all\nxi \u2208 Cc together, resulting in computational efficiency.\nBidirectional branching. In downwards and upwards BAB methods, branching is performed by remov-\ning elements from Cc and moving elements from Cc to Ff , respectively. These two branching approaches\ncan be combined into an efficient bidirectional approach by selecting a decision element and deciding upon\n5\nwhether the decision element be eliminated from Cc or moved to Ff .\nIn B3 algorithm, the decision element is selected as the one with the smallest upwards or downwards\nlower bound for upward or downward (best-first) search, respectively. To select the branching direction,\nwe note that downwards and upwards branching result in subproblems with Cn\u2212fc\u22121 and Cn\u2212f\u22121c\u22121 terminal\nnodes, respectively. In B3 algorithm, the simpler branch is evaluated first, i.e. downwards branching is\nperformed, if Cn\u2212fc\u22121 > Cn\u2212f\u22121c\u22121 or 2(n\u2212 f) > c, otherwise upwards branching is selected.\n3 Local method for Self-optimizing control\nTo present the local method for self-optimizing control, consider that the economics of the plant is charac-\nterized by the scalar objective function J(u,d), where u \u2208 Rnu and d \u2208 Rnd denote the degrees of freedom\nor inputs and disturbances, respectively. Let the linearized model of the process around the nominally\noptimal operating point be given as\ny = Gy u+GydWd d+We e (6)\nwhere y \u2208 Rny denotes the process measurements and e \u2208 Rny denotes the implementation error, which\nresults due to measurement and control error. Here, the diagonal matrices Wd and We contain the mag-\nnitudes of expected disturbances and implementation errors associated with the individual measurements,\nrespectively. The CVs c \u2208 Rnu are given as\nc = Hy = Gu+GdWd d+HWe e (7)\nwhere\nG = HGy and Gd = HG\ny\nd (8)\nIt is assumed that G = HGy \u2208 Rnu\u00d7nu is invertible. This assumption is necessary for integral control.\nWhen d and e are uniformly distributed over the set\u2225\u2225\u2225[ dT eT ]\u2225\u2225\u2225T\n2\n\u2264 1 (9)\nthe average loss is given as [13]\nLaverage(H) =\n1\n6(ny + nd)\n\u2225\u2225\u2225(HG\u02dc)\u22121HY\u2225\u2225\u22252\nF\n(10)\n6\nwhere \u2016 \u00b7 \u2016F denotes Frobenius norm and\nG\u02dc = GyJ\u22121\/2uu (11)\nY =\n[\n(Gy J\u22121uu Jud \u2212Gyd)Wd We\n]\n(12)\nHere, Juu and Jud represent \u2202\n2J\n\u2202u2\nand \u2202\n2J\n\u2202u \u2202d , evaluated at the nominally optimal operating point, respectively.\nWhen individual measurements are selected as CVs, the elements of H are restricted to be 0 or 1 and\nHHT = I (13)\nIn words, selection of a subset of available measurements as CVs involves selecting nu among ny mea-\nsurements, where the number of available alternatives is Cnuny . Using index notation, this problem can be\nstated as\nmin\nXnu\u2282Xny\nL1(Xnu) =\n\u2225\u2225\u2225G\u02dc\u22121XnuYXnu\u2225\u2225\u22252F (14)\nIn this work, we assume both ny and nd are given. Hence, the scalar 1\/(6(ny + nd)) is constant and\nis neglected in (14), as it does not depend on the selected CVs. Instead of 2-norm, as used in (9), if a\ndifferent norm is used to define the allowable set of d and e, the resulting expressions for average losses\nonly differ by scalar constants [13]. Thus, the formulation of optimization problem in (14) is independent\nof the norm used to define the allowable set of d and e.\nInstead of using individual measurements, it is possible to use linear combinations of measurements as\nCVs. In this case, the integer constraint on H \u2208 Rnu\u00d7ny is relaxed, but the condition rank(H) = nu is still\nimposed to ensure invertibility of HGy. The minimal average loss over the set (9) using measurements\ncombinations as CVs is given as [13]\nmin\nH\nLaverage =\n1\n6 (ny + nd)\nnu\u2211\ni=1\n\u03bb\u22121i\n(\nG\u02dcT (YYT )\u22121 G\u02dc\n)\n(15)\nEquation (15) can be used to calculate the minimum loss provided by the optimal combination of a given\nset of measurements. However, the use of all measurements is often unnecessary and similar losses may\nbe obtained by combining only a few of the available measurements [1, 11, 13]. Then, the combinatorial\noptimization problem involves finding the set of n among ny measurements (nu \u2264 n \u2264 ny) that can provide\nthe minimal loss for specified n. In index notation, the n measurements are selected by minimizing\nmin\nXn\u2282Xny\nL2(Xn) =\nnu\u2211\ni=1\n\u03bb\u22121i\n(\nG\u02dcTXn(YXnY\nT\nXn)\n\u22121G\u02dcXn\n)\n(16)\nwhere the scalar constant has been omitted as (14).\n7\n4 Bidirectional controlled variable selection\nAs shown in Section 3, the selection of CVs using exact local method can be seen as subset selection\nproblems. In this section, the BAB methods for solving these problems is presented. For simplicity of\nnotation, we define M(Xp) \u2208 Rp\u00d7p and N(Xp) \u2208 Rnu\u00d7nu as\nM(Xp) = R\u2212T G\u02dcXpG\u02dc\nT\nXpR\n\u22121 (17)\nN(Xp) = G\u02dcTXp(YXpY\nT\nXp)\n\u22121G\u02dcXp (18)\nwhere R is the Cholesky factor of YXpYTXp , i.e. R\nTR = YXpYTXp .\n4.1 Lower bounds\nIndividual measurements. L1 in (14) requires inversion of GXnu and thus L1(Xp) is well-defined only\nwhen GXp is a square matrix, i.e. p = nu. On the other hand, BAB methods require evaluation of loss,\nwhen the number of selected measurements differs from nu. Motivated by this drawback, an alternate\nrepresentation of L1 is derived in the following discussion. Since nu measurements are selected as CVs,\nL1 can be represented as\nL1(Xp) =\nnu\u2211\ni=1\n\u03c32i\n(\nG\u02dc\u22121XnuYXnu\n)\n(19)\n=\nnu\u2211\ni=1\n\u03bbi\n(\nG\u02dc\u22121XpYXpY\nT\nXpG\u02dc\n\u2212T\nXp\n)\n(20)\n=\nr\u2211\ni=1\n\u03bb\u22121i (N(Xp)) =\nr\u2211\ni=1\n\u03bb\u22121i (M(Xp)) (21)\nwhere r = rank(G\u02dcXp).\nIn practice, every measurement has a non-zero implementation error associated with it. Thus, based on\n(12), [We]ii 6= 0 and Y has full row rank. This implies that the inverse of YXpYTXp is well defined for all\npractical problems and the expression for L1 in (21) holds for any number of measurements. Using the\ngeneralized expression for L1, the downwards and upwards lower bounds required for the application of\nB3 algorithm are derived next.\nLemma 1 Let the matrix A\u02c6 be defined as\nA\u02c6 =\n\uf8ee\uf8f0 A b\nbT a\n\uf8f9\uf8fb (22)\n8\nwhere A \u2208 Rp\u00d7p is a Hermitian matrix, b \u2208 Rp\u00d71 and a \u2208 R. Let the eigenvalues of A and A\u02c6 be arranged\nin descending order. Then [10, Th. 4.3.8]\n\u03bbp+1(A\u02c6) \u2264 \u03bbp(A) \u2264 \u03bbp(A\u02c6) \u2264 \u03bbp\u22121(A) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bb1(A) \u2264 \u03bb1(A\u02c6) (23)\nProposition 1 (Lower bounds for L1) Consider a node S = (Ff , Cc). For L1 defined in (21),\nL1(Ff ) \u2264 min\nXnu\u2283Ff\nL1(Xnu); f < nu (24)\nL1(Ff \u222a Cc) \u2264 min\nXnu\u2282(Ff\u222aCc)\nL1(Xnu); f + c > nu (25)\nProof : To prove (24), let G\u02dcFf\u222ai =\n[\nG\u02dcTFf G\u02dc\nT\ni\n]T\nand YFf\u222ai =\n[\nYTFf Y\nT\ni\n]T\n. Further, let RTR = YFfY\nT\nFf\nand R\u02dcT R\u02dc = YFf\u222aiY\nT\nFf\u222ai (Cholesky factorization). Then, it follows that R and M(Ff ) are principal\nsubmatrices of R\u02dc and M(Ff \u222a i), respectively, obtained by deleting the last row and column of the\ncorresponding matrices. We note that r = rank(G\u02dcFf ) = f . Now, using (23), we have\n\u03bb\u22121i (M(Ff )) \u2264 \u03bb\u22121i+1(M(Ff \u222a i)); i = 1, 2, \u00b7 \u00b7 \u00b7 , f ; f < nu (26)\nThen,\nL1(Ff \u222a i) =\nf+1\u2211\ni=1\n\u03bb\u22121i (M(Ff \u222a i)) (27)\n=\n(\n\u03bb\u221211 (M(Ff \u222a i)) +\nf+1\u2211\ni=2\n\u03bb\u22121i (M(Ff \u222a i))\n)\n(28)\n\u2265\n(\n\u03bb\u221211 (M(Ff \u222a i)) +\nf\u2211\ni=1\n\u03bb\u22121i (M(Ff ))\n)\n(29)\n\u2265\nf\u2211\ni=1\n\u03bb\u22121i (M(Ff )) = L1(Ff ) (30)\nNow, (24) follows by recursive use of the above expression.\nFor (25), it can be similarly shown that M((Ff \u222a Cc) \\ i) is a principal submatrix of M(Ff \u222a Cc). Based\non (23),\n\u03bb\u22121i (M(Ff \u222a Cc)) \u2264 \u03bb\u22121i (M((Ff \u222a Cc) \\ i)); i = 1, 2, \u00b7 \u00b7 \u00b7 , nu (31)\nWe have rank(G\u02dcFf\u222aCc) = rank(G\u02dc(Ff\u222aCc)\\i) = nu. Now,\nL1((Ff \u222a Cc) \\ i) =\nnu\u2211\ni=1\n\u03bb\u22121i (M((Ff \u222a Cc) \\ i)) (32)\n\u2265\nnu\u2211\ni=1\n\u03bb\u22121i (M(Ff \u222a Cc)) = L1(Ff \u222a Cc) (33)\n9\nNow, (25) can be shown to be true through recursive use of the above expression.\nTo illustrate the implications of Proposition 1, let B represent the best available upper bound on L1(X\u2217nu).\nThen (24) implies that, if L1(Ff ) > B, the optimal solution cannot be a superset of Ff and hence all\nsupersets of Ff need not be evaluated. Similarly, if L1(Ff \u222a Cc) > B, (25) implies that the optimal\nsolution cannot be a subset of Ff \u222a Cc and hence all subsets of Ff \u222a Cc need not be evaluated. Thus,\nupwards and downwards pruning can be conduced using (24) and (25) and the optimal solution can be\nfound without complete enumeration.\nMeasurements combinations. The expression for L2 in (16) is the same as the expression for L1 in\n(21). Thus, similar to Proposition 1, it can be shown that\nL2(Ff \u222a Cc) \u2264 min\nXn\u2282(Ff\u222aCc)\nL2(Xn); f + c > n (34)\nFor selecting measurements, whose combinations can be used as CVs, the result in (34) is useful for\ndownwards pruning. Equation (25), however, also implies that when nu \u2264 f < n, L2(Ff ) decreases as the\nsubset size increases. Thus, unlike L1, the expression for L2 cannot be directly used for upwards pruning.\nIn the following proposition, a lower bound on L2 is derived, which can instead be used for upwards\npruning, whenever n\u2212 nu < f < n.\nProposition 2 (Upwards lower bound for L2) For the node S = (Ff , Cc), let\nL2(Ff ) =\nf+nu\u2212n\u2211\ni=1\n\u03bb\u22121i (N(Ff )) ; f > n\u2212 nu (35)\nThen,\nL2(Ff ) \u2264 min\nXn\u2283Ff\nXn\u2282(Ff\u222aCc)\nL2(Xn) (36)\nProof : Consider the index set Xn \u2282 (Ff \u222a Cc). For j \u2208 Xn with j \/\u2208 Ff , similar to the proof of\nProposition 1, M(Xn \\ j) is a principal submatrix of M(Xn). Based on Lemma 1, we have\n\u03bb\u22121i (M(Xn \\ j)) \u2264 \u03bb\u22121i+1(M(Xn)); i = 1, 2 \u00b7 \u00b7 \u00b7nu \u2212 1 (37)\n10\nThen,\nL2(Xn) =\nnu\u2211\ni=1\n\u03bb\u22121i (M(Xn)) (38)\n=\n(\n\u03bb\u221211 (M(Xn)) +\nnu\u2211\ni=2\n\u03bb\u22121i (M(Xn))\n)\n(39)\n\u2265\n(\n\u03bb\u221211 (M(Xn)) +\nnu\u22121\u2211\ni=1\n\u03bb\u22121i (M(Xn \\ j))\n)\n(40)\n\u2265\nnu\u22121\u2211\ni=1\n\u03bb\u22121i (M(Xn \\ j)) (41)\nThrough repeated application of (23), for Xp \u2208 Xn, Xp \/\u2208 Ff\nL2(Xn) \u2265\nnu\u2212p\u2211\ni=1\n\u03bb\u22121i (M(Xn \\Xp)) (42)\nWithout loss of generality, we can select Xp such that Ff = Xn \\Xp. Then, p = n\u2212 f and\nL2(Xn) \u2265\nnu\u2212n+f\u2211\ni=1\n\u03bb\u22121i (M(Ff )) (43)\nwhich implies (36).\nProposition 2 implies that L2(Ff ) in (35) is a lower bound on the loss corresponding to combinations of\nany n measurements obtained by appending indices to Ff and hence can be used for upwards pruning.\nIn this case, upwards pruning can only be applied to a node with f > n\u2212 nu. Thus, the BAB algorithm\nbased on L2 in (35) is referred to as partial bidirectional BAB (PB3) algorithm. Development of fully\nbidirectional BAB algorithm for selection of measurement combination as CVs is an open problem.\n4.2 Fast pruning and branching\nPropositions 1 and 2 can be used to prune the non-optimal nodes quickly. Thus, the optimal solution can\nbe found with evaluation of fewer nodes, but the solution time can still be large, as direct evaluation of\nL1 in (21) and L2 in (35) is computationally expensive.\nIndividual measurements. We note that when f < nu, M(Ff ) in (17) is invertible. Similarly for\ns = f + c > nu, N(Ss) in (18) is invertible. Thus, based on (21),\nL1(Ff ) =\nf\u2211\ni=1\n\u03bbi(M\u22121(Ff )) = trace(M\u22121(Ff )) (44)\nL1(Ss) =\nnu\u2211\ni=1\n\u03bbi(N\u22121(Ss)) = trace(N\u22121(Ss)) (45)\n11\nThe use of (44) and (45) for evaluation of lower bounds on L1 avoids computation of eigenvalues. The next\ntwo propositions relate the bounds of a node with the bounds of sub-nodes allowing pruning on sub-nodes\ndirectly and thus improving efficiency of the B3 algorithm further.\nProposition 3 (Upwards pruning for L1) Consider a node S = (Ff , Cc) and index i \u2208 Cc. Then\nL1(Ff \u222a i) = L1(Ff ) +\n\u2016zTi YFf \u2212Yi\u201622\n\u03b7i\n(46)\nwhere zi = (G\u02dcFf G\u02dc\nT\nFf\n)\u22121G\u02dcFf G\u02dc\nT\ni and \u03b7i = G\u02dci(I\u2212GTFf (G\u02dcFf G\u02dcTFf )\u22121G\u02dcFf )G\u02dcTi .\nProof : Based on (44),\nL1(Ff \u222a i) = trace(Q(GFf\u222aiGTFf\u222ai)\u22121QT ) (47)\nwhere Q is the Cholesky factor of YFf\u222aiY\nT\nFf\u222ai, i.e. Q\nTQ = YFf\u222aiY\nT\nFf\u222ai. Since GFf\u222ai =\n[\nG\u02dcTFf G\u02dc\nT\ni\n]T\n,\nusing the matrix inversion formula for partitioned matrices [10]\n(GFf\u222aiG\nT\nFf\u222ai)\n\u22121 =\n\uf8ee\uf8f0G\u02dcFf G\u02dcTFf G\u02dcFf G\u02dcTi\nG\u02dciG\u02dcTFf G\u02dciG\u02dc\nT\ni\n\uf8f9\uf8fb\u22121 (48)\n=\n\uf8ee\uf8f0(G\u02dcFf G\u02dcTFf )\u22121 + zizTi \/\u03b7i \u2212zi\/\u03b7i\nzTi \u03b7i 1\/\u03b7i\n\uf8f9\uf8fb (49)\n=\n\uf8ee\uf8f0(G\u02dcFf G\u02dcTFf )\u22121 0\n0 0\n\uf8f9\uf8fb+ 1\/\u03b7i\n\uf8ee\uf8f0 zi\n\u22121\n\uf8f9\uf8fb[zTi \u22121] (50)\nThrough simple algebraic manipulations, it can be shown that\nQ =\n\uf8ee\uf8f0R pi\n0 \u03b4i\n\uf8f9\uf8fb (51)\nwhere R is the Cholesky factor of YFfY\nT\nFf\n, pi = R\u2212TYFfY\nT\ni and \u03b4i =\n\u221a\nYiYTi \u2212 pTi pi. Thus,\nQ(GFf\u222aiG\nT\nFf\u222ai)\n\u22121QT =\n\uf8ee\uf8f0R(GFfGTFf )\u22121RT 0\n0 0\n\uf8f9\uf8fb+ 1\/\u03b7i\n\uf8ee\uf8f0Rzi \u2212 pi\n\u2212\u03b4i\n\uf8f9\uf8fb[zTi RT \u2212 pTi \u2212\u03b4i] (52)\nand\ntrace(Q(GFf\u222aiG\nT\nFf\u222ai)\n\u22121QT ) = trace(R(GFfG\nT\nFf\n)\u22121RT ) +\ntrace((Rzi \u2212 pi)(Rzi \u2212 pi)T ) + \u03b42i\n\u03b7i\n(53)\n12\nSince trace((Rzi \u2212 pi)(Rzi \u2212 pi)T ) = (Rzi \u2212 pi)T (Rzi \u2212 pi),\ntrace((Rzi \u2212 pi)(Rzi \u2212 pi)T ) + \u03b42i = zTi RTRzi \u2212 zTi RTpi \u2212 pTi Rzi +YiYTi (54)\n= zTi YFfY\nT\nFf\nzi \u2212 zTi YFfYTi \u2212YiYFf zi +YiYTi (55)\n= (zTi YFf \u2212Yi)(zTi YFf \u2212Yi)T (56)\nand the result follows.\nThe main computation load in using (46) is in Cholesky factorization and the inversion of the matrix\nG\u02dcFf G\u02dc\nT\nFf\n, which need to be calculated only once for all i \u2208 Cc. Hence, the calculation is more efficient\nthan direct calculation of L1 using (44).\nProposition 4 (Downward pruning for L1) For a node S = (Ff , Cc), let Ss = Ff\u222aCc, where s = f+c.\nFor i \u2208 Cc,\nL1(Ss \\ i) = L1(Ss) + \u2016xiN\n\u22121(Ss)\u201622\n\u03b6i \u2212 xiN\u22121(Ss)xTi\n(57)\nwhere xi = YiYTSs\\i(YSs\\iY\nT\nSs\\i)\n\u22121GSs\\i \u2212GTi and \u03b6i = Yi(I\u2212YTSs\\i(YSs\\iYTSs\\i)\u22121YSs\\i)YTi .\nProof : For simplicity of notation, define Q = YSs\\iY\nT\nSs\\i. Then,\n(YSsY\nT\nSs)\n\u22121 =\n\uf8ee\uf8f0 Q YSs\\iYTi\nYiYTSs\\i YiY\nT\ni\n\uf8f9\uf8fb\u22121 (58)\n=\n\uf8ee\uf8f0Q\u22121 +Q\u22121YSs\\iYTi YiYTSs\\iQ\u22121\/\u03b6i \u2212Q\u22121YSs\\iYTi \/\u03b6i\n\u2212YiYTSs\\iQ\u22121\/\u03b6i 1\/\u03b6i\n\uf8f9\uf8fb (59)\n=\n\uf8ee\uf8f0Q\u22121 0\n0 0\n\uf8f9\uf8fb+ 1\/\u03b6i\n\uf8ee\uf8f0Q\u22121YSs\\iYTi\n\u22121\n\uf8f9\uf8fb[YiYTSs\\iQ\u22121 \u22121] (60)\nwhere (59) is obtained using the matrix inversion formula for partitioned matrices [10]. Since G\u02dcTSs =[\nG\u02dcTSs\\i G\u02dc\nT\ni\n]\n, we have\nN(Ss) = G\u02dcTSs(YSsY\nT\nSs)\n\u22121G\u02dcSs = G\u02dc\nT\nSs\\iQ\n\u22121G\u02dcSs\\i + xix\nT\ni \/\u03b6i (61)\n= N(Ss \\ i) + xixTi \/\u03b6i (62)\nUsing matrix inversion lemma [10], we have\nN\u22121(Ss \\ i) = N\u22121(Ss) + 1\n\u03b6i \u2212 xTi N\u22121(Ss)xi\nN\u22121(Ss)xixTi N\n\u22121(Ss) (63)\n13\nwhich implies that,\ntrace(N\u22121(Ss \\ i)) = trace(N\u22121(Ss)) + trace(N\n\u22121(Ss)xixTi N\n\u22121(Ss))\n\u03b6i \u2212 xTi N\u22121(Ss)xi\n(64)\nThe result follows by using (45) and noting that trace(N\u22121(Ss)xixTi N\n\u22121(Ss)) = xTi N\n\u22121(Ss)N\u22121(Ss)xi =\n\u2016xTi N\u22121(Ss)\u201622.\nUsing (59), it can be shown that 1\/\u03b6i is the ith diagonal element of (YSsYTSs)\n\u22121 and xTi \/\u03b6i is the ith row\nof the matrix (YSsYTSs)\n\u22121G\u02dcSs . Therefore, the use of condition in (57) requires inversion of two matrices,\n(YSsYTSs) and N(Ss), which need to be calculated only once for all i \u2208 Cc. Hence, the calculation is more\nefficient than direct calculation of L1 using (45).\nThe bidirectional branching approach mentioned in Section 2.2 requires selecting a decision element, which\ncan be done directly based on the loss calculated for the super-nodes, L1(Ff \u222a i) and for the sub-nodes,\nL1(Ss \\ i). More specifically, according to the \u201cbest-first\u201d rule, for upwards-first branching, element i is\nselected as the decision element if L1(Ff \u222a i) = minj\u2208Cc L1(Ff \u222a j) or if L1(Sc \\ i) = maxj\u2208Cc L1(Ss \\ j).\nSimilarly, for downwards-first branching, element i is selected as the decision element if L1(Ff \u222a i) =\nmaxj\u2208Cc L1(Ff \u222a j) or if L1(Sc \\ i) = minj\u2208Cc L1(Ss \\ j). Between these two criteria for upwards and\ndownwards branching, the one with the larger value is less conservative and hence is adopted for the\nselection of decision element. Overall, no extra calculation is required for fast branching. The flowchart\nfor recursive implementation of the proposed B3 algorithm is available in [12].\nMeasurements combinations. As the downwards pruning criteria for minimization of L1 and L2 are\nthe same, Proposition 4 can be used for fast downwards pruning for selection of a subset of measurements,\nwhose combinations can be used as CVs. The fast upwards pruning criteria for minimization of L2 is\npresented in the next proposition.\nProposition 5 (Upwards pruning for L2) Consider a node S = (Ff , Cc) and index i \u2208 Cc. Let q =\nf + nu \u2212 n+ 1. Then\nL2(Ff \u222a i) \u2265\nq2\u2211q\nj=1 \u03bbj(N(Ff )) + \u2016si\u201622\/\u03b2i\n(65)\nwhere si = YiYTFf (YFfY\nT\nFf\n)\u22121GFf \u2212GTi and \u03b2i = Yi(I\u2212YTFf (YFfYTFf )\u22121YFf )YTi .\nProof : Similar to the proof of Proposition 4, it can be shown that N(Ff \u222ai) = N(Ff )+sisTi \/\u03b2i. According\nto [6, Th. 8.1.8], \u03bbj(N(Ff \u222a i)) = \u03bbj(N(Ff )) + tj ; j = 1, 2, \u00b7 \u00b7 \u00b7 , nu and\n\u2211nu\nj=1 tj = \u2016si\u201622\/\u03b2i; tj \u2265 0.\n14\nTherefore, a lower bound on N(Ff \u222a i) can be obtained by solving the following minimization problem:\nmin\nt1,...,tnu\nq\u2211\nj=1\n1\n\u03bbj(N(Ff )) + tj\n(66)\ns.t.\nnu\u2211\nj=1\ntj = \u2016si\u201622\/\u03b2i; tj \u2265 0, j = 1, . . . , nu (67)\nLet the Lagrangian function be defined as\nL =\nq\u2211\nj=1\n1\n\u03bbj(N(Ff )) + tj\n+ \u03bd\n\uf8eb\uf8ed nu\u2211\nj=1\ntj \u2212 \u2016si\u201622\/\u03b2i\n\uf8f6\uf8f8+ nu\u2211\nj=1\n\u00b5jtj\nThe optimality conditions for minimizing L are (see e.g. [5]):\n\u2202L\n\u2202tj\n= \u03bd \u2212 1\n(\u03bbj(N(Ff )) + tj)\n2 + \u00b5j = 0, j = 1, . . . , q (68)\n\u2202L\n\u2202tk\n= \u03bd + \u00b5k = 0, k = q + 1, . . . , nu (69)\n\u00b5jtj = 0, j = 1, . . . , nu (70)\nnu\u2211\nj=1\ntj = \u2016si\u201622\/\u03b2i (71)\nSince \u03bd 6= 0, we have \u00b5k 6= 0 and thus tk = 0 for k = q + 1, . . . , nu. Also, since tj 6= 0, we have \u00b5j = 0 for\nj = 1, . . . , q. Therefore,\n\u03bbj(N(Ff )) + tj =\n1\u221a\n\u03bd\n; j = 1, . . . , q (72)\nwhich leads to the following dual problem\nD : max\n\u03bd\n2q\n\u221a\n\u03bd \u2212 \u03bd\n\uf8eb\uf8ed q\u2211\nj=1\n\u03bbj(N(Ff )) + \u2016si\u201622\/\u03b2i\n\uf8f6\uf8f8 (73)\nThe solution of the dual problem is obtained as follows\n\u221a\n\u03bd =\nq\u2211q\nj=1 \u03bbj(N(Ff )) + \u2016si\u201622\/\u03b2i\n(74)\nNow, (65) follows by substituting for \u03bd in (73).\nThe direct computation of L2(Ff \u222a i) requires finding the eigenvalues of N(Ff \u222a i) for all i \u2208 Cc. In\ncomparison, Proposition 5 only requires computing eigenvalues of N(Ff ) and is thus much faster than\ndirect computation of L2(Ff \u222a i). Note that the relationship in (65) is an inequality, which can be\nconservative. As a BAB method spends most of its time in evaluating nodes that cannot lead to the\noptimal solution, we use the computationally cheaper albeit weaker pruning criteria in this paper. For the\nPB3 algorithm for minimization of L2, the decision element for fast branching is chosen using a similar\napproach as taken for minimization of L1.\n15\n5 Numerical Examples\nTo examine the efficiency of the proposed BAB algorithms, numerical tests are conducted using randomly\ngenerated matrices and binary distillation column case study. Programs used for loss minimization are\nlisted in Table 1. All tests are conducted on a Windows XP SP2 notebook with an Intelr CoreTM Duo\nProcessor T2500 (2.0 GHz, 2MB L2 Cache, 667 MHz FSB) using MATLABr R2008a.\nTable 1: BAB programs for comparison\nprogram description\nUP upwards pruning using (46)\nDOWN downwards pruning using (57)\nB3 bidirectional BAB by combining (46) and (57)\nPB3 partially B3 by combining (57) and (65)\n5.1 Random tests\nFour sets of random tests are conducted to evaluate the efficiency of different BAB algorithms mentioned in\nTable 1 for selection of a subset of available measurements as CVs through minimization of the local average\nloss. For each test, six random matrices are generated: three full matrices, Gy \u2208 Rny\u00d7nu , Gyd \u2208 Rny\u00d7nd\nand Jud \u2208 Rnu\u00d7nd , and three diagonal matrices, We \u2208 Rny\u00d7ny , Wd \u2208 Rnd\u00d7nd and Juu \u2208 Rnu\u00d7nu . All the\nelements of Gy, Gyd and Jud, and the diagonal elements of We and Wd are uniformly distributed between\n0 \u2212 1. To avoid ill-conditioning, the diagonal elements of Juu are uniformly distributed between 1 \u2212 10.\nFor all tests, we use nd = 5, while nu and ny are varied. For each selection problem, 100 random cases are\ntested and the average computation time and number of nodes evaluated over the 100 random cases are\nsummarized in Figure 2 for Tests 1 and 2 and Figure 3 for Tests 3 and 4, respectively.\nThe first and second tests are designed to select nu = 5 and nu = ny \u2212 5 out of ny measurements,\nrespectively. From Figure 2, it can be seen that algorithm UP is more suitable for problems involving\nselection of a few variables from a large candidate set, whilst algorithm DOWN is more efficient for\nproblems, where a few among many candidate variables need to be discarded to find the optimal solution.\nThe solution times for UP and DOWN algorithms increase only modestly with problem size, when nu <<\nny and nu \u2248 ny, respectively. The solution times for the B3 algorithm is similar to the better of UP and\nDOWN algorithms, however, its efficiency is insensitive to the kind of selection problem.\nThe third test consists of selecting nu out of ny = 2nu measurements with nu increasing from 5 to 18,\n16\n10\u22122\n100\n102\ncp\nu \ntim\ne,\n s\n(a)\n10 100 200 300 400 500\n100\n104\n108\n1012\nny\ne\nva\nlu\nat\nio\nns\n(b)\n10\u22122\n100\n102\ncp\nu \ntim\ne,\n s\n(c)\n30 60 90 120 150 180\n100\n104\n108\n1012\nny\ne\nva\nlu\nat\nio\nns\n(d)\n \n \nDOWN UP B3 BRUTE\nFigure 2: Random test 1: (a) computation time and (b) number of nodes evaluated against ny; Random\ntest 2: (c) computation time and (d) number of nodes evaluated against ny.\n17\n10\u22122\n100\n102\ncp\nu \ntim\ne,\n s\n(a)\n5 10 15 18\n100\n104\n108\n1012\nn\nu\ne\nva\nlu\nat\nio\nns\n(b)\n10\u22122\n100\n102\ncp\nu \ntim\ne,\n s\n(c)\n1 9 18 27 36\n100\n104\n108\n1012\nn\nu\ne\nva\nlu\nat\nio\nns\n(d)\n \n \nDOWN UP B3 BRUTE\nFigure 3: Random test 3: (a) computation time and (b) number of nodes evaluated against nu; Random\ntest 4: (c) computation time and (d) number of nodes evaluated against nu.\nwhile the fourth test involves selecting nu out of ny = 36 variables with nu ranging from 1 to 35. Figure 3\nindicates that while the UP and DOWN algorithms show reasonable performance for small nu, their\nperformance degrades, when nu \u2248 ny\/2. Within 1000 seconds, both UP and DOWN algorithms can only\nhandle problems with nu < 9 or ny \u2212 nu < 9. For all cases, however, the B3 algorithm exhibits superior\nefficiency and is able to solve problems up to nu = 18 within 200 seconds.\nIn summary, for selection of individual measurements as CVs by minimizing the average loss, all the\ndeveloped algorithms (UP, DOWN and B3) show much superior performance than the currently used\nbrute force method. In comparison with the UP and DOWN algorithms, the B3 algorithm shows superior\nperformance and similar efficiency for different problem dimensions including problems with nu << ny,\nnu \u2248 ny and nu \u2248 ny\/2.\n18\n5.2 Distillation column case study\nTo demonstrate the efficiency of the developed PB3 algorithm, we consider self-optimizing control of a\nbinary distillation column [15]. The objective is to minimize the relative steady-state deviations of the\ndistillate (zLtop) and bottoms (z\nH\nbtm) compositions from their nominal values, i.e.\nJ =\n(\nzLtop \u2212 zLtop,s\nzLtop,s\n)2\n+\n(\nzHbtm \u2212 zHbtm,s\nzLbtm,s\n)2\n(75)\nwhere the superscripts L and H refer to the light and heavy components and the nominal steady-state\nvalues are zLtop,s = z\nH\nbtm,s = 0.01 (99% purity). Two manipulated variables, namely reflux (u1) and vapor\nboilup rates (u2), are available for minimizing J in (75). The main disturbances are in feed flow rate\n(d1), feed composition (d2) and vapor fraction of feed (d3), which can vary between 1 \u00b1 0.2, 0.5 \u00b1 0.1\nand 1 \u00b1 0.1, respectively. The top and bottom compositions are not measured online and thus two CVs\nneeds to be identified for indirect control of the compositions. It is considered that the temperatures on\n41 trays (y1, \u00b7 \u00b7 \u00b7 , y41) are measured with an accuracy of \u00b10.5o C, whose combinations can be used as CVs\nfor implementation of self-optimizing control strategy.\nFor local analysis, the following linear model is derived:\uf8ee\uf8f0zLtop\nzHbtm\n\uf8f9\uf8fb =\n\uf8ee\uf8f01.083 1.097\n0.877 \u22120.863\n\uf8f9\uf8fbu+\n\uf8ee\uf8f00.586 1.117 1.091\n0.394 0.883 0.869\n\uf8f9\uf8fbd\nUsing the linear model and (75), the Hessian matrices required for local analysis are calculated to be\nJuu =\n\uf8ee\uf8f0 38832.119 \u221238888.107\n\u221238888.107 38951.438\n\uf8f9\uf8fb\nJud =\n\uf8ee\uf8f0 19600.452 39679.404 38863.870\n\u221219652.356 \u221239742.991 \u221238924.023\n\uf8f9\uf8fb\nwhile Wd = diag(0.2, 0.1, 0.1) and We = 0.5I41. The reader is referred to [9] for further details of this\ncase study.\nThe PB3 algorithm is used to select the 10 best measurement combinations for 2 \u2264 n \u2264 41. The trade-off\nbetween the losses of the 10 best selections and n is shown in Figure 4(a). It can be seen that when n \u2265 14,\nthe loss is less than 0.075, which is close to the minimum loss (0.052) obtained by using a combination\nof all 41 measurements. Furthermore, the reduction in loss is negligible, when combinations of more than\n20 measurements are used. Figure 4(a) also shows that the 10 best selections have similar self-optimizing\n19\n5 10 15 20 25 30 35 40\n0\n0.02\n0.04\nLo\nss\n(a)\n5 10 15 20 25 30 35 40\n0\n200\n400\n600\n800\nCP\nU \ntim\ne,\n s\n(b)\n \n \nPB3\nDOWN\n5 10 15 20 25 30 35 40\n100\n105\n1010\nN\num\nbe\nr o\nf e\nva\nlu\nat\nio\nns (c)\n \n \nPB3\nDOWN\nBRUTE\n5 10 15 20 25 30 35 40\n0\n2\n4\nNumber of measurments seleted\nD\nO\nW\nN \n: B\nP3\n(d)\n \n \ntime ratio\nevaluation number ratio\nFigure 4: (a) Average losses of 10-best measurement combinations against the number of measurements,\n(b) Comparison of computation time, (c) Comparison of number of node evaluations, and (d) Ratios of\ncomputation time and number of node evaluations required by PB3 over DOWN algorithms\n20\ncapabilities. Thus, the designer can choose the subset of measurements among these 10 best alternatives\nbased on some other important criteria, such as dynamic controllability [17].\nFigure 4(b) and (c) show the computation time and number of node evaluations for PB3 and DOWN\nalgorithms. To facilitate the comparison further, the ratios of number of node evaluations and computation\ntimes are also shown in Figure 4(d). Due to the conservativeness of the pruning condition (65), the PB3\nalgorithm is only able to reduce the number of node evaluations and hence computation time up to a\nfactor of 4 for selection problems involving selection of a few measurements from a large candidate set. It\nis expected that a less conservative or fully upwards pruning rule would improve the efficiency, but the\nderivation of such a rule is currently an open problem.\nOverall, both algorithms are very efficient and are able to reduce the number of node evaluations by\n5 to 6 orders of magnitude, as compared to the brute force search method. For example, to select 20\nmeasurements from 41 candidates, evaluation of a single alternative requires about 0.15 ms on the specified\nnotebook computer. Thus, a brute force search methods would take more than one year to evaluate all\npossible alternatives. However, both PB3 and DOWN algorithms are able to solve this problem within\n100 seconds. Hence, without the algorithms developed here, it would be practically impossible to generate\nof the trade-off curve shown in Figure 4(a).\n6 Conclusions\nSelf-optimizing control is a promising method for systematic selection of controlled variables (CVs) from\navailable measurements. In this paper, efficient bidirectional branch and bound (BAB) algorithms have\nbeen developed for selection of controlled variables (CVs) using the local average loss minimization criterion\nfor self-optimizing control. The numerical tests using randomly generated matrices and binary distillation\ncolumn case study show that the number of evaluations required by the proposed algorithms are 4 to 5\norders of magnitude lower than the current practice of CV selection using brute force search. This algorithm\nwould allow the practicing engineer to select CVs for large-dimensional problems in a computationally\ntractable manner.\nThe proposed algorithm for selection of subset of measurements, whose combinations can be used as CVs,\nis only partially bidirectional. For this problem, the development of a fully bidirectional branch and bound\nalgorithm is currently open and is an issue for future research. It is noted in [13] that for CV selection,\nthe problems involving minimization of local worst-case and average losses can be conflicting in nature.\n21\nTo overcome this difficulty, an extension of the bidirectional BAB algorithm to select CVs based on the\nbi-objective minimization of local worst-case and average losses for self-optimizing control is currently\nunder consideration.\nAcknowledgements\nThe first author gratefully acknowledges the financial support from Nanyang Technological University,\nSingapore through grant no. RG42\/06.\nReferences\n[1] V. Alstad, S. Skogestad, and E. S. Hori. Optimal measurement combinations as controlled variables.\nJ. Proc. Control, 19(1):138\u2013148, 2009.\n[2] Y. Cao and V. Kariwala. Bidirectional branch and bound for controlled variable selection: Part I.\nPrinciples and minimum singular value criterion. Comput. Chem. Engng., 32(10):2306\u20132319, 2008.\n[3] Y. Cao and P. Saha. Improved branch and bound method for control structure screening. Chem.\nEngg. Sci., 60(6):1555\u20131564, 2005.\n[4] X.-W. Chen. An improved branch and bound algorithm for feature selection. Pattern Recognition\nLetters, 24:1925\u20131933, 2003.\n[5] C. A. Floudas. Nonlinear and Mixed-Integer Optimization. Oxford University Press, Newyork, NY,\nUSA, 1995.\n[6] G. H. Golub and C. F. van Loan. Matrix Computations. The Johns Hopkins University Press,\nBaltimore, MD, 3rd edition, 1993.\n[7] I. J. Halvorsen, S. Skogestad, J. C. Morud, and V. Alstad. Optimal selection of controlled variables.\nInd. Eng. Chem. Res., 42(14):3273\u20133284, 2003.\n[8] S. Heldt. On a new approach for self-optimizing control structure design. In Proc. 7th Intl. Symposium\non ADCHEM, Istanbul, Turkey, 2009.\n[9] E. S. Hori and S. Skogestad. Selection of controlled variables: Maximum gain rule and combination\nof measurements. Ind. Eng. Chem. Res., 47(23):9465\u20139471, 2008.\n22\n[10] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, UK, 1985.\n[11] V. Kariwala. Optimal measurement combination for local self-optimizing control. Ind. Eng. Chem.\nRes., 46(11):3629\u20133634, 2007.\n[12] V. Kariwala and Y. Cao. Bidirectional branch and bound for controlled variable selection: Part II.\nExact local method for self-optimizing control. Comput. Chem. Eng., 33(8):1402\u20131412, 2009.\n[13] V. Kariwala, Y. Cao, and S. Janardhanan. Local self-optimizing control with average loss minimiza-\ntion. Ind. Eng. Chem. Res., 47(4):1150\u20131158, 2008.\n[14] P. Narendra and K. Fukunaga. A branch and bound algorithm for feature subset selection. IEEE\nTrans. Comput., C-26:917\u2013922, 1977.\n[15] S. Skogestad. Dynamics and control of distillation columns - A tutorial introduction. Trans. IChemE\nPart A, 75:539\u2013562, 1997.\n[16] S. Skogestad. Plantwide control: The search for the self-optimizing control structure. J. Proc. Control,\n10(5):487\u2013507, 2000.\n[17] S. Skogestad and I. Postlethwaite. Multivariable Feedback Control: Analysis and Design. John Wiley\n& Sons, Chichester, UK, 2nd edition, 2005.\n[18] P. Somol, P. Pudil, and J. Kittler. Fast branch & bound algorithms for optimal feature selection.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, pages 900\u2013912, 2004.\n[19] B. Yu and B. Yuan. A more efficient branch and bound algorithm for feature selection. Pattern\nRecognition, 26:883\u2013889, 1993.\nList of Figures\n1 Solution trees for selecting 2 out of 6 elements. . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2 Random test 1: (a) computation time and (b) number of nodes evaluated against ny;\nRandom test 2: (c) computation time and (d) number of nodes evaluated against ny. . . . . 17\n3 Random test 3: (a) computation time and (b) number of nodes evaluated against nu;\nRandom test 4: (c) computation time and (d) number of nodes evaluated against nu. . . . . 18\n23\n4 (a) Average losses of 10-best measurement combinations against the number of measure-\nments, (b) Comparison of computation time, (c) Comparison of number of node evaluations,\nand (d) Ratios of computation time and number of node evaluations required by PB3 over\nDOWN algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nList of Tables\n1 BAB programs for comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n24\n"}