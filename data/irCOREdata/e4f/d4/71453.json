{"doi":"10.1109\/ICSMC.2007.4413728","coreId":"71453","oai":"oai:eprints.lancs.ac.uk:929","identifiers":["oai:eprints.lancs.ac.uk:929","10.1109\/ICSMC.2007.4413728"],"title":"Architectures of evolving fuzzy rule-based classifiers","authors":["Angelov, Plamen","Zhou, Xiaowei","Filev, Dimitar","Lughofer, Edwin"],"enrichments":{"references":[{"id":16348533,"title":"A Fuzzy Controller with Evolving Structure,","authors":[],"date":"2004","doi":"10.1016\/j.ins.2003.03.006","raw":"P. Angelov, A Fuzzy Controller with Evolving Structure, Information Sciences, ISSN 0020-0255, vol.161, 2004, pp.21-35.","cites":null},{"id":16348551,"title":"A Multi-objective genetic algorithm for feature selection and granularity learning in fuzzy-rule based classification systems,","authors":[],"date":"2001","doi":"10.1109\/nafips.2001.943727","raw":"O. Cordon, F. Herrera, M. J. del Jesus, P. Villar, A Multi-objective genetic  algorithm  for  feature  selection  and  granularity  learning  in fuzzy-rule based classification systems, Proc. Joint 9 th IFSA World Congress and 20th NAFIPS Intern. Conf.,25-28 July 2001, Vol. 3,  pp. 1253-1258, Vancouver, BC, Canada, ISBN: 0-7803-7078-3. Authorized licensed use limited to: Lancaster University Library. Downloaded on December 8, 2008 at 08:43 from IEEE Xplore.  Restrictions apply.","cites":null},{"id":16348539,"title":"A Smoothed Boosting Algorithm Using Probabilistic Output Codes,","authors":[],"date":null,"doi":"10.1145\/1102351.1102397","raw":"R. Jin, J. Zhang, A Smoothed Boosting Algorithm Using Probabilistic Output  Codes,  Proc.  22 nd  International  Conference  on  Machine Learning, Bonn, Germany, pp.361-368, 2005.","cites":null},{"id":16348530,"title":"An Approach to Model-based Fault Detection in In-","authors":[],"date":"2006","doi":"10.1088\/0957-0233\/17\/7\/020","raw":"P. Angelov, V. Giglio, C. Guardiola, E. Lughofer, An Approach to Model-based Fault Detection in In- dustrial Measurement Systems with Application  to  Engine  Test  Ben-  ches,  Measurement  Science  & Technology, v.17 (7), 2006, 1809-1818.","cites":null},{"id":16348525,"title":"An approach to on-line identification of evolving Takagi-Sugeno models&quot;,","authors":[],"date":"2004","doi":"10.1109\/fuzzy.2004.1375687","raw":"P. Angelov, D. Filev, &quot;An approach to on-line identification of evolving Takagi-Sugeno  models&quot;,  IEEE  Trans.  on  Systems,  Man  and Cybernetics, part B, vol.34, No1, pp. 484-498, 2004.","cites":null},{"id":16348549,"title":"Angelov and E.Lughofer, Data-Driven Evolving Fuzzy Systems using eTS and FLEXFIS: Comparative Analysis,","authors":[],"date":null,"doi":"10.1080\/03081070701500059","raw":"P.  Angelov  and  E.Lughofer,  Data-Driven  Evolving  Fuzzy  Systems using  eTS  and  FLEXFIS:  Comparative Analysis, Intern. Journal of General Systems, to appear","cites":null},{"id":16348541,"title":"Approximate Clustering via the Mountain Method,","authors":[],"date":"1994","doi":"10.1109\/21.299710","raw":"R.  Yager  and  D.  Filev,  Approximate  Clustering  via  the  Mountain Method, IEEE Trans. on Systems and Cybernetics, vol. 24 (8), 1994","cites":null},{"id":16348535,"title":"Autonomous Self-localization in Completely Unknown Environment using Evolving Fuzzy Rule-based Classifier,","authors":[],"date":"2007","doi":"10.1109\/cisda.2007.368145","raw":"X.  Zhou,  P.  Angelov,  Autonomous  Self-localization  in  Completely Unknown Environment using Evolving Fuzzy Rule-based Classifier, st IEEE Symp. on Comp. Intelligence for Security and Defense Applic. (CISDA) 2007 , April 1-5, 2007 Honolulu, HI, USA, to appear.","cites":null},{"id":16348517,"title":"Catching Up with the Data: Research Issues in Mining Data Streams,","authors":[],"date":"2001","doi":"10.1145\/502512.502529","raw":"P. Domingos and G. Hulten. Catching Up with the Data: Research Issues in Mining Data Streams, Workshop on Research Issues in Data Mining and Knowledge Discovery, Santa Barbara, CA, 2001.","cites":null},{"id":16348544,"title":"Classification and Regression Trees, Chapman and Hall,","authors":[],"date":"1993","doi":"10.2307\/2530946","raw":"L. Breiman, J. Friedman, C.J. Stone and R.A. Olshen, Classification and Regression Trees, Chapman and Hall, Boca Raton, 1993.","cites":null},{"id":16348545,"title":"Discriminant Adaptive Nearest Neighbor Classification.","authors":[],"date":"1996","doi":"10.1109\/34.506411","raw":"T.  Hastie  and  R.  Tibshirani,  1996.  Discriminant  Adaptive  Nearest Neighbor  Classification.  IEEE  Trans.  Pattern  Anal.  Mach.  Intell. (TPAMI). Vol. 18 (6), pp. 607-616, 1996.","cites":null},{"id":16348529,"title":"Evolving Extended Naive Bayes Classifier, In:","authors":[],"date":"2006","doi":"10.1109\/icdmw.2006.74","raw":"F. Klawonn,  P. Angelov, Evolving Extended Naive Bayes Classifier, In: S. Tsumoto, C.W. Clifton, N. Zhong, X. Wu, J. Liu, B.W. Wah, Y.-M. Cheung: Proc. Sixth IEEE International Conference on Data Mining. IEEE, Los Alamitos (2006), pp. 643-647 (ISBN 0769527027).","cites":null},{"id":16348528,"title":"Evolving Fuzzy Rule-based Classifiers,","authors":[],"date":"2007","doi":"10.1109\/ciisp.2007.369172","raw":"P.  Angelov,  X.  Zhou,  F.  Klawonn,  Evolving  Fuzzy  Rule-based Classifiers,  First  2007  IEEE  International  Conference  on Computational  Intelligence  Applications  for  Signal  and  Image Processing, April 1-5, 2007, Honolulu, Hawaii, USA, to appear.","cites":null},{"id":16348531,"title":"Evolving Fuzzy Systems from Data Streams in Real-Time,","authors":[],"date":null,"doi":"10.1109\/isefs.2006.251157","raw":"P. Angelov, X. Zhou, Evolving Fuzzy Systems from Data Streams in Real-Time, Proc. 2006 International Symposium on Evolving Fuzzy Systems, UK, IEEE Press, pp.29-35, ISBN 0-7803-9719-3.","cites":null},{"id":16348524,"title":"Evolving Rule-based Models: A Tool for Design of Flexible Adaptive Systems.","authors":[],"date":"2002","doi":"10.1007\/978-3-7908-1794-2_3","raw":"P.  Angelov,  Evolving  Rule-based  Models:  A  Tool  for  Design  of Flexible Adaptive Systems. Berlin, Germany: Springer Verlag, 2002.","cites":null},{"id":16348536,"title":"Evolving Single- and Multi-Model Fuzzy Classifiers with FLEXFIS-Class,","authors":[],"date":"2007","doi":"10.1109\/fuzzy.2007.4295393","raw":"E. Lughofer, P. Angelov, X. Zhou, Evolving Single- and Multi-Model Fuzzy Classifiers with FLEXFIS-Class, IEEE Intern. Conf. on Fuzzy Syst., FUZZ-IEEE2007 23-26 July, 2007, London, England, to appear.","cites":null},{"id":16348540,"title":"FLEXFIS: A variant for incremental learning of Takagi-Sugeno fuzzy systems, in","authors":[],"date":"2005","doi":"10.1109\/fuzzy.2005.1452516","raw":"E. Lughofer and E. P. Klement, \u201cFLEXFIS: A variant for incremental learning of Takagi-Sugeno fuzzy systems, in Proc. FUZZ-IEEE 2005, Reno, Nevada, USA, 2005, pp.915-920.","cites":null},{"id":16348550,"title":"Fuzzy Identification of Systems and its Applications to Modeling and Control,","authors":[],"date":"1985","doi":"10.1016\/b978-1-4832-1450-4.50045-6","raw":"T. Takagi and M. Sugeno, Fuzzy Identification of Systems and its Applications to Modeling and Control, IEEE Trans. on Systems, Man and Cybernetics, vol. 15 (1), pp. 116-132, 1985.","cites":null},{"id":16348542,"title":"Fuzzy Model Identification based on Cluster Estimation,","authors":[],"date":"1994","doi":"10.1109\/fuzzy.1994.343644","raw":"S.  Chiu,  Fuzzy  Model  Identification  based  on  Cluster  Estimation, Journal of Intelligent and Fuzzy Systems, vol.2 (3), pp. 267-278, 1994","cites":null},{"id":16348547,"title":"Fuzzy Systems are Universal Approximators,","authors":[],"date":"1992","doi":"10.1109\/fuzzy.1992.258721","raw":"L.X. Wang, Fuzzy Systems are Universal Approximators, Proc. 1st IEEE Conf. Fuzzy Systems, pp. 1163-1169, San Diego, 1992.","cites":null},{"id":16348532,"title":"Fuzzy Systems are Universal Approximators\u201d,","authors":[],"date":"1992","doi":"10.1109\/fuzzy.1992.258721","raw":"L.-X.  Wang  \u201cFuzzy  Systems  are  Universal  Approximators\u201d,  Proc. FUZZ-IEEE, San Diego, CA, USA, pp.1163-1170, 1992.","cites":null},{"id":16348526,"title":"Identification of Evolving Rule-based Models","authors":[],"date":"2002","doi":"10.1109\/tfuzz.2002.803499","raw":"P. Angelov, R. Buswell, Identification of Evolving Rule-based Models (2002)  IEEE  Transactions  on  Fuzzy  Systems,  vol.  10,  No5, pp.667-677.","cites":null},{"id":16348548,"title":"Incremental Learning of Fuzzy Basis Function Networks with a Modified Version of Vector Quantization,","authors":[],"date":"2006","doi":null,"raw":"E. Lughofer and U. Bodenhofer, Incremental Learning of Fuzzy Basis Function Networks with a Modified Version of Vector Quantization, Proc. of IPMU 2006, Paris, France, vol. 1, pp 56-63, 2006.","cites":null},{"id":16348537,"title":"Incremental Linear Discriminant Analysis for Classification of Data Streams&quot;,","authors":[],"date":"2005","doi":"10.1109\/tsmcb.2005.847744","raw":"S.  Pang,  S.  Ozawa,  N.  Kasabov,  &quot;Incremental  Linear Discriminant Analysis for Classification of Data Streams&quot;, IEEE Trans. on Systems, Man and Cybernetics, part B, vol.35, No5, pp. 905-914, 2005.","cites":null},{"id":16348521,"title":"Learning Fuzzy Classification Rules from Data,","authors":[],"date":"2003","doi":"10.1007\/978-3-7908-1829-1_13","raw":"J. A. Roubos, M. Setnes and J. Abonyi, Learning Fuzzy Classification Rules from Data, Information Sciences\u2014Informatics and Computer Science: An International Journal, vol. 150, pp. 77-93, 2003","cites":null},{"id":16348534,"title":"Learning of Fuzzy Rules by Mountain Clustering,\u201d","authors":[],"date":null,"doi":"10.1117\/12.165030","raw":"R.  R.  Yager,  D.P.  Filev,  \u201cLearning  of  Fuzzy  Rules  by  Mountain Clustering,\u201d  Proc.  of  SPIE  Conf.  on  Application  of  Fuzzy  Logic Technology, Boston, MA, USA, pp.246-254,1993.","cites":null},{"id":16348522,"title":"Neural Networks for Pattern Recognition,","authors":[],"date":"1995","doi":"10.1145\/294828.1067910","raw":"C.  M.,  Bishop,  Neural  Networks  for  Pattern  Recognition,  Oxford University Press, Oxford, UK, 1995.","cites":null},{"id":16348543,"title":"On-line Identification of MIMO Evolving Takagi-Sugeno Fuzzy Models,","authors":[],"date":"2004","doi":"10.1109\/fuzzy.2004.1375687","raw":"P.  Angelov,  C.  Xydeas,  D.  Filev,  On-line  Identification  of  MIMO Evolving Takagi-Sugeno Fuzzy Models, Intern. Joint Conf. on Neural Networks and Intern. Conf. on Fuzzy Systems, IJCNN-FUZZ-IEEE, Budapest, Hungary, 25-29 July, 2004, 55-60, ISBN 0-7803-8354-0.","cites":null},{"id":16348515,"title":"Pattern Classification - Second Edition. Wiley-Interscience,","authors":[],"date":"2000","doi":"10.1142\/s1469026801000251","raw":"R. O. Duda, P.E. Hart, and D.G. Stork. Pattern Classification - Second Edition. Wiley-Interscience, Chichester, West Sussex, England, 2000.","cites":null},{"id":16348516,"title":"Piatetsky-Shapiro, P.~Smyth. From Data Mining to Knowledge Discovery: An Overview,","authors":[],"date":"1996","doi":"10.1145\/240455.240464","raw":"G.  Fayyad,  Piatetsky-Shapiro,  P.~Smyth.  From  Data  Mining  to Knowledge  Discovery:  An  Overview,  Advances  in  Knowledge Discovery and Data Mining, MIT Press, 1996.","cites":null},{"id":16348546,"title":"Querying and Mining Data Streams: you only get one look,","authors":[],"date":"2002","doi":"10.1145\/564793.564794","raw":"M. Garofalakis, J.Gehrke and R. Rastogi, Querying and Mining Data Streams: you only get one look, Proc. of the 2002 ACM SIGMOD Intern. Conf. on Management of Data, pp. 635-641, ACM Press, 2002.","cites":null},{"id":16348520,"title":"Rule-based Evolutionary Online Learning Systems: A Principal Approach to LCS Analysis and Design,","authors":[],"date":"2006","doi":"10.1007\/3-540-31231-5_11","raw":"M.  Butz,  Rule-based  Evolutionary  Online  Learning  Systems:  A Principal  Approach  to  LCS  Analysis  and  Design,  Physica  Verlag, v.191, Berlin, Heidelberg, Germany, 2006, ISBN 3-540-25379-3.","cites":null},{"id":16348518,"title":"The Elements of Statistical Learning: Data Mining, Inference and Prediction.","authors":[],"date":"2001","doi":"10.1111\/j.1541-0420.2010.01516.x","raw":"T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning:  Data  Mining,  Inference  and  Prediction.  Heidelberg, Germany: Springer Verlag, 2001.","cites":null},{"id":16348523,"title":"The Nature of Statistical Learning Theory,","authors":[],"date":"1995","doi":"10.1007\/978-1-4757-2440-0","raw":"V.Vapnik, The Nature of Statistical Learning Theory, Springer, 1995","cites":null},{"id":16348519,"title":"The Statistical Learning Theory,","authors":[],"date":"1998","doi":"10.1007\/978-1-4757-3264-1","raw":"V. N. Vapnik, The Statistical Learning Theory, Springer, 1998.","cites":null},{"id":16348527,"title":"Transparent Fuzzy Modelling using Fuzzy Clustering and GA\u2019s. In:","authors":[],"date":"1999","doi":"10.1109\/nafips.1999.781682","raw":"Setnes M., J. A. Roubos, Transparent Fuzzy Modelling using Fuzzy Clustering  and  GA\u2019s.  In:  Proc.  18 th  Annual  Conf.  of  the  North American Fuzzy Information Processing Society, NAFIPS, 10-12 June 1999, New York, USA, pp.198-202.","cites":null},{"id":16348538,"title":"UCI Machine Learning Repository, http:\/\/www.ics.uci.edu\/ ~mlearn\/MLRepository.html, accessed on 27","authors":[],"date":"2007","doi":null,"raw":"UCI  Machine  Learning  Repository,  http:\/\/www.ics.uci.edu\/ ~mlearn\/MLRepository.html, accessed on 27 Jan 2007.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2007-10-09","abstract":"Prediction of the properties of the crude oil distillation side streams based on statistical methods and laboratory-based analysis has been around for decades. However, there are still many problems with the existing estimators that require a development of new techniques especially for an on-line analysis of the quality of the distillation process. The nature of non-linear characteristics of the refinery process, the variety of properties to measure and control and the narrow window that normally refinery processes operates in are only some of the problems that a prediction technique should deal with in order to be useful for a practical application. There are many successful application cases that refinery units use real plant data to calibrate models. They can be used to predict quality properties of the gas oil, naphtha, kerosene and other products of a crude oil distillation tower. Some of these are distillation end points and cold properties (freeze, cloud). However, it is difficult to identify, control or compensate the dynamic process behaviour and the errors from instrumentation for an online model prediction","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/71453.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/929\/1\/getPDF4.pdf","pdfHashValue":"7108585beb07f28ddacafe148cea9fe9a42125df","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:929<\/identifier><datestamp>\n      2018-01-24T02:07:13Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D626F6F6B5F73656374696F6E<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Architectures of evolving fuzzy rule-based classifiers<\/dc:title><dc:creator>\n        Angelov, Plamen<\/dc:creator><dc:creator>\n        Zhou, Xiaowei<\/dc:creator><dc:creator>\n        Filev, Dimitar<\/dc:creator><dc:creator>\n        Lughofer, Edwin<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        Prediction of the properties of the crude oil distillation side streams based on statistical methods and laboratory-based analysis has been around for decades. However, there are still many problems with the existing estimators that require a development of new techniques especially for an on-line analysis of the quality of the distillation process. The nature of non-linear characteristics of the refinery process, the variety of properties to measure and control and the narrow window that normally refinery processes operates in are only some of the problems that a prediction technique should deal with in order to be useful for a practical application. There are many successful application cases that refinery units use real plant data to calibrate models. They can be used to predict quality properties of the gas oil, naphtha, kerosene and other products of a crude oil distillation tower. Some of these are distillation end points and cold properties (freeze, cloud). However, it is difficult to identify, control or compensate the dynamic process behaviour and the errors from instrumentation for an online model prediction.<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2007-10-09<\/dc:date><dc:type>\n        Contribution in Book\/Report\/Proceedings<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/929\/1\/getPDF4.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/ICSMC.2007.4413728<\/dc:relation><dc:identifier>\n        Angelov, Plamen and Zhou, Xiaowei and Filev, Dimitar and Lughofer, Edwin (2007) Architectures of evolving fuzzy rule-based classifiers. In: Systems, Man and Cybernetics, 2007. ISIC. IEEE International Conference on. IEEE, pp. 2050-2055. ISBN 978-1-4244-0991-4<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/929\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1109\/ICSMC.2007.4413728","http:\/\/eprints.lancs.ac.uk\/929\/"],"year":2007,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution in Book\/Report\/Proceedings","NonPeerReviewed"],"fullText":"  \n \n \nArchitectures for Evolving Fuzzy Rule-based Classifiers \n \nPlamen Angelov \nDept of Communication \nSystems, InfoLab21 \nLancaster University \nLancaster LA1 4WA,UK  \nXiaowei Zhou \nDept of Communication \nSystems, InfoLab21 \nLancaster University \nLancaster LA1 4WA,UK \nDimitar Filev \nFord Research and \nAdvanced Engineering, \nFord Motor Co \nDetroit, MI 48239, USA \n \nEdwin Lughofer \nDept of Knowledge- \nbased Mathematical Syst \nJ. Kepler University \nA-4040,Linz, AUSTRIA \nAbstract\u2014 In this paper the recently introduced evolving \nfuzzy classifier method called eClass is studied in respect to its \narchitecture and evolution of the fuzzy rule-base. The proposed \nclassifier has an open\/evolving structure and can start \u2018from \nscratch\u2019, learning and adapting to the new data samples. \nAlternatively, if an initial fuzzy rule-based classifier, generated \nbeforehand in off-line mode or provided by the operator, exists \nthen eClass can evolve this initial classifier in on-line mode. In \nother words, the fuzzy rule base will evolve incorporating new \nrules, modifying and\/or, possibly, removing some of the \npreviously existing ones. Additionally, the parameters of both, \nthe antecedent and the consequent parts are adapted. Note that \neClass can start with an empty rule-base, which is a unique \nfeature of this approach. The proposed approach is free from \nuser-specified parameters and the mechanism of forming new \nrules is very robust. In this paper, four different modelling \narchitectures are described and compared. The architectures are \nbased on i) unsupervised cluster partitions, eClassC; ii) Sugeno \nfuzzy models with singleton consequents, eClassA; iii) \nTakagi-Sugeno fuzzy models with linear consequent functions, \neClassB; and iv) a multi-model classification architecture, where \nseparate TS regression models are combined to form an overall \nclassification output of the system, eClassM. A thorough \ncomparison of the results when applying each of these \narchitectures and the results using previously existing classifiers \nhas been made using an online interactive self-adaptive image \nclassification framework.  \nKeywords: evolving fuzzy rule-based classifiers, \nincremental learning from scratch, Mountain and subtractive \nclustering, weighted recursive least squares. \nI. INTRODUCTION \nLASSIFICATION is a problem that is well studied and \nunderstood. One of the widely referenced texts is [1] \nwhere classification is treated in the context of pattern \nrecognition. In [2] classification is considered in the light of \nstatistical learning techniques. In [3] a study on fuzzy \nrule-based classifiers is given. There are numerous practical \nproblems in industrial systems, robotics, defence, bio-medical \nand other application domains which call for classification, \nsuch as fault detection and isolation, early cancer diagnosis, \nproduct quality monitoring and control, machine health \nmonitoring and prognostics, automatic target recognition, etc. \n[4,5].   \n The classifier can be considered as a  mapping from the \nfeature domain onto the class labels domain. A number of \ndifferent types of classifiers exists which use different \napproaches to perform this mapping such as: i) linear \ndiscriminant analysis [1]; ii) fuzzy rule-based classifiers [6], \n[7],[39]; iii) decision trees (e.g. C4.5 [31], CART [32]; iv) \nneural networks-based [8]; v) support vector machines [9], \netc. Practically all of them assume training in batch mode \n(when all the training data are known including their class \nlabels). In many practical applications, however, we deal with \ndata streams coming from sensory readings or Internet etc. \n[3]. Additionally, even if the data is available off-line as in \nmarket basket analysis, genome data etc. the volume of this \ndata is huge and prohibits the direct use of well established \nlearning methods [4]. Very often storing the complete data is \npractically impossible. This requires addressing the problem \nof classification of streaming data in real-time [34]. \nIt is well known that fuzzy rule-based systems are \nuniversal function approximators [35]; they are suitable for \nextracting interpretable knowledge. Therefore, they are \nviewed as a promising framework for designing effective and \npowerful classifiers. The type of classifiers that can be built \nusing the recently introduced evolving fuzzy rule-based \nsystems [10],[11] can be called evolving [12] which differs \nfrom \u2018evolutionary\u2019. Evolving fuzzy rule-based classifiers \ndevelop and adapt in on-line mode the non-linear \nclassification surface. Evolutionary\/genetic algorithms have \nrecently been used for design of fuzzy rule-based systems in \ngeneral [13] and classifiers in particular [6],[39]. They are \nbased on the off-line optimization of one or more criteria in \ndesigning the fuzzy rule-base (classifier) using paradigms that \nstem from Nature such as mutation, crossover, and \nreproduction. Evolving in the sense that we use it in our paper \nand related works includes self-organising, self-developing in \nterms of the classifier (rule-base) structure. In this sense this \nparadigm can be considered as a higher level of adaptation \n(adaptation is usually related to parameters not to the structure \nof the systems [15]). Note, that similar principles were used by \nthe authors in developing evolving classifiers also in [14] and \n[23]. The concept is taken further in this paper comparing to \n[14] by analysing different possible architectures of eClass. \nComparing to [23] the backbone of the approach is different \u2013 \nwe use here and in [14] the evolving fuzzy Takagi-Sugeno, \neTS approach while in [23] we extended FLEXFIS [27] and \nits modification FLEXFIS-Mod [36] to the classification case \n(called FLEXFIS-Class), both families originally designed for \nfuzzy regression modelling tasks. The eTS family of evolving \nTS models (eTS, MIMO-eTS, exTS) has been recently \nC \n20501-4244-0991-8\/07\/$25.00\/\u00a92007 IEEE\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on December 8, 2008 at 08:43 from IEEE Xplore.  Restrictions apply.\n  \n \napplied successfully to a number of identification [11,18], \ntime-series prediction [18], fault detection [17], and control \n[19] problems.  \n In this paper four different architectures are presented, \nstudied, and compared in Section II: i) eClassC \u2013classification \nbased on eClustering combined with nearest neighbour type \nof classification (so called \u2018winner takes all\u2019); ii) eClassA \u2013 \nevolving fuzzy rule-based classifier with zero order Sugeno \nconsequents (singletons that represent the class labels); iii) \neClassB \u2013 evolving fuzzy rule-based classifier with first order \nTS (locally linear) consequents; iv) eClassM \u2013 multiple model \nclassifier. In Section III the methodology and algorithms for \nevolving these structures with sample-wise loaded data are \npresented.  \nThe experimental results demonstrated in Section IV \ninclude a well known benchmark problem and a real-life data \nof a self-adaptive online image classification framework. The \nresults demonstrate that the proposed evolving fuzzy rule-base \nclassifier, eClass in all of its modifications has a very good \nclassification performance; it is computationally very \nefficient, and is, thus, suitable for real-time applications such \nas classification streaming data, robotic applications, e.g. \ntarget and landmark recognition, real-time machine health \nmonitoring and prognostics, fault detection and diagnostics \netc. This approach is transparent, linguistically interpretable, \nand applicable to both fully unsupervised and partially \nsupervised learning. While the low order architectures \n(eClassC and eClassA) are computationally superior, with \nhigh transparency (low number of fuzzy rules and parameters) \nand simple structure, the performance of eClassB and \neClassM are superior to all the other structures and to the \npreviously existing classifiers. The multiple model structure, \neClass M has similar performance to eClassA but a more \ncomplicated structure \nII. THE PROPOSED ARCHITECTURES OF ECLASS \nThe proposed classifier, eClass uses fuzzy rule-bases as a \nframework. The antecedent part concerns the features and the \nconsequent part differs for different types of eClass. The \nfollowing general form of fuzzy rules is used: \n( )i\ni\nnn\nii\nConseqTHEN\nxisxANDANDxisxIFR )(...)(: **11\n  (1) \nwhere Tnxxxx ],...,,[ 21= is the vector of features; iR  \ndenotes the ith fuzzy rule; i=[1, N]; N is the number of fuzzy \nrules; ( )*ijj xisx denotes the jth fuzzy set of the ith fuzzy rule; \nj=[1,n]; *ix is the prototype (focal point) of the ith\n \n rule \nantecedent. \nNote that the type of the classifier depends on the type of \nthe consequent and can be: \na) Associated with the nearest cluster (eClassC) \nii ClusterConseq = , i=[1,N]           (1a) \nb) Zero order MIMO Takagi-Sugeno (TS) type [30] when \nthe consequents are the class labels (eClassA):  \n       [ ]Ti Miii LLLConseq 00201=   (1b) \nwhere L denotes a binary (0\/1) class label;  \nc) First order MIMO TS type [30] when the consequents are \nlinear classifiers (eClassB): \n        [ ]\nT\ni\nnM\ni\nn\ni\nn\ni\nM\nii\ni\nM\nii\nTi xConseq\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n\uf8f9\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8f0\n\uf8ee\n=\n\u03b1\u03b1\u03b1\n\u03b1\u03b1\u03b1\n\u03b1\u03b1\u03b1\nK\nKKKK\nK\nK\n21\n11211\n00201\n,1  (1c) \nd) M first order TS type classifiers (eClassM) when each \nclassifier predicts the degree of membership to that class: \n        [ ][ ]Ti\nn\niiTi\nj xConseq 00201,1 \u03b1\u03b1\u03b1 K=  (1c) \nwhere  i=[1,N]; j=[1,M]  \nNote that the number of fuzzy rules, N is not necessarily the \nsame as the number of classes, M. For eClassC the number of \nclusters formed, N can be more, less or equal to the number of \nclasses, M. Since there are no labels provided (the learning is \nunsupervised) there is no direct link between the two. For \neClass A, B, and M there is a requirement to have at least one \nfuzzy rule per class and, therefore, the following relation holds \nfor these types of classifiers:  \n        MN \u2265 ; i=[1,N]; j=[1,M]  (2) \nThe firing degree for each fuzzy rule is determined as a \nt-norm (a form to represent the logical AND) [21]:  \n        ],1[;)(\n1\nNix\nn\nj\nj\ni\nj\ni\n== \u220f\n=\n\u00b5\u03c4   (3) \nwhere ij\u00b5 is the membership value that describes the degree \nof association with the ith prototype. We assume it to be of \nGaussian form due to its generalization capabilities [1]: \n2\n2\n1\n\uf8f7\n\uf8f7\n\uf8f8\n\uf8f6\n\uf8ec\n\uf8ec\n\uf8ed\n\uf8eb\n\u2212\n=\ni\nj\nijd\ni\nj e\n\u03c3\u00b5\n i=[1,N];  j=[1,n]  (4) \nwhere ijd is the Euclidean distance between a sample \nand the prototype (focal point) of the ith fuzzy rule; ij\u03c3  is the \nspread of the membership function, which also represents the \nradius of the zone of influence of the fuzzy rule.  \nThe spread of the membership \u03c3 can be determined using \nthe data scatter [18] per cluster:  \n1;)(1 0\n1\n2*\n,\n== \u2211\n=\ni\nS\nj\nj\ni\nl\nk\nl\nk\nj\nk\nxxd\nS\n\u03c3\u03c3   (5) \nwhere i=[1,n] is the number of clusters; )( *\n, j\ni xxd  denotes \n2051\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on December 8, 2008 at 08:43 from IEEE Xplore.  Restrictions apply.\n  \n \nthe distance from cluster centre to new sample assigned into \nthis cluster.  \n \nThe scatter can be updated recursively by [14,18]: \n( ) ( ) ( )( )212*,212 ),(1 lkkil\nk\nl\nk\nl\nk xxdS \u2212\u2212\n\u2212+= \u03c3\u03c3\u03c3  (6) \nThe nearest neighbour (also known as \u2018winner-takes-all\u2019) \nclassifier is used to determine the label of the winning class \n[13]: \na. For eClassC the sample is associated with the \nnearest cluster (thus associating with its label if \nany, note we do not need to know this label): \n( ) ],1[;maxarg*\n1\nNij i\nN\ni\n==\n=\n\u03c4          (7a) \nwhere j*\u2208[1,N] denotes the index of the winning cluster. \nb. For eClassA the label of the winning prototype is \ntaken: \n( ) jN\nj\nj\ni\nM\ni\njLL \u03bb\n1\n*\n1\nmaxarg*;max*\n=\n=\n==         (7b) \nwhere  ],1[;\/\n1\nNi\nN\nj\nj\nii\n== \u2211\n=\n\u03c4\u03c4\u03bb denotes the normalised \nfiring level of the ith fuzzy rule. \nc. For eClassB the label of the winning prototype is \ntaken: \n( )iMi yL 1max* ==                (7c) \nwhere ],1[;\n1\n0\n1\nMlxaay\nn\nj\njjlM\nN\ni\ni\nl =\uf8f7\n\uf8f7\n\uf8f8\n\uf8f6\n\uf8ec\n\uf8ec\n\uf8ed\n\uf8eb\n+= \u2211\u2211\n==\n\u03bb  is the output of \nthe multi-input-multi-output (MIMO) TS model used [30]. \nd. For eClassM the label of the winning prototype is \nalso determined by (7c) but instead of one MIMO \ntype TS classifier [14] we build M separate \nmulti-input-single-output (MISO) TS classifiers \nper class. This procedure is also called fuzzy \nregression of an indicator matrix [23], as it acts on \na specific transferred form of the original input \ndata.  \nIII. METHODOLOGY AND PROCEDURE FOR EVOLVING THE \nCLASSIFIERS \nA. Methodology \nEvolving the fuzzy classifiers of eClass type can start \u2018from \nscratch\u2019 or from an initial classifier. eClassC stands out as a \nfully unsupervised classifier. As such it is a clustering \nalgorithm, which is on-line and evolving (the number of \nclusters is not pre-specified). The algorithm is influenced by \nthe Mountain [28] and subtractive clustering approaches [29]. \nIt is described in more details elsewhere [11],[18]. The basic \nidea is to measure the potential of a prototype to become a new \ncluster centre and to compare this with the potentials of the \nexisting cluster centres so far. The potential is a measure of \ndata density (in the feature space for the case of a classifier). \nThat is a prototype is formed around a representative sample \nwith feature vector similar to which there are many other \nsamples. Prototypes are also formed around samples that will \nensure coverage of the feature space. \n The three other types of classifiers (eClassA, eClassB, and \neClassM) are based on the evolving TS type fuzzy rule-based \nsystems of MIMO [30] or MISO [11] type with singletons \n(zero order) or linear (first order) outputs of the so called eTS \nfamily [11],[18],[30]. The fuzzy rule-base build in this way is \nused to approximate the classification surface by a non-linear \nregression over the features. In essence, this approach \ncombines the evolving clustering as described above with a \nfuzzily weighted mixture of recursive least square (wRLS) \nestimators [11]. Details of this algorithm are also given \nelsewhere [11],[18],[30].  \nA special contemplation regarding the application of the \nalgorithms for learning  eTS model family [11], [18], and [30] \nfollows from the definitions (7b) and (7c) of the eClassB and \neClassM classifiers.  Since max aggregation of the normalized \nfiring levels j\u03bb  is used in the procedures for calculation of the \nwinning prototype (7b) and (7c), it is reasonable to consider \nalso max guided aggregation of the TS sub-system \ncontributions during the learning phase.  This can be \naccomplished by replacing the conventional normalization of \nthe firing levels of the rules: ],1[;\/\n1\nNi\nN\nj\nj\nii\n== \u2211\n=\n\u03c4\u03c4\u03bb  \nin the inferred TS model output: \n\u03b8\u03c8 Ty =\n                 \nwhere [ ]TTRTT pipipi\u03b8 ,...,, 21= is a vector formed by the \nsub-model parameters and TTeR\nT\ne\nT\ne xxx ],...,,[ 21 \u03bb\u03bb\u03bb\u03c8 =  is \na vector of the inputs that are weighted by the normalized \nfiring levels of the rules, i\u03bb , i=[1,R], by a max-like \nnormalization \n],1[i;\/ N\n1j\nj\n)i(i N=\u2211=\n=\n\u03b1\u03b1 \u03c4\u03c4\u03bb ,  \nwhere parameter \u03b1 > 1 determines an increasing  level of \nmax-like aggregation of the firing levels.   For higher values of \n\u03b1 the higher firing levels i\u03c4 's are reinforced providing higher \nweights i\u03bb to the respective subsystems in the TS model.   \nTherefore, the eTS learning algorithm is adapted to the \nclassification objective.  For \u03b1 = 1 the mean type aggregation \nof the subsystems that is characteristic for approximation type \nof applications of the eTS models occurs. \nNote that eClassC coincides with eClassA, if the class \nlabels are known, because according to (7a) and (7b): \n2052\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on December 8, 2008 at 08:43 from IEEE Xplore.  Restrictions apply.\n  \n \n( ) ],1[;\/maxargmaxarg\n111\nNi\nN\nj\nj\ni\nN\ni\ni\nN\ni\n=\uf8f7\n\uf8f7\n\uf8f8\n\uf8f6\n\uf8ec\n\uf8ec\n\uf8ed\n\uf8eb\n= \u2211\n=\n==\n\u03c4\u03c4\u03c4  \nIn case when the labels are not known (a fully unsupervised \nclassification based on the nearest neighbour principle) the \nlearning is based on all data samples. The learning when the \nlabels are known (eClassA) is per class. \nB. Procedures \nIn this paper we describe the procedures for each of the \nproposed evolving fuzzy rule-based classifiers. All algorithms \nhave one characteristic in common: the rule evolution and \nupdate of the antecedents takes place in the feature space, i.e. \nclusters evolve if the new incoming data is highly descriptive \nor if it expands the coverage of the fuzzy rule-base. The major \ndifference lies in the update scheme for the consequent parts. \nIn eClassC it is virtually not existent as such; in eClassA it is a \nlabel, singleton that has an integer (binary) value; in eClassB \nand eClassM it is (locally linear).  \n \nAlgorithm 1 eClassC \n1) Initialise the classifier by either of: \na. The first sample, x1 (assign potential P1:=1)  \nb. An initial rule-base \n2) Start a loop while there are new data samples do: \na. Read the feature vector, xk \nb. Associate the sample with the nearest \ncluster using (7a) \nc. Calculate its potential, Pk by [11,21]: \n( ) ( ) )1(21\n1\n\u2212+\u2212+\u2212\n\u2212\n=\nkk\nk\nxP\nkkk\nkk \u03b3\u03b2\u03b1  (8) \nwhere ( )\u2211\n=\n=\nn\nj\nj\nkk x\n1\n2\n\u03b1 ; 0; 111 =+= \u2212\u2212 \u03b2\u03b1\u03b2\u03b2 kkk   \n             \nj\nk\nn\nj\nj\nkk x \u03c7\u03b3 \u2211\n=\n=\n1\n; 0; 111 =+= \u2212\u2212\njj\nk\nj\nk\nj\nk x \u03c7\u03c7\u03c7         \nd. Update the potential of the existing cluster \nprototypes by [11,21]: \n( ) ( ) ( )( ) ( )\u2211\n=\n\u2212\u2212\n\u2212\n\u2212++\u2212\n\u2212\n=\nn\nj j\nk\ni\nk\ni\nkk\ni\nkk\ni\nkki\nkk\nxxxPxPk\nxPk\nxP\n1\n2\n**\n1\n*\n1\n*\n1*\n2\n1\n (9) \ne. Compare the two and  \ni. Add a new cluster  \n  ( ) ( ) ],1[;)()()()( ** nixPxPORxPxPIF ikkkikkk =\u2200<>  (10) \nii. Replace a cluster  \n( )],1[,;)(];,1[, 1 njjexNiiIF kji =\u2200>=\u2203 \u2212\u00b5   (11) \niii. ELSE do not change the cluster \nstructure  \nAlgorithm 2 eClassA \n1) Initialise the classifier by either of: \na. the first data sample per class with the pair \n(feature vector and the class label), \nz1=[x1,L1]. \nb. An initial rule-base. \n2) Start a loop while there are new data samples do: \na. Read the feature vector, xk \nb. Determine the winning class using (7b) \nc. Calculate the potential of the pair, \nzk=[xk,Lk] using (8) in respect to zk \nd. Update the potential of the existing \nprototypes by (9) in respect to zk \ne. Compare the two and \ni. Add a new fuzzy rule IF (10)  \nii. Replace a rule IF (11) holds \niii. ELSE do not change the rule-base \nf. Increment the time, k\u2190k+1 \nAlgorithm 3 eClassB differs from Algorithm 2 in step 2)b. \nonly which is according to (7c) not (7b), whereas the linear \nparameters ija are updated by weighted recursive least \nsquares (wRLS), exploiting local learning [18],[30], \ntriggering more flexibility during learning and transparency of \nthe consequent functions.  \nAlgorithm 4 eClassMM is based on Algorithm 3 applied to \nM separate multi-input-single-output (MISO) TS classifiers \nper class [23]. For classification that class is taken, whose \ncorresponding output value is maximal (Figure 1).  \n \nFigure 1 A graphical interpretation of eClassB and eClassM \nIV. EXPERIMENTAL RESULTS \nThe proposed classifier, eClass was tested on a well known \nbenchmark problem and on a real-life problem. It should be \nnoted that despite the clearly off-line nature of these \nbenchmark problems they were used to test the proposed \nclassifier in order to have some comparison.  \n2053\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on December 8, 2008 at 08:43 from IEEE Xplore.  Restrictions apply.\n  \n \nA. Wine Reproduction Data \nThe commonly used benchmark problem Wine \nReproduction data set [25] is tested in order to illustrate the \nlevel of performance of the proposed evolving classifiers in \ncomparison with other popular classification algorithms. The \ndata set contains the chemical analysis of wines derived from \nthree different cultivars. There are three classes, 178 samples \nwith 13 continuous numerical features available in the data \nset. Original order of the data set from [25] is used, and no \nprior feature selection is performed.  \nThe results of testing eClass with Wine data set are \ntabulated in Table I, along with results from some comparable \nclassifiers. \nThe result shows that eClassC and eClassA are more \ncomputational efficient and their rule bases have better \ninterpretability; while the first order Takagi-Sugeno rule \nbased, eClassB and eClassM have better accuracy of \nclassification. Please note that the rule numbers listed for \neClassM is the summation of rule numbers of each MISO \nsub-models. \nB. On-line Image Classification  \nIn this section an application example is given, which \nincludes an automatically self-reconfigurable and adaptive \nfault detection framework for images which classifies each \nimage as good or bad, and evolves the classifier upon \noperator's feedback and the data. The images are taken from \nan online production process with a high frequency with the \naim to supervise the system, as they may show errors in a \nproduction process. This framework including \npre-processing, segmentation and classification is shown in \nFigure 2. \n \nFig 2. Dynavis framework for on-line images classification. \n \nIn principle, each type of image may be processed through \nthe classification framework as shown in Fig. 1. The only \nassumption is that a master image is available: the purpose is \nto generate deviation images by subtracting newly recorded \nimages from the master one in order to be able to classify the \nimage into good or bad (depending on the structure and \ncharacteristics of the deviation pixels). \nFor the evaluation of our approaches we applied image data \nfrom a CD-imprint production process, where faults due to \nweak colours, wrong palettes etc. should be detected within a \nprocess frequency of about 1 Hz. The data stream comprises \n1164 images that were recorded one by one. eClass was \nevolved on-line starting \u2018from scratch\u2019, so it was able to \nclassify from the second sample onwards. In order to compare, \nhowever, with the other approaches which require batch \nlearning over certain data set we have evolved eClass with the \nfirst 776 images and stopped evolution (fixed the rule-base). \nWith the remaining 388 samples we made classifications only \n(no learning and evolution). Seventeen aggregated features \nwere extracted, describing the distribution, density, shape etc. \nof the pixel fragments in the deviation images. \nThe miss-classification rates on this test data set are \ndemonstrated in Table II. The superiority in terms of low \ncomputational costs (time), high precision, and low \ncomplexity (low number of fuzzy rules) of eClass family is \nclearly visible. \nV. CONCLUSIONS \nIn this paper, four different fuzzy model architectures were \npresented, whose inner structures and parameters are evolved \nand incrementally updated. In this way, four different schemes \nbased on eClass procedure were presented, namely: eClassC, \neClassA, eClassB and eClassM.  \nBoth tests on Wine data and on the adaptive image data shows \nthat the proposed techniques for training fuzzy rule-base classifiers \nhave the advantage of that evolving its structure from scratch \nwithout losing much precision (classification rate). In the online \nevolving mode, the performance slightly deteriorates, but is \ncomparable to the results of the well-known and renowned batch \nmodelling approaches CART [32] and probabilistic NN [33]. The \nfirst order classifiers eClassB and eClassM have results of the same \nlevel of precision as the best of these classifiers. The results \nTABLE II \nRESULTS FOR ON-LINE IMAGE CLASSIFICATION \n \nClass. \nRate # rules Total time, s \nCART [32] 91.24% - Off-line \nPNN [33] 90.46% - Off-line \nFLEXFISClass SM [23] 83.33% 39 1.58 \nFLEXFISClass MM [23] 90.98% 62 3.56 \nFLEXFISClass MM* [23] 91.24% 62 3.56 \neClassC 76.28% 12 0.05 * \neClassA 79.70% 12 0.05 * \neClassB 91.24% 7 0.28 * \neClassM 91.24% 14 0.53 * \n*    Running time for the last 388 samples with fixed eClass model; the test is \ncarried out on a laptop computer with a CPU 3.0GHz. \n \nTABLE I \nRESULTS FOR WINE DATA \n Class. Rate # rules Total time, s  \niPCA [24] 80.3% 7* - \nSmooth Boost [26] 86.1% - - \neClassC 63.48% 9 0.92** \neClassA 90.45% 12 1.18** \neClassB 94.38% 9 3.52** \neClassM 97.19% 28 7.18** \n* number of eigen-values (note that they are not transparent as the fuzzy rules \nand do not represent the features directly) \n** The test is carried out on a laptop computer with a CPU 1.6GHz; the time is \nfor processing all the data samples \n \n2054\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on December 8, 2008 at 08:43 from IEEE Xplore.  Restrictions apply.\n  \n \ndemonstrate that the proposed evolving fuzzy rule-base classifier, \neClass in any of its modifications has a very good classification \nperformance; it is computationally very efficient, and is, thus, \nsuitable for real-time applications such as classification streaming \ndata, robotic applications, e.g. target and landmark recognition, \nmachine health monitoring and prognostics, fault detection and \ndiagnostics etc. This approach is transparent, linguistically \ninterpretable, and applicable to both fully unsupervised and partially \nsupervised learning. While the low order architectures (eClassC \nand eClassA) are computationally superior, with high transparency \n(low number of fuzzy rules and parameters) and simple structure, the \nperformance of eClassB using multi-input- multi-output, MIMO TS \nis superior to all the other structures and to the previously existing \nclassifiers. The multiple model structure, eClassM has similar \nperformance to eClassA with MIMO TS but a more complicated \nstructure.   \nACKNOWLEDGEMENTS \nThis work was supported by the European Commission \n(project Contract No. STRP016429, acronym DynaVis). This \npublication reflects only the author's view. \nREFERENCES \n[1] R. O. Duda, P.E. Hart, and D.G. Stork. Pattern Classification - Second \nEdition. Wiley-Interscience, Chichester, West Sussex, England, 2000. \n[2] G. Fayyad, Piatetsky-Shapiro, P.~Smyth. From Data Mining to \nKnowledge Discovery: An Overview, Advances in Knowledge \nDiscovery and Data Mining, MIT Press, 1996. \n[3] P. Domingos and G. Hulten. Catching Up with the Data: Research \nIssues in Mining Data Streams, Workshop on Research Issues in Data \nMining and Knowledge Discovery, Santa Barbara, CA, 2001. \n[4] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical \nLearning: Data Mining, Inference and Prediction. Heidelberg, \nGermany: Springer Verlag, 2001. \n[5] V. N. Vapnik, The Statistical Learning Theory, Springer, 1998. \n[6] M. Butz, Rule-based Evolutionary Online Learning Systems: A \nPrincipal Approach to LCS Analysis and Design, Physica Verlag, \nv.191, Berlin, Heidelberg, Germany, 2006, ISBN 3-540-25379-3. \n[7] J. A. Roubos, M. Setnes and J. Abonyi, Learning Fuzzy Classification \nRules from Data, Information Sciences\u2014Informatics and Computer \nScience: An International Journal, vol. 150, pp. 77-93, 2003 \n[8] C. M., Bishop, Neural Networks for Pattern Recognition, Oxford \nUniversity Press, Oxford, UK, 1995. \n[9] V.Vapnik, The Nature of Statistical Learning Theory, Springer, 1995 \n[10] P. Angelov, Evolving Rule-based Models: A Tool for Design of \nFlexible Adaptive Systems. Berlin, Germany: Springer Verlag, 2002. \n[11] P. Angelov, D. Filev, \"An approach to on-line identification of evolving \nTakagi-Sugeno models\", IEEE Trans. on Systems, Man and \nCybernetics, part B, vol.34, No1, pp. 484-498, 2004. \n[12] P. Angelov, R. Buswell, Identification of Evolving Rule-based Models \n(2002) IEEE Transactions on Fuzzy Systems, vol. 10, No5, \npp.667-677. \n[13] Setnes M., J. A. Roubos, Transparent Fuzzy Modelling using Fuzzy \nClustering and GA\u2019s. In: Proc. 18th Annual Conf. of the North \nAmerican Fuzzy Information Processing Society, NAFIPS, 10-12 June \n1999, New York, USA, pp.198-202. \n[14] P. Angelov, X. Zhou, F. Klawonn, Evolving Fuzzy Rule-based \nClassifiers, First 2007 IEEE International Conference on \nComputational Intelligence Applications for Signal and Image \nProcessing, April 1-5, 2007, Honolulu, Hawaii, USA, to appear. \n[15] G. C. Goodwin, K. S. Sin, Adaptive Filtering, Prediction, and Control, \nPrentice Hall, ISBN-10: 013004069X, 1984. \n[16] F. Klawonn,  P. Angelov, Evolving Extended Naive Bayes Classifier, \nIn: S. Tsumoto, C.W. Clifton, N. Zhong, X. Wu, J. Liu, B.W. Wah, \nY.-M. Cheung: Proc. Sixth IEEE International Conference on Data \nMining. IEEE, Los Alamitos (2006), pp. 643-647 (ISBN 0769527027). \n[17] P. Angelov, V. Giglio, C. Guardiola, E. Lughofer, An Approach to \nModel-based Fault Detection in In- dustrial Measurement Systems with \nApplication to Engine Test Ben- ches, Measurement Science & \nTechnology, v.17 (7), 2006, 1809-1818.  \n[18] P. Angelov, X. Zhou, Evolving Fuzzy Systems from Data Streams in \nReal-Time, Proc. 2006 International Symposium on Evolving Fuzzy \nSystems, UK, IEEE Press, pp.29-35, ISBN 0-7803-9719-3. \n[19] L.-X. Wang \u201cFuzzy Systems are Universal Approximators\u201d, Proc.  \nFUZZ-IEEE, San Diego, CA, USA, pp.1163-1170, 1992. \n[20] P. Angelov, A Fuzzy Controller with Evolving Structure, Information \nSciences, ISSN 0020-0255, vol.161, 2004, pp.21-35. \n[21] R. R. Yager, D.P. Filev, \u201cLearning of Fuzzy Rules by Mountain \nClustering,\u201d Proc. of SPIE Conf. on Application of Fuzzy Logic \nTechnology, Boston, MA, USA, pp.246-254,1993. \n[22] X. Zhou, P. Angelov, Autonomous Self-localization in Completely \nUnknown Environment using Evolving Fuzzy Rule-based Classifier, \n1st IEEE Symp. on Comp. Intelligence for Security and Defense Applic. \n(CISDA) 2007 , April 1-5, 2007 Honolulu, HI, USA, to appear. \n[23] E. Lughofer, P. Angelov, X. Zhou, Evolving Single- and Multi-Model \nFuzzy Classifiers with FLEXFIS-Class, IEEE Intern. Conf. on Fuzzy \nSyst., FUZZ-IEEE2007 23-26 July, 2007, London, England, to appear. \n[24] S. Pang, S. Ozawa, N. Kasabov, \"Incremental Linear Discriminant \nAnalysis for Classification of Data Streams\", IEEE Trans. on Systems, \nMan and Cybernetics, part B, vol.35, No5, pp. 905-914, 2005. \n[25] UCI Machine Learning Repository, http:\/\/www.ics.uci.edu\/ \n~mlearn\/MLRepository.html, accessed on 27 Jan 2007. \n[26] R. Jin, J. Zhang, A Smoothed Boosting Algorithm Using Probabilistic \nOutput Codes, Proc. 22nd International Conference on Machine \nLearning, Bonn, Germany, pp.361-368, 2005. \n[27] E. Lughofer and E. P. Klement, \u201cFLEXFIS: A variant for incremental \nlearning of Takagi-Sugeno fuzzy systems, in Proc. FUZZ-IEEE 2005, \nReno, Nevada, USA, 2005, pp.915-920. \n[28] R. Yager and D. Filev, Approximate Clustering via the Mountain \nMethod, IEEE Trans. on Systems and Cybernetics, vol. 24 (8), 1994 \n[29] S. Chiu, Fuzzy Model Identification based on Cluster Estimation, \nJournal of Intelligent and Fuzzy Systems, vol.2 (3), pp. 267-278, 1994 \n[30] P. Angelov, C. Xydeas, D. Filev, On-line Identification of MIMO \nEvolving Takagi-Sugeno Fuzzy Models, Intern. Joint Conf. on Neural \nNetworks and Intern. Conf. on Fuzzy Systems, IJCNN-FUZZ-IEEE, \nBudapest, Hungary, 25-29 July, 2004, 55-60, ISBN 0-7803-8354-0. \n[31] J. R. Quinlan, C4.5: Programs for Machine Learning, Morgan \nKaufmann Publishers Inc, U.S.A., 1993. \n[32] L. Breiman, J. Friedman, C.J. Stone and R.A. Olshen, Classification  \nand Regression Trees, Chapman and Hall, Boca Raton, 1993. \n[33] T. Hastie and R. Tibshirani, 1996. Discriminant Adaptive Nearest  \nNeighbor Classification. IEEE Trans. Pattern Anal. Mach. Intell. \n(TPAMI). Vol. 18 (6), pp. 607-616, 1996. \n[34] M. Garofalakis, J.Gehrke and R. Rastogi, Querying and Mining Data \nStreams: you only get one look, Proc. of the 2002 ACM SIGMOD \nIntern. Conf. on Management of Data, pp. 635-641, ACM Press, 2002. \n[35] L.X. Wang, Fuzzy Systems are Universal Approximators, Proc. 1st \nIEEE Conf. Fuzzy Systems, pp. 1163-1169, San Diego, 1992. \n[36] E. Lughofer and U. Bodenhofer, Incremental Learning of Fuzzy Basis \nFunction Networks with a Modified Version of Vector Quantization, \nProc. of IPMU 2006, Paris, France, vol. 1, pp 56-63, 2006. \n[37] P. Angelov and E.Lughofer, Data-Driven Evolving Fuzzy Systems \nusing eTS and FLEXFIS: Comparative Analysis, Intern. Journal of \nGeneral Systems, to appear \n[38] T. Takagi and M. Sugeno, Fuzzy Identification of Systems and its \nApplications to Modeling and Control, IEEE Trans. on Systems, Man \nand Cybernetics, vol. 15 (1), pp. 116-132, 1985. \n[39] O. Cordon, F. Herrera, M. J. del Jesus, P. Villar, A Multi-objective \ngenetic algorithm for feature selection and granularity learning in \nfuzzy-rule based classification systems, Proc. Joint 9th IFSA World \nCongress and 20th NAFIPS Intern. Conf.,25-28 July 2001, Vol. 3,  pp. \n1253-1258, Vancouver, BC, Canada, ISBN: 0-7803-7078-3. \n \n \n2055\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on December 8, 2008 at 08:43 from IEEE Xplore.  Restrictions apply.\n"}