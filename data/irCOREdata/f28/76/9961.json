{"doi":"10.1145\/1102351.1102465","coreId":"9961","oai":"oai:www.era.lib.ed.ac.uk:1842\/3700","identifiers":["oai:www.era.lib.ed.ac.uk:1842\/3700","10.1145\/1102351.1102465"],"title":"Learning Discontinuities with Product-of-Sigmoids for Switching between Local Models","authors":["Toussaint, Marc","Vijayakumar, Sethu"],"enrichments":{"references":[{"id":1034380,"title":"A greedy approach to identi\ufb01cation of piecewise a\ufb03ne models.","authors":[],"date":"2003","doi":"10.1007\/3-540-36580-x_10","raw":"Bemporad, A., Garulli, A., Paoletti, S., & Vicino, A. (2003). A greedy approach to identi\ufb01cation of piecewise a\ufb03ne models. Hybrid Systems: Computation and Control; LNCS (pp. 97\u2013112). Springer.","cites":null},{"id":1034428,"title":"Constructive incremental learning from only local information.","authors":[],"date":"1998","doi":"10.1162\/089976698300016963","raw":"Schaal, S., & Atkeson, C. (1998). Constructive incremental learning from only local information. Neural Computation, 10, 2047\u20132084.","cites":null},{"id":1034403,"title":"Empirical evaluation of the improved Rprop learning algorithm.","authors":[],"date":"2003","doi":"10.1016\/s0925-2312(01)00700-7","raw":"Igel, C., & H\u00a8 usken, M. (2003). Empirical evaluation of the improved Rprop learning algorithm. Neurocomputing, 50(C),, 105\u2013123.","cites":null},{"id":1034412,"title":"Evolution of voronoi-based fuzzy controllers.","authors":[],"date":"2004","doi":"10.1007\/978-3-540-30217-9_55","raw":"Kavka, C., & Schoenauer, M. (2004). Evolution of voronoi-based fuzzy controllers. 8th International Conference on Parallel Problem Solving from Nature (PPSN VIII), Birmingham, UK; LNCS. Springer.","cites":null},{"id":1034398,"title":"Generalized additive models.","authors":[],"date":"1990","doi":"10.1214\/ss\/1177013604","raw":"Hastie, T., & Tibshirani, R. (1990). Generalized additive models. Chapman and Hall, New York.","cites":null},{"id":1034406,"title":"Hierarchical mixtures of experts and the EM algorithm.","authors":[],"date":"1994","doi":"10.1007\/978-1-4471-2097-1_113","raw":"Jordan, M. I., & Jacobs, R. A. (1994). Hierarchical mixtures of experts and the EM algorithm. Neural Computation, 6, 181\u2013214.","cites":null},{"id":1034420,"title":"Learning switching linear models of human motion.","authors":[],"date":"2000","doi":null,"raw":"Pavlovic, V., Rehg, J. M., & MacCormick, J. (2000). Learning switching linear models of human motion. NIPS (pp. 981\u2013987).","cites":null},{"id":666274,"title":"Locally weighted projection regression: An o(n) algorithm for incremental real time learning in high dimensional space.","authors":[],"date":"2000","doi":null,"raw":"Vijayakumar, S., & Schaal, S. (2000). Locally weighted projection regression: An o(n) algorithm for incremental real time learning in high dimensional space.","cites":null},{"id":666275,"title":"Multiple paired forward and inverse models for motor control.","authors":[],"date":"1998","doi":"10.1016\/S0893-6080(98)00066-5","raw":"Wolpert, D., & Kawato, M. (1998). Multiple paired forward and inverse models for motor control. Neural Networks, 11, 1317\u20131329.","cites":null},{"id":1034387,"title":"Random sample consensus: A paradigm for model \ufb01tting with applications to image analysis and automated cartography.","authors":[],"date":"1981","doi":"10.1145\/358669.358692","raw":"Fischler, M. A., & Bolles, R. C. (1981). Random sample consensus: A paradigm for model \ufb01tting with applications to image analysis and automated cartography. Comm. of the ACM, 24, 381\u2013395.","cites":null},{"id":1034383,"title":"SIMPLS: An alternative approach to partial least squares regression.","authors":[],"date":"1993","doi":"10.1016\/0169-7439(93)85002-x","raw":"de Jong, S. (1993). SIMPLS: An alternative approach to partial least squares regression. Chemometrics and Intelligent Laboratory Systems, 18, 251\u2013263.","cites":null},{"id":1034393,"title":"Variational learning for switching state-space models.","authors":[],"date":"1998","doi":"10.1162\/089976600300015619","raw":"Ghahramani, Z., & Hinton, G. (1998). Variational learning for switching state-space models. Neural Computation, 12, 963\u2013996.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-08-31T13:57:56Z","abstract":"Sensorimotor data from many interesting physical interactions comprises discontinuities. While existing locally weighted learning approaches aim at learning smooth function","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/9961.pdf","fullTextIdentifier":"http:\/\/homepages.inf.ed.ac.uk\/svijayak\/publications\/toussaint-ICML2005.pdf","pdfHashValue":"81e41fa38cf21c2699a5c9300effdb1204e7b00f","publisher":"ACM Press    New York","rawRecordXml":"<record><header><identifier>\n    \n        \n            \n                oai:www.era.lib.ed.ac.uk:1842\/3700<\/identifier><datestamp>\n                2010-08-31T15:17:22Z<\/datestamp><setSpec>\n                com_1842_102<\/setSpec><setSpec>\n                col_1842_3391<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nLearning Discontinuities with Product-of-Sigmoids for Switching between Local Models<\/dc:title><dc:creator>\nToussaint, Marc<\/dc:creator><dc:creator>\nVijayakumar, Sethu<\/dc:creator><dc:subject>\nswitching models<\/dc:subject><dc:description>\nSensorimotor data from many interesting physical interactions comprises discontinuities. While existing locally weighted learning approaches aim at learning smooth functions<\/dc:description><dc:date>\n2010-08-31T13:57:56Z<\/dc:date><dc:date>\n2010-08-31T13:57:56Z<\/dc:date><dc:date>\n2005<\/dc:date><dc:date>\n2010-08-31T13:57:56Z<\/dc:date><dc:type>\nConference Paper<\/dc:type><dc:identifier>\n1-59593-18<\/dc:identifier><dc:identifier>\n0376<\/dc:identifier><dc:identifier>\nhttp:\/\/homepages.inf.ed.ac.uk\/svijayak\/publications\/toussaint-ICML2005.pdf<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/1842\/3700<\/dc:identifier><dc:identifier>\nhttp:\/\/doi.acm.org\/10.1145\/1102351.1102465<\/dc:identifier><dc:publisher>\nACM Press    New York<\/dc:publisher>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":["switching models"],"subject":["Conference Paper"],"fullText":"Learning Discontinuities with Products-of-Sigmoids for\nSwitching between Local Models\nMarc Toussaint mtoussai@inf.ed.ac.uk\nSethu Vijayakumar sethu.vijayakumar@ed.ac.uk\nSchool of Informatics, University of Edinburgh, The King\u2019s Buildings, May\ufb01eld Road, Edinburgh EH9 3JZ, UK.\nAbstract\nSensorimotor data from many interesting\nphysical interactions comprises discontinu-\nities. While existing locally weighted learning\napproaches aim at learning smooth functions,\nwe propose a model that learns how to switch\ndiscontinuously between local models. The\nlocal responsibilities, usually represented by\nGaussian kernels, are learned by a product\nof local sigmoidal classi\ufb01ers that can repre-\nsent complex shaped and sharply bounded re-\ngions. Local models are incrementally added.\nA locality prior constrains them to learn only\nlocal data\u2014which is the key ingredient for in-\ncremental learning with local models.\n1. Introduction\nLocally weighted learning techniques have successfully\nbeen employed as incremental, non-parametric ap-\nproximation schemes for high-dimensional regression\nproblems (Schaal & Atkeson, 1998; Vijayakumar &\nSchaal, 2000). Their robustness and e\ufb03cient online\nversions are crucial in robotic domains where, for in-\nstance, an inverse model of an articulated dynamic\nrobot has to be learned in real-time. Such models map\na high-dimensional state (e.g., joint angles and veloc-\nities) and a desired change of state to the required\nmotor signals (torques).\nWhile typically such mappings are assumed to be\nsmooth, in real world scenarios, there are many inter-\nesting cases where the functions of interest are truly\ndiscontinuous. Some examples include contact with\nother objects (and the ground), with other parts of the\nbody, or with \u201cjoint limits\u201d. In fact, many interesting\ninteractions with the environment manifest themselves\nAppearing in Proceedings of the 22\nnd International Confer-\nence on Machine Learning, Bonn, Germany, 2005. Copy-\nright 2005 by the author(s)\/owner(s).\nthrough discontinuities in the sensorimotor data.\nSwitching between models can be modeled by intro-\nducing a latent switch variable i that indicates which\nmodel is responsible. Several existing methods address\na scenario where i can be inferred from the tempo-\nral context by learning a latent temporal transition\nmodel P(i0|i), e.g., in the context of state space mod-\nels (Ghahramani & Hinton, 1998; Pavlovic et al., 2000)\nor multiple inverse models (Wolpert & Kawato, 1998).\nHere we want to address another scenario, where i\ncan be inferred directly from the input x (e.g., the\nrobot\u2019s con\ufb01guration or sensor readings) by learning a\nprobabilistic model P(i|x). Learning a deterministic\npartitioning of the input space x 7\u2192 i has been ad-\ndressed before: (Bemporad et al., 2003) use a greedy\nsearch algorithm to \ufb01nd a polygonal partitioning con-\nstrained by an absolute error bound. (Kavka & Schoe-\nnauer, 2004) recently proposed a Voronoi cell parti-\ntioning with local fuzzy controllers learned with an\nEvolutionary Algorithm. Also decision trees (Model\nTrees, (Quinlan, 1992)) are possible choices to rep-\nresent a partitioning. None of these approaches pro-\nvide a probabilistic model for P(i|x) or for the local\nregression models (e.g., to account for outliers) and\nnone of them provides an incremental learning scheme,\nwhere P(i|x) (or the deterministic partitioning) can be\nadapted locally without interference e\ufb00ects.\nOur approach will introduce a product-of-sigmoids\nmodel of P(i|x) where the boundary between two\nneighboring models can be adapted independently\nfrom all others. A standard EM-algorithm is derived\nfrom a probabilistic regression model which includes a\nbackground noise (outlier) model as a mixture. Un-\nlike standard mixture models, we introduce a locality\nprior which prevents non-neighboring local models to\n\u2018compete for data\u2019\u2014this greatly reduces interference\nin the learning process and is one of the key features\nof local learning techniques that allows for incremen-\ntal learning (Schaal & Atkeson, 1998). In that re-\nspect, we follow the approach of locally weighted learn-Learning Discontinuities with Products-of-Sigmoids for Switching between Local Models\ning (Vijayakumar & Schaal, 2000), where previously a\nGaussian kernel associated with each local model rep-\nresented its responsibility (although it is used as an\naveraging weight rather than a probabilistic mixture\ncoe\ufb03cient). Our key extension to these approaches is\nthat, using a product of local sigmoidal classi\ufb01ers as a\nmodel for the responsibilities P(i|x) instead of the typ-\nically Gaussian kernel, we can learn complex shaped,\nsharply bounded responsibility regions for each local\nmodel.\nThe next section will give a brief overview of the model\nwhile Sections 3 and 4 introduce the details of learning\na family of local models and learning the responsibili-\nties. Section 5 brie\ufb02y addresses the complexity of the\nmodel. In Section 6, we demonstrate the performance\nof the algorithm on test functions and a physical in-\nteraction problem.\n2. Overview of the model\nThe complete model will be composed of n submod-\nels \u03c6i, i = 1,..,n, and a set of sigmoidal functions\n\u03c8ij(x) \u2208 [0,1] that switch between two neighboring\nsubmodels i and j (see Fig. 1). The \u201ccell\u201d associated\nto a model i is bounded by the adjacent sigmoids \u03c8ij,\nsimilar to the Voronoi cells of (Kavka & Schoenauer,\n2004). However, adapting Voronoi sites incrementally\nto design appropriate boundaries is a complex coupled\nproblem. Therefore, we represent every boundary by a\nseparate sigmoid \u03c8ij; hence, incorporating the ability\nto represent even non-convex polygonal regions.\nGiven this setup, we will have to: (1) learn the set\n(or family) of submodels \u03c6i such that it is su\ufb03cient to\nexplain the data, and (2) learn the sigmoidal switch-\ning functions \u03c8ij between neighboring models. Fol-\nlowing the policy to reduce interference, we decouple\nthe learning process accordingly on two levels. On\nthe \ufb01rst level (family learning) the goal is to learn a\nset of submodels such that for every datum at least\none of them is a su\ufb03cient model. Family learn-\ning will assume a simple, uninformed locality prior\nPl(i|x) to formulate a generative mixture model and\nderive the EM-updates. The second level uses the\nlearned family of submodels to learn an accurate pre-\ndictive model Ps(i|x) with the product-of-sigmoids ap-\nproach. Separating these two levels prevents interfer-\nence problems\u2014typical for mixture of experts mod-\nels (Jordan & Jacobs, 1994)\u2014that arise when using a\nyet incorrect (gating) model Ps(i|x) for the responsi-\nbility inference in the E-step: If initially the learned\nmodel Ps(i|x) is incorrect, the inferred responsibilities\nare corrupted and the submodels \u03c6i are assigned hard\nto learn data. No stable set of submodels might de-\n\u03c812\n\u03c61 \u03c62\nFigure 1. The complete model is composed of local models\n\u03c6i and classi\ufb01ers \u03c8ij between neighboring models (e.g., the\nsolid line indicates a learned linear classi\ufb01cation). Every\nlocal model receives responsibility in a region bounded by\nadjacent classi\ufb01ers (dotted lines).\nvelop and based on this a better model Ps(i|x) could\nhardly be learned.\n3. Learning a family of models\nThe \ufb01rst level goal of our algorithm is to learn a fam-\nily \u03a6 = {\u03c61,..,\u03c6N} of models such that every datum\ncan be explained by at least one model. Thinking in\nterms of a generative model, this can be captured as\nfollows. First, an input x is drawn from some distribu-\ntion P(x). Then, with a probability \u000f, the output y is\npure noise modeled by the uniform distribution U(y).\nOtherwise, with a probability 1\u2212\u000f, the ith model of\nthe family is selected according to a prior Pl(i|x) and\nproduces an output P(y|i,x). Formally,\nP(y|x) = (1 \u2212 \u000f)\nN X\ni=1\nPl(i|x) P(y|i,x) + \u000f P(y|i = 0) ,\nP(y|i,x) =\n1\n\u221a\n2\u03c0 \u03c3\nexp\n\u001a\n\u2212\n|y \u2212 \u03c6i(x)|2\n2\u03c32\n\u001b\n,\nP(y|i = 0) = U(y) .\nThe latent variable i, which we will call the respon-\nsibility index, thus decides on which model produced\nthe data. Here, i = 0 corresponds to the case of the\nnoise model.\nOn the level of family learning we assume a locality\nprior Pl(i|x), describing which model i may possibly\nbe responsible for a given input x. This prior is rather\nuninformed and is not learned from data\u2014the next\nsection will exactly focus on replacing this prior by\na more predictive model Ps(i|x) learned from data.\nThe locality prior implements a constraint such that\nthe same model \u03c6i cannot be responsible for remote,\ndisconnected regions of the input domain. This clearly\nfollows the principle of localized learning, aiming to\nreduce interference, and is also a necessary constraint\nfor the switching model explained later.\nLet ci denote the mean input (center) on which model\n\u03c6i has been trained on. For a given input x, the ithLearning Discontinuities with Products-of-Sigmoids for Switching between Local Models\ni is not eligible for x i is eligible for x\nci ci\nx\ncj cj\nx\nFigure 2. The constraint implemented by the locality\nprior: A model with center ci is eligible for an input x\nonly when there does not exist another model with center\ncj \u201cbetween\u201d ci and x. A notion of \u201cbetween\u201d is that the\nscalar product in equation (1) is negative (which is equiv-\nalent to saying cj is not in the ball between x and ci).\nmodel is eligible (i.e., a candidate for explaining the\ndata) if and only if there does not exist a jth model\nwhich has its center \u201cbetween\u201d ci and x. More pre-\ncisely,\nPl(i|x) = 0 \u21d0\u21d2 \u2203j : hx \u2212 cj,ci \u2212 cji < 0 , (1)\nwhere h\u00b7,\u00b7i is the scalar product in input space. This\nconstraint is best explained by Fig. 2. Equal prob-\nability is then associated to all eligible models, i.e.,\nPl(i|x) is uniform over all eligible i\u2019s. It would be\nstraight-forward to introduce smoother locality con-\nstraints, but since this prior worked su\ufb03ciently and is\nparameter-free, we stick to this simple option.\nHaving setup this model, learning follows the standard\nEM-machinery. For the E-step, given a current family\nof models and a new training datum (x,y), we can\ninfer a posterior on the latent index i for this datum\nusing Bayes rule:\n\u2200i6=0 : P(i|y,x) =\n1 \u2212 \u000f\nZ\nP(y|i,x) Pl(i|x), (2)\nP(i = 0|y,x) =\n\u000f\nZ\nP(y|i = 0) , (3)\nwhere Z normalizes over i (including i = 0). As an M-\nstep, model \u03c6i is then trained on the datum weighted\nby the P(i|y,x) (we give more details on the local\nmodels below). It becomes clear how the locality prior\nin equation (2) reduces interference: a model i that is\nnon-local to x has a weighting zero, is not trained,\nand does not participate in the competition to model\nthe data.\nNew models are added incrementally to the family\nwhen needed: We use the MAP index\n\u02c6 i = argmaxi P(i|y,x) (4)\nto label the training datum with the most likely model.\nA zero MAP index \u02c6 i = 0 indicates that the training\ndata is \u201cyet unmodeled\u201d. All such data is collected\ninto a set until it has a desired size su\ufb03cient for a new\nhypothesis. The generation of new family members is\ndone, similarly to RANSAC (Fischler & Bolles, 1981),\nas follows: A datum (x,y) is selected randomly from\nthe set of unmodeled data. Then the K closest neigh-\nbors1 of (x,y) in this batch (w.r.t. Euclidean distance\nin input space) are determined. These K data points\nare chosen as initial training data for the new hypoth-\nesis. Thereafter, all points in the set of unmodeled\ndata are reconsidered and the MAP index \u02c6 i recalcu-\nlated for them according to equation (2), pretending\nthe hypothesis is a member of the family. If the new\nhypothesis receives a su\ufb03cient count of responsibilities\n(it basically only has to compete with the noise model)\nthis modeled data is deleted from the batch and the hy-\npothesis added to the family. We considered 10d as a\nthreshold for su\ufb03cient counts of responsibilities. This\nthreshold naturally limits the number of instantiated\nmodels depending on the data. Additionally, a prun-\ning of models that do not explain a su\ufb03cient amount\nof data is possible based on how often a model receives\nthe MAP index. We did not use such heuristics though\nin the experiments.\nWe also de\ufb01ne an error measure, the family error,\nwhich is the average squared error of the best \ufb01tting\neligible model: For a test data set {(xk,yk)}M\nk=1 we\nde\ufb01ne\nEf =\nM X\nk=1\n|yk \u2212 \u03c6ik(xk)|2 , where\nik = argmax\ni\nPl(i|xk) P(yk|i,xk) .\nIn this way, the performance of the underlying family\nlearning can be monitored independently in the train-\ning phase.\n3.1. The local models\nThe general scheme of family learning can be realized\nwith any type of models \u03c6i. In the experiments, we will\nchoose \u03c6i to be linear functions, learned with Partial\nLeast Squares (PLS) regression (de Jong, 1993). How-\never, there exist standard techniques to blend between\nlocal linear functions in order to learn non-linearities\n(Schaal & Atkeson, 1998). Future version of our model\nwill be extended to be able to do both, blending and\nswitching. Here, we want to focus only on the novel\naspect of the new model: the switching.\nThe su\ufb03cient statistics for PLS need to be collected\nin a weighted fashion: The variables of these statistics\nare the norm W, the input and output means mx and\nmy, and correlation matrices V and C. All of these\n1We choose K to be a random Poisson number with\nmean 3d, where d is the input dimensionality.Learning Discontinuities with Products-of-Sigmoids for Switching between Local Models\nare initialized with zero. When the model receives\nan input x, a target output y, and a weighting w (=\nP(i|y,x)) as new training data, the su\ufb03cient statistics\nare accumulated as\nW \u2190 W + w ,\nmx \u2190 mx + wx , my \u2190 my + wy ,\nV \u2190 V + wxxT , C \u2190 C + wxy .\nClearly, the total input mean is mx\/W, the output\nmean is my\/W, the input covariance matrix V\/W \u2212\nmx mT\nx\/W2 and the input-output covariance matrix\nC\/W \u2212 mx my\/W2. Whenever the model has to be\nevaluated, the SIMPLS algorithm (de Jong, 1993) \ufb01ts\na linear regression to these statistics by computing\na projection matrix P that maps x onto a lower-\ndimensional representation and a matrix Q that maps\nthis representation to the \ufb01nal output. The composi-\ntion QP : x 7\u2192 y is the learned linear model.\nPartial Least Squares can also be realized as an online\nlearning scheme (Vijayakumar & Schaal, 2000) where\nthe statistics are accumulated in the same way, but\nthe projections P are learned by continuous adapta-\ntion. The bene\ufb01t of PLS is that the projections P\nare learned in such a way that they are most \u201cinfor-\nmative\u201d (most correlated) with the output\u2014which is a\nvery powerful dimensionality reduction method to base\nthe actual regression on. In addition, the multivariate\nregression breaks down into a series of univariate re-\ngressions due to the orthogonality of the projection\ndirections.\n4. Products-of-sigmoids for switching\nOn the second level of our algorithm, the goal is to\nlearn a predictive model Ps(i|x) of the latent respon-\nsibility index i that is more precise than the locality\nprior Pl(i|x). One may think of exactly the same gen-\nerative process as described in the previous section ex-\ncept that Pl(i|x) is replaced with Ps(i|x).\nTraditionally, in localized learning one associates a ker-\nnel \u03b1i(x) (typically Gaussian) to each local model such\nthat P(i|x) (or a model\u2019s \u201cweighting\u201d) is proportional\nto \u03b1i(x), but normalized over i. The shape and size\nof such kernels can also be learned (Schaal & Atkeson,\n1998). Generally though, the shapes of responsibility\nregions may be complex and sharply bounded. Follow-\ning the localized learning principle we still want this\nclassi\ufb01cation be represented in a localized way.\nOur approach is to represent such kernels as a prod-\nuct of sigmoidal classi\ufb01ers that are arranged locally\naround a model i. More precisely, given some data\nit is easy to decide whether two models are \u201cpoten-\n(a) (b)\n(c) (d)\nFigure 3. Kernels that can be represented as a product-of-\nsigmoids. Let s(x) = 1\/[1 + exp(\u2212x)]. (a): s(x)s(\u2212x) is\ncompared with the Gaussian .25 exp(\u2212x\n2\/5) (dashed line).\n(b): s(3(x + 5))s(\u22123(x \u2212 5)), the steepness of the kernel\nboundaries can be tuned with the slope of the linear func-\ntion \u03b7ij. (c): The product s(x)s(\u2212x)s(y)s(\u2212y) in 2 dimen-\nsions is very smooth and almost radially symmetric. (d):\ns(2(x+2))s(2(\u2212x\/1.5+y +2))s(2(\u2212x\/1.5\u2212y +2)) is dis-\nplayed. In principle any polygonal kernel shape with any\nboundary steepness can be constructed.\ntially neighbored\u201d\u2014namely whether there exists data\nfor which both models are eligible\u2014based on their cen-\nters. For each pair (ij) of neighbored models, we learn\na sigmoidal function \u03c8ij(x), where \u03c8ij \u2261 1\u2212\u03c8ji. The\nproduct of such sigmoids around a submodel i then\nde\ufb01nes Ps(i|x) as\nPs(i|x) =\n1\nZ0\nY\nj\n\u03c8ij(x) , (5)\n\u03c8ij(x) =\n1\n1 + exp[\u2212\u03b7ij(x)]\n,\nwhere Z0 normalizes over i. As indicated, the sigmoids\n\u03c8ij are de\ufb01ned by a scalar functions \u03b7ij which we will\nassume to be linear (which makes \u03c8 essentially a stan-\ndard perceptron). Fig. 3 illustrates some kernels that\ncan be represented as products of sigmoids.\nThe sigmoids \u03c8ij(x) are meant to represent the likeli-\nhood that a model i rather than j is responsible for an\ninput x, conditioned on the fact that either i or j is\nresponsible. The product combination is comparable\nto an and voting. The MAP index\u02c6 i we inferred in the\nprevious section is now used to train these sigmoids:\nFor every datum that is labeled with a non-zero MAP\nindex \u02c6 i, all sigmoids \u03c8ij adjacent to the ith model are\ntrained to classify this datum as 1 (or, equivalently, the\n\u03c8ji\u2019s are trained to classify this datum as 0). The pa-Learning Discontinuities with Products-of-Sigmoids for Switching between Local Models\nrameters of the sigmoid (basically the weights of the\nperceptron) are trained with a fast gradient descent\nbased adaptation (Rprop, (Igel & H\u00a8 usken, 2003)).\n4.1. Error measures\nIn addition to the family error, we de\ufb01ne the classi\ufb01-\ncation error Ec, which counts how often the product\nof sigmoids correctly predicts \u03c6\u02c6 i to be the best \ufb01tting\nmodel for a given input: Let us de\ufb01ne\ni\u2217 = argmax\ni\nPs(i|x) (6)\nas the maximum likelihood predicted index, as opposed\nto the MAP index \u02c6 i = argmaxiP(i|y,x) given the tar-\nget output y. Then the classi\ufb01cation error gives the\nratio of data for which i\u2217 6=\u02c6 i.\nOur model could be interpreted as a deterministic\nfunction (instead of a probabilistic model) where the\noutput y\u2217 is given by the predicted best model, y\u2217 =\n\u03c6i\u2217(x). The MSE w.r.t. this function could be used\nas an error measure for our model. However, this\nis inconsistent with the probabilistic framework and\nin the case of discontinuous functions, also mislead-\ning. For instance, consider the Heavy-side function\nh(x < 0) = 0 and h(x \u2265 0) = 1. Very close to the\ndiscontinuity our model might predict an output of 0\nor 1, each with a probability of 50%. Thereby, with a\nchance of 50% it produces a squared error of 1, or in\naverage 1\n2. In contrast, a model that predicts an out-\nput of 1\n2 only produces a squared error of 1\n4 although\nit predicts consistently wrong. From the probabilis-\ntic point of view, it is clear that it makes no sense\nto assume a (unimodal) Gaussian output noise model\nat discontinuities. Instead, displaying both, the family\nand classi\ufb01cation error, gives more insight and directly\nre\ufb02ects the likelihood of the model.\n5. Computational issues\nFor every training point, an E-step (eq. (2)) has to\ncalculate responsibilities for each model by computing\nthe product of the locality prior Pl(i|x) and the model\nlikelihood Pi(y|x). Ordinarily, these would have to\nbe evaluated for each model i. However, since the\nlocality prior is non-zero only for a limited number\nof eligible models, we can reduce the computational\ncost by maintaining a graph data structure where each\nnode is a local linear model \u03c6i and each edge represents\na sigmoid \u03c8ij between neighboring models. The graph\nallows us to quickly decide which models are eligible\n(by \ufb01nding the closest node and considering its graph\nneighbors)\u2014the likelihoods Pi(y|x) need be evaluated\nonly for those models. Evaluation of a likelihood is\n(a)\n(b)\nFigure 4. (a) A 1D test function with d=1, l=10, \u03c3=0.1.\nLearned switching model after 20 iterations on 1000 train-\ning data points. (b) The averaged switching model: y(x) = P\ni Ps(i|x)\u03c6i(x) compared to LWPR.\nO(d) for a linear model in d dimensions.\nFor the M-step (for every training point), the statistics\nof each of the eligible models has to be updated using\nincremental PLS weighted with the non-zero respon-\nsibility. In (Vijayakumar & Schaal, 2000) details are\nfound on how to realize incremental PLS in O(d) by\nadapting the number p of projections needed for re-\ngression (as a result, the su\ufb03cient statistics are d \u00d7 p\nmatrices rather than d\u00d7d). For every training datum,\nwe also adapt the sigmoids \u03c8\u02c6 ij adjacent to the MAP\nmodel \u03c6\u02c6 i, which is also O(d) when implemented using\ngradient based adaptation.\n6. Experiments\n6.1. Discontinuous test functions\nFirst we tested our algorithm on random, piecewise lin-\near, discontinuous test functions. A test function has\n3 parameters: the input dimensionality d, the num-\nber l of linear pieces it is composed of, and the output\nnoise \u03c3. They are generated as follows. We draw lLearning Discontinuities with Products-of-Sigmoids for Switching between Local Models\nFigure 5. A 2D test function composed of 10 pieces. The\nwire-frame represents the learned switching model which\ndeviates from the true test function (gray-shaded surface)\nonly at a few corners. The noise level was \u03c3 = .01.\nbasis points \u02c6 xi, i = 1,..,l uniformly from the input\ndomain [\u22121,1]d. To each basis point \u02c6 xi we associate\na random o\ufb00set \u03b20\ni \u223c U([\u22121,1]) and a random vector\n\u03b2i \u223c U([\u22121,1]n), where U is the uniform distribu-\ntion. Thus, each i represents a random linear function\nx 7\u2192 \u03b20\ni +h\u03b2i,x\u2212 \u02c6 xii. For a given input x, the target\noutput is then the output of the ith linear piece with\nminimal distance |x \u2212 \u02c6 xi| plus Gaussian noise with\nstandard deviation \u03c3.\nFig. 4(a) display a 1D test function (dashed) with\n10 linear pieces together with the 1000 training data\npoints sampled from it and the approximation (solid)\nlearned by our algorithm (i.e., the most likely pre-\ndicted output y\u2217). The \ufb01t between the learned model\nand the true test function is almost perfect, except\nof the region around x \u2248 \u2212.4, where the true test\nfunction has as a step which is not recognized by the\nmodel. Given the noise variance of the actual training\ndata though, this is not surprising.\nFig. 4(b) displays the same 1D test function,\nnow compared against a model learned by Locally\nWeighted Projection Regression (LWPR, (Vijayaku-\nmar & Schaal, 2000)) and an \u201caveraged output\u201d of\nour model, y(x) =\nP\ni Ps(i|x)\u03c6i(x). As mentioned\nin the section 4.1, this averaging is probabilistically\nquestionable. Still, displaying this output on the one\nhand gives a concrete idea of the sigmoids, in particu-\nlar their slopes, that are implicitly learned, and on the\nother hand makes it more comparable to the weighted\naveraging usually employed by local regression models.\nFig. 5 displays a 2D test function showing how the\ncomplexity of the kernels (or cells) may increase in\nhigher dimensions.\nIn Fig. 6 we display results for 10 independent trials\non random test functions in 2, 5 and 10 dimensions.\nThe family error (upper graphs) consistently decays\n(a)\n(b)\nFigure 6. The family error (a) and the classi\ufb01cation error\n(b) for 10 runs on random 2D, 5D and 10D test functions\nwith l = 10 and \u03c3 = .1. We used 1000 training point in\nthe 2D case and 10000 in the 5D and 10D. Each iteration\ncorresponds to one cycle through the training data set.\nThe curves are average errors standard deviations for the\n10 runs on independent test data sets.\nto the noise level (\u03c32 = 0.01). (A family error below\nthe noise level is possible because the MAP index \u02c6 i\nis a posterior given the target output.) The classi\ufb01-\ncation is not perfect but still rather robust as shown\nby the variance between trials. The 10D case (with a\nconsiderably large input space [\u22121,1]10) indicates that\neventually this classi\ufb01cation becomes the challenge in\nhigh dimensions.\n6.2. Physical interaction problem\nWe use a realistic physical simulation2 of a movement\nsystem with contact dynamics (cf. Fig. 7(a)) to test our\nalgorithm. The con\ufb01guration has 4 degrees of freedom,\nan angular position and velocity for both levers. As a\ncontrol signal, we applied random Gaussian torques on\nthe cylindrical lever. When the cylindrical lever comes\nin contact and pushes the black lever, contact forces\n2The Open Dynamics Engine (http:\/\/ode.org\/) written\nby Russell Smith et al.Learning Discontinuities with Products-of-Sigmoids for Switching between Local Models\n(a) (b) (c)\nFigure 7. (a) The physical con\ufb01guration we used to generate the data. The 1D control signal speci\ufb01es the torque applied\non the (light coloured) cylindrical lever which rotates friction-less about the axis. An unactuated (black) lever rotates\nabout the same axis with signi\ufb01cant friction. No torques are exerted on the black lever unless the cylinder pushes it. The\nfamily error (b) and the classi\ufb01cation error (c) for the physical interaction problem.\ndiscontinuously change the dynamics of the whole sys-\ntem.\nThe problem is to learn the inverse dynamic model\nwhich maps from the current state (positions and ve-\nlocities of both levers) and the desired acceleration (of\nthe cylindrical lever) to the motor signal that is needed\nto realize this acceleration. 10000 data points were\ncollected for training. Eventually, the model instanti-\nated three submodels to learn the data. The family er-\nror given in Fig. 7 corresponds to the noise-level of the\nphysical simulation; the classi\ufb01cation error shows that\nonly very few of the independent 10000 test points\nwere wrongly associated. More insight is gained by\ninvestigating how the sigmoidal classi\ufb01ers learned to\nsplit up the data among the local models. In Fig. 8,\nthe full training data set is displayed, as well as the\nsplit up. We \ufb01nd that the data is separated in three\nparts corresponding to the three submodels. The sim-\nplest part is the contact-free data (with non-extremal\nrelative angle, Fig. 8(b)) which is learned as a linear\nrelationship between desired acceleration and motor\nsignal. The two other parts correspond to the contact\nsituations (from the left and from the right). Here, the\ntarget motor signal additionally depends on the cur-\nrent velocities (due to the black lever\u2019s friction) and\ntwo separate models are learned for the cases of left\nand right contact.\n7. Discussion\nThe presented model addresses the problem of hand-\nling the discontinuities that naturally arise, for ex-\nample, in sensorimotor data during interaction with\na structured environment. Our model extends ear-\nlier local incremental learning approaches in several\nways: The responsibility region associated with each\nlocal model (learned with the product-of-sigmoids) has\na much more versatile boundary shape compared to\ntypical Gaussian kernels. Problems associated with\ninitialization of kernel shapes or widths and the heuris-\ntic choice of an ad hoc number of submodels are cir-\ncumvented by the robust incremental allocation of new\nmodels.\nIn incremental learning problems, locality is the key\nto reduce interference since one local model can be\nadapted to new local data without destroying remote\nmodels. The sigmoid-of-products representation of\nP(i|x) follows this principle: A new datum leads to\nan adaptation of only the local classi\ufb01ers around \u02c6 i and\nleaves remote classi\ufb01ers una\ufb00ected. This contrasts to\npreviously investigated global approaches (Bemporad\net al., 2003; Quinlan, 1992) to learn a deterministic\npartitioning of the input space.\nLearning a family of models was realized on a separate\nlevel, decoupled from the learning of the responsibil-\nity prediction which is built on top. The \ufb01xed locality\nprior Pl(i|x) that we introduced at the level of fam-\nily learning has a decoupling e\ufb00ect which contrasts\nto standard mixture models: Since the prior is zero\nfor non-local data, non-neighboring submodels do not\ncompete for modeling this data. For instance, when a\nnew submodel is added to the family, it only interferes\nwith its direct neighbors (which may have to readapt)\nbut does not interfere with remote submodels. This is\nthe key ingredient for incremental learning with local\nmodels.\nFinally, we assumed the local models to be linear. If\none can specify an appropriate set of basis functions\n(like low order polynomials) for the given problem,\nthen it is straight-forward to extend the approach to\nlocal non-linear models following Generalized Additive\nModels (Hastie & Tibshirani, 1990). Generally, ex-\nisting approaches addressed the problem of blending\n(with associated kernels) local linear models in order to\nrepresent smooth non-linear function. Here we focused\non how to discontinuously switch between linear mod-\nels. A future version of our model will have to combineLearning Discontinuities with Products-of-Sigmoids for Switching between Local Models\n(a) (b)\nFigure 8. (a) The (normalized) training data collected from the physical simulation projected on the desired acceleration\n(which is an input), the relative angle between the levers (which is not an input but used here for better visibility), and\nthe motor signal (which is the target output of the inverse model). When the relative angle is extremal (around \u00b11 in\nthe given scale) the two levers have contact (from the left or from the right). (b) The separation of the test data set\nas learned by the products-of-sigmoids. The model learned to associate the test data points to the three di\ufb00erent local\nmodels \u03c6i. More precisely, the test data is split up according to the maximum likelihood predicted index i\n\u2217 (cf. eq. (6)),\nwhich is computed from the product of the learned sigmoids (cf. eq. (5)). The data labeled with \u201cmodel B\u201d corresponds\nto the contact free dynamics and is a linear dependency between desired acceleration and motor signal. The data assigned\nto models A and C correspond to contact from the left or the right, respectively.\nboth, blending and switching, in order to represent dis-\ncontinuous and piece-wise non-linear functions.\nAcknowledgments\nWe would like to thank the anonymous reviewer for\ntheir helpful comments. The \ufb01rst author is grateful\nto the German Research Foundation (DFG) for the\nEmmy Noether fellowship TO 409\/1-1.\nReferences\nBemporad, A., Garulli, A., Paoletti, S., & Vicino, A.\n(2003). A greedy approach to identi\ufb01cation of piece-\nwise a\ufb03ne models. Hybrid Systems: Computation\nand Control; LNCS (pp. 97\u2013112). Springer.\nde Jong, S. (1993). SIMPLS: An alternative approach\nto partial least squares regression. Chemometrics\nand Intelligent Laboratory Systems, 18, 251\u2013263.\nFischler, M. A., & Bolles, R. C. (1981). Random sam-\nple consensus: A paradigm for model \ufb01tting with\napplications to image analysis and automated car-\ntography. Comm. of the ACM, 24, 381\u2013395.\nGhahramani, Z., & Hinton, G. (1998). Variational\nlearning for switching state-space models. Neural\nComputation, 12, 963\u2013996.\nHastie, T., & Tibshirani, R. (1990). Generalized addi-\ntive models. Chapman and Hall, New York.\nIgel, C., & H\u00a8 usken, M. (2003). Empirical evaluation\nof the improved Rprop learning algorithm. Neuro-\ncomputing, 50(C),, 105\u2013123.\nJordan, M. I., & Jacobs, R. A. (1994). Hierarchical\nmixtures of experts and the EM algorithm. Neural\nComputation, 6, 181\u2013214.\nKavka, C., & Schoenauer, M. (2004). Evolution of\nvoronoi-based fuzzy controllers. 8th International\nConference on Parallel Problem Solving from Nature\n(PPSN VIII), Birmingham, UK; LNCS. Springer.\nPavlovic, V., Rehg, J. M., & MacCormick, J. (2000).\nLearning switching linear models of human motion.\nNIPS (pp. 981\u2013987).\nQuinlan, J. R. (1992). Learning with Continuous\nClasses. 5th Australian Joint Conference on Arti-\n\ufb01cial Intelligence (pp. 343\u2013348).\nSchaal, S., & Atkeson, C. (1998). Constructive incre-\nmental learning from only local information. Neural\nComputation, 10, 2047\u20132084.\nVijayakumar, S., & Schaal, S. (2000). Locally weighted\nprojection regression: An o(n) algorithm for incre-\nmental real time learning in high dimensional space.\nProc. Int. Conf. on Machine Learning (ICML) (pp.\n1079\u20131086).\nWolpert, D., & Kawato, M. (1998). Multiple paired\nforward and inverse models for motor control. Neu-\nral Networks, 11, 1317\u20131329."}