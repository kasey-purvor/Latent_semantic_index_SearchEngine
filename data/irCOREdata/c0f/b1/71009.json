{"doi":"10.1109\/TSP.2005.847844","coreId":"71009","oai":"oai:eprints.lancs.ac.uk:8195","identifiers":["oai:eprints.lancs.ac.uk:8195","10.1109\/TSP.2005.847844"],"title":"Exact Bayesian curve fitting and signal segmentation.","authors":["Fearnhead, Paul"],"enrichments":{"references":[{"id":16371866,"title":"Adaptive Filtering and Change Detection.","authors":[],"date":"2000","doi":"10.1002\/0470841613","raw":"F. Gustafsson, Adaptive Filtering and Change Detection. New York: Wiley, 2000.","cites":null},{"id":16371875,"title":"Adaptive sequential segmentation of piecewise stationary time series,\u201d","authors":[],"date":"1983","doi":"10.1016\/0020-0255(83)90008-7","raw":"U. Appel and A. V. Brandt, \u201cAdaptive sequential segmentation of piecewise stationary time series,\u201d Inf. Sci., vol. 29, pp. 27\u201356, 1983. Paul Fearnhead received the B.A. and D.Phil. degrees in mathematics from the University of Oxford, Oxford, U.K. Since 2001, he has been a Lecturer in statistics at Lancaster University, Lancaster, U.K. Prior to this, he was a research associate with the Mathematical Genetics group, University of Oxford. His research interests include computational statistical methods and, in particular, particle \ufb01lters and the use of \ufb01lters within Markov chain Monte Carlo methods. He is also interested in modeling and inference in population genetics. Authorized licensed use limited to: IEEE Xplore. Downloaded on February 19, 2009 at 08:06 from IEEE Xplore.  Restrictions apply.","cites":null},{"id":16371824,"title":"An introduction to hidden Markov models,\u201d","authors":[],"date":"1986","doi":"10.1109\/massp.1986.1165342","raw":"L. R. Rabiner and B. H. Juang, \u201cAn introduction to hidden Markov models,\u201d IEEE Acoust., Speech, Signal Process. Mag., vol. ASSP-34, no. 1, pp. 4\u201315, Jan. 1986.","cites":null},{"id":16371864,"title":"Automatic Bayesian curve \ufb01tting,\u201d","authors":[],"date":"1998","doi":"10.1111\/1467-9868.00128","raw":"D. G. T. Denison, B. K. Mallick, and A. F. M. Smith, \u201cAutomatic Bayesian curve \ufb01tting,\u201d J. R. Statist. Soc., ser. B, vol. 60, pp. 333\u2013350, 1998.","cites":null},{"id":16371830,"title":"Bayesian analysis for change point problems,\u201d","authors":[],"date":"1993","doi":"10.2307\/2290726","raw":", \u201cA Bayesian analysis for change point problems,\u201d J. Amer. Statist. Soc., vol. 88, pp. 309\u2013319, 1993.","cites":null},{"id":16371816,"title":"Bayesian curve \ufb01tting using MCMC with applications to signal segmentation,\u201d","authors":[],"date":"2002","doi":"10.1109\/78.984776","raw":"E. Punskaya, C. Andrieu, A. Doucet, and W. J. Fitzgerald, \u201cBayesian curve \ufb01tting using MCMC with applications to signal segmentation,\u201d IEEE Trans. Signal Process., vol. 50, no. 3, pp. 747\u2013758, Mar. 2002. Authorized licensed use limited to: IEEE Xplore. Downloaded on February 19, 2009 at 08:06 from IEEE Xplore.  Restrictions apply.2166 IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 53, NO. 6, JUNE 2005","cites":null},{"id":16371839,"title":"Bayesian inference for partially observed stochastic epidemics,\u201d","authors":[],"date":"1999","doi":"10.1111\/1467-985x.00125","raw":"P. D. O\u2019Neill and G. O. Roberts, \u201cBayesian inference for partially observed stochastic epidemics,\u201d J. R. Statist. Soc., ser. A, vol. 162, pp. 121\u2013129, 1999.","cites":null},{"id":16371848,"title":"Bayesian Theory.","authors":[],"date":"1994","doi":"10.1002\/9780470316870","raw":"J. M. Bernardo and A. F. M. Smith, Bayesian Theory. Chichester, U.K.: Wiley, 1994.","cites":null},{"id":16371872,"title":"Design and comparative study of some sequential jump detection algorithms for digital signals,\u201d","authors":[],"date":"1983","doi":"10.1109\/tassp.1983.1164131","raw":"M. Basseville and A. Benveniste, \u201cDesign and comparative study of some sequential jump detection algorithms for digital signals,\u201d IEEE Trans. Acoust., Speech, Signal Process., vol. ASSP-31, no. 2, pp. 521\u2013535, Apr. 1983.","cites":null},{"id":16371869,"title":"Detection of Abrupt Changes: Theory and Applications.","authors":[],"date":"1993","doi":"10.2307\/1269388","raw":"M. Basseville and I. V. Nikiforov, Detection of Abrupt Changes: Theory and Applications. Englewood Cliffs, NJ: Prentice-Hall, 1993.","cites":null},{"id":16371857,"title":"Error bounds for convolutional codes and an","authors":[],"date":"1967","doi":"10.1109\/tit.1967.1054010","raw":"A. J. Viterbi, \u201cError bounds for convolutional codes and an asymptoticallyoptimumdecodingalgorithm,\u201dIEEETrans.Inf.Theory,vol.IT-13, no. 2, pp. 260\u2013269, Apr. 1967.","cites":null},{"id":16371822,"title":"Exact and Ef\ufb01cient Inference for Multiple Changepoint Problems.","authors":[],"date":"2004","doi":"10.1007\/s11222-006-8450-8","raw":"P. Fearnhead. (2004) Exact and Ef\ufb01cient Inference for Multiple Changepoint Problems. [Online]. Available: http:\/\/www.maths.lancs.ac.uk\/~fearnhea\/","cites":null},{"id":16371842,"title":"Exact \ufb01ltering for partially-observed continuous-time Markov models,\u201d","authors":[],"date":"2004","doi":"10.1111\/j.1467-9868.2004.05561.x","raw":"P. Fearnhead and L. Meligkotsidou, \u201cExact \ufb01ltering for partially-observed continuous-time Markov models,\u201d J. R. Statist. Soc., ser. B, vol. 66, pp. 771\u2013789, 2004.","cites":null},{"id":16371833,"title":"Exact sampling with coupled Markov chains and applications to statistical mechanics,\u201d in Random Structures Algorithms,","authors":[],"date":"1996","doi":"10.1002\/(sici)1098-2418(199608\/09)9:1\/2<223::aid-rsa14>3.0.co;2-o","raw":"J. G. Propp and D. B. Wilson, \u201cExact sampling with coupled Markov chains and applications to statistical mechanics,\u201d in Random Structures Algorithms, 1996, vol. 9, pp. 223\u2013252.","cites":null},{"id":16371854,"title":"Hierarchical Bayesian analysis of changepoint problems,\u201d","authors":[],"date":"1992","doi":"10.2307\/2347570","raw":"B. P. Carlin, A. E. Gelfand, and A. F. M. Smith, \u201cHierarchical Bayesian analysis of changepoint problems,\u201d Appl. Statist., vol. 41, pp. 389\u2013405, 1992.","cites":null},{"id":16371862,"title":"Ideal spatial adaptation by wavelet shrinkage,\u201d","authors":[],"date":"1994","doi":"10.1093\/biomet\/81.3.425","raw":"D. L. Donoho and I. M. Johnstone, \u201cIdeal spatial adaptation by wavelet shrinkage,\u201d Biometrika, vol. 81, pp. 425\u2013455, 1994.","cites":null},{"id":16371851,"title":"On Bayesian analysis of mixtures with an unknown number of components,\u201d","authors":[],"date":"1997","doi":"10.1111\/1467-9868.00146","raw":"S. Richardson and P. J. Green, \u201cOn Bayesian analysis of mixtures with an unknown number of components,\u201d J. R. Statist. Soc., ser. B, vol. 59, pp. 731\u2013792, 1997.","cites":null},{"id":16371836,"title":"Perfect sampling for the wavelet reconstruction of signals,\u201d","authors":[],"date":"2002","doi":"10.1109\/78.978388","raw":"C. C. Holmes and D. G. T. Denison, \u201cPerfect sampling for the wavelet reconstruction of signals,\u201d IEEE Trans. Signal Process., vol. 50, no. 2, pp. 337\u2013344, Feb. 2002.","cites":null},{"id":16371827,"title":"Product partition models for change point problems,\u201d","authors":[],"date":"1992","doi":"10.1214\/aos\/1176348521","raw":"D. Barry and J. A. Hartigan, \u201cProduct partition models for change point problems,\u201d Ann. Statist., vol. 20, pp. 260\u2013279, 1992.","cites":null},{"id":16371819,"title":"ReversiblejumpMarkovchainMonteCarlo computation and Bayesianmodeldetermination,\u201dBiometrika,vol.82,pp.711\u2013732,1995.","authors":[],"date":null,"doi":null,"raw":"P.Green, \u201cReversiblejumpMarkovchainMonteCarlo computation and Bayesianmodeldetermination,\u201dBiometrika,vol.82,pp.711\u2013732,1995.","cites":null},{"id":16371845,"title":"Trans-dimensional Markov chain Monte Carlo,\u201d in Highly Structured Stochastic Systems,","authors":[],"date":"2003","doi":null,"raw":"P. J. Green, \u201cTrans-dimensional Markov chain Monte Carlo,\u201d in Highly Structured Stochastic Systems, P. J. Green, N. L. Hjort, and S. Richardson, Eds. Oxford, U.K.: Oxford Univ. Press, 2003.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2005-06","abstract":"We consider regression models where the underlying functional relationship between the response and the explanatory variable is modeled as independent linear regressions on disjoint segments. We present an algorithm for perfect simulation from the posterior distribution of such a model, even allowing for an unknown number of segments and an unknown model order for the linear regressions within each segment. The algorithm is simple, can scale well to large data sets, and avoids the problem of diagnosing convergence that is present with Monte Carlo Markov Chain (MCMC) approaches to this problem. We demonstrate our algorithm on standard denoising problems, on a piecewise constant AR model, and on a speech segmentation problem","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/71009.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/8195\/1\/getPDF.pdf","pdfHashValue":"167f38f21395c6b7912ae9c4efb9cbfef5bce8a9","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:8195<\/identifier><datestamp>\n      2018-01-24T03:16:59Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Exact Bayesian curve fitting and signal segmentation.<\/dc:title><dc:creator>\n        Fearnhead, Paul<\/dc:creator><dc:subject>\n        QA Mathematics<\/dc:subject><dc:description>\n        We consider regression models where the underlying functional relationship between the response and the explanatory variable is modeled as independent linear regressions on disjoint segments. We present an algorithm for perfect simulation from the posterior distribution of such a model, even allowing for an unknown number of segments and an unknown model order for the linear regressions within each segment. The algorithm is simple, can scale well to large data sets, and avoids the problem of diagnosing convergence that is present with Monte Carlo Markov Chain (MCMC) approaches to this problem. We demonstrate our algorithm on standard denoising problems, on a piecewise constant AR model, and on a speech segmentation problem.<\/dc:description><dc:date>\n        2005-06<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/8195\/1\/getPDF.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/TSP.2005.847844<\/dc:relation><dc:identifier>\n        Fearnhead, Paul (2005) Exact Bayesian curve fitting and signal segmentation. IEEE Transactions on Signal Processing, 53 (6). pp. 2160-2166. ISSN 1053-587X<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/8195\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1109\/TSP.2005.847844","http:\/\/eprints.lancs.ac.uk\/8195\/"],"year":2005,"topics":["QA Mathematics"],"subject":["Journal Article","PeerReviewed"],"fullText":"2160 IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 53, NO. 6, JUNE 2005\nExact Bayesian Curve Fitting\nand Signal Segmentation\nPaul Fearnhead\nAbstract\u2014We consider regression models where the underlying\nfunctional relationship between the response and the explanatory\nvariable is modeled as independent linear regressions on disjoint\nsegments. We present an algorithm for perfect simulation from the\nposterior distribution of such a model, even allowing for an un-\nknown number of segments and an unknown model order for the\nlinear regressions within each segment. The algorithm is simple,\ncan scale well to large data sets, and avoids the problem of di-\nagnosing convergence that is present with Monte Carlo Markov\nChain (MCMC) approaches to this problem. We demonstrate our\nalgorithm on standard denoising problems, on a piecewise constant\nAR model, and on a speech segmentation problem.\nIndex Terms\u2014Changepoints, denoising, forward-backward al-\ngorithm, linear regression, model uncertainty, perfect simulation.\nI. INTRODUCTION\nA. Overview\nREGRESSION problems are common in signal processing.The aim is to estimate, from noisy measurements, a func-\ntional relationship between a response and a set of explanatory\nvariables. We consider the approach of [1], who model this func-\ntional relationship as a sequence of linear regression models on\ndisjoint segments. Both the number and position of the segments\nand the order and parameters of the linear regression models are\nto be estimated. A Bayesian approach to this inference is taken.\nIn [1], Bayesian inference is performed via the reversible\njump MCMC methodology of [2]. We consider applying re-\ncently developed perfect simulation ideas [3] to this problem.\nThese ideas are closely related to the Forward\u2013Backward algo-\nrithm [4] and methods for product partition models [5], [6]. To\ndefine segments, we need to assume that the response can be or-\ndered linearly through \u201ctime.\u201d (Whereas time may be artifical,\nin estimating a polynomial relationship between a response and\nan explanatory variable, time can be defined so that the order\nthat responses are observed is in increasing value of the explana-\ntory variable.) The perfect simulation algorithm requires inde-\npendence between segments and utilizes the Markov property\nof changepoint models in such cases. It involves a recursion for\nthe probability of the data from time onwards, conditional on\na changepoint immediately before time , given similar quan-\ntities for all times after . Once these probabilities have been\ncalculated for all , simulating from the posterior distribution of\nthe number and position of the changepoints is straight forward.\nManuscript received August 13, 2003; revised July 27, 2004. The associate\neditor coordinating the review of this manuscript and approving it for publica-\ntion was Zixiang Xiong.\nThe author is with the Department of Mathematics and Statistics, Lancaster\nUniversity, Lancaster, LA1 4YF U.K. (e-mail: p.fearnhead@lancaster.ac.uk).\nDigital Object Identifier 10.1109\/TSP.2005.847844\nThis approach to perfect simulation is different from the more\ncommon approach based on coupling from the past [7], which\nhas been used, for example, on the related problem of recon-\nstructing signals via wavelets [8].\nWe develop the existing methodology by allowing for model\nuncertainty within each segment. We also implement a Viterbi\nversion of the algorithm to perform maximum a posteriori\n(MAP) estimation. The advantages of our approach over\nMCMC are that we have the following.\ni) The perfect simulation algorithm draws independent\nsamples from the true posterior distribution and avoids\nthe problems of diagnosing convergence that occur\nwith MCMC.\nii) The recursions of the algorithm are generic and simpler\nthan designing efficient MCMC moves.\niii) The computational cost of the algorithm can scale lin-\nearly with the length of data analyzed, thus making it\napplicable for large data sets.\nThe first advantage is particularly important. There are a\nnumber of examples of published results from MCMC analyses\nthat have later proven to be inaccurate because the MCMC\nalgorithm had not been run long enough (for example, compare\nthe results of [9] and [10] or those of [2] with those of [11]).\nOur approach avoids this problem by enabling iid draws from\nthe true posterior (the goal of Bayesian inference). Thus, it can\nbe viewed as enabling \u201cexact Bayesian\u201d inference for these\nproblems.\nIn Bayesian inference, the posterior distribution depends on\nthe choice of prior distribution. When inference is made con-\nditional on a specific model, uninformative priors can then be\nchosen so that the posterior reflects the information in the data.\nThe regression problem we address involves model choice, and\nfor such problems, uninformative priors do not exist, as the\nchoice of prior affects the Bayes factor between different com-\npeting models. The use of uninformative priors for the param-\neters can severely penalize models with larger numbers of pa-\nrameters (see [12] for more details).\nOne approach to choosing priors for inference problems that\ninclude model uncertainty is to let the data inform the choice of\nprior [1], [13], for example, by using a hierarchical model with\nhyperpriors on the prior parameters [14]. However, the inclusion\nof hyperpriors on the regression parameters violates the inde-\npendence assumption required for perfect simulation. We sug-\ngest two possible approaches for choosing the prior parameters.\nThe simpler is based on a recursive use of the perfect simulation\nalgorithm, with the output of a preliminary run of the algorithm\nbeing used to choose the prior parameters. Alternatively, if hy-\nperpriors are used, the perfect simulation algorithm can then be\n1053-587X\/$20.00 \u00a9 2005 IEEE\nAuthorized licensed use limited to: IEEE Xplore. Downloaded on February 19, 2009 at 08:06 from IEEE Xplore.  Restrictions apply.\nFEARNHEAD: EXACT BAYESIAN CURVE FITTING AND SIGNAL SEGMENTATION 2161\nincorporated within a simple MCMC algorithm, which mixes\nover the hyperparameters.\nThe outline of the paper is as follows. In Section II we de-\nscribe our modeling assumptions together with the methodology\nfor exact simulation and MAP estimation; we also describe how\nto use the exact simulation algorithm within MCMC for the\ncase of Bayesian inference with hyperpriors. In Section III, we\ndemonstrate our method on standard denoising problems and on\nspeech segmentation.\nII. MODEL AND METHOD\nA. Model\nOur model is based on that of [1]. We assume we have ob-\nservations . Throughout, we use the nota-\ntion to denote , which is the th to th entries of\nthe vector . Given segments, defined by the ordered change-\npoints , with and , we model the\nobservations , which are associated with the th seg-\nment by a linear regression of order . Denote the parame-\nters by the vector , and the matrix of basis functions by .\nThen, we have\nwhere is a vector of independent and identically dis-\ntributed (iid) Gaussian random variables with mean 0 and vari-\nance . For examples, see the polynomial regression model of\nSection III-A or the auto regression model of Section III-B.\nThe number and positions of the changepoints and the order,\nparameters, and variance of the regression model for each seg-\nment are all assumed to be unknown. We introduce conjugate\npriors for each of these.\nThe prior on the changepoints is given by\nfor and for\n, for some probability . For the th regression parameter of\nthe th segment , we have a normal prior with mean 0 and\nvariance , independent of all other regression parameters.\nWe assume an Inverse-Gamma prior for the noise variances\nwith parameters and . The priors are independent for\ndifferent segments. Finally, we constrain and introduce\nan arbitrary discrete prior , again assuming independence\nbetween segments.\nWe now describe how perfect simulation can be performed\nfor this model. We then discuss how the data can be used to\nchoose the prior parameters of the model .\nB. Perfect Simulation\n1) Recursions: Define for\nchangepoint at\nand . The model of Section II-A has a Markov\nproperty that enables to be calculated in terms of , for\n, by averaging over the position of the next changepoint\nafter .\nConsider a segment with observations for and a\nlinear regression model order . Let be the\nmatrix of basis vectors for the th-order linear regression model\non this segment. Let Diag be the prior vari-\nance on the regression parameters for this segment, and let be\nthe identity matrix. Define\nand\nFinally, define\nis a segment, model order\n(1)\nwhere (1) is obtained by integrating out the regression parame-\nters and variance.\nThen, for\n(2)\nThe intuition behind this recursion is that (suppressing the con-\nditioning on a changepoint at for notational convenience)\nnext changepoint at\nno further changepoints\nThe respective joint probabilities are given by the two sets of\nsums over the model order that appears on the right-hand side\nof (2). See [3] for a formal proof of this recursion.\n2) Simulation: Once the s have been calculated for\n, it is straightforward to recursively simulate the\nchangepoints and linear regression orders. To simulate the\nchangepoints, we set , and then recursively simulate\n, given for until for some value\nof . The conditional posterior distribution of , given\n, is\nfor , and\nAuthorized licensed use limited to: IEEE Xplore. Downloaded on February 19, 2009 at 08:06 from IEEE Xplore.  Restrictions apply.\n2162 IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 53, NO. 6, JUNE 2005\nFor the th segment, the posterior distribution of the model order\nis given by\n3) Viterbi Algorithm: A Viterbi algorithm [15] can be used\nto calculate the MAP estimate of the changepoint positions and\nmodel orders. Define\nchangepoint at MAP estimate for\nif and 0 otherwise, and .\nThen\nwhere the maximum is taken over , and\n. Define and to be the values of\nand , which achieve the maximum. Then, the MAP estimate of\nthe changepoints and the model orders\ncan be obtained recursively by the following:\na) Set and .\nb) While : i) set ; ii) set\n; iii) set , and go to (b).\nThe MAP estimate produced by this algorithm may have a\ndifferent number of segments than the MAP estimate of the\nnumber of segments (see Section III-B). An alternative approach\nto MAP estimation is to fix the number of segments to the MAP\nestimate , say, and calculate the MAP estimate of the position\nof the changepoints and the model orders conditional on seg-\nments.\nA simple adaptation of the above algorithm can perform such\na conditional MAP estimation of the changepoints and model\norders. Define\nchangepoint at\nsegments after MAP estimate for\nThen\nand for\nwhere the maximum is taken over , and\n. Define and as the values of\nand that achieve the maximum (with ). Then, the\nMAP estimate of the changepoints and model orders can be\nobtained by the following.\na) Set , , and .\nb) While , i) set , ii) set\n, iii) set , , and go to (b).\n4) Implementation: As written, (2) suffers from numerical\ninstabilities. This can be overcome by calculating re-\ncursively, using\nand\nEvaluating can be achieved in computational\ntime, which is , as the matrix multiplications involved in\ncalculating the s can be done recursively. For example,\nthe term required for can be calculated from the\nequivalent term required for . This is also true for\nthe and terms required in .\nWhen calculating the values, we store the values for\nthat we have calculated. These stored values can then\nbe used in the simulation stage of the algorithm. An efficient\nalgorithm for simulating large samples from the posterior dis-\ntribution once the values have been calculated is given in\n[3]. The main computational cost of the perfect simulation is that\nof evaluating the recursions to calculate the s; once calcu-\nlated, simulating samples are computationally very cheap.\nThe computational complexity of the recursion for the s\nis . However, computational savings can be made in gen-\neral as the terms in (2) tend to become negligible for sufficiently\nlarge . We suggest truncating the sum in (2) at term when\n(3)\nbecomes smaller than some predetermined value, for example,\n.\nIn a limiting regime, where data is observed over longer\ntime periods (as opposed to observations being made more fre-\nquently) such that as increases, the number of changepoints\nincreases linearly with , this simplification is likely to make\nthe algorithm\u2019s complexity . See Section III for empirical\nevidence of this.\nC. Hyperpriors and MCMC\nWe have described an algorithm for perfect simulation from\nthe model of Section II-A. While this algorithm produces iid\ndraws from the true posterior distribution of the model, the use-\nfulness of the approach and the model will depend on the choice\nof prior parameters\nThe choice of these parameters defines the Bayes factors for the\ncompeting models and, hence, the posterior distribution from\nwhich it is sampled.\nThe approach of [13], which is also used by [1], lets the data\nchoose the prior parameters. In these two papers, this is achieved\nby introducing uninformative hyperpriors on the prior param-\neters. Unfortunately, using hyperpriors introduces dependence\nAuthorized licensed use limited to: IEEE Xplore. Downloaded on February 19, 2009 at 08:06 from IEEE Xplore.  Restrictions apply.\nFEARNHEAD: EXACT BAYESIAN CURVE FITTING AND SIGNAL SEGMENTATION 2163\nbetween the segments, such that the approach of Section II-B is\nno longer directly applicable.\nOne solution is to use the results from a preliminary analysis\nof the data to choose the prior parameters. Thus, we implement\nthe perfect simulation algorithm using a default choice of the\nprior parameters . New values of can be chosen based on the\nperfect samples of the number of changepoints as well as the\nregression variance and parameters. For example, the values\ncould be chosen so that the prior means of the regression vari-\nance, parameters, and number of changepoints are close to the\nposterior means from the preliminary analysis. We denote such\nestimated values of the prior parameters by . If necessary, this\napproach could be iterated a number of times until there is little\nchange in the posterior means. We call this a recursive approach.\nA less ad hoc approach, which mimics that of [1], is to use a\nhyperprior for . A simple MCMC algorithm in this case is as\nfollows.\na) Update the number of segments , the changepoints,\nand model orders conditional on .\nb) Update the s and s for conditional\non , the changepoints, the model orders, and .\nc) Update conditional on , the changepoints, the\nmodel orders, and, for , the s and s.\nIf conjugate hyperpriors are used (for example, those of [1] or\nof Section III), then Gibbs updates can be used in steps b) and\nc). The perfect simulation algorithm can be used in step a) to\nsimulate the changepoints and model orders from the full condi-\ntional, given . However, we advocate a more efficient (in terms\nof computing time) approach, which is to use an independence\nproposal from the posterior distribution conditional on the prior\nparameters being .\nWe test and compare the accuracy and efficiency of both the\nrecursive and MCMC approaches on a number of examples in\nSection III.\nIII. EXAMPLES\nA. Polynomial Regression\nFor our first class of examples, we assume that for each seg-\nment, there is a polynomial relationship between the response\nand the explanatory variable. Here, we assume that the response\nis either constant, linear, or quadratic. For a segment consisting\nof observations and explanatory variables , the\nmatrix of basis vectors for the quadratic relationship is\ntaken to be\n.\n.\n.\n.\n.\n.\n.\n.\n.\nwhere\nand for , 2, 3\nFig. 1. (Top) Blocks function and observations and (bottom) estimates\n(posterior means) based on the recursive and MCMC approaches. The two\nestimates are almost exact and indistinguishable on the plot. The average\nsquare error of the two estimates are 0.0045 and 0.0043, respectively.\nFig. 2. (Top) Heavisine function and observations and estimates (posterior\nmeans) based on the (bottom) recursive and MCMC approaches. The two\nestimates are almost exact and indistinguishable on the plot. The average\nsquare error of the two estimates are 0.0266 and 0.0264, respectively.\nwhich is the mean value of the th power of the explanatory\nvariables .\nThe reason for this choice of model is that the basis vectors\nare orthogonal, and for a given segment, the regression parame-\nters are independent under the posterior distribution. This helps\nwith the interpretability of the parameter estimates and slightly\nreduces the computational cost of perfect simulation. The first-\nand second-order models are obtained by taking the first and\nfirst two columns of , respectively.\nWe tested our algorithm on the four test data sets of [16].\nThese have been previously analyzed by [17] and [1], among\nothers. Each data set consists of 2048 equally spaced observa-\ntions of an underlying functional relationship (for example, see\nFigs. 1 and 2). The noise variance was 1.0 throughout, which\ngives a signal-to-noise ratio of 7 in each case. In our simulation\nstudy, we focused primarily on the computational aspects of our\napproach. The accuracy of inference from a related model to the\none we use for these test data sets is given in [1].\nWe set as in [1]. Initial parameter values were\n, , and . We first tried the re-\ncursive approach, with two preliminary runs being used to ob-\ntain an estimate of the prior parameter values . Second, we im-\nplemented the MCMC approach, assuming an Inverse-Gamma\nprior on the s, a uniform prior on , and an improper Jeffreys\u2019\nprior on . In implementing step a) of the MCMC algorithm of\nAuthorized licensed use limited to: IEEE Xplore. Downloaded on February 19, 2009 at 08:06 from IEEE Xplore.  Restrictions apply.\n2164 IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 53, NO. 6, JUNE 2005\nTABLE I\nRESULTS OF ANALYSIS OF THE FOUR TEST DATA SETS\nSection II-C, we used proposals from the posterior distribution\nconditional on .\nIn calculating the s for each data set, we truncated the\nsum in (2) when (3) was less than . By varying this cutoff,\nwe could tell that any inaccuracies introduced were negligible.\nFor each data set, evaluating the recursions to calculate the\ns took less than 10 sec on a 900-MHz Pentium PC.\nSummaries of the computational aspects of the perfect simu-\nlation and the MCMC algorithms are given in Table I. These in-\nclude the average number of terms calculated in the sum of (2)\n; the autocorrelation time of the MCMC algorithm , and\nthe acceptance probability of the independence sampler. The\nautocorrelation time was calculated as the maximum estimated\ntime for all the prior parameters.\nThe average number of terms calculated for the sums in (2)\nwas much less in all cases than the roughly 1000 that would\noccur if no truncation of the sum was used. The average number\nof terms depends primarily on the average length of the seg-\nments for realizations that have non-negligible posterior proba-\nbility. For example, the Heavisine function had fewest segments\n(as few as 5) and, hence, the most terms, whereas, for example,\nBumps had many more segments (at least 35) and, thus, fewer\nterms.\nThe MCMC algorithm mixed extremely well in all cases. The\nacceptance probabilities of the independence sampler in part a)\nof the algorithm were high (close to 1 for Blocks and Heavisine).\nThe autocorrelation times were also low because they were close\nto 1 for Blocks and Heavisine, which suggests near iid samples\nfrom the target posterior distribution.\nFor each dataset, the estimates based on the recursive ap-\nproach and those based on the MCMC approach were almost\nidentical. For example, the two estimates for the Blocks and the\nHeavisine data sets are shown in Figs. 1 and 2. The average mean\nsquare errors of the estimates were also almost identical in all\ncases. As can be seen from these figures, the reconstruction of\nthe Blocks and Heavisine functions are very good.\nB. Auto Regressive Processes\nOur second example is based on an analyzing data from\na piecewise constant AR process. Such models are used for\nspeech data [18]. We considered models of order up to 3. For\na segment consisting of observation , the matrix of basis\nvectors for the third-order model is\n.\n.\n.\n.\n.\n.\n.\n.\n.\nFig. 3. (Top) Simulated AR process and (bottom) the posterior distribution of\nthe position of the changepoints. The true changepoint positions are denoted by\ndashed lines.\nTABLE II\nMAP ESTIMATES OF CHANGEPOINTS AND MODEL ORDER\nFig. 4. (Top) Speech data with conditional MAP estimates of the changepoints\ngiven by vertical dashed lines and (bottom) posterior distribution of the\nchangepoint positions.\nThe matrices for the first- and second-order models consist of\nthe first and first two columns of , respectively.\nWe simulated 2000 data points from a model with nine break-\npoints. The data is shown in Fig. 3. We only used the recursive\napproach to analyze this data, as the MCMC approach had pro-\nduced a negligible difference for the polynomial regression ex-\namples.\nAs above, we implemented the recursions for the s by\ntruncating the sums when (3) was less than . On average,\n250 terms were required in the summation. For simplicity, we\nsummarize our results in terms of the MAP estimate of the\nchangepoint positions and the model orders (see Table II) and\nthe posterior distribution of the changepoint positions (see\nFig. 3).\nAuthorized licensed use limited to: IEEE Xplore. Downloaded on February 19, 2009 at 08:06 from IEEE Xplore.  Restrictions apply.\nFEARNHEAD: EXACT BAYESIAN CURVE FITTING AND SIGNAL SEGMENTATION 2165\nTABLE III\nCHANGEPOINT POSITIONS FOR DIFFERENT METHODS\nFig. 5. Changepoint positions of the conditional MAP estimate (solid lines)\nand those estimated by Punskaya et al. (dashed lines).\nThe MAP estimate of the changepoint positions and model\norders consists of only eight changepoints, whereas the MAP\nestimate of the number of changepoints is 9 (posterior proba-\nbility 0.48). The MAP estimates given in Table II are conditional\non there being nine changepoints. For the unconditional MAP\nestimate, the seventh changepoint is missed, but otherwise, the\nestimates of the changepoint positions and model orders are un-\nchanged. The MAP estimate incorrectly infers the model order\nfor segments 7 and 8. In each case, the MAP estimate is one\nless than the true model order, and the AR coefficients that are\nincorrectly estimated as 0 are both small (0.1 and 0.2).\nC. Speech Segmentation\nWe also used our method to analyze a real speech signal [19],\nwhich has also previously been analyzed in the literature [1],\n[18], [19]. We analyzed the data under a piecewise AR model\nthat allowed the AR orders of between 1 and 6 for each segment.\nWe implemented the recursive approach, where an initial run of\nthe exact simulation algorithm is used to construct suitable prior\nparameters.\nThe signal, MAP estimates of the changepoints, and posterior\ndistribution of the changepoints are given in Fig. 4. A compar-\nison of our estimates of the changepoint positions to previous\nesimates are shown in Table III, where we give our MAP esti-\nmates, both conditional and unconditional, on the MAP estimate\nfor the number of changepoints.\nOur MAP estimates are similar to those of Punskaya et al.\n[1], except for the inclusion by the conditional MAP estimate\nof an extra changepoint near the beginning of the signal and the\ninclusion by both MAP estimates of an extra changepoint near\nthe end of the signal. Fig. 5 shows these two regions and the\nestimated changepoints from the different methods.\nIV. CONCLUSION\nWe have presented a novel algorithm for performing exact\nBayesian inference for regression models, where the underlying\nfunction relationship consists of independent linear regressions\non disjoint segments. The algorithm is both scalable and easy to\nimplement. It avoids the problems of diagnosing convergence\nthat are common with MCMC methods.\nWe have focused on models suggested by [1], but the algo-\nrithm can be applied more generally. The main requirement is\nthat of independence between the parameters associated with\neach segment.\nThe regression problem we have addressed involves model\nuncertainty. In practice, the accuracy of Bayesian inference for\nsuch model-choice problems depends on the choice of prior. We\nconsidered two approaches to choosing these priors, both based\non letting the data inform the choice of prior parameters. In our\nexamples, we found that the simpler of the two (the recursive ap-\nproach) performs as well as the approach based on introducing\nhyperparameters, and we would suggest such an approach in\npractice.\nWe have also demonstrated how MAP estimates of the\nchangepoints can be obtained. There are two ways of defining\nthe MAP estimate, depending on whether or not the MAP\nestimate of the number of changepoints is first calculated, and\nthen, the changepoints are inferred, conditional on this number\nof changepoints. In some cases, these different approaches\ncan give different estimates for the number and position of the\nchangepoints: For example, when there is a likely changepoint\nin some period of time but there is a lot of uncertainty over when\nthis changepoint occured, conditioning on the MAP number\nof changepoints will pick up a changepoint during this period\nof time, but it may be omitted otherwise (see Section III-B).\nNote that for the related problem of inferring changepoints in\ncontinuous time, it would clearly be correct to conditon on\nthe number of changepoints, as it is inappropriate to compare\njoint densities of positions of changepoints that are of different\ndimension.\nACKNOWLEDGMENT\nThe author would like to thank E. Punskaya for providing the\nspeech data.\nREFERENCES\n[1] E. Punskaya, C. Andrieu, A. Doucet, and W. J. Fitzgerald, \u201cBayesian\ncurve fitting using MCMC with applications to signal segmentation,\u201d\nIEEE Trans. Signal Process., vol. 50, no. 3, pp. 747\u2013758, Mar. 2002.\nAuthorized licensed use limited to: IEEE Xplore. Downloaded on February 19, 2009 at 08:06 from IEEE Xplore.  Restrictions apply.\n2166 IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 53, NO. 6, JUNE 2005\n[2] P. Green, \u201cReversible jump Markov chain Monte Carlo computation and\nBayesian model determination,\u201d Biometrika, vol. 82, pp. 711\u2013732, 1995.\n[3] P. Fearnhead. (2004) Exact and Efficient Inference for\nMultiple Changepoint Problems. [Online]. Available:\nhttp:\/\/www.maths.lancs.ac.uk\/~fearnhea\/\n[4] L. R. Rabiner and B. H. Juang, \u201cAn introduction to hidden Markov\nmodels,\u201d IEEE Acoust., Speech, Signal Process. Mag., vol. ASSP-34,\nno. 1, pp. 4\u201315, Jan. 1986.\n[5] D. Barry and J. A. Hartigan, \u201cProduct partition models for change point\nproblems,\u201d Ann. Statist., vol. 20, pp. 260\u2013279, 1992.\n[6] , \u201cA Bayesian analysis for change point problems,\u201d J. Amer. Statist.\nSoc., vol. 88, pp. 309\u2013319, 1993.\n[7] J. G. Propp and D. B. Wilson, \u201cExact sampling with coupled Markov\nchains and applications to statistical mechanics,\u201d in Random Structures\nAlgorithms, 1996, vol. 9, pp. 223\u2013252.\n[8] C. C. Holmes and D. G. T. Denison, \u201cPerfect sampling for the wavelet\nreconstruction of signals,\u201d IEEE Trans. Signal Process., vol. 50, no. 2,\npp. 337\u2013344, Feb. 2002.\n[9] P. D. O\u2019Neill and G. O. Roberts, \u201cBayesian inference for partially ob-\nserved stochastic epidemics,\u201d J. R. Statist. Soc., ser. A, vol. 162, pp.\n121\u2013129, 1999.\n[10] P. Fearnhead and L. Meligkotsidou, \u201cExact filtering for partially-ob-\nserved continuous-time Markov models,\u201d J. R. Statist. Soc., ser. B, vol.\n66, pp. 771\u2013789, 2004.\n[11] P. J. Green, \u201cTrans-dimensional Markov chain Monte Carlo,\u201d in\nHighly Structured Stochastic Systems, P. J. Green, N. L. Hjort, and S.\nRichardson, Eds. Oxford, U.K.: Oxford Univ. Press, 2003.\n[12] J. M. Bernardo and A. F. M. Smith, Bayesian Theory. Chichester,\nU.K.: Wiley, 1994.\n[13] S. Richardson and P. J. Green, \u201cOn Bayesian analysis of mixtures with\nan unknown number of components,\u201d J. R. Statist. Soc., ser. B, vol. 59,\npp. 731\u2013792, 1997.\n[14] B. P. Carlin, A. E. Gelfand, and A. F. M. Smith, \u201cHierarchical Bayesian\nanalysis of changepoint problems,\u201d Appl. Statist., vol. 41, pp. 389\u2013405,\n1992.\n[15] A. J. Viterbi, \u201cError bounds for convolutional codes and an asymptoti-\ncally optimum decoding algorithm,\u201d IEEE Trans. Inf. Theory, vol. IT-13,\nno. 2, pp. 260\u2013269, Apr. 1967.\n[16] D. L. Donoho and I. M. Johnstone, \u201cIdeal spatial adaptation by wavelet\nshrinkage,\u201d Biometrika, vol. 81, pp. 425\u2013455, 1994.\n[17] D. G. T. Denison, B. K. Mallick, and A. F. M. Smith, \u201cAutomatic\nBayesian curve fitting,\u201d J. R. Statist. Soc., ser. B, vol. 60, pp. 333\u2013350,\n1998.\n[18] F. Gustafsson, Adaptive Filtering and Change Detection. New York:\nWiley, 2000.\n[19] M. Basseville and I. V. Nikiforov, Detection of Abrupt Changes: Theory\nand Applications. Englewood Cliffs, NJ: Prentice-Hall, 1993.\n[20] M. Basseville and A. Benveniste, \u201cDesign and comparative study of\nsome sequential jump detection algorithms for digital signals,\u201d IEEE\nTrans. Acoust., Speech, Signal Process., vol. ASSP-31, no. 2, pp.\n521\u2013535, Apr. 1983.\n[21] U. Appel and A. V. Brandt, \u201cAdaptive sequential segmentation of piece-\nwise stationary time series,\u201d Inf. Sci., vol. 29, pp. 27\u201356, 1983.\nPaul Fearnhead received the B.A. and D.Phil. de-\ngrees in mathematics from the University of Oxford,\nOxford, U.K.\nSince 2001, he has been a Lecturer in statistics at\nLancaster University, Lancaster, U.K. Prior to this,\nhe was a research associate with the Mathematical\nGenetics group, University of Oxford. His research\ninterests include computational statistical methods\nand, in particular, particle filters and the use of\nfilters within Markov chain Monte Carlo methods.\nHe is also interested in modeling and inference in\npopulation genetics.\nAuthorized licensed use limited to: IEEE Xplore. Downloaded on February 19, 2009 at 08:06 from IEEE Xplore.  Restrictions apply.\n"}