{"doi":"10.1080\/0968776042000259564","coreId":"14173","oai":"oai:generic.eprints.org:610\/core5","identifiers":["oai:generic.eprints.org:610\/core5","10.1080\/0968776042000259564"],"title":"INQUIRE: a case study in evaluating the potential of online MCQ tests in a discursive subject","authors":["Clarke, Sophie","Lindsay, Katharine","McKenna, Chris","New, Steve"],"enrichments":{"references":[{"id":196380,"title":"A computer-based formative a strategy for a basic statistics model,","authors":[],"date":"1998","doi":"10.1080\/03098269885787","raw":"Charman, D. & Elmes, A. (1998b) A computer-based formative a strategy for a basic statistics model, Journal of Geography in Higher Education, 22(3), 381\u2013385.","cites":null},{"id":1042435,"title":"Academic approaches and attitudes towards CAA: a qualitative study,","authors":[],"date":"2001","doi":null,"raw":"McKenna, C. (2001a) Academic approaches and attitudes towards CAA: a qualitative study, Fifth International Computer Assisted Assessment Conference, Loughborough University.","cites":null},{"id":196377,"title":"Assessment 2000: towards a pluralistic approach to assessment, in: M. Birenbaum & F. Douchy (Eds) Alternatives in assessment of achievements, learning processes and prior knowledge","authors":[],"date":"1996","doi":"10.1007\/978-94-011-0657-3_1","raw":"Birenbaum, M. (1996) Assessment 2000: towards a pluralistic approach to assessment, in: M. Birenbaum & F. Douchy (Eds) Alternatives in assessment of achievements, learning processes and prior knowledge (London, Kluwer).","cites":null},{"id":196376,"title":"Classroom assessment techniques (2nd edn)","authors":[],"date":"1993","doi":null,"raw":"Angelo, T. A. & Cross, K. P. (1993) Classroom assessment techniques (2nd edn) (San Francisco, CA, Jossey-Bass).","cites":null},{"id":196379,"title":"Computer based assessment (Vol 2): case studies in science and computing (University of Plymouth,","authors":[],"date":"1998","doi":null,"raw":"Charman, D. & Elmes, A. (Eds) (1998a) Computer based assessment (Vol 2): case studies in science and computing (University of Plymouth, SEED Publications).","cites":null},{"id":196378,"title":"Computer-based learning and assessment: a palaeontological case study with outcomes and implications,","authors":[],"date":"1997","doi":"10.1016\/s0098-3004(97)00025-3","raw":"Boyle, A., Byron, D. & Paul, C. (1997) Computer-based learning and assessment: a palaeontological case study with outcomes and implications, Computers and Geosciences, 23, 573\u2013580.","cites":null},{"id":1042433,"title":"Evaluation of health assessment skills using a computer videodisc interactive programme,","authors":[],"date":"1996","doi":null,"raw":"Mansen, T. J. & Haank, S. W. (1996) Evaluation of health assessment skills using a computer videodisc interactive programme, Journal of Nursing Education, 35(8), 382\u2013383.","cites":null},{"id":196385,"title":"Evaluation of learning technology implementation, in: N. Mogey (Ed.) Evaluation studies (Learning Technology Dissemination Initiative). Available online at: http:\/\/ www.icbl.hw.ac.uk\/ltdi\/evalstudies\/esevalimp.pdf","authors":[],"date":"1998","doi":null,"raw":"Jackson, B. (1998)  Evaluation of learning technology implementation, in: N. Mogey (Ed.) Evaluation studies  (Learning Technology Dissemination Initiative). Available online at: http:\/\/ www.icbl.hw.ac.uk\/ltdi\/evalstudies\/esevalimp.pdf Leon, P. (2002) Log onto student motivation, The Times Higher Educational Supplement, 10 March 2002.","cites":null},{"id":1042437,"title":"Focus groups as qualitative research","authors":[],"date":"1988","doi":"10.4135\/9781412963909.n178","raw":"Morgan D. L. (1988) Focus groups as qualitative research (London, Sage).","cites":null},{"id":196381,"title":"Formative assessment in basic geographical statistics module, in: D. Charman & A. Elmes (Eds) Computer based assessment (Vol 2): case studies in science and computing (University of Plymouth,","authors":[],"date":"1998","doi":null,"raw":"Charman, D. & Elmes, A. (1998c) Formative assessment in basic geographical statistics module, in: D. Charman & A. Elmes (Eds) Computer based assessment (Vol 2): case studies in science and computing (University of Plymouth, SEED Publications).","cites":null},{"id":1042439,"title":"Interpreting qualitative data, methods for analysing talk, text, interaction","authors":[],"date":"2001","doi":"10.2307\/329190","raw":"Silverman, D. (2001) Interpreting qualitative data, methods for analysing talk, text, interaction (London, Sage).","cites":null},{"id":1042436,"title":"Introducing computers into the assessment process: what is the impact upon academic practice?","authors":[],"date":"2001","doi":null,"raw":"McKenna, C. (2001b) Introducing computers into the assessment process: what is the impact upon academic practice? Higher Education Close Up Conference 2, Lancaster University.","cites":null},{"id":1042438,"title":"Student perceptions of the learning benefits of computer-assisted assessment: a case study in electronic engineering, in:","authors":[],"date":"1999","doi":null,"raw":"Sambell, K., Sambell, A. & Graham, S. (1999) Student perceptions of the learning benefits of computer-assisted assessment: a case study in electronic engineering, in: S. Brown, P. Race & J. Bull (Eds) Computer-assisted assessment in higher education (Birmingham, SEDA).","cites":null},{"id":1042434,"title":"The introduction of computer-based testing on an engineering technology course, Assessment and Evaluation","authors":[],"date":"1996","doi":"10.1080\/0260293960210107","raw":"Martin, J. B., McCaffery, K. & Lloyd, D. (1996) The introduction of computer-based testing on an engineering technology course, Assessment and Evaluation in Higher Education, 21, 83\u201390.","cites":null},{"id":196384,"title":"The LTDI evaluation cookbook (Glasgow, Learning Technology Dissemination Initiative).","authors":[],"date":"1998","doi":null,"raw":"Harvey, J. (1998) The LTDI evaluation cookbook (Glasgow, Learning Technology Dissemination Initiative).","cites":null},{"id":196383,"title":"Usability engineering","authors":[],"date":"2000","doi":"10.1145\/353519.343076","raw":"Faulkner, X. (2000) Usability engineering (New York, Palgrave).","cites":null},{"id":196382,"title":"Using CAA for formative assessment, in:","authors":[],"date":"1999","doi":null,"raw":"Charman, D. (1999) Using CAA for formative assessment, in: S. Brown, P. Race & J. Bull (Eds) Computer-assisted assessment in higher education (Birmingham, SEDA).","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2004","abstract":"There has been a wealth of investigation into the use of online multiple-choice questions as a means of summative assessment, however the research into the use of formative MCQs by the same mode of delivery still remains patchy. Similarly, research and implementation has been largely concentrated within the Sciences and Medicine rather than the more discursive subjects within the Humanities and Social Sciences. The INQUIRE (Interactive Questions Reinforcing Education) Evaluation Project was jointly conducted by two groups at the University of Oxford-the Said Business School and the Academic Computing Development Team to evaluate the use of online MCQs as a mechanism to reinforce and extend student learning. This initial study used a small set of highly focused MCQ tests that were designed to complement an introductory series of first-year undergraduate management lectures. MCQ is a simple and well-established technology, and hence the emphasis was very much on situating the tests within the student experience. The paper will cover how the online MCQs are intended to fit into the Oxford Undergraduate study agenda, and how a simple evaluation was executed and planned to investigate their usage and impact. The chosen method of evaluation was to combine focus groups with automated online methods of tracking, and the paper discusses the findings of both of these","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14173.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/610\/1\/ALT_J_Vol12_No3_2004_INQUIRE_%20a%20case%20study%20in%20evalu.pdf","pdfHashValue":"fe36d64bbb448d37db962732cff826588b3dff4f","publisher":"Taylor and Francis Ltd","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:610<\/identifier><datestamp>\n      2011-04-04T09:05:41Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/610\/<\/dc:relation><dc:title>\n        INQUIRE: a case study in evaluating the potential of online MCQ tests in a discursive subject<\/dc:title><dc:creator>\n        Clarke, Sophie<\/dc:creator><dc:creator>\n        Lindsay, Katharine<\/dc:creator><dc:creator>\n        McKenna, Chris<\/dc:creator><dc:creator>\n        New, Steve<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        There has been a wealth of investigation into the use of online multiple-choice questions as a means of summative assessment, however the research into the use of formative MCQs by the same mode of delivery still remains patchy. Similarly, research and implementation has been largely concentrated within the Sciences and Medicine rather than the more discursive subjects within the Humanities and Social Sciences. The INQUIRE (Interactive Questions Reinforcing Education) Evaluation Project was jointly conducted by two groups at the University of Oxford-the Said Business School and the Academic Computing Development Team to evaluate the use of online MCQs as a mechanism to reinforce and extend student learning. This initial study used a small set of highly focused MCQ tests that were designed to complement an introductory series of first-year undergraduate management lectures. MCQ is a simple and well-established technology, and hence the emphasis was very much on situating the tests within the student experience. The paper will cover how the online MCQs are intended to fit into the Oxford Undergraduate study agenda, and how a simple evaluation was executed and planned to investigate their usage and impact. The chosen method of evaluation was to combine focus groups with automated online methods of tracking, and the paper discusses the findings of both of these.<\/dc:description><dc:publisher>\n        Taylor and Francis Ltd<\/dc:publisher><dc:date>\n        2004<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/610\/1\/ALT_J_Vol12_No3_2004_INQUIRE_%20a%20case%20study%20in%20evalu.pdf<\/dc:identifier><dc:identifier>\n          Clarke, Sophie and Lindsay, Katharine and McKenna, Chris and New, Steve  (2004) INQUIRE: a case study in evaluating the potential of online MCQ tests in a discursive subject.  Association for Learning Technology Journal, 12 (3).  pp. 249-260.  ISSN 0968-7769 (print)\/1741-1629 (online)     <\/dc:identifier><dc:relation>\n        10.1080\/0968776042000259564<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/610\/","10.1080\/0968776042000259564"],"year":2004,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"ALT-J, Research in Learning Technology\nVol. 12, No. 3, September 2004\nISSN 0968\u20137769 (print)\/ISSN 1741\u20131629 (online)\/04\/030249\u201312\n\u00a9 2004 Association for Learning Technology\nDOI: 10.1080\/0968776042000259564\nINQUIRE: a case study in evaluating the \npotential of online MCQ tests in a \ndiscursive subject\nSophie Clarkea*, Katharine Lindsaya, Chris McKennab & \nSteve Newb\naAcademic Computing Development Team, University of Oxford, UK;  bSa\u00efd Business \nSchool, University of Oxford, UK\nTaylor and Francis LtdCALT120305.sgm10.1080\/ 968776042000259564ALT-J, Research in Learning Technology0968 7769 (pri t)\/1741-1629 (onli e)Original Article2 04ssoci tion for Learning Techno ogy23 000Sept mber 20 4SophieClarkecademic Co puting Dev lopment TeamOxford University Computing Services13 Banbury RdOxfordOX2 6NNacdt@oucs.ox.ac.uk\nThere has been a wealth of investigation into the use of online multiple-choice questions as a means\nof summative assessment, however the research into the use of formative MCQs by the same mode\nof delivery still remains patchy. Similarly, research and implementation has been largely\nconcentrated within the Sciences and Medicine rather than the more discursive subjects within the\nHumanities and Social Sciences. The INQUIRE (Interactive Questions Reinforcing Education)\nEvaluation Project was jointly conducted by two groups at the University of Oxford\u2014the Sa\u00efd\nBusiness School and the Academic Computing Development Team to evaluate the use of online\nMCQs as a mechanism to reinforce and extend student learning. This initial study used a small set\nof highly focused MCQ tests that were designed to complement an introductory series of first-year\nundergraduate management lectures. MCQ is a simple and well-established technology, and hence\nthe emphasis was very much on situating the tests within the student experience. The paper will\ncover how the online MCQs are intended to fit into the Oxford Undergraduate study agenda, and\nhow a simple evaluation was executed and planned to investigate their usage and impact. The\nchosen method of evaluation was to combine focus groups with automated online methods of\ntracking, and the paper discusses the findings of both of these.\nIntroduction\nMultiple Choice Question (MCQ) tests are one of the most widely used teaching\ntools, and have translated very successfully into the online environment; there are\ncountless free and commercial products, and nearly every VLE has the built-in\ncapacity to deliver online MCQ tests. Uptake has generally been dominated by the\nSciences and Medicine, with less interest from the Humanities and Social Sciences\n*Corresponding author: Academic Computing Development Team, Oxford University Computing\nServices, 13 Banbury Rd, Oxford OX2 6NN, UK. Email: acdt@oucs.ox.ac.uk\n250 S. Clarke et al.\n(McKenna, 2001a). This has been attributed to a belief that MCQ tests are not well\nsuited to discursive subjects, and furthermore that they propagate in students the\nmindset that their education is about the retention and regurgitation of facts\n(McKenna, 2001b). The type of opinions expressed in McKenna (2001b) are typical\nof the reservations academics in these subjects have about MCQ tests. They are not\nconvinced as to the suitability of MCQ tests for their subject area, and if MCQ tests\ncan be of use then they are applicable only at a very unsophisticated level: \u2018I do think\nthat if you want to understand the basics of a course, it\u2019s a good way of getting a basic\nanswer\u2019 (McKenna citing Weldon, 2001b).\nThis paper outlines a small-scale case study at the Sa\u00efd Business School, University\nof Oxford, which aimed to investigate further the potential for using online MCQ\ntests to support a discursive subject. The intention for these tests, or rather informal\nquizzes, was to look beyond the application of MCQ quizzes for testing the retention\nof numerous facts to see if they could be used to deepen student understanding, and\nto see how they might complement the traditional teaching methods used within the\nundergraduate course. The evaluation carried out concentrated on the impact of the\nformative assessment on student learning, as well as the student and staff preferences\nand attitudes where substantial work has been carried out in other studies (Charman,\n1999).\nBackground\nUndergraduate teaching at the Sa\u00efd Business School is very much in the model of the\ntraditional Oxford tutorial system. Students have short periods of intense formal prac-\ntitioner input in the form of tutorials, and these are complemented by lectures. Outside\nof this small amount of high-intensity formal input, students are expected to manage\ntheir own studies, including a large amount of directed and also self-discovered\nreading, writing essays for tutorials, and preparing for examinations. There are two\naspects to this system that influenced the decision to investigate the potential for using\nMCQ tests further. Firstly, in common with most UK Higher Education institutions,\nthere is an increasing pressure from students to provide as many learning resources\nas possible. In particular, students had requested additional ways in which to learn\nand judge their progress during periods of low contact time with their tutors\u2014\nespecially in the lead up to examinations. A second factor in the decision to investigate\nMCQ tests is the importance of the material covered in the tutorials and lectures. A\ngreat deal of emphasis is placed on the tutorials and lectures, and it is important that\nthe ideas and concepts presented in them are grasped by the student. There is little\nor no opportunity for this to be done at a later date, as least not as part of the formal\ncurriculum. Tutorials are viewed as high priority by both students and staff, whereas\nlectures are accorded lower priority by students when other commitments interfere,\nand so it was deemed especially important that the MCQ quizzes would support\nstudents who did not make the lecture.\nThe reasons above highlight why the Business School decided to trial online\nformative assessment as a means of providing students with an additional resource\nEvaluating the potential of online MCQ tests 251\nto support the cycle of lectures, essay-writing and tutorials. The pilot implementa-\ntion of MCQ quizzes to support the lectures is discussed here, and in particular the\nresults of the evaluation carried out to investigate their first implementation in the\nundergraduate curriculum. The scope of the pilot involved the use of online MCQ\nquizzes to support the \u2018Introduction to Management\u2019 course, taken by all under-\ngraduate students. The study evaluated the use of an online MCQ test alongside\nthe lectures to investigate students\u2019 use of the resources. The aim was to develop\nfurther quizzes to support more of the undergraduate curriculum if they proved\nsuccessful.\nThe MCQ quizzes had several key characteristics. The questions themselves were\ncarefully crafted to address the educational objectives outlined below. Each individ-\nual potential answer for each MCQ question needed to be able to have its own feed-\nback, and in both the questions and answers there was a requirement for the ability\nto include hyperlinks and images. A variety of MCQ software was available at the\nUniversity, and software choice was regarded as relatively insignificant at the start of\nthe project. However, the feedback and HTML inclusion requirements dramatically\nlimited the choice of suitable software. The solution finally chosen was Quia, an\nonline activity creation tool hosted on the service providers web site. Quia fulfilled the\naforementioned requirements, and was also suitably easy to use for non-technical\nquestion authors.\nAims and objectives\nThere were four areas in which it was thought that the MCQ quizzes could help\nstudents, and the pilot tests were carefully written with these aims in mind. Investi-\ngating the success in fulfilling these aims was used as the basis for choosing an evalu-\nation methodology. The four aims it was hoped that the MCQ quizzes could address\nare outlined below.\nThe reinforcement of learning from other pedagogic elements\nAs previously discussed, the pressure for lectures and tutorials to deliver effectively\nis high, but there are many factors that can contribute to whether or not a student\nsucceeds with a particular subject. Lectures can vary in quality across the course,\nand are more likely to fall by the wayside completely in terms of student\u2019s priori-\nties when other commitments press, such as assignment deadline and extra-curric-\nular activities. Logistically the structure of the course means that there can be\nmany months between the last formal teaching input on a particular topic and the\nexam.\nIt was envisioned that the MCQ quizzes would provide students with a further\nopportunity to cover the lecture material when and where it suited them. For\nstudents who did not make the original lecture, they would have extra help in\ncomprehending the lecture handout, and a way of measuring their own level of\nunderstanding.\n252 S. Clarke et al.\nCementing students\u2019 understanding\nMCQ quizzes have a role in terms of testing that students have grasped the key facts\nand ideas. Additionally, this is a practical safeguard as the lectures are designed to\nchallenge the students rather than merely transmit information. In addition to cover-\ning the core concepts, MCQ questions can be used to cement students\u2019 understand-\ning of the more subtle points in the lectures, particularly through the use of the\ncarefully constructed feedback.\nDeepening students\u2019 knowledge\nThe MCQ quizzes provide an excellent opportunity to present entirely new material\nto students, leading them beyond their reading lists. For the more curious students,\nthey can provide some framing to new areas that there might not be time to cover in\nthe tutorials, and can guide the student towards credible resources that they may not\notherwise have found. Necessary to this end is the ability to link out to online\nresources (the links made ranged from company web sites to journal articles in the\ndepartment electronic library provision).\nFraming educational expectations\nMCQ quizzes provide another way for course leaders to convey to students (explicitly\nand implicitly) what is expected of them. Many first-year undergraduate students are\nsurprised and daunted to find that they are expected to read complex journal articles\nand whole books. The MCQ quizzes can reinforce the message of what they are\nexpected to read by basing questions on those materials and by linking out to a wider\nvariety of literature than is present on the reading lists. In general, students can be\npointed in particular directions without being obviously spoon-fed.\nExample question and answer with feedback\nIn the lecture, you learned that even in recent years the shift to working in large organiza-\ntions continues. What do you think that this has meant for the number of hours that people\nwork?\nIn other words, do you believe that it is true or false that working for larger organisations\nmeans that people work longer hours than they would have if they were working for smaller\ncompanies or for themselves?\n(Hint: You might want to consult this study on long term employment trends: http:\/\/\nphe.rockefeller.edu\/work_less\/)\n\u2022True\n\u2022False\nAnswer: False\nFeedback: For a critical view of the conclusion that people are working fewer total\nhours, see Juliet Schor\u2019s book The Overworked American.\nThere were several potential pitfalls identified with respect to the MCQ quizzes, and\nattempts were made to avoid these, particularly in the writing of the questions. There\nEvaluating the potential of online MCQ tests 253\nis a danger that by picking out particular areas (either deliberately or inadvertently),\nthe quizzes could send misleading clues to students about what is and isn\u2019t important.\nThis is exacerbated by the students\u2019 tendency to be very strategic and exam-focused\nwhen considering how best to spend their study time. A second potential problem to\navoid was writing overly simple questions with black or white answers, or indeed the\nopposite issue of inadvertently adding subtle nuances that some students may\nwrongly pick-up. To avoid these pitfalls questions were authored collaboratively and\ntested before they were made available to the students.\nMethodology\nThis section outlines the evaluation methodology used to consider the usage of the\nsix pilot MCQ quizzes over the course of a term. The aim of using evaluation tech-\nniques was to assess if the quizzes delivered any or all of the four perceived aims\noutlined above. This type of subjective evaluation is notoriously difficult to design\nand carry out. The expectation for the evaluation was to get an initial indication of\nhow well the pilot tests had worked. From there any particularly promising, or\ninteresting, aspects could be identified to be investigated in greater depth and with\nmore resources (possibly with a larger group of students over more than one\ncourse).\nA range of evaluation methods were considered, and each assessed against its\npotential to illuminate how well the MCQ quizzes had addressed the four primary\naims. A focus on methods with a proven track record in educational research and\nthe evaluation of technology-based resources led to a shortlist of six possibilities:\nsystem logs, automated tracking of link following, confidence logs, 1-minute\npapers, video observation, focus groups (Angelo & Cross, 1993; Harvey, 1998).\nThe shortlist was then considered again to judge which methods could address\nmultiple aims at once. Between the chosen methods they would be required to\naddress all of the four aims, and to provide a combination of qualitative and\nquantitative data.\nAccess logs, the tracking of link following and focus groups were chosen as the\nmost appropriate. Techniques that attempted to quantify student knowledge in\nsome way (confidence logs and 1-minute papers) were discarded on the grounds of\ndifficulty of application within the resources available, and also because of the prob-\nlems inherent in using these methods to judge improvement against the stated aims.\nVideo observation of students was also decided against as the focus was more\ntowards the cumulative result of accessing all six quizzes rather than in analysing the\nspecific answers chosen in any one sitting. Similarly, there was no particular concern\nwith software usability at this stage, an area in which video footage would be partic-\nularly applicable.\nAccess logs provide an unobtrusive method of mapping the use patterns of individ-\nuals. They give a useful picture of how the quizzes were used in relation to the timing\nof other aspects of the course (lectures and tutorials), and also provide quantitative\ndata to back up the focus groups. There are data protection issues surrounding this,\n254 S. Clarke et al.\nand students were asked to give their consent to tracking using anonymous but\ndistinct usernames, i.e. student1, student2. The specifics logged by this method were\nthe details as to the date, time and frequency with which each username accessed each\ntest.\nTracking of link following was used to measure whether or not the students moved\nbeyond the boundaries of their recommended reading and used the hyperlinks\nembedded in the questions, feedback and answers. Code was embedded directly\nwithin each hyperlink, this took the student to the page advertised but via a script that\nrecorded the following of that link in a database. Tracking done in this way is\nextremely quick and is not visually obvious or intrusive to the student. However,\nethics again came into play and student consent was necessary.\nFinally, focus groups were chosen as the best method of getting the students\nopinions on the quizzes and judging the effect on their learning and understanding\n(Morgan, 1988). The focus groups were chaired by a course leader familiar with\nthe curriculum with some lines of investigation intended to assess student\nunderstanding of the lecture topics. To help ascertain to some degree the effects of\nthe MCQ quizzes as opposed to the learning bestowed by the course as a whole, it\nwas decided to use three sample groups. Because the quiz aims were as yet\nunproven, and because they would be made available to all students at a later date\nif beneficial, it was considered to be an ethically acceptable strategy. A self-selecting\ngroup of thirty student volunteers were involved in the evaluation. The groups are\ndescribed below:\nThe control group\nThe control group students went through course in the same way as those not\ninvolved in the trial. They attended the lectures and tutorials in the usual way, but\nhad no access to the MCQ quizzes.\nThe paper group\nThis group of students completed the lectures and tutorials. In addition, after each\nlecture they were given a printed version of the MCQ test, with answers provided on\na separate sheet. Online material referenced in the questions was given as printed\nURLs, and references to journal articles and books were given in the form of tradi-\ntional written citations.\nThe online group\nThis group of students completed the lectures and tutorials. After each lecture, an\nonline MCQ test was released. Students were given immediate feedback after\nanswering each question, URLs were provided as hyperlinks and journal articles and\nbooks were also provided as hyperlinks wherever possible. Each student had an indi-\nvidual but anonymous account as described earlier.\nEvaluating the potential of online MCQ tests 255\nResults analysis\nThe investigation took place over six weeks, with the staggered release of six sets of\nquizzes coinciding with the six introductory lectures to the course. Throughout this\ntime the logs and tracking were analysed and at the end of the six-week period the\nstudents were brought together in their individual focus groups, not only to discuss\nthe impact of the quizzes, but also for analysis of their level of knowledge and\nunderstanding concerning the concepts and ideas of the lectures that they had been\nto. The results that emerged gave a valuable insight into how a resource such as\nformative online multiple-choice questions can be used to reinforce lectures and\nenhance student learning. What follows is a results analysis of the focus groups,\ntracking and logs in relation to the four aims posed, and a discussion of other relevant\nfindings.\nThe reinforcement of learning from other pedagogic elements\nTo monitor whether the quizzes were successful as a pedagogy to reinforce learning,\nthe usage of the quizzes was analysed as well as students\u2019 attitudes towards using them\nas an additional study resource. The paper group voiced their reluctance to use the\nquizzes. It was inconvenient to pick up the quiz sheets after their lectures and they\nwere generally put to the bottom of the pile of paper accumulating on their desks.\nBeing delivered on paper was not appealing as the quizzes lacked a high level of\ninteraction, students found them bland and boring and none admitted to going back\nto them after completing them the once, although some filed them away for \u2018revision\npurposes\u2019.\nFor the online quizzes the collected logs were examined to show usage. Whilst such\nan analytical method can only provide a broad overview of the students\u2019 use patterns,\nthe statistics collected provided some valuable information. In some individual cases\nstudents accessed all the quizzes for the first time a couple of days before the focus\ngroup session. Here it could be argued that their behaviour was affected by the\nknowledge that they were being tracked and of the impending focus group, their\nattention drawn more to the act of evaluation rather than the act of learning\n(Faulkner, 2000, p. 171). For the remainder of the students the statistics showed that\nthe quizzes were subjected to light usage; 100% of these students accessed all of the\nquizzes, but only 40% accessed them more than once. During the focus group session\nthe general consensus was that the quizzes were a valuable resource for many\nstudents: \nStudent C: \u2026 some people use different libraries, different readings and different newspa-\npers, I think all of us\u2014we all like choose how we study and stuff so it\u2019s, I think, it\u2019s valuable\nto have this as an option as well. Cos as I said before, it\u2019s not for everyone, but for sure it\u2019s\nvaluable for a lot [of students].\nThe lack of repeated use of the quizzes was largely due to the difficulty students had\nin perceiving how they would form a cemented part of their studies, as it did not fit\ninto the rigorous essay writing and examination process. Some students did go back\n256 S. Clarke et al.\nand redo certain quizzes where they felt they needed to improve. However, like the\npaper group, the majority left it at one attempt with the forethought that such a\nresource would be more useful in the period leading up to examinations. With this in\nmind students commented that they would like the quizzes to be \u2018a little more based\non the exam\u2019 rather than being based on the lectures.\nThe times at which the students accessed the tests showed a strong pattern of late-\nnight quizzing. The majority of log-in times occurred between 8pm and 12am in the\nevening. On questioning the students in the focus group it became clear that the\nprocess of taking a short quiz provided a work-related distraction for them whilst they\nwere engaged in another activity, such as preparing for a tutorial or waiting to meet\nfriends: \nStudent A: It\u2019s like something you can do [so] that you feel like you\u2019re doing something\nuseful \u2026\nStudent B: It\u2019s kind of like in-between time \u2026 after dinner doing work and maybe you go\nout later to meet your friends or something.\nCementing students\u2019 understanding\nThe understanding of the key concepts covered during the lectures differed between\nthe three groups. The control group and the paper group had extremely patchy\nretention of the details of the lectures. They had yet to grasp the key concepts and\nfacts that had been delivered and generally had to bounce off each other when\nrecalling lecture topics: \u2018I don\u2019t really remember much of it but when people start\nsaying stuff it starts coming back\u2019. The paper group thought that the quizzes were\n\u2018good as a memory trigger\u2019, but there was little evidence within the focus group\nsessions that they had remembered significantly more than their peers. It was clear\nthat the students\u2019 knowledge was built primarily through essay writing and tutorials\nrather than analysing and going back to the lecture content. In the lectures they\nsought specific facts and ideas which would help them with that week\u2019s essay: \u2018he\u2019s\n[the lecturer] probably going to be mentioning something that is very relevant to your\nessay \u2026 and then you want to capture certain points\u2019.\nThe online group were noticeably more forward in their focus group when\ndiscussing topics that specifically related to the lecture content and more readily\nrecalled subtle points that had been addressed in the lectures, as well as key examples\nand facts. The group all agreed that the online quizzes had provided a way of helping\nthem remember the lectures: \nStudent A: I think it\u2019s a good thing to go back, sit down and answer questions, because if\nyou answer questions you think about it more than when you just sit down and read the\nnotes again.\nStudent B: And then sometimes you miss things, at lectures, you know you\u2019re not always\nconcentrating. So, you know, when you read the questions \u2026 they help you think about it\nmore and, more in-depth.\nStudent C: Yeah, I think it\u2019s good because, it\u2019s good to think about it more, and it\u2019s the\nright kind of style, it is for learning like. I think he raises key points that he wants you to\nget out of the lecture, impressions.\nEvaluating the potential of online MCQ tests 257\nStudent C: Sometimes I felt that.\nStudent D: Yeah, it kind of triggered your memory about, I waited about a week, two until\nafter the lectures to do it \u2026 I remembered it better in my mind, doing that. Cos, I think if\nI\u2019d done it straight away, it would have kind of all been one thing and I think I would have\njust forgotten it all after a while.\nInterestingly, the images used in the quizzes provided a particularly useful hook with\nthe lecture material, jogging their memories and reinforcing ideas and facts that had\nbeen discussed. \nStudent A: I would use as many pictures, illustrations as you did, because they can some-\ntimes just give you that little bit that makes you remember them. Cos when you see a\npicture, a photograph of Thomas Edison, you think, oh hang on, yeah, General Electrics,\nbecause it was on the slide. But when they weren\u2019t on the slides, they sort of confuse you\na bit, because you think I must have seen that before, but you haven\u2019t.\nDeepening students\u2019 knowledge\nTo achieve a deeper understanding of the course students were given links to journal\narticles and company websites. In the paper-based quizzes these links were written\ndown as URLs or references to hard copy books and journals. The students found\ntyping out the URLs \u2018annoying\u2019 and if they were not actually sat at a computer at the\ntime of taking the quiz they were unlikely to follow them up. Likewise, the references\nto hard copy books and journals were not pursued as they felt they were reading\nenough with the set tutorial texts they were given each week : \u2018I don\u2019t do any more\nreading than I have to\u2019. As a result the students found it difficult linking the lecture\ntopics that they could remember to relevant wider material outside of their reading\nlists.\nThe link tracking showed that the students taking the online quizzes did follow the\nlinks to the outside resources. Interestingly the students refuted this\u2014denying that\nthey had read the articles and company profiles: \nInterviewer: \u2026 for the ones [questions] which had the links, which could take you off and\ngive you something more, long to read, did anyone look at those?\nStudent E: No, I didn\u2019t.\nStudent C: I read some, I just didn\u2019t include them.\nStudent A: Depends how long they were. Sometimes I read through them but sometimes\nI thought, ooh that\u2019s too long, for the moment.\nIt is clear that many students just clicked-and-glanced at the wider reading materi-\nals. This, they considered, did not constitute learning as there was no depth of\nreading or interpretation. However, it was evident that this very mundane prac-\ntice made the students acknowledge that there was a wider pool of resources that\nthey could, and were expected to, consult within their study practices. Links to\nscholarly articles that the students felt relevant to their essays received a greater\nnumber of hits than others and the company websites. Many students commented\nthat they had added these to their browser \u2018favourites\u2019 folder to read at a \u2018later\ndate\u2019.\n258 S. Clarke et al.\nFraming educational expectations\nThrough the focus group sessions it was clear that students were starting to take on\nthe expectations of the department. There was little difference between the three\ngroups indicating that the quizzes were a precarious factor in framing the students\u2019\neducational expectations. There was a slight difference in the online focus group\nsession where there was a strong general consensus that they were expected to read\nmuch more than they were already doing, perhaps as a result of following the links\nplaced to deepen their knowledge.\nFurther findings of the evaluation\nAll three focus group sessions brought to light some interesting points concerning the\nnature of the students\u2019 study patterns at the university. Although not directly related\nto the research questions of this study, these highlighted some pedagogic concerns\nwhich are relevant to the development of online learning tools. Firstly, it was clear\nthat undergraduate students study habits were entirely governed by the tutorial and\nessay writing timetable\u2014each academic week revolved around the process of reading\nand writing for the next tutorial. This rigid study routine gives students little incentive\nto read any wider than the set papers for their forthcoming tutorials, rarely consulting\nfurther reading lists or searching for wider materials themselves.\nSecondly, it became evident that the lecture handout played a talismanic role in the\nstudents study process. The students were immensely grateful for the lecture handout.\nFor them it holds a vitally important place in their resources for study, representing a\nwalkthrough of the lecture. However, to the lecturer it only really offers an overview\nfrom which they expect their students to expand their knowledge through further read-\ning and discussion. Because the lectures were engaging and entertaining, students\ntook few, if any, notes and tended to sit back and enjoy the class; they felt they could\nuse the handout later as a reference point. Interestingly, although the handout appears\nto be an important resource, students can remember very little of the content of their\nlectures when quizzed. Filed away and retrieved nearer the examination period it\nprovides an outline of what needs to be regurgitated for a particular topic.\nThe rigorous structure of lecture-tutorial-examination presents to the advocator of\ne-learning a problem of materiality. When given a set reading list and lecture handout\nstudents are inclined to do little more than follow the lines of investigation that these\nboth represent. This structure is not conducive to wider thinking and obtaining a\ndeeper knowledge. Additionally, learners want to make sure that the skills and capa-\nbilities they acquire through using study resources are relevant to their summative\ncourse marks and it is not clear to them how optional e-learning such as formative\nquizzes can contribute to their degree.\nConclusions\nLectures are a fundamental part of the university teaching system \u2013 they provide an\noverview of a course\u2019s topics and key ideas, and guide students towards areas of\nEvaluating the potential of online MCQ tests 259\nfurther research and investigation. However, in a university system that also\nincludes high quality tutorials it is often the case that lectures are put aside when\nthere are more pressing academic activities and events in a student\u2019s life. From the\nINQUIRE evaluation it is strongly indicated that reinforcing the content of the\nlectures through formative assessment can act to cement students\u2019 understanding of\nkey concepts and ideas. The value clearly lies in the \u2018jogging\u2019 effect that the quizzes\nhad on the students\u2019 memories, and fundamentally the mode of delivery influenced\nhow effective this was. Good practice in university teaching places a high emphasis\non the entertainment value of a lecture with its quality of being engaging and thus\nmemorable, the implication being that memorable lectures are more effective in\nterms of student learning. We need to realize that the same is true for learning\nresources.\nThis evaluation project, like others before it (e.g. Mansen & Haak, 1996; Martin\net al., 1996; Charman & Elsmes, 1998a) concerned chiefly the attitudes and\npreferences of the students towards using formative online assessment. It also\nattempted to address the impact of this type of assessment on student learning\nthrough comparing the knowledge and understanding of the three student groups.\nThe evaluation of the effectiveness of learning technology is particularly complex\nand is fraught with difficulties: the success will vary, depending on the scope of\nprevious knowledge, attitudes, and conceptions which particular learners bring to\nthe learning situation, and the larger context in which the learning situation is\nembedded (Jackson, 1998). The qualitative results of this evaluation suggest that\nonline MCQs can reinforce learning from other pedagogic elements and cement\nstudent understanding in subjects that were not traditionally thought to lend\nthemselves to such a model (McKenna, 2001b; Leon, 2002). However, to clearly\nestablish the positive impact of this method of formative assessment there must be\nfurther investigation of a quantifiable nature. The most valid option in the current\neducational system is to chart examination performance, as Charman states \u2018the\nonly real way to assess this quantitatively is to compare performances in summative\nassessments\u2019 (Charman, 1999). It will be highly interesting to observe over the next\nfew years if there is a marked difference in the module examination results attribut-\nable to the online quizzes.1\nThe use of formative online MCQs could potentially offer measurable benefits in\nsupporting humanities and social science students in their study. However, there is\nstill much to do if this is to become fully integrated within the study experience. It is\nstill widely perceived both by education establishments and students that all forms of\nassessment are a means to measure knowledge and are inextricable from certification.\nThe aims of these quizzes, however was to compliment traditional teaching and\ndeepen understanding rather than to assess competence in a subject. There is still\nmuch work to do if there is to be a shift in our assessment paradigms (Birenbaum\n1996; Sambell et al., 1999) and the benefits of formative assessment can be realized\nand integrated into the rigid study structures posed by many higher education\nsystems. Although this was a small-scale evaluation it has laid some useful founda-\ntions for further research in this area.\n260 S. Clarke et al.\nNotes\n1. For similar studies that have compared exam performance, see Boyle et al., 1997; Paul & Boyle,\n1998; and Charmen & Elmes, 1998c,d.\nReferences\nAngelo, T. A. & Cross, K. P. (1993) Classroom assessment techniques (2nd edn) (San Francisco, CA,\nJossey-Bass).\nBirenbaum, M. (1996) Assessment 2000: towards a pluralistic approach to assessment, in:\nM. Birenbaum & F. Douchy (Eds) Alternatives in assessment of achievements, learning processes\nand prior knowledge (London, Kluwer).\nBoyle, A., Byron, D. & Paul, C. (1997) Computer-based learning and assessment: a palaeontolog-\nical case study with outcomes and implications, Computers and Geosciences, 23, 573\u2013580.\nCharman, D. & Elmes, A. (Eds) (1998a) Computer based assessment (Vol 2): case studies in science\nand computing (University of Plymouth, SEED Publications).\nCharman, D. & Elmes, A. (1998b) A computer-based formative a strategy for a basic statistics\nmodel, Journal of Geography in Higher Education, 22(3), 381\u2013385.\nCharman, D. & Elmes, A. (1998c) Formative assessment in basic geographical statistics module,\nin: D. Charman & A. Elmes (Eds) Computer based assessment (Vol 2): case studies in science and\ncomputing (University of Plymouth, SEED Publications).\nCharman, D. (1999) Using CAA for formative assessment, in: S. Brown, P. Race & J. Bull (Eds)\nComputer-assisted assessment in higher education (Birmingham, SEDA).\nFaulkner, X. (2000) Usability engineering (New York, Palgrave).\nHarvey, J. (1998) The LTDI evaluation cookbook (Glasgow, Learning Technology Dissemination\nInitiative).\nJackson, B. (1998)  Evaluation of learning technology implementation, in: N. Mogey (Ed.) Evalu-\nation studies (Learning Technology Dissemination Initiative). Available online at: http:\/\/\nwww.icbl.hw.ac.uk\/ltdi\/evalstudies\/esevalimp.pdf\nLeon, P. (2002) Log onto student motivation, The Times Higher Educational Supplement, 10 March\n2002.\nMansen, T. J. & Haank, S. W. (1996) Evaluation of health assessment skills using a computer\nvideodisc interactive programme, Journal of Nursing Education, 35(8), 382\u2013383.\nMartin, J. B., McCaffery, K. & Lloyd, D. (1996) The introduction of computer-based testing on\nan engineering technology course, Assessment and Evaluation in Higher Education, 21, 83\u201390.\nMcKenna, C. (2001a) Academic approaches and attitudes towards CAA: a qualitative study, Fifth\nInternational Computer Assisted Assessment Conference, Loughborough University.\nMcKenna, C. (2001b) Introducing computers into the assessment process: what is the impact\nupon academic practice? Higher Education Close Up Conference 2, Lancaster University.\nMorgan D. L. (1988) Focus groups as qualitative research (London, Sage).\nSambell, K., Sambell, A. & Graham, S. (1999) Student perceptions of the learning benefits of\ncomputer-assisted assessment: a case study in electronic engineering, in: S. Brown, P. Race &\nJ. Bull (Eds) Computer-assisted assessment in higher education (Birmingham, SEDA).\nSilverman, D. (2001) Interpreting qualitative data, methods for analysing talk, text, interaction\n(London, Sage).\n"}