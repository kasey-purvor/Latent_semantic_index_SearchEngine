{"doi":"10.1016\/j.compchemeng.2007.11.011","coreId":"140997","oai":"oai:dspace.lib.cranfield.ac.uk:1826\/3072","identifiers":["oai:dspace.lib.cranfield.ac.uk:1826\/3072","10.1016\/j.compchemeng.2007.11.011"],"title":"Bidirectional branch and bound for controlled variable selection. Part I: principles and minimum singular value criterion.","authors":["Cao, Yi","Kariwala, Vinay"],"enrichments":{"references":[{"id":37923486,"title":"A branch and bound algorithm for feature subset selection.","authors":[],"date":"1977","doi":"10.1109\/tc.1977.1674939","raw":"Narendra, P., & Fukunaga, K. (1977). A branch and bound algorithm for feature subset selection. IEEE Transactions on Computers, 26(9), 917\u2013922.","cites":null},{"id":37923491,"title":"A more ef\ufb01cient branch and bound algorithm for feature selection.","authors":[],"date":"1993","doi":"10.1016\/0031-3203(93)90054-z","raw":"Yu, B., & Yuan, B. (1993). A more ef\ufb01cient branch and bound algorithm for feature selection. Pattern Recognition, 26(6), 883\u2013889.","cites":null},{"id":37923490,"title":"A review of methods for input\/output selection.","authors":[],"date":"2001","doi":"10.1016\/s0005-1098(00)00181-3","raw":"Van de Wal, M., & de Jager, B. (2001). A review of methods for input\/output selection. Automatica, 37(4), 487\u2013510.","cites":null},{"id":37923470,"title":"An improved branch and bound algorithm for feature selection.","authors":[],"date":"2003","doi":"10.1016\/s0167-8655(03)00020-5","raw":"Chen, X.-W. (2003). An improved branch and bound algorithm for feature selection. Pattern Recognition Letters, 24(12), 1925\u20131933.","cites":null},{"id":37923487,"title":"An improved branch and bound algorithm for feature subset selection.","authors":[],"date":"1988","doi":"10.2307\/2347512","raw":"Ridout, M. S. (1988). An improved branch and bound algorithm for feature subset selection. Applied Statistics, 37(1), 139\u2013147.","cites":null},{"id":37923468,"title":"An input pre-screening technique for control structure selection.","authors":[],"date":"1997","doi":"10.1016\/s0098-1354(96)00296-7","raw":"Cao, Y., & Rossiter, D. (1997). An input pre-screening technique for control structure selection. Computers & Chemical Engineering, 21(6), 563\u2013569.","cites":null},{"id":37923484,"title":"Branch and bound methods for control structure design.","authors":[],"date":"2006","doi":"10.1016\/s1570-7946(06)80238-5","raw":"Kariwala, V., & Skogestad, S. (2006). Branch and bound methods for control structure design. In Proceedings of the 16th ESCAPE and 9th international symposium on PSE Kookos, I. K., & Perkins, J. D. (2001). Heuristic-based mathematical programming framework for control structure selection. Industrial & Engineering Chemistry Research, 40, 2079\u20132088. Luyben,M.L.,&Floudas,C.A.(1994).Analyzingtheinteractionofdesignand control. 1. A multiobjective framework and application to binary distillation column. Computers & Chemical Engineering, 18(10), 933\u2013969.","cites":null},{"id":37923488,"title":"Branch and reduce optimization navigator (BARON) user\u2019s manual v 4. 0.","authors":[],"date":"2000","doi":null,"raw":"Sahinidis, N. V. (2000). Branch and reduce optimization navigator (BARON) user\u2019s manual v 4. 0. Urbana, IL, USA: University of Illinois at UrbanaChampaign.","cites":null},{"id":37923469,"title":"Improved branch and bound method for control structure screening.","authors":[],"date":"2005","doi":"10.1016\/j.ces.2004.10.025","raw":"Cao, Y., & Saha, P. (2005). Improved branch and bound method for control structure screening. Chemical Engineering Science, 60(6), 1555\u2013 1564.","cites":null},{"id":37923472,"title":"Linear Algebra and its Applications, 422(2\/3), 349\u2013359. Floudas,C.A.(1995).Nonlinearandmixed-integeroptimization:Fundamental and applications.","authors":[],"date":null,"doi":null,"raw":"Linear Algebra and its Applications, 422(2\/3), 349\u2013359. Floudas,C.A.(1995).Nonlinearandmixed-integeroptimization:Fundamental and applications. New York, NY, USA: Oxford University Press.","cites":null},{"id":37923467,"title":"Linear least squares solution by householder transformations. Numerische Mathematik, 7(3), 269\u2013276.Page 14 of 14 Accepted Manuscript Please cite this article in press as:","authors":[],"date":"1965","doi":"10.1007\/bf01436084","raw":"Businger, P. A., & Golub, G. H. (1965). Linear least squares solution by householder transformations. Numerische Mathematik, 7(3), 269\u2013276.Page 14 of 14 Accepted Manuscript Please cite this article in press as: Cao, Y., Kariwala, V., Bidirectional branch and bound for controlled variable selection, Comput Chem Eng (2008), doi:10.1016\/j.compchemeng.2007.11.011 ARTICLE IN PRESS +Model CACE-3580; No.of Pages14 14 Y. Cao, V. Kariwala \/ Computers and Chemical Engineering xxx (2008) xxx\u2013xxx Cao, Y., & Kariwala, V. (2007, November). B3MSV. MATLAB File Exchange. Available at http:\/\/www.mathworks.com\/matlabcentral\/\ufb01leexchange\/ loadFile.do?objectId=17480&objectType=\ufb01le.","cites":null},{"id":37923483,"title":"Matrix analysis.","authors":[],"date":"1985","doi":"10.1017\/cbo9780511810817","raw":"Horn, R. A., & Johnson, C. R. (1985). Matrix analysis. Cambridge, UK: Cambridge University Press.","cites":null},{"id":37923482,"title":"of controlled variables.","authors":[],"date":"1996","doi":null,"raw":"Golub, G. H., & Van Loan, C. F. (1996). Matrix computations (3rd ed.). Baltimore, MD, USA: The Johns Hopkins University Press. Halvorsen,I.J.,Skogestad,S.,Morud,J.C.,&Alstad,V.(2003).Optimalselection of controlled variables. Industrial & Engineering Chemistry Research, 42(14), 3273\u20133284.","cites":null},{"id":37923485,"title":"Optimal sensor location in motion control of \ufb02exibly supported long reach manipulators.","authors":[],"date":"1997","doi":"10.1115\/1.2802382","raw":"Mavroidis, C., Dubowsky, S., & Thomas, K. (1997). Optimal sensor location in motion control of \ufb02exibly supported long reach manipulators. Journal of Dynamic Systems, Measurement and Control, 119(4), 718\u2013726.","cites":null},{"id":37923489,"title":"Plantwide control: The search for the self-optimizing control structure.","authors":[],"date":"2000","doi":"10.1016\/s0959-1524(00)00023-8","raw":"Skogestad, S. (2000). Plantwide control: The search for the self-optimizing control structure. Journal of Process Control, 10(5), 487\u2013507. Skogestad,S.,&Postlethwaite,I.(1996).Multivariablefeedbackcontrol:Analysis and design (1st ed.). Chichester, UK: John Wiley & Sons. Somol,P.,Pudil,P.,&Kittler,J.(2004).Fastbranch&boundalgorithmsforoptimal feature selection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(7), 900\u2013912.","cites":null},{"id":37923464,"title":"Studies on selection of controlled variables.","authors":[],"date":"2005","doi":null,"raw":"Alstad, V. (2005). Studies on selection of controlled variables. PhD thesis, Norwegian University of Science and Technology, Trondheim, Norway. Available at http:\/\/www.nt.ntnu.no\/users\/skoge\/publications\/thesis\/2005 alstad\/. Araujo,A.,&Skogestad,S.(2007).ApplicationofplantwidecontroltotheHDA process. I. Steady-state optimization and self-optimizing control. Control Engineering Practice, 15(10), 1222\u20131237.","cites":null},{"id":37923471,"title":"Subset selection for matrices.","authors":[],"date":"2007","doi":"10.1016\/j.laa.2006.08.034","raw":"De Hoog, F. R., & Mattheij, R. M. M. (2007). Subset selection for matrices.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2008-10-17T00:00:00Z","abstract":"The minimum singular value (MSV) rule is a useful tool for selecting controlled\nvariables (CVs) from the available measurements. However, the application of the\nMSV rule to large-scale problems is difficult, as all feasible measurement\nsubsets need to be evaluated to find the optimal solution. In this paper, a new\nand efficient branch and bound (BAB) method for selection of CVs using the MSV\nrule is proposed by posing the problem as a subset selection problem. In\ntraditional BAB algorithms for subset selection problems, pruning is performed\ndownwards (gradually decreasing subset size). In this work, the branch pruning\nis considered in both upward (gradually increasing subset size) and downward\ndirections simultaneously so that the total number of subsets evaluated is\nreduced dramatically. Furthermore, a novel bidirectional branching strategy to\ndynamically branch solution trees for subset selection problems is also\nproposed, which maximizes the number of nodes associated with the branches to be\npruned. Finally, by replacing time-consuming MSV calculations with novel\ndeterminant based conditions, the efficiency of the bidirectional BAB algorithm\nis increased further. Numerical examples show that with these new approaches,\nthe CV selection problem can be solved incredibly fast","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/140997.pdf","fullTextIdentifier":"http:\/\/dx.doi.org\/10.1016\/j.compchemeng.2007.11.011","pdfHashValue":"8997a5d000d4d5afa32b1037d7a5343a2e0261ff","publisher":"Elsevier Science B.V., Amsterdam.","rawRecordXml":"<record><header><identifier>\noai:dspace.lib.cranfield.ac.uk:1826\/3072<\/identifier><datestamp>2016-07-12T10:56:40Z<\/datestamp><setSpec>hdl_1826_19<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>Bidirectional branch and bound for controlled variable selection. Part I: principles and minimum singular value criterion.<\/dc:title><dc:creator>Cao, Yi<\/dc:creator><dc:creator>Kariwala, Vinay<\/dc:creator><dc:subject>Branch and bound<\/dc:subject><dc:subject>Control structure design<\/dc:subject><dc:subject>Controlled variables<\/dc:subject><dc:subject>Combinatorial optimization<\/dc:subject><dc:subject>Minimum singular value<\/dc:subject><dc:subject>Self-optimizing control<\/dc:subject><dc:description>The minimum singular value (MSV) rule is a useful tool for selecting controlled\nvariables (CVs) from the available measurements. However, the application of the\nMSV rule to large-scale problems is difficult, as all feasible measurement\nsubsets need to be evaluated to find the optimal solution. In this paper, a new\nand efficient branch and bound (BAB) method for selection of CVs using the MSV\nrule is proposed by posing the problem as a subset selection problem. In\ntraditional BAB algorithms for subset selection problems, pruning is performed\ndownwards (gradually decreasing subset size). In this work, the branch pruning\nis considered in both upward (gradually increasing subset size) and downward\ndirections simultaneously so that the total number of subsets evaluated is\nreduced dramatically. Furthermore, a novel bidirectional branching strategy to\ndynamically branch solution trees for subset selection problems is also\nproposed, which maximizes the number of nodes associated with the branches to be\npruned. Finally, by replacing time-consuming MSV calculations with novel\ndeterminant based conditions, the efficiency of the bidirectional BAB algorithm\nis increased further. Numerical examples show that with these new approaches,\nthe CV selection problem can be solved incredibly fast.<\/dc:description><dc:publisher>Elsevier Science B.V., Amsterdam.<\/dc:publisher><dc:date>2012-10-03T23:01:39Z<\/dc:date><dc:date>2012-10-03T23:01:39Z<\/dc:date><dc:date>2008-10-17T00:00:00Z<\/dc:date><dc:type>Article<\/dc:type><dc:identifier>Yi Cao, Vinay Kariwala, Bidirectional branch and bound for controlled variable\nselection. Part I: Principles and minimum singular value criterion, Computers &\nChemical Engineering, Volume 32, Issue 10, 17 October 2008, Pages 2306-2319<\/dc:identifier><dc:identifier>0098-1354<\/dc:identifier><dc:identifier>http:\/\/dx.doi.org\/10.1016\/j.compchemeng.2007.11.011<\/dc:identifier><dc:identifier>http:\/\/dspace.lib.cranfield.ac.uk\/handle\/1826\/3072<\/dc:identifier><dc:language>en_UK<\/dc:language><dc:rights>This is the author\u2019s version of a work that was accepted for publication in Computers &\nChemical Engineering. Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may have been made to this work since it was submitted for publication. A definitive version was subsequently published in Computers &\nChemical Engineering, Volume 32, Issue 10, 17 October 2008, Pages 2306-2319 DOI:10.1016\/j.compchemeng.2007.11.011<\/dc:rights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0098-1354","0098-1354"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2008,"topics":["Branch and bound","Control structure design","Controlled variables","Combinatorial optimization","Minimum singular value","Self-optimizing control"],"subject":["Article"],"fullText":"Accepted Manuscript\nTitle: Bidirectional branch and bound for controlled variable\nselection Part I. Principles and minimum singular value\ncriterion\nAuthors: Yi Cao, Vinay Kariwala\nPII: S0098-1354(09)00035-0\nDOI: doi:10.1016\/j.compchemeng.2009.01.014\nReference: CACE 3770\nTo appear in: Computers and Chemical Engineering\nReceived date: 10-6-2008\nRevised date: 11-1-2009\nAccepted date: 21-1-2009\nPlease cite this article as: Cao, Y., & Kariwala, V., Bidirectional branch\nand bound for controlled variable selection Part I. Principles and minimum\nsingular value criterion, Computers and Chemical Engineering (2008),\ndoi:10.1016\/j.compchemeng.2009.01.014\nThis is a PDF file of an unedited manuscript that has been accepted for publication.\nAs a service to our customers we are providing this early version of the manuscript.\nThe manuscript will undergo copyediting, typesetting, and review of the resulting proof\nbefore it is published in its final form. Please note that during the production process\nerrors may be discovered which could affect the content, and all legal disclaimers that\napply to the journal pertain.\nCA\nt\ns\np\nd\ns\nd\nb\nb\ns\n\u00a9\nK\n1\nc\nc\no\nC\ne\nt\nt\ni\ns\nr\ne\na\np\n0\ndus\ncri\npt\nARTICLE IN PRESS+ModelACE-3580; No. of Pages 14\nAvailable online at www.sciencedirect.com\nComputers and Chemical Engineering xxx (2008) xxx\u2013xxx\nBidirectional branch and bound for controlled variable selection\nPart I. Principles and minimum singular value criterion\nYi Cao a,\u2217, Vinay Kariwala b\na School of Engineering, Cranfield University, Cranfield, Bedford MK43 0AL, UK\nb Division of Chemical & Biomolecular Engineering, Nanyang Technological University, Singapore 637459, Singapore\nReceived 18 July 2007; received in revised form 12 November 2007; accepted 13 November 2007\nbstract\nThe minimum singular value (MSV) rule is a useful tool for selecting controlled variables (CVs) from the available measurements. However,\nhe application of the MSV rule to large-scale problems is difficult, as all feasible measurement subsets need to be evaluated to find the optimal\nolution. In this paper, a new and efficient branch and bound (BAB) method for selection of CVs using the MSV rule is proposed by posing the\nroblem as a subset selection problem. In traditional BAB algorithms for subset selection problems, pruning is performed downwards (gradually\necreasing subset size). In this work, the branch pruning is considered in both upward (gradually increasing subset size) and downward directions\nimultaneously so that the total number of subsets evaluated is reduced dramatically. Furthermore, a novel bidirectional branching strategy toanynamically branch solution trees for subset selection problems is also proposed, which maximizes the number of nodes associated with theranches to be pruned. Finally, by replacing time-consuming MSV calculations with novel determinant based conditions, the efficiency of theidirectional BAB algorithm is increased further. Numerical examples show that with these new approaches, the CV selection problem can be\nolved incredibly fast.\n2007 Elsevier Ltd. All rights reserved.\nomb\nV\ni\nf\ni\nt\np\nT\no\np\np\nw\nm\nh\nAcc\nep\nted\n Meywords: Branch and bound; Control structure design; Controlled variables; C\n. Introduction\nControl structure design (CSD) deals with the selection of\nontrolled, manipulated and measured variables, and the inter-\nonnections linking them. During the past few decades, a number\nf tools have been developed for various problems arising in\nSD; see e.g. Skogestad and Postlethwaite (1996). Though\nxtremely useful, a common feature of most of these tools is\nhat they require evaluation of all the possible alternatives. As\nhe number of alternatives grows rapidly with the problem size,\nn many practical situations it is impossible to do an exhaustive\nearch to find the optimal control structure. Thus, effective algo-\nithms are required to find the globally optimal solution without\nnumerating all the possible alternatives.A\nPlease cite this article in press as: Cao, Y., Kariwala, V., Bidirectional bra\n(2008), doi:10.1016\/j.compchemeng.2007.11.011\nIn this paper, we consider the selection of controlled vari-\nbles (CVs) from the available set of measurements. For this\nroblem, a survey of most of the available methods is given by\n\u2217 Corresponding author. Tel.: +44 1234 750111; fax: +44 1234 754685.\nE-mail address: y.cao@cranfield.ac.uk (Y. Cao).\nn\nr\nn\ni\nt\nM\np\n098-1354\/$ \u2013 see front matter \u00a9 2007 Elsevier Ltd. All rights reserved.\noi:10.1016\/j.compchemeng.2007.11.011inatorial optimization; Minimum singular value; Self-optimizing control\nan de Wal and de Jager (2001). Recently, Skogestad (2000)\nntroduced the concept of self-optimizing control, which is use-\nul for selecting CVs. The central idea is to select CVs such that\nn presence of disturbances, the loss incurred in implementing\nhe operational policy by holding these CVs at constant set-\noints is minimal, as compared to the use of an online optimizer.\nhe choice of CVs based on the general non-linear formulation\nf self-optimizing control can be time-consuming. To quickly\nre-screen the alternatives, Skogestad and Postlethwaite (1996)\nresented the approximate minimum singular value (MSV) rule,\nhere the CVs are selected such that the MSV of the scaled gain\natrix is maximum among different alternatives. The MSV rule\nas recently been revisited in Halvorsen, Skogestad, Morud, and\nlstad (2003). Though efficient for evaluation of a single alter-\native in comparison with the non-linear formulation, the MSV\nule still suffers from computational intractability as the MSV\need to be evaluated for every feasible subset of measurementsPage 1 of 14\nnch and bound for controlled variable selection, Comput Chem Eng\nn order to find the optimal solution. For example, the selec-\nion of 5 CVs from 50 measurements requires C550 \u2248 2.2 \u00d7 106\nSV evaluations. This drawback of the MSV rule motivates the\nresent work.\nMARTICLE IN+ModelCACE-3580; No. of Pages 14\n2 Y. Cao, V. Kariwala \/ Computers and Chem\nNomenclature\na column vector (lower case bold face letter)\nA matrix (upper case bold face letter)\nB best available lower bound on J\nC candidate set of a node\nCnm binomial coefficient of m choose n\nF fixed set of a node\nG full m \u00d7 n matrix for row selection\nGX sub-matrix of G consisting of rows with indices\nin set X\nJ selection criterion (to be maximized)\n\u00afJn(X) upper bound on J for all n-element subsets of X\nm total number of candidate variables\nn target size of the subset to be selected\nQ defined as Q = GGT\nQX,Y sub-matrix of Q consisting of rows and columns\nwith indices in sets X and Y, respectively\nS union of the sets F and C, i.e. S = F \u222a C\nS a two-tuple, S = (F,C) represents a node in the\nsearch tree\nXt subscript t represents the size of the set X\nXi superscript i represents the index of the sub- or\nsuperset obtained from X\nGreek symbols\n\u03b1Si ith downwards pruning index based on set S\n\u03b2Fi ith upwards pruning index based on fixed set F\n\u03bbi ith largest eigenvalue of a square matrix\n\u03bb minimum eigenvalue of a square matrix\n\u03c3i ith largest singular value of a matrix\ns\nr\nC\no\ni\nt\nB\np\nu\na\nt\nu\n2\nb\ne\nt\na\ni\na\no\nT\nd\no\ni\nP\nr\nm\nc\ne\ni\nL\nh\nu\nI\nt\na\nu\nm\na\np\no\nr\nC\na\nt\ns\na\nS\nC\ni\nu\na\nt\no\np\n(\n(Ac\nce\npte\nd \u03c3 minimum singular value of a matrix\nKariwala and Skogestad (2006) have demonstrated that the\ncaling of the gain matrix, required for application of the MSV\nule, can be performed prior to CV selection. In this sense,\nV selection based on MSV rule can be seen as selection\nf the rows of a matrix such that the MSV of the result-\nng square matrix is maximum among all the alternatives. For\nhis problem, an early sub-optimal solution was proposed by\nusinger and Golub (1965) based on QR factorization. Other\nossibilities for obtaining sub-optimal solutions include the\nse of non-square relative gain array (Cao & Rossiter, 1997)\nnd a sequential approa h, where one row is selected at a\nime such that the least non-zero singular value is maximum,\nntil the resulting matrix is square (Kariwala & Skogestad,\n006).\nThe problem of MSV maximization by row selection can also\ne seen as a subset selection problem, for which many differ-\nnt algorithms are available in literature (Chen, 2003). Among\nhese available methods, however, only the exhaustive searchPlease cite this article in press as: Cao, Y., Kariwala, V., Bidirectional bra\n(2008), doi:10.1016\/j.compchemeng.2007.11.011\nnd branch and bound (BAB) method guarantee global optimal-\nty (Chen, 2003). In comparison to exhaustive search, a BAB\npproach gains its efficiency by pruning certain branches with-\nut evaluating nodes (subsets) associated with those branches.an\nus\ncri\npt\n PRESS\nical Engineering xxx (2008) xxx\u2013xxx\no solve subset selection problems, a BAB algorithm was first\nescribed by Narendra and Fukunaga (1977) in the context\nf pattern recognition. This algorithm has been subsequently\nmproved by several researchers; see e.g. Chen (2003), Somol,\nudil, and Kittler (2004), and Yu and Yuan (1993). This algo-\nithm is powerful because any subset selection problem with a\nonotonic relationship between the subset size and selection\nriterion can be solved using this method.\nThe usefulness of BAB method for solving mixed integer lin-\nar and non-linear programming (MILP and MINLP) problems\ns well-known (Floudas, 1995; Nikolaos & Sahinidis, 2000).\nuyben and Floudas (1994) and Kookos and Perkins (2001)\nave applied the BAB algorithm to problems arising in CSD\nsing the general MINLP and MILP frameworks, respectively.\nn these approaches, the lower and upper bounds on the objec-\nive functions are computed by relaxing the integer variables\ns continuous variables. The optimization problems resulting\npon integer relaxation can be non-convex and thus global opti-\nality cannot be guaranteed. In comparison, the use of the BAB\nlgorithm of Narendra and Fukunaga (1977) for subset selection\nroblems is advantageous, as the need for solving the relaxed\nptimization problems is avoided. Furthermore, the BAB algo-\nithm can also be easily tailored to suit the specific problems in\nSD as demonstrated in Cao and Saha (2005), Cao, Rossiter,\nnd Owens (1998), and Kariwala and Skogestad (2006), where\nhe globally optimal solution is obtained without exhaustive\nearch.\nThat MSV satisfies monotonicity requirement and renders the\npplication of BAB method has been shown by Cao et al. (1998).\nimilar to most of the problems dealing with subset selection,\nao et al. (1998) use downwards pruning (gradually decreas-\nng subset size). Logically, pruning can also be conducted\npwards (gradually increasing subset size). Recently, Kariwala\nnd Skogestad (2006) showed that MSV satisfies upward mono-\nonicity and proposed a new BAB method. For maximization\nf MSV, we introduce the following novel concepts in this\naper:\na) Bidirectional pruning: Intuitively, the upwards pruning\nbased BAB method is more efficient than the downwards\npruning based BAB method, when only a few variables\nare to be selected from a large number of measurements\nand vice versa. We show that the solution tree for upward\n(downward) search contains a hidden downward (upward)\nstructure. This observation enables us to combine the two\npruning strategies such that nodes that cannot lead to an opti-\nmal solution are discarded more quickly and fewer nodes\nneed to be evaluated.\nb) Bidirectional branching: The efficiency of a BAB method\nalso depends strongly on the branching strategy. By ana-\nlyzing advantages and disadvantages of the upwards and\ndownwards branching principles, a novel bidirectional\nbranching strategy is developed. The proposed branchingPage 2 of 14\nnch and bound for controlled variable selection, Comput Chem Eng\napproach dynamically decides upon the best search direction\n(upwards or downwards) and ordering of sub- or super-\nnodes such that the number of nodes associated with a branch\nto be pruned is maximized.\na IN+ModelC\nChem\n(\nt\nr\nt\np\nt\nr\nI\n6\na\n2\nl\nd\nm\nB\ns\na\nt\nr\ni\nt\np\nn\ne\nC\nr\nm\nm\n(\nm\n1\nt\nc\nM\ns\nc\ns\np\np\na\nS\nc\nt\nS\ni\n6\n2\ns\nB\n2\nb\np\ns\nJ\nS\ns\nv\nb\nc\ns\ne\nr\nd\n2\nb\nX\nJ\nF\nB\nT\nJ\no\nw\nR\np\ns\n(\nJ\nt\na\nJAc\nce\npte\nd M\nARTICLEACE-3580; No. of Pages 14\nY. Cao, V. Kariwala \/ Computers and\nc) Determinant condition: BAB method spends most of its time\nin evaluation of non-optimal nodes. The number of float-\ning point operations required for MSV calculation increases\npolynomially (circa O(n3) for n \u00d7 n matrix; Golub & Van\nLoan, 1996). We present a computationally inexpensive con-\ndition based on determinants, which can be used to quickly\ndecide upon whether expansion of a node can lead to the\noptimal solution.\nThese three novel features greatly improve the efficiency of\nhe BAB method in finding the optimal set of CVs using the MSV\nule. The improvement in computational efficiency is so substan-\nial that for the benchmark Hydrodealkylation of Toluene (HDA)\nrocess (Araujo & Skogestad, 2007), which requires selec-\nion of 8 CVs among 129 measurements, the proposed method\nequires less than 0.1 s on a Windows XP SP2 notebook with an\nntel\u00aeCore TM Duo Processor T2500 (2.0 GHz, 2 MB L2 Cache,\n67 MHz FSB) using MATLAB\u00ae R2006b. In comparison, the\nvailable methods (Cao et al., 1998; Kariwala & Skogestad,\n006) require more than an hour for solving the same prob-\nem. Hence, the concepts of bidirectional pruning, branching and\neterminant condition reduce the solution time by four orders of\nagnitude for the HDA process. We also show that the proposed\nAB algorithm scales well with problem dimensions using\neveral randomly generated matrices. In this sense, the BAB\nlgorithm proposed in this paper enables the practicing engineer\no efficiently select CVs for large-scale processes using the MSV\nule.\nThough in this paper, the problem of MSV maximization\ns considered for selection of individual measurements as CVs,\nhis problem has many other engineering applications. For exam-\nle, in the local self-optimizing control framework, the designer\needs to select a specified number of measurements, whose lin-\nar combinations found using null space method can be used as\nVs. Alstad (2005) has shown that these measurements can be\neasonably selected by squaring a matrix such that the MSV is\naximized. Similarly, based on controllability arguments, MSV\naximization through selection of columns of the gain matrix\nor rows of its transpose) is useful for identifying appropriate\nanipulated variables for CSD (Skogestad and Postlethwaite,\n996). Mavroidis, Dubowsky, and Thomas (1997) have shown\nhat the sensor locations for long reach manipulator systems\nan be optimally selected by MSV maximization. De Hoog and\nattheij (2007) discuss the use of MSV maximization for least\nquares problems for ill-conditioned matrices. In summary, the\nomputationally efficient method for MSV maximization pre-\nented in this paper is applicable to a wide variety of engineering\nroblems.\nThe rest of the paper is organized as follows. The general\nrinciples of unidirectional BAB methods for subset selection\nre illustrated through upward and downward solution trees in\nection 2. Then, the novel bidirectional pruning and branching\noncepts are proposed in Section 3. These concepts are appliedPlease cite this article in press as: Cao, Y., Kariwala, V., Bidirectional bra\n(2008), doi:10.1016\/j.compchemeng.2007.11.011\no the MSV rule and efficient BAB algorithms are developed in\nection 4. Developed algorithms are tested with several numer-\ncal examples in Section 5 and the work is concluded in Section\n.\no\nF\n(\nsnu\nsc\nrip\nt\n PRESS\nical Engineering xxx (2008) xxx\u2013xxx 3\n. Unidirectional branch and bound\nIn this section, we formally define the problem of subset\nelection and briefly introduce the principles of unidirectional\nAB methods for solving the same.\n.1. Subset selection\nLet Xm = {xi|i = 1, 2, . . . , m} be an m-element set and Xn\ne an n-element subset selected from Xm. A subset selection\nroblem with criterion J is to find the globally optimal n-element\nubset, X\u2217n such that\n(X\u2217n) = maxXn\u2282XmJ(Xn) (1)\nince there are Cnm = m!\/[(m \u2212 n)!n!] candidate n-element sub-\nets available for selection, the total number of candidates grows\nery rapidly as m and n increase and thus exhaustive search\necomes unviable for large m and n. BAB is one of the effi-\nient approaches, which are able to find the globally optimal\nubset without exhaustive search. BAB search can be conducted\nither downwards or upwards, where the elements are gradually\nemoved from or added to the subsets, respectively, until the\nesired subset size is reached.\n.2. Downward branch and bound\nTo present the downward BAB approach, let \u00afJn(Xs), s > n,\ne a downwards upper bound on J over all n-element subsets of\ns:\n\u00af\nn(Xs) \u2265 maxXn\u2286XsJ(Xn) (2)\nurther, let B be a lower bound of J(X\u2217n):\n\u2264 J(X\u2217n) (3)\nhen, it follows that\n(Xn) < J(X\u2217n), \u2200Xn \u2286 Xs, if \u00afJn(Xs) < B (4)\nCondition (4) indicates that none of subsets of Xs can be the\nptimal subset. Therefore, Xs and its subsets can be discarded\nithout further evaluation.\nemark 1 (Downward monotonicity). The above BAB princi-\nle requires an estimation of a downwards upper bound on the\nelection criterion J over all n-element subsets of a given set, i.e.\n2). However, if J satisfies downward monotonicity, i.e.:\n(Xs) \u2264 J(Xt), if Xs \u2286 Xt (5)\nhen a simple downwards upper bound estimation can be adopted\ns\n\u00af\nn(Xs) = J(Xs) (6)\nThe first downward BAB subset selection algorithm basedPage 3 of 14\nnch and bound for controlled variable selection, Comput Chem Eng\nn an asymmetrical solution tree was proposed by Narendra and\nukunaga (1977) and further improved by several researchers\nChen, 2003; Somol et al., 2004; Yu & Yuan, 1993). To remove\nubset redundancy, the tree is constructed asymmetrically, i.e.\nARTICLE IN PRESS+ModelCACE-3580; No. of Pages 14\n4 Y. Cao, V. Kariwala \/ Computers and Chemical Engineering xxx (2008) xxx\u2013xxx\nr m =\nn\ne\ns\nn\na\nl\ne\nd\nw\nn\nb\nl\ns\n(\nfi\nc\nb\nD\ns\ni\nD\nc\nr\ne\nc\na\nD\nF\nd\nf\nd\nF\nC\no\nn\nR\nt\nr\na\nS\ni\ni\ns\nF\nw\ni\nc\nt\ni\n3\n2\na\no\nu\nt\nb\nu\nJ\nT\nJ\nC\na\nf\nR\ns\nJ\nt\naAc\nce\npte\nd M\nFig. 1. Solution trees fo\nodes at the same level have different number of branches. An\nxample of a downwards solution tree for selecting a 2-element\nubset from a 6-element set is shown in Fig. 1. In this tree, the top\node is the root of the tree, which represents the set containing\nll the available elements. From the top to the bottom, at each\nevel, a node represents a subset obtained by eliminating one\nlement from its parent set. Labels at nodes denote the elements\niscarded there. The bottom nodes of the tree are terminal nodes\nhich represent all possible n-element subsets. For selecting an\n-element subset from anm-element set, (m \u2212 n) elements are to\ne discarded. Hence, the downward search tree has (m \u2212 n + 1)\nevels. All nodes at the same level have the same subset size.\nFor simple representation of the asymmetrical tree and recur-\nive implementation of downwards BAB algorithm Cao et al.\n1998) and Cao and Saha (2005) introduced the concepts of\nxed and candidate sets of a node. In this paper, these con-\nepts are instrumental in derivation of bidirectional pruning and\nranching strategies discussed in Section 3.\nefinition 1 (Downwards fixed set F). The downwards fixed\net, F \u2286 X is the largest subset of X, whose elements are always\nncluded in all the sub-nodes of X.\nefinition 2 (Downwards candidate set C). The downwards\nandidate set, C \u2286 X is the set of elements which can be freely\nemoved from X, i.e. C = X \\ F , where the symbol \\ denotes\nxclude.\nFixed set and candidate set are complementary, i.e. s = f +\nif Xs = Ff \u222a Cc. The downward solution tree is branched\nccording to the following branching rule.\nefinition 3 (Downwards branching rule). The node Xs =\nf \u222a Cc has (n \u2212 f + 1) branches (sub-nodes), which are\nenoted as Xis\u22121 = Fifi \u222a Cici = Xs \\ xi, i = 1, 2, . . . , (n \u2212+ 1), arranged from the left to the right. Fixed sets and can-\nidate sets of sub-nodes are defined as\ni\nfi\n= Ff \u222a {x1, \u00b7 \u00b7 \u00b7 , xi\u22121} (7)\ni\nci\n= Cc \\ {x1, \u00b7 \u00b7 \u00b7 , xi} (8)\nCao and Saha (2005) have shown that the branching rulePlease cite this article in press as: Cao, Y., Kariwala, V., Bidirectional bra\n(2008), doi:10.1016\/j.compchemeng.2007.11.011\nf Definition 3 is complete without any redundancies, i.e. each\n-element subset belongs to one and only one branch.\nemark 2 (Hidden upward structure). According to (7), for\nhe sub-nodes originating from the same parent node, elements\nJ\naan\nus\ncri\npt\n6 and n = 2 selection.\nemoved from the candidate sets of the left sub-nodes are passed\ns fixed elements to the right sub-nodes (Cao et al., 1998; Cao &\naha, 2005). Therefore, the fixed set of the ith sub-node has (f +\n\u2212 1) elements, i.e. fi = f + i \u2212 1 and Fifi = Fif+i\u22121. More\nmportantly, the fixed sets of all the sub-nodes derived from the\name parent satisfy the following relationship:\n1\nf \u2282 F2f+1 \u2282 \u00b7 \u00b7 \u00b7 \u2282 Fn\u2212f+1n (9)\nhere F1f is the fixed set of the leftmost sub-node and F\nn\u2212f+1\nn\ns the fixed set of the rightmost sub-node. This relationship indi-\nates that there is a hidden upwards (fixed sets expand from left\no right) structure in the downwards search tree. This property\ns exploited to derive a bidirectional pruning strategy in Section\n.1.\n.3. Upward branch and bound\nLogically, subset selection can also be performed upwards\nlthough this has not been discussed in the literature up to best of\nur knowledge. An upward search starts from an empty set, grad-\nally expands the superset element-by-element, until it reaches\nhe required subset size.\nTo present the principle of upward BAB, let B be a lower\nound of J(X\u2217n) as defined in (3) and \u00afJn(Xs), s < n, be an\npwards upper bound of J over all n-element supersets of Xs:\n\u00af\nn(Xs) \u2265 maxXn\u2287XsJ(Xn) (10)\nhen\n(Xn) < J(X\u2217n), \u2200Xn \u2287 Xs, if \u00afJn(Xs) < B (11)\nondition (11) ensures that none of supersets of Xs can be glob-\nlly optimal. Thus, Xs and its supersets can be pruned without\nurther consideration.\nemark 3 (Upward monotonicity). If the selection criterion\natisfies the upward monotonicity:\n(Xs) \u2264 J(Xt), if Xs \u2287 Xt (12)\nhen a simple upwards upper bound estimation can be adopted\nsPage 4 of 14\nnch and bound for controlled variable selection, Comput Chem Eng\n\u00af\nn(Xs) = J(Xs) (13)\nThe upward BAB method can also be represented using an\nsymmetric tree. An example of such a tree for selecting 2 ele-\na IN+ModelC\nChem\nm\nt\ne\ne\na\nb\nh\nl\na\nD\nh\nD\nd\nf\nl\nD\nd\na\na\na\nf\nC\nI\nC\nu\ns\nP\nr\ne\nP\nt\na\nX\nF\no\no\nn\nn\na\nn\nb\nC\nb\nC\nS\nC\nn\nt\ns\nf\nw\nt\nR\nc\ns\nc\nC\nS\nf\n(\nT\ns\nc\nf\nd\n3\nw\no\n3\ns\ni\nt\ns\nn\nS\ns\na\nf\np\np\nn\nb\nsAc\nce\npte\nd M\nARTICLEACE-3580; No. of Pages 14\nY. Cao, V. Kariwala \/ Computers and\nents from a 6-element set is shown in Fig. 1(b). In an upward\nree, the bottom node is the root of the tree, which represents an\nmpty set. A node represents a superset obtained by adding one\nlement to its parent set. Labels at nodes denote the elements\ndded there. At each level, the sizes of the supersets increase\ny one until the top level is reached, where the terminal nodes\nave n elements. Thus, from bottom to top, the tree has (n + 1)\nevels. The fixed and candidate sets for upward BAB are defined\ns follows.\nefinition 4 (Upwards fixed set F). The upwards fixed set, F\nas the same elements as the node itself, i.e. F = X.\nefinition 5 (Upwards candidate set C). The upwards candi-\nate set, C is the set of elements which are not in X, but can be\nreely selected to append X.\nThe upward solution tree is branched according to the fol-\nowing branching rule.\nefinition 6 (Upwards branching rule). A node Xf with candi-\nate set Cc has (f + c \u2212 n + 1) branches (super-nodes), which\nre denoted as Xif+1 = Xf \u222a xi, i = 1, 2, . . . , (f + c \u2212 n + 1)\nnd are arranged from the left to the right. Here, Fif+1 = Xif+1\nnd the candidate set of the ith super-node, Cici is defined by the\nollowing branching rule:\ni\nci\n= Cc \\ {x1, . . . , xi} (14)\nn words, Cici is constructed by eliminating those elements of\nc, which have been fixed in its left super-nodes.\nSimilar to the downward BAB method, the branching rule for\npward BAB method is also complete without redundancies, as\nhown next.\nroposition 1 (Upward branch completeness). The branching\nule in Definition 6 is complete without any redundancies, i.e.\nach n-element subset belongs to one and only one branch.\nroof. Let F(Xf ) denote all the supersets generated by\nhe node Xf . Consider, 1 \u2264 i < j \u2264 (f + c \u2212 n + 1). Then,\nccording to the branching rule in Definition 6, xi \u2208Xs, for all\ns \u2208F(Xif+1). However, xi \/\u2208 Xt , for all Xt \u2208F(Xjf+1). Thus,\n(Xif+1) \u2229 F(Xjf+1) = \u2205, i.e. no superset belongs to more than\nne branch. This proves that each n-element subset belongs to\nnly one branch.\nNow, as the solution tree is properly branched so that there is\no redundancy in the terminal nodes, it suffices to prove that the\numbers of n-element subsets that can be reached from any node\nnd its super-nodes are the same. We note that Xf has Cn\u2212fc valid\n-element supersets, whilst Cn\u2212f\u22121c\u2212i valid n-element supersets\nelong to its ith super-node Xif+1. According to the identity\nn\nm + Cn\u22121m = Cnm+1, the total number of n-element supersets that\nelong to the two rightmost super-nodes of Xf is\nn\u2212f\u22121 n\u2212f\u22121 n\u2212f n\u2212f\u22121 n\u2212fPlease cite this article in press as: Cao, Y., Kariwala, V., Bidirectional bra\n(2008), doi:10.1016\/j.compchemeng.2007.11.011\nn\u2212f\u22121 + Cn\u2212f = Cn\u2212f + Cn\u2212f = Cn\u2212f+1 (15)\nimilarly, the three rightmost super-nodes of Xf contain\nn\u2212f\nn\u2212f+1 + Cn\u2212f\u22121n\u2212f+1 = Cn\u2212fn\u2212f+2 (16)\nt\np\nd\nanu\nsc\nrip\nt\n PRESS\nical Engineering xxx (2008) xxx\u2013xxx 5\n-element supersets. By recursively applying these arguments,\nhe number of n-element supersets that belong to at least one of\nuper-nodes Xif+1, i = 1, 2, . . . , (f + c \u2212 n + 1) is given as\n+c\u2212n+1\u2211\nk=1\nCn\u2212f\u22121c\u2212k = Cn\u2212fc (17)\nhich is same as the number of n-element supersets of Xf and\nhus shows the upward branch completeness. \u0002\nemark 4 (Hidden downward structure). According to (14),\ni = c \u2212 i and for the super-nodes derived from the same parent\net, the candidate set of a left super-node is a superset of the\nandidate sets of all the super-nodes on its right:\n1\nc\u22121 \u2283 C2c\u22122 \u2283 \u00b7 \u00b7 \u00b7 \u2283 Cf+c\u2212n+1n\u2212f\u22121 (18)\nimilarly, the unions of the fixed and candidate sets satisfy the\nollowing relationship:\nF1f+1 \u222a C1c\u22121) \u2283 (F2f+1 \u222a C2c\u22122) \u2283 \u00b7 \u00b7 \u00b7\n\u2283 (Fc+f\u2212n+1f+1 \u222a Cf+c\u2212n+1n\u2212f\u22121 ) (19)\nhe above relationship shows that there is a hidden downward\ntructure (from left to right) in an upwards search tree, where the\nandidate sets and, the unions of fixed and candidate sets shrink\nrom left to right. Similar to Remark 2, this property is used to\nerive a bidirectional pruning strategy in Section 3.1.\n. Bidirectional branch and bound\nIn this section, the attractive features of the upward and down-\nard approaches are combined to improve the overall efficiency\nf the BAB method.\n.1. Bidirectional pruning\nThe efficiency of a BAB approach depends on its pruning\ntrategy. Downward and upward search approaches provide two\nndependent pruning strategies, as given in (4) and (11), respec-\nively. According to the Remark 2, there is a hidden upward\ntructure in the downward tree, where the fixed sets of the sub-\nodes of the same branch expand upwards from left to right.\nimilarly, following Remark 4, there is a hidden downward\ntructure in the upward tree, where the union of super-nodes\nnd its candidate sets in the same branch shrink downwards\nrom left to right. Therefore, it is possible to adopt the upwards\nruning strategy in a downward solution tree and the downwards\nruning strategy in an upward solution tree so that non-optimal\nodes can be discarded more quickly and fewer nodes need to\ne evaluated. In this way, the efficiency of BAB search can be\nignificantly improved.\nMore specifically, consider a nodeX = F \u222a C in a downwardPage 5 of 14\nnch and bound for controlled variable selection, Comput Chem Eng\nree. If \u00afJn(X) < B, then, according to (4) downwards pruning is\nerformed, i.e. all the sub-nodes of X are pruned, as shown in the\nashed box of Fig. 2(a). On the other hand, if \u00afJn(F ) < B, then\nccording to (11) and the hidden upward structure, an upwards\nARTICLE IN PRESS+ModelCACE-3580; No. of Pages 14\n6 Y. Cao, V. Kariwala \/ Computers and Chemical Engineering xxx (2008) xxx\u2013xxx\npruni\np\nt\nu\nJ\nt\nc\nh\nt\nT\np\nt\nP\nc\nb\ns\ni\no\na\nA\nb\nd\nc\nr\nt\nT\ns\ns\nd\ns\nb\ne\nL\na\nJ\nJ\nt\nd\nS\nP\nn\no\nt\na\nf\nL\na\nJ\nJ\nt\nd\nS\na\ne\nt\nf\ns\nd\ns\ni\na\nr\np\nb\npAc\nce\npte\nd M\nFig. 2. Bidirectional pruning in solution trees. (a) Bidirectional\nruning can be conducted, i.e. the sub-nodes and all the nodes to\nhe right of X are removed, as shown in the gray box of Fig. 2(a).\nThe bidirectional pruning strategy is also applicable to an\npward tree. For the node X = F with the candidate set C, if\n\u00af\nn(F ) < B, normal upwards pruning is conducted, as shown in\nhe gray box of Fig. 2(b). In addition, if the downwards pruning\nondition \u00afJn(F \u222a C) < B is satisfied, according to (4) and the\nidden downward structure, the super-nodes and all the nodes to\nhe right of X are pruned, as shown in the dashed box of Fig. 2(b).\nhe findings of this section are summarized by the following\nroposition, where a node in a solution tree is represented by a\nwo-tuple, S = (F,C).\nroposition 2 (Bidirectional pruning). A node S = (F,C) may\nontain the globally optimal subset, i.e. X\u2217n \u2282 (F \u222a C) only if\noth of the requirements Jn(F ) > B and \u00afJn(F \u222a C) > B are\natisfied simultaneously.\nIn comparison to unidirectional BAB methods, Proposition 2\ns more efficient for pruning non-optimal nodes at an early stage\nf the search. The key requirement for using Proposition 2 is the\nvailability of both upwards and downwards upper bounds on J .\ns shown in Section 4.1, these two upper bounds for MSV can\ne obtained based on monotonicity. Finding the upwards and\nownwards upper bounds for other commonly used selection\nriteria is currently under research.\nFor a node S = (F,C), downwards pruning is an operation to\neduce the size of C, whilst upwards pruning is the manipulation\no increase the fixed set size by moving an element from C to F.\nhe downwards and upwards pruning strategies are uniformly\ntated in the following lemmas, which are applicable to both the\nolution trees simultaneously. Instead of pruning a node, as is\none traditionally, the proposed results discuss pruning on the\nub- and super-nodes. This is not only more efficient in pruning\nut also facilitates the development of efficient upper bound\nstimation algorithms as discussed in Section 4.\nemma1 (Downwards subset pruning). Let B be the best avail-Please cite this article in press as: Cao, Y., Kariwala, V., Bidirectional bra\n(2008), doi:10.1016\/j.compchemeng.2007.11.011\nble lower bound of J. Consider a node, S = (Ff ,Cc) with\n\u00af\nn(Ff \u222a Cc) > B. For xi \u2208Cc, if\n\u00af\nn(Ff \u222a Cc \\ xi) < B (20)\nt\ni\nP\nxan\nus\ncri\nptng in downward tree. (b) Bidirectional pruning in upward tree.\nhen downwards pruning is performed by replacing S with Si\nefined as\ni = (Ff \u222a xi, Cc \\ xi) (21)\nroof. Based on the branching rule in Definition 3, the terminal\nodes of (Ff ,Cc) are the same as the union of the terminal nodes\nf (Ff ,Cc \\ xi) and (Ff \u222a xi, Cc \\ xi). When (20) holds, any\nerminal node of (Ff ,Cc \\ xi) cannot be the optimal solution\nnd thus it suffices to evaluate only the node (Ff \u222a xi, Cc \\ xi)\nurther. \u0002\nemma 2 (Upwards superset pruning). Let B be the best avail-\nble lower bound of J. Consider a node, S = (Ff ,Cc) with\n\u00af\nn(Ff ) > B. For xi \u2208Cc, if\n\u00af\nn(Ff \u222a xi) < B (22)\nhen upwards pruning is performed by replacing S with Si\nefined as\ni = (Ff ,Cc \\ xi) (23)\nThe proof of Lemma 2 is similar to the proof of Lemma 1\nnd is omitted.\nThe pruning approaches described in Lemmas 1 and 2 are\nquivalent to placing the sub- and super-node, respectively, at\nhe leftmost position of the solution tree before pruning it. There-\nore, in general, they are more efficient than the traditional\ntrategy of pruning directly from a structured tree. For example,\nownwards pruning element 2 in the downward tree removes\nub-nodes shown in the dashed box of Fig. 2(a), which elim-\nnates only four terminal nodes. On the other hand, with the\npproach given in Lemma 1, C26 \u2212 C15 = 10 terminal nodes are\nemoved.\nAt a specific node, S = (F,C) with given B, the downwards\nruning only depends on F \u222a C. Therefore, all elements in C can\ne simultaneously checked against (20). Similarly, the upwards\nruning only depends on F. Hence, the upwards pruning condi-Page 6 of 14\nnch and bound for controlled variable selection, Comput Chem Eng\nion (22) can also be simultaneously determined on all elements\nn C. These insights lead to Proposition 3.\nroposition 3 (Multiple pruning). If there are k elements,\n1, . . . , xk, which satisfy (20), then these k elements can be\naARTICLE IN PRESS+ModelCACE-3580; No. of Pages 14\nY. Cao, V. Kariwala \/ Computers and Chemical Engineering xxx (2008) xxx\u2013xxx 7\nching\ns\nS\nS\nc\nS\ns\np\nc\nu\nB\ni\ni\n3\nb\nr\n(\nr\na\na\nn\nB\nd\nb\nB\nb\nt\nt\na\ni\ns\nc\nw\ni\no\ne\nc\nt\nw\nt\nb\nn\ne\nt\no\nm\nt\no\ns\nr\np\nr\nt\nr\nfi\nf\nr\nt\nt\nb\ns\ntwo sub-nodes with Cnm\u22121 (downward) and Cn\u22121m\u22121 (upward) ter-\nminal nodes, respectively. If Cnm\u22121 > Cn\u22121m\u22121, then the upward\nsub-node can be evaluated first and vice versa. For example, inAc\nce\npte\nd M\nFig. 3. Bidirectional branching in solution trees. (a) Binary bran\nimultaneously pruned downwards by replacing S with\n\u2032 = (Ff \u222a {x1, . . . , xk}, Cc \\ {x1, . . . , xk}) (24)\nimilarly, if k elements, x1, . . . , xk satisfy (22), then all elements\nan be simultaneously pruned upwards by replacing S with\n\u2032 = (Ff ,Cc \\ {x1, . . . , xk}) (25)\nWhen successful upwards pruning in (25) is conducted, the\net F \u222a C shrinks. Thus the downwards pruning check can be\nerformed again on the new set F \u222a C. Similarly, after suc-\nessful downwards pruning in (24), the set F expands and the\npwards pruning check can be conducted again on the new set F.\ny applying these two pruning strategies alternately, the min-\nmal candidate set can be obtained. An algorithm that shows\nmplementation of this idea is given in Section 4.2.\n.2. Bidirectional branching\nThe efficiency of a BAB approach is also strongly related to its\nranching strategy. This happens as the search tree is asymmet-\nic, i.e. nodes on the left of the search tree have more sub-nodes\nsuper-nodes) than those on the right. There is no compulsory\nule for a given nodeS = (F,C) to be on the left or on the right of\nsearch tree. However, it is desired that a node to be pruned has\ns many sub-nodes or super-nodes as possible so that the total\number of nodes to be evaluated is minimized. In bidirectional\nAB, this issue is addressed by dynamically selecting the search\nirection (upwards or downwards) and using a simplest-first plus\nest-first search strategy so that the efficiency of bidirectional\nAB is maximized.\nA node that satisfies the conditions in Proposition 2 needs to\ne evaluated further either by expanding it in the upward direc-\nion or by shrinking it in the downward direction. To decide upon\nhe search direction, we organize the solution tree bidirection-\nlly using the concepts of fixed and candidate sets. As shown\nn Fig. 3(a), subsets of the root node, which has C26 2-element\nubsets, can be divided into two groups. The left group with C25\nandidates is obtained by removing element 1 from the root,Please cite this article in press as: Cao, Y., Kariwala, V., Bidirectional bra\n(2008), doi:10.1016\/j.compchemeng.2007.11.011\nhilst the right group with C15 terminal nodes is derived by fix-\nng element 1. Similarly, in the upward tree, the left group is the\nne with element 1 fixed whilst the right group is the one with\nlement 1 removed.nu\nsc\nrip\ntin downward tree. (b) Bidirectional branching in upward tree.\nIn general, the operation of discarding an element from the\nandidate set or fixing this element by moving this element from\nhe candidate set to the fixed set, represents a binary branching,\nhich can be applied to any node of a solution tree. Such a solu-\nion tree for a C26-selection problem is shown in Fig. 4. In the\ninary tree, the decision element for each node is denoted under-\neath the node. The left sub-node is the one with the decision\nlement removed whilst the right sub-mode is constructed with\nhe decision element fixed. The total number of terminal nodes\nf a sub-node is marked in the circle. All terminal nodes are\narked with gray circles. Both upward and downward solution\nrees can be uniformly represented in the binary tree. Evaluation\nf the right sub-node increases the fixed set size, hence repre-\nents an upward search, whilst evaluation of the left sub-node\neduces the candidate set resulting in a downward search. Com-\naring Fig. 4 with Fig. 3(a) and (b), it can been seen that the\night-first search strategy on the downward tree is equivalent to\nhe right-first search strategy on the binary tree. Similarly the\night-first search strategy on the upward tree is the same as left-\nrst search strategy on the binary tree. To take advantage of this\nact, search on the binary tree can be freely chosen to be either\night-first or left-first at each node based on a certain criterion\no achieve maximum efficiency.\nThe maximum efficiency is achieved by pruning as many\nerminal nodes as possible with as little evaluations as possi-\nle. Therefore, an intuitive rule is to perform the simplest-first\nearch. In Fig. 4, a node with Cnm terminal elements results inPage 7 of 14\nnch and bound for controlled variable selection, Comput Chem Eng\nFig. 4. A bidirectional branching example.\n IN+ModelC\n8 Chem\nt\nb\nm\no\nf\nu\nd\na\no\nt\nl\nr\nn\nt\na\np\n(\nt\nl\nr\nw\n4\nu\nt\na\nh\n\u03c3\nI\ns\nw\nt\nr\n4\nf\nT\nr\nL\nA\nw\nt\nT\n\u03bb\nP\nA\n[\n\u03c3\n\u03c3\nP\n\u03c3\nT\n(\n\u03c3\na\nR\no\np\np\nf\nw\nc\nc\nc\nt\ns\nr\nn\nt\ne\nt\n(\ne\n4\ni\nJ\na\nm\nu\nl\n(\nnAc\nce\npte\nd M\nARTICLEACE-3580; No. of Pages 14\nY. Cao, V. Kariwala \/ Computers and\nhe binary tree shown in Fig. 4, most of the times search should\ne performed right-first except at the node with C23 terminal ele-\nents, where the left sub-node should be evaluated first. Based\nn this discussion, the bidirectional search rule is represented as\nollows:\npward search, if 2(n \u2212 f ) \u2264 c (26)\nownward search, if 2(n \u2212 f ) > c (27)\nEfficiency can be improved further by selecting an appropri-\nte element as the decision element at each sub-node. For upward\nr downward search, the decision element should be selected as\nhe one with largest upwards upper bound given by (10) or the\nargest downwards upper bound given by (2) (best-first search),\nespectively. In traditional downwards BAB algorithms, sub-\nodes are usually sorted according to their criterion values so\nhat from left to right, criterion values of sub-nodes are in an\nscending order. This way, nodes which are more likely to be\nruned have more sub-nodes and overall efficiency is improved\nRidout, 1988). However, sorting may incur significant computa-\nional overheads especially when the candidate set is relatively\narge. The decision element selection strategy proposed here\nequires comparison of only the maximum or minimum values,\nhich is more efficient than sorting.\n. Bidirectional controlled variable selection\nIn this section, the combinatorial problem of selecting CVs\nsing the MSV rule is addressed using the BAB method. Using\nhe MSV rule, selecting CVs is equivalent to selecting n rows of\nn m \u00d7 n matrix G such that the selected n \u00d7 n sub-matrix GX\u2217n\nas the largest MSV among all possible n \u00d7 n sub-matrices, i.e.:\n(GX\u2217n ) \u2265 \u03c3(GXn ) \u2200Xn \u2282 {1, 2, . . . , m} (28)\nn this case, the subset selection problem is equivalent to the\nelection of n natural numbers from the first m natural numbers,\nhere each selected number represents a row index. Based on\nhis observation, the elements of different sets, e.g. F and C,\nepresent row indices in the following discussion.\n.1. Monotonicity\nThe BAB method used in this paper requires that the objective\nunction or its upper bound has monotonicity (see Section 2).\no show that an upper bound on MSV satisfies the monotonicity\nequirement, we need the following lemma.\nemma 3. Let the matrix \u02c6A be defined as\n\u02c6 =\n[\nA b\nbT a\n]\nhere A\u2208Rp\u00d7p is a Hermitian matrix, b\u2208Rp\u00d71 and a\u2208R. Let\nhe eigenvalues of A and \u02c6A be arranged in descending order.Please cite this article in press as: Cao, Y., Kariwala, V., Bidirectional bra\n(2008), doi:10.1016\/j.compchemeng.2007.11.011\nhen (Horn & Johnson, 1985, Th. 4.3.8):\np+1( \u02c6A) \u2264 \u03bbp(A) \u2264 \u03bbp( \u02c6A) \u2264 \u03bbp\u22121(A) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bb1(A) \u2264 \u03bb1( \u02c6A)\n(29)\nn\ni\nl\ncan\nus\ncri\npt\n PRESS\nical Engineering xxx (2008) xxx\u2013xxx\nroposition 4 (Bidirectional monotonicity of MSV). For\n\u2208Rp\u00d7n and b\u2208Rn\u00d71, let the singular values of A and\nAT b ]T be arranged in descending order. Then\np+1\n([\nA\nbT\n])\n\u2264 \u03c3p(A) if p < n (30)\nn\n([\nA\nbT\n])\n\u2265 \u03c3n(A) if p \u2265 n (31)\nroof. To prove (30), we note that when p < n:\np+1\n([\nA\nbT\n])\n= \u03bb1\/2p+1\n([\nAAT Ab\nbTAT bTb\n])\nhen, using (29), it follows that (30) holds. The inequality in\n31) can be proven by noting that when p \u2265 n:\nn\n([\nA\nbT\n])\n= \u03bb1\/2n (ATA + bbT) = \u03bb1\/2n\n([\nAAT Ab\nbTAT bTb\n])\nnd applying (29). \u0002\nemark 5. Proposition 4 shows that for a full rank matrix\nf size m \u00d7 n with m > n, to select p rows from m, if\n\u2264 n the MSV satisfies upward monotonicity, whilst if\n\u2265 n, the MSV satisfies downward monotonicity. Therefore,\nor p = n, the MSV satisfies both upward and down-\nard monotonicity and bidirectional BAB can be efficiently\nonducted.\nTo understand the significance of (30) and (31) in the\nontext of MSV maximization problem using BAB method,\nonsider two index sets I and J with p and q elements, respec-\nively, where p < n < q. If \u03c3p(GI ) < B, (30) implies that all\nquare sub-matrices of G that can be generated by adding\nows to GI cannot contain the optimal solution and hence\need not be evaluated. Similarly, if \u03c3n(GJ ) < B, (31) implies\nhat all square sub-matrices of G that can be generated by\nliminating rows from GJ cannot contain the optimal solu-\nion and hence need not be evaluated. Thus, using (30) and\n31), the optimal solution can be found without complete\nnumeration.\n.2. Fast pruning algorithms\nBidirectional pruning requires the test of two different prun-\nng conditions: \u00afJn(GF\u222aC) < B for downwards pruning and\n\u00af\nn(GF ) < B for upwards pruning, where B is the best avail-\nble lower bound on J. The inequalities in (30) and (31) imply\nonotonicity and hence simplify upper bound calculations. The\nse of these inequalities, however, involves calculation of singu-\nar values, which still requires O(n3) floating point operations\nflops) for ann \u00d7 nmatrix (Golub & Van Loan, 1996). We recog-\nize that the calculation of MSV is only necessary for terminalPage 8 of 14\nnch and bound for controlled variable selection, Comput Chem Eng\nodes to update B. For non-terminal nodes, it suffices to ver-\nfy whether they may contain a terminal node that gives a better\nower bound than B. In this section, we derive determinant based\nonditions and fast algorithms based on these conditions. For the\na IN+ModelC\nChem\nn\nfi\nb\nP\nd\nG\n\u03c3\n\u03b2\nw\ns\nP\n1\n=\nN\nV\nd\nI\na\nd\n(\nd\nk\nB\nt\no\na\no\nu\nA\nG\ns\n1\n2\n3\n4\n5\n6\na\nf\nc\n&\nt\nd\ns\n4\nh\nP\nb\nG\n\u03c3\n\u03b1\nw\ns\nP\nd\n(\n\u03c3Ac\nce\npte\nd M\nARTICLEACE-3580; No. of Pages 14\nY. Cao, V. Kariwala \/ Computers and\node under consideration, these algorithms are used to efficiently\nnd the sub-nodes or super-nodes that can be pruned or need to\ne considered further.\nroposition 5 (Upwards pruning). Let the fat matrix GF\u222ai\nefined as GF\u222ai = [GTF GTi ]\nT\n, where GF \u2208R(p\u22121)\u00d7n and\ni \u2208R1\u00d7n with p \u2264 n. For a positive scalar B, if \u03c3p\u22121(GF ) =\n(GF ) > B:\nF\ni := GiGTi \u2212 GiGTF (GFGTF \u2212 B2Ip\u22121)\n\u22121GFGTi < B2\n\u21d4 \u03c3p(GF\u222ai) = \u03c3(GF\u222ai) < B (32)\nhere \u03b2Fi denotes the ith upwards pruning index based on the\net F.\nroof. Using the Schur determinant formula (Horn & Johnson,\n985):\ndet(GF\u222aiGTF\u222ai \u2212 B2Ip)\n= det\n[\nGFGTF \u2212 B2Ip\u22121 GFGTi\nGiGTF GiGTi \u2212 B2\n]\n(33)\ndet(GFGTF \u2212 B2Ip\u22121)(\u03b2Fi \u2212 B2) (34)\now, denoting the eigenvalue decomposition of GFGTF as\n\u0002VT, we have\net(GFGTF \u2212 B2Ip\u22121) = det(\u0002\u2212 B2Ip\u22121)\n=\np\u22121\u220f\nk=1\n(\u03bbk(GFGTF ) \u2212 B2) =\np\u22121\u220f\nk=1\n(\u03c32k (GF ) \u2212 B2) (35)\nf \u03c3(GF ) > B, (\u03c32k (GF ) \u2212 B2) > 0 for all k = 1, 2, . . . , (p \u2212 1)\nnd thus det(GFGTF \u2212 B2Ip\u22121) > 0. Similar to (35),\net(GF\u222aiGTF\u222ai \u2212 B2Ip) =\n\u220fp\nk=1(\u03c32k (GF\u222ai) \u2212 B2) =\n\u03c32(GF\u222ai) \u2212 B2)\n\u220fp\u22121\nk=1 (\u03c32k (GF\u222ai) \u2212 B2). Since \u03c3(GF ) > B,\nue to the interlacing property in (29), \u03c3k(GF\u222ai) > B for\n= 1, 2, . . . , (p \u2212 1) and thus \u220fp\u22121k=1 (\u03c32k (GF\u222ai) \u2212 B2) > 0.\nased on (34), the signs of (\u03c32(GF\u222ai) \u2212 B2) and (\u03b2Fi \u2212 B2) are\nhe same and (32) follows. \u0002\nIn condition (32), the matrix inverse needs to be calculated\nnly once to evaluate all the candidates generated by appending\nrow to G. Therefore, it is more efficient than direct calculation\nf MSV. An implementation of the upwards pruning algorithm\nsing the Cholesky factorization is described as follows.\nlgorithm 1 (Upwards pruning). Pre-calculate and store Q =Please cite this article in press as: Cao, Y., Kariwala, V., Bidirectional bra\n(2008), doi:10.1016\/j.compchemeng.2007.11.011\nGT before search, where G is the original matrix for row\nelection.\n. At a node S = (F,C), perform the Cholesky factorization,\nRTR = QF,F \u2212 B2If , where QF,F represents the sub-matrix\nof Q constructed by selecting rows and columns with indices\nin F;\nS\ni\u220f\n(\ncnu\nsc\nrip\nt\n PRESS\nical Engineering xxx (2008) xxx\u2013xxx 9\n. If the factorization fails, (QF,F \u2212 B2If ) is not positive def-\ninite or \u03c3(GF ) \u2264 B. Therefore, the whole node should be\npruned, i.e. return without any further calculation;\n. For i\u2208C, calculate x = R\u2212TQF,i using forward substitution\nthrough the lower-triangular matrix RT;\n. Calculate \u03b2Fi = Qi,i \u2212 xTx;\n. If \u03b2Fi \u2264 B2, perform pruning, i.e. remove index i from can-\ndidate set C;\n. Go to Step 3 through all i\u2208C.\nFor a node with f fixed elements and c candidate elements, the\nbove algorithm requires about (f + f 3\/3 + cf 2 + 2cf + c) =\n3\/3 + c(f + 1)2 + f flops, where f and f 3\/3 are related to the\nomputation ofQF,F \u2212 B2If and Cholesky factorization (Golub\nVan Loan, 1996, p. 145), respectively, cf 2 is associated with\nhe forward substitution (Step 3) and c(2f + 1) for Step 4. A\nirect MSV approach require evaluation of the eigenvalues of c\nquare matrices of size (f + 1) \u00d7 (f + 1), hence requires about\nc(f + 1)3\/3 flops (Golub & Van Loan, 1996, Chapter 8) and\nence is very inefficient in comparison with Algorithm 1.\nroposition 6 (Downwards pruning). Let the tall matrix GS\ne defined as GS = [GTS\\i GTi ]T, where GS\\i \u2208Rp\u00d7n and\ni \u2208R1\u00d7n with p \u2265 n. For a positive scalar B, if \u03c3n(GS) =\n(GS) > B:\nS\ni : = 1 \u2212 Gi(GTSGS \u2212 B2In)\n\u22121GTi < 0 \u21d4 \u03c3n(GS\\i)\n= \u03c3(GS\\i) < B (36)\nhere \u03b1Si denotes the ith downwards pruning index based on the\net S.\nroof. We first note that GTSGS = GTS\\iGS\\i + GTi Gi and thus\ndet(GTS\\iGS\\i \u2212 B2In)\n= det(GTSGS \u2212 B2In \u2212 GTi Gi)\n= det(GTSGS \u2212 B2In) det(In \u2212 (GTSGS \u2212 B2In)\n\u22121GTi Gi)\n= det(GTSGS \u2212 B2In)(1 \u2212 GI (GTSGS \u2212 B2In)\n\u22121GTi )\n= det(GTSGS \u2212 B2In)\u03b1Si (37)\nSimilar to the proof of Proposition 5, \u03c3(GS) > B \u21d2\net(GTSGS \u2212 B2In) > 0 and det(GTS\\iGS\\i \u2212 B2In) =\n\u03c32(GS\\i) \u2212 B2)\n\u220fn\u22121\nk=1(\u03c32k (GS\\i) \u2212 B2). We note that\ni(GS) = \u03bb1\/2i\n([\nGS\\iGTS\\i GS\\iGTi\nGiGTS\\i GiGTi\n])\n; i = 1, 2, . . . , n\nince \u03c3(G) = \u03c3n(G) > B, due to the interlacing property\nn (29), \u03c3k(GS\\i) > B for k = 1, 2, . . . , (n \u2212 1) and thus\nn\u22121 2 2Page 9 of 14\nnch and bound for controlled variable selection, Comput Chem Eng\nk=1(\u03c3k (GS\\i) \u2212 B ) > 0. Based on (37), the signs of\n\u03c32(GS\\i) \u2212 B2) and \u03b1Si are the same and (36) follows. \u0002\nSimilar to upwards pruning, the matrix inverse needs to be\nalculated only once to evaluate all the candidates generated by\n IN+ModelC\n1 Chem\nr\nv\np\na\nA\nS\ni\n(\n1\n2\n3\n4\n5\n6\nd\nr\nn\nc\nt\ns\nd\n&\nA\nt\nb\ng\nT\nb\nR\nb\nF\ns\ni\nc\nd\np\nR\n(\nu\na\nS\nw\nt\ns\nb\nfl\ns\nA\nu\nn\nc\n1\n2\n3\n4\n5\n6\n7\n8\n9\nm\nw\nf\nn\nt\nm\ne\ni\ni\nc\nt\n4\n1\nw\nT\nv\np\no\nI\nt\no\ni\nm\ns\npAc\nce\npte\nd M\nARTICLEACE-3580; No. of Pages 14\n0 Y. Cao, V. Kariwala \/ Computers and\nemoving a row from GS in condition (36). Hence, it is also a\nery efficient pruning test. An implementation of the downwards\nruning algorithm using the Cholesky factorization is described\ns follows.\nlgorithm 2 (Downwards pruning). At a node S = (F,C), let\n= F \u222a C, GS be the corresponding matrix with all s rows\nndexed by S and n be the number of variables to be selected\nthe number of rows of GS).\n. Calculate the Cholesky factorization, RTR = GTSGS \u2212\nB2In;\n. If the factorization fails, \u03c3(GS) \u2264 B; therefore, the whole\nnode should be pruned, i.e. return without any further calcu-\nlation;\n. For i\u2208C, calculate x = R\u2212TGTi using forward substitution\nthrough the lower-triangular matrix RT;\n. Calculate \u03b1Si = 1 \u2212 xTx;\n. If \u03b1Si \u2264 0, perform pruning by fixing i, i.e. move index i from\nC to F;\n. Go to step 3 through all i\u2208C.\nFor a node with f fixed elements and c candi-\nate elements, s = f + c, and the above algorithm\nequires about (n3\/3 + 2sn2 + n + cn2 + 2cn + c) =\n3\/3 + 2sn2 + c(n + 1)2 + n flops. Here, (n3\/3 + 2sn2 + n),\nn2 and (2cn + c) flops are related to Steps 1, 3 and 4, respec-\nively. Since GS is a s \u00d7 n matrix, it results in c sub-matrices of\nize (s \u2212 1) \u00d7 n upon elimination of one row from C. Hence,\nirectly calculating MSV requires 2cn2(s \u2212 1 + n) flops (Golub\nVan Loan, 1996, p. 254), which is much more expensive than\nlgorithm 2.\nIt is worth noting that both the conditions (32) and (36) require\nhat the MSV of the current node is larger than the best available\nound B. In both of Algorithms 1 and 2, such a requirement is\nuaranteed to be satisfied if the Cholesky factorization exists.\nherefore, any evaluation of MSV on the non-terminal nodes\necomes unnecessary.\nemark 6 (Multiple pruning). The value of \u03b2Fi depends on\noth F and B whilst that of \u03b1Si depends on S = F \u222a C and B.\nor the same B, multiple upwards pruning can be conducted\nimultaneously because F is unchanged in any upwards prun-\nng. Similarly, for a given B, multiple downwards pruning is\narried out independently since S remains the same during any\nownwards pruning. These features are useful in parallel com-\nuting.\nemark 7 (Updating pruning indices). If either F is expanded\ndue to downwards pruning or upwards branching) or B is\npdated after an upwards pruning test has been conducted, then\nnew upwards pruning test has to be performed to update \u03b1Si .\nimilarly, if either S shrinks (due to upwards pruning or down-\nards branching) or B is updated after a downwards pruningPlease cite this article in press as: Cao, Y., Kariwala, V., Bidirectional bra\n(2008), doi:10.1016\/j.compchemeng.2007.11.011\nest has been performed, then a new downwards pruning check\nhould be carried out to update \u03b2Fi .\nThe upwards and downwards pruning algorithms can be com-\nined to form a bidirectional pruning algorithm, where the three\nC\nb\n\u03b2an\nus\ncri\npt\n PRESS\nical Engineering xxx (2008) xxx\u2013xxx\nags are introduced to check whether a pruning test is necessary;\nee Remark 7.\nlgorithm 3 (Bidirectional pruning). Initially set down-flag,\np-flag and bound-flag to true. Assume the target is to select\nelements. At a node S = (F,C) with f fixed indices and c\nandidate indices, let r = n \u2212 f .\n. while (down-flag or up-flag or bound-flag are true) and r > 0\nand c \u2212 r > 0, do\n. if down-flag or bound-flag are true, execute Algorithm 2;\n. if Algorithm 2 fails, break the loop else set down-flag to false;\n. if any element has been moved from C to F, set up-flag to\ntrue;\n. if (up-flag or bound-flag are true) and r > 0, execute\nAlgorithm 1;\n. if Algorithm 1 fails, break the loop else set up-flag to false;\n. if any element has been removed from C, set down-flag to\ntrue;\n. set bound-flag to false;\n. end of while loop\nAlgorithm 3 results in the largest candidate set C whose ele-\nents may be removed or fixed to produce sub- or super-nodes\nith upper bounds larger than the best available bound B. There-\nore, the bidirectional algorithm ensures that only the necessary\nodes are expanded. This feature is particularly important for\nhe cases where r = 1 or c \u2212 r = 1, i.e. where only one ele-\nent needs to be fixed or removed. In such cases, selecting any\nlement from C produced by Algorithm 3 always leads to an\nmproved bound. Particularly, selecting the element correspond-\nng to the largest \u03b1Si for c \u2212 r = 1 cases and fixing the element\norresponding to the largest \u03b2Fi for r = 1 cases often leads to\nhe best subset due to Corollary 1 discussed in Section 4.3.\n.3. Fast branching algorithms\nFor a given node S = (F,C) and lower bound B, Algorithms\nand 2 can be used to efficiently find the subsets and supersets,\nhich can be discarded without computation of singular values.\no branch efficiently, however, it is still required that the MSV\nalues for these subsets and supersets be computed and com-\nared; see Section 3.2. The selection of the decision element\nnly requires the relative rankings of the sub- and super-nodes.\nt is possible to obtain approximate rankings using bounds on\nhe MSV values of the sub- and super-nodes. Note that the use\nf approximate rankings reduces the computational load signif-\ncantly, and only affects efficiency but not optimality. With this\notivation, we next establish relationships between MSV of\nupersets and subsets of a node and the upward and downwards\nruning indices obtained from Algorithms 1 and 2, respectively.Page 10 of 14\nnch and bound for controlled variable selection, Comput Chem Eng\norollary 1 (Upwards upper bound and downwards lower\nound). For \u03b2Fi and \u03b1Si defined in (32) and (36), respectively:\nF\ni \u2265 \u03c32(GF\u222ai) = \u03c32f+1(GF\u222ai) (38)\na IN PRESS+ModelC\nChemical Engineering xxx (2008) xxx\u2013xxx 11\n\u03b1\nP\n\u220f\nk\nA\n\u03b2\nS\np\no\nm\n\u03b1\n(\nb\nM\nc\n(\n\u03b2\nt\nn\nb\ni\nC\nf\nr\nL\nr\na\np\nF\nb\nA\nK\nTable 1\nBranch and bound programs for MSV maximization\nAlgorithm Description\nBB1 Original program developed in Cao et al. (1998)\nBB2 Original program developed in Kariwala and Skogestad (2006)\nUP Upwards pruning using determinant condition (32)\nD\nB\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\nR\na\nu\na\ni\nu\ni\nt\n5Ac\nce\npte\nd M\nARTICLEACE-3580; No. of Pages 14\nY. Cao, V. Kariwala \/ Computers and\nS\ni \u2264\n\u03c32(GS\\i) \u2212 B2\n\u03c32(GS) \u2212 B2 =\n\u03c32n(GS\\i) \u2212 B2\n\u03c32n(GS) \u2212 B2\n(39)\nroof. Based on the proof of Proposition 5, we note that\nn\n=1\n(\u03bbk(GF\u222aiGTF\u222ai) \u2212 B2) = (\u03b2Fi \u2212 B2)\nn\u22121\u220f\nk=1\n(\u03bbk(GFGTF ) \u2212 B2)\nccording to the interlacing property of eigenvalues in (29):\nF\ni \u2212 B2 = (\u03bb(GF\u222aiGTF\u222ai) \u2212 B2)\nn\u22121\u220f\nk=1\n\u03bbk(GF\u222aiGTF\u222ai) \u2212 B2\n\u03bbk(GFGTF ) \u2212 B2\n\u2265 \u03bb(GF\u222aiGTF\u222ai) \u2212 B2 \u21d2 \u03b2Fi \u2265 \u03bb(GF\u222aiGTF\u222ai)\n= \u03c32(GF\u222ai)\nimilarly, based on the proof of Proposition 6 and the interlacing\nroperty of eigenvalues in (29):\nn\u220f\nk=1\n(\u03bbk(GTS\\iGS\\i) \u2212 B2) = \u03b1Si\nn\u220f\nk=1\n(\u03bbk(GTSGS) \u2212 B2In) \u21d2 \u03b1Si\n= \u03bb(G\nT\nS\\iGS\\i) \u2212 B2\n\u03bb(GTSGS) \u2212 B2\nn\u22121\u220f\nk=1\n\u03bbk(GTS\\iGS\\i) \u2212 B2\n\u03bbk(GTSGS) \u2212 B2\n\u21d2 \u03b1Si\n\u2264 \u03bb(G\nT\nS\\iGS\\i) \u2212 B2\n\u03bb(GTSGS) \u2212 B2\n, \u03b1Si \u2264\n\u03c32(GS\\i) \u2212 B2\n\u03c32(GS) \u2212 B2 \u0002\nAccording to Corollary 1, \u03b1Si is related to the lower bound\nn the MSV values of the sub-nodes and the decision ele-\nent can be selected by finding the sub-node with the largest\nS\ni (best to fix) for upward-first branching, or the smallest \u03b1Si\nbest to discard) for downward-first branching based on the\nest-first rule. Similarly, \u03b2Fi provides an upper bound on the\nSV values of the super-nodes and thus the decision element\nan be selected by finding the super-node with the smallest \u03b2Fi\nbest to discard) for downward-first branching, or the largest\nF\ni (best to fix) for upward-first branching. Hence, the evalua-\nion of the MSV can be completely eliminated for non-terminal\nodes. Moreover, according to Remark 7 these indices need to\ne updated (recalculated) only when either the lower bound B\ns updated or for \u03b2Fi , i\u2208C when some rows are moved from\nto F, or for \u03b1Si , i\u2208C when some elements are eliminated\nrom C.\nCombining all the aforesaid techniques, we next present a\necursive implementation of bidirectional BAB algorithm, B3.\net G be a m \u00d7 n matrix with m > n. The problem is to select n\nows such that the MSV of the resulting square matrix is largest\nmong all the square matrices that can be obtained from G. In\nreparation stage, one needs to calculate Q = GGT, initialize\n2Please cite this article in press as: Cao, Y., Kariwala, V., Bidirectional bra\n(2008), doi:10.1016\/j.compchemeng.2007.11.011\n= \u2205, C = {1, \u00b7 \u00b7 \u00b7 ,m}, B = 0, and set down-flag, up-flag and\nound-flag to true.\nlgorithm 4 (Bidirectional branch and bound (B3); Cao &\nariwala, 2007). Assume the given node S = (F,C).\nr\nr\nmnu\nsc\nrip\nt\nOWN Downwards pruning using determinant condition (36)\n3 The bidirectional branch and bound algorithm (Algorithm 4)\n1. evaluate f and c as the number of elements of F and C,\nrespectively. Set r = n \u2212 f and S = F \u222a C.\n2. while r > 0 and c > r do\n3. Algorithm 3;\n4. if fails or r < 0 or c < r return;\n5. if r = 0 or c = r break the while loop;\n6. if r > 1 go to step 9, else select i = arg max \u03b2Fi , i\u2208C;\n7. update bound B2 = max(B2, \u03bb(QF\u222ai,F\u222ai)) and set bound-\nflag to true if the bound is updated;\n8. remove i from C, set down-flag to true, continue the while\nloop;\n9. if r \u2212 c > 1 go to step 12, else select i = arg max \u03b1Si , i\u2208C;\n0. update bound B2 = max(B2, \u03bb(QS\\i,S\\i)) and set bound-\nflag to true if the bound is updated;\n1. move i from C to F, set up-flag to true, continue the while\nloop;\n2. if 2r > c go to step 15, else select i = arg min \u03b1Si , i\u2208C;\n3. call Algorithm 4 with the fixed set F \u222a i, the candidate set\nC \\ i and up-flag being true;\n4. remove i from C, set down-flag to true, continue the while\nloop;\n5. select i = arg min \u03b2Fi , i\u2208C;\n6. call Algorithm 4 with the fixed set F, the candidate set C \\ i\nand down-flag being true.\n7. move i from C to F, set up-flag to true;\n8. end of the while loop;\n9. if r = c update boundB2 = max(B2, \u03bb(QF\u222aC,F\u222aC)) and set\nbound-flag to true if the bound is updated;\n0. if r = 0 update bound B2 = max(B2, \u03bb(QF,F )) and set\nbound-flag to true if the bound is updated;\n1. return\nemark 8. As indicated in Cao et al. (1998), by changing B to\nvector with p criterion values arranged in ascending order and\nsing the first element as the lower bound for comparison, B3\nlgorithm can be used to find the best p subsets. This extension\ns particularly useful for CV selection, where local analysis is\nsed to identify promising candidates and the final set of CVs\ns selected by evaluating the loss for these candidates based on\nhe non-linear model.\n. Numerical examplesPage 11 of 14\nnch and bound for controlled variable selection, Comput Chem Eng\nTo examine the efficiency of the bidirectional BAB algo-\nithm, numerical tests are conducted using random data and a\neal self-optimizing control case study. Programs used for loss\ninimization or MSV maximization are listed in Table 1. As\nrip\nt\nARTICLE IN PRESS+ModelCACE-3580; No. of Pages 14\n12 Y. Cao, V. Kariwala \/ Computers and Chemical Engineering xxx (2008) xxx\u2013xxx\nF agains\no luated\na\np\nf\nX\n(\nR\n5\nc\ns\ne\nt\ne\nT\nf\nb\np\nc\no\na\na\ns\no\nt\nm\ne\nD\na\n(\nt\nm\n2\nc\na\nr\np\nc\no\nt\na\nm\nt\ns\na\ns\nm\nt\n5\nt\nS\nc\nh\na\nl\nr\nT\na\noAc\nce\npte\nd M\nig. 5. Random test 1: selection of n out of 2n variables, (a) computation time\nut of 40 variables, (c) computation time against n and (d) number of nodes eva\nll programs provide the same optimal solution, we only com-\nare the computational efficiency of different algorithms in the\nollowing discussion. All tests were conducted on a Windows\nP SP2 notebook with an Intel\u00aeCore TM Duo Processor T2500\n2.0 GHz, 2 MB L2 Cache, 667 MHz FSB) using MATLAB\u00ae\n2006b.\n.1. Random tests\nFour sets of random tests are conducted to evaluate the effi-\niency of the proposed B3 algorithm. The first test consists of\nelecting n out of 2n variables, where n varies from 5 to 20. For\nach n, 100 2n \u00d7 n random matrices are generated. For this test,\nhe average computation time and the average number of nodes\nvaluated are plotted against n in Fig. 5(a) and (b), respectively.\nhe second test is to select n out of 40 variables with n ranging\nrom 1 to 39. The average computation time and average num-\ner of node evaluations calculated over 100 random cases are\nlotted against n in Fig. 5(c) and (d), respectively. In order to\nompare the efficiency of proposed BAB algorithms, the number\nf node evaluations required by the brute force search (marked\ns BRUTE) is also shown in Fig. 5(b) and (d). For clarity, the\nverage computation time required by brute force search is not\nhown in Fig. 5(c) and (d), but note that the computation time\nf brute force search is (approximately) directly proportional to\nhe number of node evaluations, i.e. number of node evaluations\nultiplied by the computation time required for a single MSV\nvaluation.\nFrom Fig. 5, it can been seen that the evaluation count of\nOWN algorithm is similar to BB1 algorithm (Cao et al., 1998),\nnother downward algorithm, where power iteration methodPlease cite this article in press as: Cao, Y., Kariwala, V., Bidirectional bra\n(2008), doi:10.1016\/j.compchemeng.2007.11.011\nGolub & Van Loan, 1996) is used to avoid direct MSV calcula-\nions. However, the determinant based UP algorithm is much\nore efficient than BB2 algorithm (Kariwala & Skogestad,\n006), another upward algorithm based on direct MSV cal-\nl\nI\na\nean\nus\nct n and (b) number of nodes evaluated against n; Random test 2: selection of n\nagainst n.\nulations. In terms of computation time, both DOWN and UP\nlgorithms demonstrate the efficiency of the determinant algo-\nithms. Furthermore, by combining upward and downwards\nruning, the bidirectional B3algorithm exhibits superior effi-\niency, extending the manageable selection size beyond C2040.\nThe third and fourth random tests are designed to select 5\nut of n and (n \u2212 5) out of n variables, respectively. Each selec-\nion problem is tested for 100 randomly generated matrices. The\nverage computation time and average evaluation count are sum-\narized in Fig. 6(a)\u2013(d), respectively. All figures clearly show\nhat upward and downward pruning based algorithms are more\nuitable to select a few variables and discard a few variables from\nlarge candidate set, respectively. However, for both kinds of\nelection problems, the bidirectional algorithm improves perfor-\nance dramatically and its efficiency is relatively insensitive to\nhe nature of the selection problem.\n.2. HDA case study\nThe developed algorithms are also applied to select CVs for\nhe HDA process, which was used as a case study in Kariwala and\nkogestad (2006). Details of self-optimizing control of this pro-\ness can be found in Araujo and Skogestad (2007). The process\nas 8 degrees of freedom, i.e. 8 CVs are to be selected from 129\nvailable measurements. It is interesting to note that the prob-\nem has C8129 \u2248 1.52 \u00d7 1012 alternatives, whereas MATLAB\u00ae\nequires about 0.05 ms to calculate the MSV of an 8 \u00d7 8 matrix.\nherefore, if the problem were to be solved using the brute force\npproach, it would require more than 2 years to find the globally\nptimal solution.\nThe computation times required to solve this selection prob-Page 12 of 14\nnch and bound for controlled variable selection, Comput Chem Eng\nem using various BAB algorithms are compared in Table 2.\nt is clear that for this case study, upward approaches have an\ndvantage over downward ones in terms of the number of nodes\nvaluated. This is reasonable, as very few variables are to be\na\nrip\nt\nARTICLE IN PRESS+ModelCACE-3580; No. of Pages 14\nY. Cao, V. Kariwala \/ Computers and Chemical Engineering xxx (2008) xxx\u2013xxx 13\nFig. 6. Random test 3: selection of 5 out of n variables, (a) computation time again\n(n \u2212 5) out of n variables, (c) computation time against n and (d) number of nodes ev\nTable 2\nComputation time and number of evaluations for the HDA case study\nProgram cpu time (s) Evaluation\nBB1 2337.6 5,035,735\nBB2 3809.0 809,673\nUP 41.98 177,811\nD\nB\ns\nm\nr\ni\nr\nc\n6\nb\nd\ni\nt\nd\nh\nb\nB\ns\np\nl\nv\nr\ne\nc\ni\nc\nb\ni\na\ns\nu\np\ns\nl\na\nA\nm\nt\nA\np\nS\nR\nAAc\nce\npte\nd M\nOWN 294.51 1,254,891\n3 0.046 263\nelected from a large number of alternatives. The novel deter-\ninant pruning algorithms also demonstrate their efficiency by\neducing evaluation count significantly. The most astonishing\nmprovement of performance is achieved by bidirectional algo-\nithm, B3, which reduces both computation time and evaluation\nount by four orders of magnitude.\n. Conclusions\nIn this paper, theoretical frameworks for unidirectional and\nidirectional branch and bound (BAB) principles have been\neveloped based on the concepts of fixed and candidate sets. By\nntroducing the upward BAB concept, a hidden upward struc-\nure in the traditional downward BAB solution tree and a hidden\nownward structure in the proposed upward BAB solution tree\nave been identified. These developments lead to the concepts of\nidirectional pruning and branching, and a novel bidirectional\nAB scheme. The proposed concepts are independent of the\nelection criterion, i.e. they can be applied to any subset selection\nroblem.Please cite this article in press as: Cao, Y., Kariwala, V., Bidirectional bra\n(2008), doi:10.1016\/j.compchemeng.2007.11.011\nFor self-optimizing control problems, the minimum singu-\nar value (MSV) criterion has been adopted to select controlled\nariables (CVs) from the available measurements. The crite-\nion satisfies bidirectional monotonicity and thus upper bound\nA\nBnu\nscst n and (b) number of nodes evaluated against n; Random test 4: selection of\naluated against n.\nstimation can be simplified. A set of novel determinant based\nonditions have been developed to accelerate pruning, either\nn the upward or downward directions. By combining these\nonditions with the bidirectional BAB principles, an efficient\nidirectional BAB (B3) algorithm has been developed. Numer-\ncal examples clearly show the superior performance of B3\nlgorithm. Particularly for the HDA case study, B3 algorithm\nuccessfully reduces both the computation time and the eval-\nation count by four orders of magnitude, as compared to the\nreviously available methods.\nAlthough MSV maximization is a simple criterion for CV\nelection, it may fail to identify the CVs providing minimal local\noss in some cases. This fact was pointed out by Halvorsen et\nl. (2003), who also derived expressions for exact local loss.\nbidirectional BAB algorithm based on the exact local loss\ninimization has been developed, which will be presented in\nhe second part of this work.\ncknowledgement\nThe second author gratefully acknowledges the financial sup-\nort from Office of Finance, Nanyang Technological University,\ningapore through grant no. M52120046.\neferences\nlstad, V. (2005). Studies on selection of controlled variables. PhD thesis, Nor-\nwegian University of Science and Technology, Trondheim, Norway. Avail-\nable at http:\/\/www.nt.ntnu.no\/users\/skoge\/publications\/thesis\/2005 alstad\/.Page 13 of 14\nnch and bound for controlled variable selection, Comput Chem Eng\nraujo, A., & Skogestad, S. (2007). Application of plantwide control to the HDA\nprocess. I. Steady-state optimization and self-optimizing control. Control\nEngineering Practice, 15(10), 1222\u20131237.\nusinger, P. A., & Golub, G. H. (1965). Linear least squares solution by house-\nholder transformations. Numerische Mathematik, 7(3), 269\u2013276.\n IN+ModelC\n1 Chem\nC\nC\nC\nC\nC\nD\nF\nG\nH\nH\nK\nK\nL\nM\nN\nR\nS\nS\nS\nSARTICLEACE-3580; No. of Pages 14\n4 Y. Cao, V. Kariwala \/ Computers and\nao, Y., & Kariwala, V. (2007, November). B3MSV. MATLAB File Exchange.\nAvailable at http:\/\/www.mathworks.com\/matlabcentral\/fileexchange\/\nloadFile.do?objectId=17480&objectType=file.\nao, Y., & Rossiter, D. (1997). An input pre-screening technique for control\nstructure selection. Computers & Chemical Engineering, 21(6), 563\u2013569.\nao, Y., Rossiter, D., & Owens, D. H. (1998). Globally optimal control\nstructure selection using branch and bound method. In Proceedings of IFAC-\nsymposium on DYCOPS 5 (pp. 183\u2013188).\nao, Y., & Saha, P. (2005). Improved branch and bound method for con-\ntrol structure screening. Chemical Engineering Science, 60(6), 1555\u2013\n1564.\nhen, X.-W. (2003). An improved branch and bound algorithm for feature\nselection. Pattern Recognition Letters, 24(12), 1925\u20131933.\ne Hoog, F. R., & Mattheij, R. M. M. (2007). Subset selection for matrices.\nLinear Algebra and its Applications, 422(2\/3), 349\u2013359.\nloudas, C. A. (1995). Nonlinear and mixed-integer optimization: Fundamental\nand applications. New York, NY, USA: Oxford University Press.\nolub, G. H., & Van Loan, C. F. (1996). Matrix computations (3rd ed.). Balti-\nmore, MD, USA: The Johns Hopkins University Press.\nalvorsen, I. J., Skogestad, S., Morud, J. C., & Alstad, V. (2003). Optimal selec-\ntion of controlled variables. Industrial & Engineering Chemistry Research,\n42(14), 3273\u20133284.Ac\nce\npte\nd M\nPlease cite this article in press as: Cao, Y., Kariwala, V., Bidirectional bra\n(2008), doi:10.1016\/j.compchemeng.2007.11.011\norn, R. A., & Johnson, C. R. (1985). Matrix analysis. Cambridge, UK: Cam-\nbridge University Press.\nariwala, V., & Skogestad, S. (2006). Branch and bound methods for control\nstructure design. In Proceedings of the 16th ESCAPE and 9th international\nsymposium on PSE\nV\nYrip\nt\n PRESS\nical Engineering xxx (2008) xxx\u2013xxx\nookos, I. K., & Perkins, J. D. (2001). Heuristic-based mathematical program-\nming framework for control structure selection. Industrial & Engineering\nChemistry Research, 40, 2079\u20132088.\nuyben, M. L., & Floudas, C. A. (1994). Analyzing the interaction of design and\ncontrol. 1. A multiobjective framework and application to binary distillation\ncolumn. Computers & Chemical Engineering, 18(10), 933\u2013969.\navroidis, C., Dubowsky, S., & Thomas, K. (1997). Optimal sensor location\nin motion control of flexibly supported long reach manipulators. Journal of\nDynamic Systems, Measurement and Control, 119(4), 718\u2013726.\narendra, P., & Fukunaga, K. (1977). A branch and bound algorithm for feature\nsubset selection. IEEE Transactions on Computers, 26(9), 917\u2013922.\nidout, M. S. (1988). An improved branch and bound algorithm for feature\nsubset selection. Applied Statistics, 37(1), 139\u2013147.\nahinidis, N. V. (2000). Branch and reduce optimization navigator (BARON)\nuser\u2019s manual v 4. 0. Urbana, IL, USA: University of Illinois at Urbana-\nChampaign.\nkogestad, S. (2000). Plantwide control: The search for the self-optimizing\ncontrol structure. Journal of Process Control, 10(5), 487\u2013507.\nkogestad, S., & Postlethwaite, I. (1996). Multivariable feedback control: Anal-\nysis and design (1st ed.). Chichester, UK: John Wiley & Sons.\nomol, P., Pudil, P., & Kittler, J. (2004). Fast branch & bound algorithms for opti-\nmal feature selection. IEEE Transactions on Pattern Analysis and MachinePage 14 of 14\nan\nus\nc\nnch and bound for controlled variable selection, Comput Chem Eng\nIntelligence, 26(7), 900\u2013912.\nan de Wal, M., & de Jager, B. (2001). A review of methods for input\/output\nselection. Automatica, 37(4), 487\u2013510.\nu, B., & Yuan, B. (1993). A more efficient branch and bound algorithm for\nfeature selection. Pattern Recognition, 26(6), 883\u2013889.\n"}