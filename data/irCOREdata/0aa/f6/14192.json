{"doi":"10.1080\/0968776042000339772","coreId":"14192","oai":"oai:generic.eprints.org:587\/core5","identifiers":["oai:generic.eprints.org:587\/core5","10.1080\/0968776042000339772"],"title":"A review of computer-assisted assessment","authors":["Conole, Gr\u00e1inne","Warburton, Bill"],"enrichments":{"references":[{"id":450698,"title":"1) Universities blamed for exam fiasco, Times Higher Education Supplement,","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":1042642,"title":"12) Litigation fees top \u00a315m as academic disputes grow, Times Higher Education Supplement,","authors":[],"date":"2004","doi":null,"raw":"Baty, P. (2004, March 12) Litigation fees top \u00a315m as academic disputes grow, Times Higher Education Supplement, 2004, p. 7.","cites":null},{"id":1042683,"title":"A model for computer-based assessment: the Catherine wheel principle, Assessment and Evaluation","authors":[],"date":"2000","doi":"10.1080\/713611427","raw":"Zakrzewski, S. & Steven, C. (2000) A model for computer-based assessment: the Catherine wheel principle, Assessment and Evaluation in Higher Education, 25, 201\u2013215.","cites":null},{"id":1042667,"title":"A summary of methods of item analysis","authors":[],"date":"2002","doi":null,"raw":"McAlpine, M. (2002c) A summary of methods of item analysis (Luton, CAA Centre).","cites":null},{"id":1042638,"title":"A taxonomy for learning, teaching, and assessing: a revision of Bloom\u2019s taxonomy of educational objectives","authors":[],"date":"2001","doi":null,"raw":"Anderson, L., Krathwohl, D. & Bloom, B. (2001) A taxonomy for learning, teaching, and assessing: a revision of Bloom\u2019s taxonomy of educational objectives (New York, Longman).","cites":null},{"id":1042669,"title":"Academic approaches and attitudes towards CAA: a qualitative study,","authors":[],"date":"2001","doi":null,"raw":"McKenna, C. (2001) Academic approaches and attitudes towards CAA: a qualitative study, in: M.","cites":null},{"id":1042663,"title":"An evaluation of a computer adaptive test in a UK university context,","authors":[],"date":"2003","doi":null,"raw":"Lilley, M. & Barker, T. (2003) An evaluation of a computer adaptive test in a UK university context, in: J. Christie (Ed.) 7th International CAA Conference, Loughborough University, 8\u20139 July 2003.","cites":null},{"id":450200,"title":"Are mathematics exam results affected by the mode of delivery?,","authors":[],"date":"2002","doi":"10.1080\/0968776020100109","raw":"Fiddes, D. J., Korabinski, A. A., McGuire, G. R., Youngson, M. A. & McMillan, D. (2002) Are mathematics exam results affected by the mode of delivery?, ALT-J, 10(6), 1\u20139.","cites":null},{"id":1042655,"title":"Assessment as a catalyst for innovation, invited keynote and paper for the Quality Assurance Agency for Higher Education, Assessment Workshop 5. Available online at: http:\/\/ www.qaa.ac.uk\/scottishenhancement\/events\/default.htm (accessed 10","authors":[],"date":"2004","doi":null,"raw":"Conole, G. (2004b) Assessment as a catalyst for innovation, invited keynote and paper for the Quality Assurance Agency for Higher Education, Assessment Workshop 5. Available online at: http:\/\/ www.qaa.ac.uk\/scottishenhancement\/events\/default.htm (accessed 10 September 2004).","cites":null},{"id":450702,"title":"Assessment for learning: quality and taxonomies,","authors":[],"date":"1995","doi":"10.1080\/02602939508565719","raw":"Imrie, B. (1995) Assessment for learning: quality and taxonomies, Assessment and Evaluation in Higher Education, 20, 171\u2013189.","cites":null},{"id":1042647,"title":"Assuring quality computer-based assessment development in UK higher education,","authors":[],"date":"2003","doi":null,"raw":"Boyle, A. & O\u2019Hare, D. (2003) Assuring quality computer-based assessment development in UK higher education, in: J. Christie (Ed.)  7th International CAA Conference,  Loughborough University, 8\u20139 July 2004.Review of computer-assisted assessment 29 Brown, G., Bull, J. & Pendlebury, M. (1997) Assessing student learning in higher education (Routledge, London).","cites":null},{"id":1042678,"title":"Authority","authors":[],"date":"2003","doi":"10.4324\/9780203464908_chapter_11","raw":"Scottish Qualifications Authority (2003)  Guidelines for online assessment in further education (Glasgow, Scottish Qualifications Agency).","cites":null},{"id":1042664,"title":"Automated free text marking with paperless school,","authors":[],"date":"2002","doi":null,"raw":"Mason, O. & Grove-Stephensen, I. (2002) Automated free text marking with paperless school, in: M. Danson (Ed.) 6th International CAA Conference, Loughborough University, 2\u20133 July 2001.","cites":null},{"id":1042650,"title":"Available online at: www.caacentre.ac.uk\/dldocs\/final_report.pdf (accessed16","authors":[],"date":"2003","doi":null,"raw":"Available online at: www.caacentre.ac.uk\/dldocs\/final_report.pdf (accessed16 March 2003).","cites":null},{"id":1042653,"title":"Blueprint for computer-assisted assessment (London,","authors":[],"date":"2004","doi":"10.4324\/9780203464687","raw":"Bull, J. & McKenna, C. (2004) Blueprint for computer-assisted assessment (London, RoutledgeFalmer) Christie, J. (2003) Automated essay marking for content\u2014does it work?, in J. Christie. (Ed.) 7th International CAA Conference, Loughborough, 8\u20139 July 2001.","cites":null},{"id":1042680,"title":"CAA in UK HEIs: the state of the art,","authors":[],"date":"2003","doi":null,"raw":"Warburton, W. & Conole, G. (2003) CAA in UK HEIs: the state of the art, in: J. Christie (Ed.) 7th International CAA Conference, University of Loughborough, 8\u20139 July 2003.","cites":null},{"id":450195,"title":"CAA must be more than multiple-choice tests for it to be academically credible?,","authors":[],"date":"2001","doi":null,"raw":"Davies, P. (2001) CAA must be more than multiple-choice tests for it to be academically credible?, in: M. Danson & C. Eabry (Eds) 5th International CAA Conference, Loughborough University.","cites":null},{"id":450192,"title":"Campus wide setup of question mark perception (V2.5) at the Katholieke Universiteit Leuven (Belgium)\u2014facing a large scale implementation,","authors":[],"date":"2002","doi":null,"raw":"Cosemans, D., Van Rentergem, L., Verburgh, A. & Wils, A. (2002) Campus wide setup of question mark perception (V2.5) at the Katholieke Universiteit Leuven (Belgium)\u2014facing a large scale implementation, in: M. Danson (Ed.) 5th International CAA Conference, University of Loughborough, 2\u20133 July 2001.","cites":null},{"id":1042648,"title":"Code of practice for the use of information technology (IT) in the delivery of assessments","authors":[],"date":"2002","doi":"10.3403\/02533256","raw":"BSI (2002) Code of practice for the use of information technology (IT) in the delivery of assessments (London, BSI).","cites":null},{"id":1042652,"title":"Computer-assisted assessment centre (TLTP3) update, in:","authors":[],"date":"2000","doi":null,"raw":"Bull, J. & Mckenna, C. (2000) Computer-assisted assessment centre (TLTP3) update, in: M. Danson (Ed.) 4th International CAA Conference, Loughborough University, 21\u201322 June 2000.","cites":null},{"id":1042651,"title":"Computer-assisted assessment centre update,","authors":[],"date":"2001","doi":null,"raw":"Bull, J. & Hesketh, I. (2001) Computer-assisted assessment centre update, in M. Danson & C. Eabry (Eds) 5th International CAA Conference, Loughborough University, 2\u20133 July 2001.","cites":null},{"id":450771,"title":"Computer-based testing\u2014building the foundation for future assessment","authors":[],"date":"2002","doi":null,"raw":"Mills, C., Potenza, M., Fremer, J. & Ward, C. (Eds) (2002) Computer-based testing\u2014building the foundation for future assessment (New York, Lawrence Erlbaum Associates).","cites":null},{"id":450777,"title":"Computing Ltd","authors":[],"date":"2004","doi":null,"raw":"Question Mark Computing Ltd (2004) Perception: Windows based authoring, Available online at: http:\/\/www.questionmark.com\/uk\/perception\/authoring_windows.htm (accessed 29 August 2004).","cites":null},{"id":1042682,"title":"Construct validity of free response and machine-scorable forms of a test,","authors":[],"date":"1980","doi":"10.1111\/j.1745-3984.1980.tb00811.x","raw":"Ward, W. C., Frederiksen, N. & Carlson, S. B. (1980) Construct validity of free response and machine-scorable forms of a test, Journal of Educational Measurement, 7(1), 11\u201329.","cites":null},{"id":450699,"title":"Critical issues in online, large-scale assessment: An exploratory study to identify and refine issues. Unpublished Ph.D. thesis,","authors":[],"date":"2002","doi":null,"raw":"Hambrick, K. (2002) Critical issues in online, large-scale assessment: An exploratory study to identify and refine issues. Unpublished Ph.D. thesis, Capella University.","cites":null},{"id":1042666,"title":"Design requirements of a databank","authors":[],"date":"2002","doi":null,"raw":"McAlpine, M. (2002b) Design requirements of a databank (Luton, CAA Centre).","cites":null},{"id":450198,"title":"End short contract outrage, MPs insist, Times Higher Education Supplement.","authors":[],"date":"2002","doi":null,"raw":"Farrer, S. (2002) End short contract outrage, MPs insist, Times Higher Education Supplement.","cites":null},{"id":450704,"title":"Experiences of assessing LMU students over the web.","authors":[],"date":"1998","doi":null,"raw":"Kennedy, N. (1998)  Experiences of assessing LMU students over the web.  Leeds Metropolitan University.","cites":null},{"id":1042674,"title":"Final report for the Item Banks Infrastructure Study (IBIS)","authors":[],"date":"2004","doi":null,"raw":"Sclater, N. (Ed.) (2004) Final report for the Item Banks Infrastructure Study (IBIS) (Bristol, JISC).","cites":null},{"id":450696,"title":"Formative and summative confidence-based assessment,","authors":[],"date":"2003","doi":null,"raw":"Gardner-Medwin, A. & Gahan, M. (2003) Formative and summative confidence-based assessment, in: J. Christie (Ed.) 7th International CAA Conference, Loughborough University, 8\u20139 July 2003.","cites":null},{"id":197085,"title":"Guidelines for computer-based testing","authors":[],"date":"2003","doi":null,"raw":"ATP (2003) Guidelines for computer-based testing (Washington, DC, ATP).","cites":null},{"id":1042641,"title":"Higher education: a critical business (Buckingham,","authors":[],"date":"1997","doi":null,"raw":"Barnett, R. (1997) Higher education: a critical business (Buckingham, Society for Research into Higher Education and the Open University Press).","cites":null},{"id":450193,"title":"Implementing online assessment in an emerging MLE: a generic guidance document with practical examples","authors":[],"date":"2003","doi":null,"raw":"Danson, M. (2003) Implementing online assessment in an emerging MLE: a generic guidance document with practical examples (Bristol, JISC).","cites":null},{"id":1042681,"title":"Implementing perception across a large university: getting it right, Perception European Users Conference (Edinburgh, Question Mark Computing).","authors":[],"date":"2004","doi":null,"raw":"Warburton, W. & Harwood, I. (2004) Implementing perception across a large university: getting it right, Perception European Users Conference (Edinburgh, Question Mark Computing).","cites":null},{"id":1042643,"title":"Inexorable and inevitable: the continuing story of technology and assessment.","authors":[],"date":"2002","doi":"10.1002\/9780470712993.ch11","raw":"Bennett, R. E. (2002a) Inexorable and inevitable: the continuing story of technology and assessment.  Journal of Technology, Learning and Assessment,  1. Available online at: http:\/\/ www.bc.edu\/research\/intasc\/jtla\/journal\/pdf\/v1n1_jtla.pdf (accessed 17 July 2004).","cites":null},{"id":1042640,"title":"Informative reports\u2014 experiences from the Pass-IT project, in:","authors":[],"date":"2004","doi":null,"raw":"Ashton, H. S., Beavers, C. E., Schofield, D. K. & Youngson, M. A. (2004) Informative reports\u2014 experiences from the Pass-IT project, in: M. Ashby and R. Wilson (Eds) Proceedings of the 8th International Computer\u2013Assisted Assessment Conference, Loughborough University, 6\u20137 July 2004.","cites":null},{"id":1042677,"title":"Integrating CAA within the University of Ulster,","authors":[],"date":"2002","doi":null,"raw":"Stevenson, A., Sweeney, P., Greenan, K. & Alexander, S. (2002) Integrating CAA within the University of Ulster, in: M. Danson (Ed.) Proceedings of the 6th International conference on computer-assisted assessment, University of Loughborough, 4\u20135 July 2002.","cites":null},{"id":450778,"title":"Interoperability with CAA: does it work in practice?,","authors":[],"date":"2002","doi":null,"raw":"Sclater, N., Low, B. & Barr, N. (2002) Interoperability with CAA: does it work in practice?, in: M. Danson (Ed.) Proceedings of the 6th International Conference on Computer-Assisted Assessment, Loughborough University, 4\u20135 July 2002.","cites":null},{"id":1042646,"title":"Item selection and application in higher education,","authors":[],"date":"2002","doi":null,"raw":"Boyle, A., Hutchison, D., O\u2019Hare, D. & Patterson, A. (2002) Item selection and application in higher education, in: M. Danson (Ed.), 6th International CAA Conference (Loughborough, Loughborough University).","cites":null},{"id":450194,"title":"Large scale implementation of question mark perception (V2.5)\u2014experiences at","authors":[],"date":"2001","doi":null,"raw":"Danson, M., Dawson, B. & Baseley, T. (2001) Large scale implementation of question mark perception (V2.5)\u2014experiences at Loughborough University, in: M. Danson (Ed.)  6th International CAA Conference, Loughborough University, 2\u20133 July 2001.","cites":null},{"id":1042675,"title":"Learning technology in transition\u2014from individual enthusiasm to institutional implementation (Swets and Zeitlinger,","authors":[],"date":"2003","doi":null,"raw":null,"cites":null},{"id":1879081,"title":"Learning technology in transition\u2014from individual enthusiasm to institutional implementation (Swets and Zeitlinger, The Netherlands).","authors":[],"date":"2003","doi":null,"raw":"Seale, J. (Ed.) (2003) Learning technology in transition\u2014from individual enthusiasm to institutional implementation (Swets and Zeitlinger, The Netherlands).","cites":null},{"id":450199,"title":"Multiple choice for honours-level students?, in:","authors":[],"date":"1999","doi":null,"raw":"Farthing, D. & McPhee, D. (1999) Multiple choice for honours-level students?, in: M. Danson (Ed.) 3rd International CAA Conference, Loughborough University, 16\u201317 June 1999.","cites":null},{"id":1042668,"title":"Multiple response questions\u2014allowing for chance in authentic assessments,","authors":[],"date":"2003","doi":null,"raw":"McAlpine, M. & Hesketh, I. (2003) Multiple response questions\u2014allowing for chance in authentic assessments, in: J. Christie (Ed.) 7th International CAA Conference, Loughborough University, 8\u20139 July 2000.","cites":null},{"id":450773,"title":"Online assessment: creating communities and opportunities, in:","authors":[],"date":"1999","doi":null,"raw":"O\u2019Reilly, M. & Morgan, C. (1999) Online assessment: creating communities and opportunities, in: S. Brown, P. Race & J. Bull (Eds) Computer-assisted assessment in higher education (London, Kogan Page), 149\u2013161.","cites":null},{"id":1042639,"title":"Piloting summative web assessment in secondary education,","authors":[],"date":"2003","doi":null,"raw":"Ashton, H. S., Schofield, D. K. & Woodgar, S. C. (2003) Piloting summative web assessment in secondary education, in: J. Christie (Ed.) Proceedings of the 7th International Computer\u2013Assisted Assessment Conference, Loughborough University, 8\u20139 July 2003.","cites":null},{"id":1042665,"title":"Principles of assessment","authors":[],"date":"2002","doi":null,"raw":"McAlpine, M. (2002a) Principles of assessment (Luton, CAA Centre).","cites":null},{"id":450697,"title":"Processing differences as a function of item difficulty in verbal analogy performance,","authors":[],"date":"1987","doi":"10.1037\/0022-0663.79.3.212","raw":"Gitomer, D. H., Curtis, M. E., Glaser, R. & Lensky, D. B. (1987) Processing differences as a function of item difficulty in verbal analogy performance, Journal of Educational Psychology, 79, 212\u2013219.30 G. Conole and B. Warburton Goddard, A. (2002, November 1) Universities blamed for exam fiasco, Times Higher Education Supplement, p. 7.","cites":null},{"id":450703,"title":"QTI Enterprise v1.1 specs, examples and schemas. Available online at: http:\/\/ www.imsglobal.org\/specificationdownload.cfm (accessed 2","authors":[],"date":"2003","doi":null,"raw":"IMS (2003)  QTI Enterprise v1.1 specs, examples and schemas.  Available online at: http:\/\/ www.imsglobal.org\/specificationdownload.cfm (accessed 2 March 2003).","cites":null},{"id":1042662,"title":"Question and test interoperability: an update on national and international developments,","authors":[],"date":"2001","doi":null,"raw":"Lay, S. & Sclater, N. (2001) Question and test interoperability: an update on national and international developments, in: M. Danson & C. Eabry (Eds)  5th International CAA Conference, Loughborough University, 2\u20133 July 2001.","cites":null},{"id":1042654,"title":"Report on the effectiveness of tools for e-learning, report for the JISC commissioned Research Study on the Effectiveness of Resources, Tools and Support Services used by Practitioners in Designing and Delivering E-Learning Activities.","authors":[],"date":"2004","doi":null,"raw":"Conole, G. (2004a) Report on the effectiveness of tools for e-learning, report for the JISC commissioned Research Study on the Effectiveness of Resources, Tools and Support Services used by Practitioners in Designing and Delivering E-Learning Activities.","cites":null},{"id":1042676,"title":"Results of a","authors":[],"date":"1997","doi":null,"raw":"Stephens, D. & Mascia, J. (1997) Results of a (1995) Survey into the use of computer-assisted assessment in institutions of higher education in the UK, University of Loughborough.","cites":null},{"id":1042661,"title":"Rethinking university teaching a conversational framework for the effective use of learning technologies (2nd edn)","authors":[],"date":"2002","doi":"10.4324\/9780203304846","raw":"Laurillard, D. (2002,) Rethinking university teaching a conversational framework for the effective use of learning technologies (2nd edn) (London, RoutledgeFalmer).","cites":null},{"id":197084,"title":"Standards for educational and psychological testing","authors":[],"date":"1999","doi":"10.3102\/0013189x07308810","raw":"AERA (1999) Standards for educational and psychological testing (Washington, DC, AERA). ALT (2003) The future of higher education: the Association for Learning Technologies response to the white paper. Available online at: http:\/\/www.alt.ac.uk\/docs\/he_wp_20030429_final.doc (accessed 19 December 2003).","cites":null},{"id":450770,"title":"Student assessment in higher education: a handbook for assessing performance (London,","authors":[],"date":"1998","doi":null,"raw":"Miller, A., Imrie, B. & Cox, K. (1998) Student assessment in higher education: a handbook for assessing performance (London, Kogan Page).","cites":null},{"id":450705,"title":"Taxonomy of educational objectives the classification of educational goals","authors":[],"date":"1964","doi":"10.1177\/001316446502500324","raw":"Krathwohl, D., Bloom, B. & Masia, B. (1964) Taxonomy of educational objectives the classification of educational goals (London, Longman).","cites":null},{"id":1042645,"title":"Taxonomy of educational objectives: the classification of educational goals. Handbook 1: cognitive domain","authors":[],"date":"1956","doi":"10.1177\/001316446502500324","raw":"Bloom, B., Engelhart, M., Furst, E., Hill, W. & Krathwohl, D. (1956) Taxonomy of educational objectives: the classification of educational goals. Handbook 1: cognitive domain (London, Longman).","cites":null},{"id":450772,"title":"Testing medium, validity and test performance,","authors":[],"date":"1998","doi":null,"raw":"Outtz, J. L. (1998) Testing medium, validity and test performance, in: M. D. Hakel (Ed.) Beyond multiple choice evaluating alternative to traditional testing for selection  (New York, Lawrence Erlbaum Associates).","cites":null},{"id":1042679,"title":"The TOIA Project web site. Available online at: http:\/\/www.toia.ac.uk (accessed 31","authors":[],"date":"2004","doi":null,"raw":"TOIA (2004) The TOIA Project web site. Available online at: http:\/\/www.toia.ac.uk (accessed 31 August 2004).","cites":null},{"id":450196,"title":"There\u2019s no confidence in multiple-choice testing,","authors":[],"date":"2002","doi":null,"raw":"Davies, P. (2002) There\u2019s no confidence in multiple-choice testing, in: M. Danson (Ed.) 6th International CAA Conference, Loughborough University, 4\u20135 July 2002.","cites":null},{"id":450701,"title":"Thinking the unthinkable: using project risk management when introducing computer-assisted assessments,","authors":[],"date":"2004","doi":null,"raw":"Harwood, I. & Warburton, W. (2004) Thinking the unthinkable: using project risk management when introducing computer-assisted assessments, in: M. Ashby & R. Wilson (Eds)  8th International CAA Conference, Loughborough University, 6\u20137 July 2004.","cites":null},{"id":1042649,"title":"TLTP85 implementation and evaluation of computer-assisted assessment: final report.","authors":[],"date":"2001","doi":null,"raw":"Bull, J. (2001) TLTP85 implementation and evaluation of computer-assisted assessment: final report.","cites":null},{"id":1042670,"title":"TRIADS experiences and Developments,","authors":[],"date":"2002","doi":null,"raw":"McKenzie, D., Hallam, B., Baggott, G. & Potts, J. (2002) TRIADS experiences and Developments, in: M. Danson (Ed.) 6th International CAA Conference, Loughborough University, 2\u20133 July 2002.","cites":null},{"id":450776,"title":"University of Bath quality audit report. Available online at: http:\/\/www.qaa.ac.uk\/ revreps\/instrev\/bath\/comms.htm (accessed 4","authors":[],"date":"1998","doi":null,"raw":"QAA. (1998)  University of Bath quality audit report.  Available online at: http:\/\/www.qaa.ac.uk\/ revreps\/instrev\/bath\/comms.htm (accessed 4 September 2003).","cites":null},{"id":450779,"title":"User requirements of the ultimate online assessment engine,","authors":[],"date":"2003","doi":"10.1016\/s0360-1315(02)00132-x","raw":"Sclater, N. & Howie, K. (2003) User requirements of the ultimate online assessment engine, Computers and Education, 40, 285\u2013306.","cites":null},{"id":450197,"title":"Using computer-aided assessment to test higher level learning outcomes,","authors":[],"date":"2001","doi":null,"raw":"Duke-Williams, E. & King, T. (2001) Using computer-aided assessment to test higher level learning outcomes, in: M. Danson & C. Eabry (Eds)  5th International CAA Conference, Loughborough University, 2\u20133 July 2001.","cites":null},{"id":1042644,"title":"Using electronic assessment to measure student performance. The State Education Standard, National Association of State Boards of Education.","authors":[],"date":"2002","doi":null,"raw":"Bennett, R. E. (2002b) Using electronic assessment to measure student performance. The State Education Standard, National Association of State Boards of Education.","cites":null},{"id":450774,"title":"WebCT and online assessment: the best thing since SOAP?,","authors":[],"date":"2003","doi":null,"raw":"Pain, D. & Leheron, K. (2003) WebCT and online assessment: the best thing since SOAP?, Educational Technology and Society, 6, 62\u201371.","cites":null},{"id":450191,"title":"What are the affordances of Information and communication technologies?,","authors":[],"date":"2004","doi":"10.1080\/0968776042000216183","raw":"Conole, G. & Dyke, M. (2004) What are the affordances of Information and communication technologies?, ALT-J, 12(2), 111\u2013123.","cites":null},{"id":450700,"title":"What happens when computer-assisted assessment goes wrong?,","authors":[],"date":"2004","doi":null,"raw":"Harwood, I. (2004) What happens when computer-assisted assessment goes wrong?,  British Journal of Educational Technology, submitted.","cites":null},{"id":450775,"title":"What\u2019s in a name? A new hierarchy for question types,","authors":[],"date":"2002","doi":null,"raw":"Paterson, J. S. (2002) What\u2019s in a name? A new hierarchy for question types, in: M. Danson (Ed.) 6th International CAA Conference, Loughborough University, 4\u20135 July 2002.Review of computer-assisted assessment 31 Pritchett, N. (1999) Effective question design, in: S. Brown, P. Race & J. Bull (Eds) Computerassisted assessment in higher education (London, Kogan Page).","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2005","abstract":"Pressure for better measurement of stated learning outcomes has resulted in a demand for more frequent assessment. The resources available are seen to be static or dwindling, but Information and Communications Technology is seen to increase productivity by automating assessment tasks. This paper reviews computer-assisted assessment (CAA) and suggests future developments. A search was conducted of CAA-related literature from the past decade to trace the development of CAA from the beginnings of its large-scale use in higher education. Lack of resources, individual inertia and risk propensity are key barriers for individual academics, while proper resourcing and cultural factors outweigh technical barriers at the institutional level","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14192.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/587\/1\/ALT_J_Vol13_No1_2005_A%20review%20of%20computer_as.pdf","pdfHashValue":"333ce1489b967696538200641ac0f4afabc51266","publisher":"Taylor and Francis Ltd","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:587<\/identifier><datestamp>\n      2011-04-04T09:07:11Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/587\/<\/dc:relation><dc:title>\n        A review of computer-assisted assessment<\/dc:title><dc:creator>\n        Conole, Gr\u00e1inne<\/dc:creator><dc:creator>\n        Warburton, Bill<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        Pressure for better measurement of stated learning outcomes has resulted in a demand for more frequent assessment. The resources available are seen to be static or dwindling, but Information and Communications Technology is seen to increase productivity by automating assessment tasks. This paper reviews computer-assisted assessment (CAA) and suggests future developments. A search was conducted of CAA-related literature from the past decade to trace the development of CAA from the beginnings of its large-scale use in higher education. Lack of resources, individual inertia and risk propensity are key barriers for individual academics, while proper resourcing and cultural factors outweigh technical barriers at the institutional level.<\/dc:description><dc:publisher>\n        Taylor and Francis Ltd<\/dc:publisher><dc:date>\n        2005<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/587\/1\/ALT_J_Vol13_No1_2005_A%20review%20of%20computer_as.pdf<\/dc:identifier><dc:identifier>\n          Conole, Gr\u00e1inne and Warburton, Bill  (2005) A review of computer-assisted assessment.  Association for Learning Technology Journal, 13 (1).  pp. 17-31.  ISSN 0968-7769 (print)\/1741-1629 (online)     <\/dc:identifier><dc:relation>\n        10.1080\/0968776042000339772<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/587\/","10.1080\/0968776042000339772"],"year":2005,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"ALT-J, Research in Learning Technology\nVol. 13, No. 1, March 2005, pp. 17\u201331\nISSN 0968-7769 (print)\/ISSN 1741-1629 (online)\/05\/010017\u201315\n\u00a9 2005 Association for Learning Technology\nDOI: 10.1080\/0968776042000339772\nA review of computer-assisted \nassessment\nGr\u00e1inne Conole* and Bill Warburton\nUniversity of Southampton, UK\nTaylor and Francis LtdCALT130103.sgm10.1080\/ 968776042000339772ALT-J, Research in Learning Technology0968 7769 (pri t)\/1741-1629 (onli e)Original Article2 05ssoci tion for Learning Techno ogy3 000Janua y 2005G \u00e1in eConoleSch l of Educa onU iversity f SouthamptonHighfieldSouthamptonSO17 1BJ\nPressure for better measurement of stated learning outcomes has resulted in a demand for more\nfrequent assessment. The resources available are seen to be static or dwindling, but Information and\nCommunications Technology is seen to increase productivity by automating assessment tasks. This\npaper reviews computer-assisted assessment (CAA) and suggests future developments. A search\nwas conducted of CAA-related literature from the past decade to trace the development of CAA\nfrom the beginnings of its large-scale use in higher education. Lack of resources, individual inertia\nand risk propensity are key barriers for individual academics, while proper resourcing and cultural\nfactors outweigh technical barriers at the institutional level.\nIntroduction\nAssessment is a critical catalyst for student learning (for example, Brown et al.,\n1997) and there is considerable pressure on higher education institutions to\nmeasure learning outcomes more formally (Farrer, 2002; Laurillard, 2002). This\nhas been interpreted as a demand for more frequent assessment. The potential for\nInformation and Communications Technology (ICT) to automate aspects of learn-\ning and teaching is widely acknowledged, although promised productivity benefits\nhave been slow to appear (Conole, 2004a; Conole & Dyke, 2004). Computer-\nassisted assessment (CAA) has considerable potential both to ease assessment load\nand provide innovative and powerful modes of assessment (Brown et al., 1997; Bull\n& McKenna, 2004), and as the use of ICT increases there may be \u2018inherent difficul-\nties in teaching and learning online and assessing on paper\u2019 (Bull, 2001; Bennett,\n2002a).\nThis review of CAA sits within a context of increased ICT use, student diversity,\nfinancial constraints and the shift from quality assurance to quality enhancement.\nThe review presents a fresh look at the topic by describing key features of CAA,\n*Corresponding author. School of Education, University of Southampton, Highfield, Southampton,\nSO17 1BJ, UK. Email: g.c.conole@soton.ac.uk\n18 G. Conole and B. Warburton\nidentifying issues and highlighting potential areas for research. It describes progress\nmade in identifying and addressing critical factors associated with implementing\nCAA.\nCAA in context\nThe shift towards online testing is well documented (for example, Bennett, 2002a)\nand different forms of CAA are illustrated in Figure 1. Bull and McKenna recently\ndefined CAA as \u2018the use of computers for assessing student learning\u2019 (2004).\nComputer-based assessment involves a computer program marking answers that\nwere entered directly into a computer, whereas optical mark reading uses a computer\nto mark scripts originally composed on paper. Portfolio collection is the use of a\ncomputer to collect scripts or written work. Computer-based assessment can be\nsubdivided into stand-alone applications that only require a single computer, appli-\ncations that work on private networks and those that are designed to be delivered\nacross public networks such as the web (online assessment).\nFigure 1. Different types of CAASix ways in which the strategic application of a learning technology such as CAA\nmay add value to the efficiency and effectiveness of the learning process have been\nidentified, along with six factors that may adversely influence it (ALT, 2003). The\nissues around CAA are similar to those identified for other learning technologies in\nterms of design and delivery and associated support needs (Seale, 2003). CAA has\nobvious similarities with the development of Managed Learning Environments in\nterms of the encountered difficulty of institutional implementation and wide-scale\nFigure 1. Different types of CAA\nReview of computer-assisted assessment 19\nuse (Sommerlad et al., 1999). However, CAA differs from other learning technolo-\ngies in that the stakes are much higher, particularly where it is used for examinations\n(QAA, 1998).\nStudents are perceived as increasingly litigious (Baty, 2004), and the clear scoring\nschemes of objective tests open the results of CAA tests to scrutiny that can render\ndeficits in practice, apparently highlighting the need for risk analysis and management\nstrategies (Zakrzewski & Steven, 2000).\nMethodology\nA search was conducted of CAA-related literature from the past decade to trace its\ndevelopment from the beginnings of large-scale use. Criteria for inclusion were direct\nor indirect reference to the implementation or evaluation of large-scale CAA. Some\nearlier material was also included because it forms the foundation of the literature.\nSearch keywords were fed into electronic literature indexes, catalogues and search\nengines. The review is presented as a narrative tracing the progress made in identifying\ncritical factors associated with implementing CAA and in overcoming operational and\ncultural barriers. The paper considers the findings from the literature in terms of CAA\ndevelopments, exploring research activities around the design, delivery and analysis\nof online assessments. It identifies barriers and enablers to the uptake of CAA and\nconsiders emergent patterns of uptake as well as the associated implementation issues.\nCategorising assessment\nA multiple choice item consists of four elements: the stem of the question, options,\ncorrect responses and distractors (Figure 2). Tests are collections of subject-specific\nitems, possibly drawn from item banks (Sclater, 2004). There are a variety of different\nquestion types (e.g. multiple choice, multiple response, hotspot, matching, ranking,\ndrag and drop, multiple steps and open ended) and feedback mechanisms (including\nautomatic feedback in objective testing, model answers, annotated test, or mixed\nmode with intervention from the teacher).\nFigure 2. Test selection from an item bankAssessment can be categorised as either summative (administered for grading\npurposes) or formative (to give feedback to assist the learning process). Diagnostic\nassessment is used by tutors to determine students\u2019 prior knowledge, and self-\nassessment is where students reflect on their understanding (O\u2019Reilly & Morgan,\n1999; Bull & McKenna, 2004). Other categorisations include formal\/informal\n(invigilated or not) and final\/continuous (at the end of a course or throughout).\nSclater and Howie distinguish six different applications of CAA: \u2018credit bearing\u2019 or\nhigh-stakes summative tests, continuous assessment, authenticated or anonymous\nself-assessment, and diagnostic tests that evaluate the student\u2019s knowledge before\nthe course to assess the effectiveness of the teaching (Sclater & Howie, 2003).\nSix kinds of cognitive learning outcomes are distinguishable according to Bloom\net al.\u2019s (1956) taxonomy: knowledge recall is at the most fundamental level, rising\nthrough comprehension, application, analysis, synthesis and evaluation. Others have\n20 G. Conole and B. Warburton\nextended or adapted this (Krathwohl et al., 1964; Imrie, 1995; Anderson et al., 2001).\nAlthough tutors in higher education assess the full range of outcomes (Miller et al.,\n1998, p. 56), it is the view of many that higher education should specialise in devel-\noping the highest level skills; for example, evaluative (\u2018critical\u2019) skills (Barnett, 1997).\nOutcomes at the lower end of Bloom\u2019s taxonomy are traditionally assessed on a\nconvergent basis (i.e. only one \u2018correct\u2019 answer), while higher-order outcomes are\nmost readily assessed divergently (a range of informed responses and analyses is\npermissible) (McAlpine, 2002a). Convergent assessments can be readily constructed\nusing objective items, but divergent assessment has traditionally relied on longer writ-\nten answers or essays, the automated marking of which presents real challenges (for\nexample, Mason & Grove-Stephensen, 2002).\nCurrent research and development activities\nBull and McKenna (2004) provide a valuable overview of CAA, McAlpine (2002a)\noutlines the basic principles of assessment and Mills et al. (2002) describe some of the\ncurrent research directions.\nCAA research and development activities can be grouped into three areas. The first\ncentres on design, ranging from the development of individual items through to the\nspecification of full CAA systems. The second is concerned with implementation and\ndelivery. The third focuses on analysis, test scoring and the development of appropri-\nate reporting systems.\nDesign\nItem development\nCAA allows for more complex item types compared with paper-based assessments,\nincluding the use of audiovisual materials and more complex interactions between\nFigure 2. Test selection from an item bank\nReview of computer-assisted assessment 21\nlearner and computer. One finding from the literature is that direct translation of\npaper-based assessments into online assessments is inappropriate; there is a need to\nrevisit question formulation, reflecting on what it is intended to test. The process of\ncreating CAA questions therefore raises fundamental issues about the nature of\npaper-based questions as well. The use of steps for some kinds of questions (for exam-\nple in mathematics) has also proved valuable in terms of enabling teachers and\nresearchers to get a better understanding of the student learning experience and how\nthey tackle questions (Ashton et al., 2003). This raises important issues about how\nCAA software systems record and report on student interactions. The reporting\nmechanisms available within CAA systems provide richer data about the students\nthan were available from paper-based assessments. Thus the development of online\nassessments has important pedagogical implications.\nThe format of an assessment affects validity, reliability and student performance.\nPaper and online assessments may differ in several respects. Studies have compared\npaper-based assessments with computer-based assessments to explore this (for exam-\nple, Ward et al., 1980; Outtz, 1998; Fiddes et al., 2002). In particular, the Pass-IT\nproject has conducted a large-scale study of schools and colleges in Scotland, across\na range of subject areas and levels (Ashton et al., 2003, 2004). Findings vary accord-\ning to the item type, subject area and level. Potential causes of mode effect include\nthe attributes of the examinees, the nature of the items, item ordering, local item\ndependency and the test-taking experience of the student. Additionally there may be\ncognitive differences and different test-taking strategies adopted for each mode.\nUnderstanding these issues is important for developing strategies for item develop-\nment as well as to produce guidelines for developing appropriate administrative\nprocedures or statistically adjusting item parameters.\nIn contrast to marking essays, marking objective test scripts is a simple repetitive\ntask and researchers are exploring methods of automating assessment. Objective test-\ning is now well established in the United States and elsewhere for standardised testing\nin schools, colleges, professional entrance examinations and for psychological testing\n(Bennett, 2002b; Hambrick, 2002).\nThe limitations of item types are an ongoing issue. A major concern related to the\nnature of objective tests is whether multiple choice questions (MCQs) are really suit-\nable for assessing higher-order learning outcomes in higher education students\n(Pritchett, 1999; Davies, 2002), and this is reflected in the opinions of both academ-\nics and quality assurance staff (Bull, 1999; Warburton & Conole, 2003). The most\noptimistic view is that item-based testing may be appropriate for examining the full\nrange of learning outcomes in undergraduates and postgraduates, provided sufficient\ncare is taken in their construction (Farthing & McPhee, 1999; Duke-Williams &\nKing, 2001). MCQs and multiple response questions are still the most frequently\nused question types (Boyle et al., 2002; Warburton & Conole, 2003) but there is\nsteady pressure for the use of \u2018more sophisticated\u2019 question types (Davies, 2001).\nWork is also being conducted on the development of computer-generated items\n(Mills et al., 2002). This includes the development of item templates precise enough\nto enable the computer to generate parallel items that do not need to be individually\n22 G. Conole and B. Warburton\ncalibrated. Research suggests that some subject areas are easier to replicate than\nothers\u2014lower-level mathematics, for example, in comparison with higher-level\ncontent domain areas.\nItem banks\nItem banks are collections of questions, often produced collaboratively across a\nsubject domain that can be grouped according to difficulty, the type of skill or topic.\nSclater and colleagues have recently conducted a detailed review of item bank devel-\nopments within the United Kingdom (Sclater, 2004), covering metadata, security,\naccess and authentication and legal issues. Sclater positions item banks as the crucial\ndriver of CAA, and McAlpine (2002b) argues for the routine adoption of item banks\nas a means of countering challenges from students about fairness, validity, security or\nquality assurance. Other researchers have looked at setting up, maintaining and\nadapting item banks (Mills et al., 2002). Security issues are particularly important\nwith high-stakes assessments; use of larger pools is one strategy that can help deter\ncheating. Controlling item exposure and maintaining the security of item banks is\nlikely to continue to be an active area of research and development.\nComputer-adaptive testing\nObjective items\u2019 ability to explore the limits of a participant\u2019s ability is developed by\ncomputer-adaptive testing (CAT). CAT involves issuing questions of a difficulty level\nthat depends on the test-taker\u2019s previous responses. If a question is answered\ncorrectly, the estimate of his\/her ability is raised and a more difficult question is\npresented, and vice versa, giving the potential to test a wide range of student ability\nconcisely. For instance, Lilley and Barker (2003) constructed a database of 119 peer-\nreviewed items and gave both \u2018traditional\u2019 (non-adaptive) and CAT tests to 133\nstudents drawing on Item Response Theory as a model. Students\u2019 results from the\nCAT test correlated well with their results from the traditional version and they did\nnot find the CAT test unfair. Because CAT items are written to test particular levels\nof ability, they have the potential to deliver more accurate and reliable results than\ntraditional tests.\nCAA software systems\nCAA software tools vary in how they support the design, delivery and analysis of online\nassessments and differ in cost, flexibility and scalability. The TOIA system provides\na range of tools for managing and reporting on the assessment process and has\nproduced a free Question and Test Interoperability (QTI)-compliant system (TOIA,\n2004). TRIADS delivers different question styles in a variety of modes to facilitate\nthe testing of higher order learning skills (McKenzie et al., 2002). Commercial tools,\nlike Perception (Question Mark Computing Ltd, 2004), represent considerable\ninvestments in terms of initial outlay and maintenance agreements. CAA systems vary\nReview of computer-assisted assessment 23\nwidely in the number of question types supported and in the control administrators\nhave in scheduling assessments. For example, Perception supports 18 objective item\ntypes, TOIA nine (although the commercial version has more) and Blackboard six.\nAssessment tools in Virtual Learning Environments tend to be less sophisticated but\nhave proved important in getting practitioners using CAA. More costly systems tend\nto be sold on the basis of commensurate scalability and flexibility with dedicated\nsupport; however, scalability is still problematic (Danson et al., 2001; Cosemans et al.,\n2002; Stevenson et al., 2002; Harwood, 2004). Software also varies in the number of\nassessments that can be taken simultaneously (Question Mark Computing Ltd,\n2004).\nThe earliest CAA systems were stand-alone applications, whereas current systems\nare either connected by a private network or are delivered by browser. Although most\nCAA assessments are still constructed from MCQs (Warburton & Conole, 2003)\nthere have long been demands for more flexible question types, including those that\nmight be difficult or impossible to rendered on paper (Bull & Hesketh, 2001; Davies,\n2001; Bennett, 2002b). This is reflected in the increasing number of question types\nsupported by CAA software, including \u2018new\u2019 question types. It is clearly in the inter-\nests of vendors to maximise the number of item types supported by their product.\nThis is seen by users as an important metric of flexibility and a means by which\nvendors differentiate themselves from their competitors, although many are simply\nelaborations of basic question types, which makes it difficult to compare CAA\nproducts (Paterson, 2002).\nInteroperability\nInteroperability is important for transferring questions between systems. Practitioners\nare using different ICT tools to support their teaching, and hence may want to design\nquestions within one tool and deliver tests in another. One contentious issue is\nwhether current CAA systems are truly interoperable (Sclater et al., 2002).\nLay and Sclater (2001) identify two more reasons for interoperability. First,\nwhether the item banks will be accessible when current CAA systems are no longer in\nuse, and second whether student assessment data can be transferred to institutional\nstudent records system. Another important driver for interoperability is to preserve\nusers\u2019 investments in existing questions and tests when moving between institutions\nor to different CAA systems. The IMS (2003) Consortium\u2019s QTI specification is a\nvaluable starting point, but clearly there is a need for further work (Sclater et al., 2002;\nSclater & Howie, 2003).\nDelivery\nCompliance with published standards for CAA practice\nThe recent code of practice for the use of information technology in the delivery of\nassessments (BS 7988: 2002) acknowledges that increased use of CAA \u2018has raised\n24 G. Conole and B. Warburton\nissues about the security and fairness of IT-delivered assessments, as well as resulting\nin a wide range of different practices\u2019 (BSI, 2002, p. 11). It aims to enhance the status\nof CAA and encourage use by demonstrating its fairness, security, authenticity and\nvalidity. However, the code\u2019s focus on the delivery of CAA tests could lead to the\nrelative neglect of earlier stages in the preparation and quality assurance of assess-\nments. As Boyle and O\u2019Hare (2003, p. 72) contend, \u2018A poor assessment, delivered\nappropriately, would [still] conform to BS\u2019. They identified the American Educa-\ntional Research Association (2003) Standards for Educational and Psychological Testing,\nthe Association of Test Publishers Guidelines for Computer-based Testing and the Scot-\ntish Qualifications Authority (2003) Guidelines for Online Assessment in Further Educa-\ntion as better guides to practice.\nScaling up CAA\nAn obstacle to the uptake of CAA is that it is often implemented by individuals on an\nad hoc basis with no overarching strategy or institutional IT infrastructure. This may\ndelay or prevent embedding (Bull, 2001; Boyle & O\u2019Hare, 2003). Bull asserts that\n\u2018Retooling is a challenge which impacts on research and development, requiring a\nhigh level of resourcing for academic and support staff in order to maintain pace with\ntechnological and software developments\u2019 (2001, p. 11). The risks of small-scale\ndevelopment include practitioner isolation and under-funding, although possible\nbenefits include practitioners being in control of the process (Kennedy, 1998).\nHigher education institutions that are implementing CAA centrally encounter risks\nand benefits on a different scale (Danson et al., 2001; Cosemans et al., 2002; Warbur-\nton & Harwood, 2004). Scaling up for full-scale institutional deployment covers every\npossible use and seems likely to depend more upon the resolution of cultural than\ntechnical issues. Bull (2001, p. 11) points out that \u2018the organisational and pedagogical\nissues and challenges surrounding the take-up of CAA often outweigh the technical\nlimitations of software and hardware.\u2019 This finding is mirrored in the issues associated\nwith the uptake of other learning technologies (for example, Seale, 2004).\nFor individual practitioners, concerns about risks are likely to continue (Harwood\n& Warburton, 2004). While operational barriers might be overcome with incremental\nadvances in technology, cultural obstacles are proving more durable.\nCritical factors governing the uptake of CAA\nTraditional assessment practices are now reasonably well understood. Even so, not\nall traditional assessments run smoothly (for example, Goddard, 2002). Many barri-\ners and enablers for traditional assessment are also relevant to CAA. The emergence\nof CAA has forced the re-examination of these dormant issues in traditional practice.\nAn argument for establishing good CAA practice at an institutional level is that it trig-\ngers the re-examination of assessment practice generally (Bull & McKenna, 2004).\nSeveral approaches have been taken to identifying factors governing uptake.\nStephens and Mascia conducted the first UK survey of CAA use in 1995 using a\nReview of computer-assisted assessment 25\n10-item questionnaire that attracted 445 responses. They identified the need for\ninstitutional support (training and resourcing), allowing time to develop CAA tests,\nmaking CAA a fully integrated part of existing assessment procedures (rather than\nan afterthought) and subject-related dependencies. Important operational factors\nwere familiarisation with the tools, well-planned procedures that addressed security\nand reliability issues, and involvement of support staff (Stephens & Mascia, 1997).\nFour years later the CAA Centre conducted a national survey of higher education\nthat focussed on use and attitudes. The survey built on that of Stephens and Mascia\n(Bull, 1999) but had over twice as many items, many of which were multi-part. It\nattracted more than 750 responses from academics, quality assurance staff and staff\ndevelopers (McKenna, 2001). Warburton and Conole (2003) piloted an adapted,\nonline version of the 1999 survey, which received 50 responses, mostly from\nacademic CAA enthusiasts.\nThe greatest institutional barrier was seen to be cost both in terms of personal time\nand the expense of commercial software. Unrealistic expectations coupled with inher-\nent conservatism, and lack of technical and pedagogic support were also cited.\nRespondents were less concerned with Managed Learning Environment integration,\nsecurity or copyright issues. Another obstacle was the perceived steep learning curve\nassociated with the technology and constructing specialised CAA question types. Of\nparticular concern was the difficulty of constructing objective items that reliably\nassess higher-learning outcomes (cf. Boyle & O\u2019Hare, 2003). There was a perceived\ncredibility gap between what CAA proponents promise and what respondents\nthought could be delivered; lack of support, cultural resistance and technophobia\nwere cited less often. Related issues about usability, academics working in isolation\nand individual inertia were also raised. Subject-specific shared question banks and the\nvalue of exemplars were cited as important drivers for the large-scale uptake of CAA,\nbut the provision of \u2018evangelists\u2019 and adherence to institutional guidelines was\nthought less crucial.\nAcademic commitment was cited as an important enabler; faculty support for CAA\nseems limited; external funding is the principle way support for CAA at this level is\nrendered. Other important factors included the need to embed CAA within normal\nteaching. Effective interoperability (particularly between CAA systems and Virtual\nLearning Environments) and integration of multimedia were also cited. Most systems\nwere web-based, although many respondents delivered CAA using closed networks,\nand a small percentage used optical mark reading. Only one-third were invigilated,\nand most of the summative CAA tests restricted the percentage weighting to one-third\nor less of the overall mark, although it was noteworthy that some tests were worth up\nto 100%. CAA was used to test to a range of group sizes including very large groups\n(more than 200 students). Subject-specific differences in the uptake of CAA were\nobvious (Bull, 1999; Warburton & Conole, 2003). Interestingly, Quality Assurance\nstaff identified few enabling factors, perhaps indicating their largely negative percep-\ntion of CAA (Bull, 1999; Bull & McKenna, 2000; Bull & Hesketh, 2001).\nThe centrality of cultural issues was evident, with 90% of barriers and 65% of\nenablers being identified as cultural in the 1999 survey. Hambrick\u2019s (2002) study of\n26 G. Conole and B. Warburton\nlarge-scale applications of formal online assessment in the US K-12 school system are\nsplit equally between cultural and operational factors. Zakrzewski and Steven (2000)\nconducted a formal risk assessment; one-third of the factors they identified were\ncultural.\nAnalysis\nItem analysis and scoring\nOne of the benefits of CAA is the opportunity to record student interactions and anal-\nyse these to provide a richer understanding of learning. A variety of analyses can be\nrun to assess how well individual questions or students perform. Weak items can then\nbe eliminated or teacher strategies adapted. Automatically recorded data can be used\nin a variety of ways; for example, looking at the relationship between answering speed\nand accuracy. Care is needed, however, in interpreting results, for incorrect responses\nmay indicate a more sophisticated understanding by the student than might at first\nappear; for example, incorrect use of grammar in a foreign language test might be a\nresult of higher cognitive understanding by the student. Gitomer et al. (1987) found\nboth high-ability and low-ability examinees increased their processing time for more\ndifficult items, but that low-ability examinees spent more time on encoding the stem\nwhile high-ability examinees spent more time on subsequent processing.\nAssessments need to be both valid and reliable. An advantage of CAA is that it\noffers consistency in marking. A range of methods are possible for scoring from\nsimple allocation of a mark to the correct response through to varied, compound and\nnegative scoring. Two main methods are used for item statistics, Classical Test\nTheory and Latent Trait Analysis (LTA) (Rasch analysis and Item Response\nTheory). The former is simpler and evaluates at the level of a test, whereas the latter\nlooks at individual questions. More details on these can be found elsewhere\n(McAlpine, 2002c; Mills et al., 2002). Boyle et al (2002) explore the use of Classical\nTest Theory, Item Response Theory and Rasch analysis with a set of 25 questions\nused by 350 test-takers. They concluded that the present approach by many practi-\ntioners to CAA of neglecting the rigorous quality assurance of items is untenable, and\nthat this is particularly problematic for high-stakes assessment. Boyle and O\u2019Hare\n(2003) recommend that training in item construction and analysis should be\nobligatory for staff who are involved in developing CAA tests and that items should\nbe peer-reviewed and trialled before use. Statistics can be used to aid curriculum\ndesign and quality control (Bull & McKenna, 2004). Feedback of this type can be\nmotivating in terms of enabling a student to identify areas of weakness and evaluate\ntheir performance against stated learning outcomes.\nConcerns about the risk of students guessing answers are addressed in two main\nways: first by discounting a test\u2019s guess factor, and second by adjusting the marking\nscheme away from simple tariffs where \u2018one correct answer equals one mark\u2019 to\ninclude negative marking. Confidence-based assessment is where marks awarded for\na response are predicted on a student\u2019s confidence that the correct response has been\ngiven (Davies, 2002; Gardner-Medwin & Gahan, 2003; McAlpine & Hesketh, 2003).\nReview of computer-assisted assessment 27\nConcerns about the risk of cheating in summative tests might be reduced by strat-\negies such as providing \u2018blinker screens\u2019 and proper invigilation, by interleaving\nparticipants taking different tests and by randomising item and response order (BSI,\n2002; Bull & McKenna, 2004; Pain & LeHeron, 2003). Other tactics include the use\nof item banks.\nFuture directions\nResearch is still needed into understanding the differences between paper-based and\nonline assessments across different subject areas and levels. Additionally we need to\nexplore how this knowledge can be used to produce more effective questions and\nappropriate administration of tests. Greater understanding is also needed into effec-\ntive question design and how this links to learning outcomes. It seems likely that CAA\nwill continue to require specialised skills in particular for the construction of good\nitems. Concerns about cost-benefit may be partially addressed by the integration of\nother technologies such as multimedia as indicated by the graphical, animated\n(FLASH) and open-ended JAVA item types provided by more recent CAA systems\n(Question Mark Computing Ltd, 2004; TOIA, 2004).\nAutomated essay marking may address concerns about the difficulty of assessing\nhigher-level outcomes. However, this is not currently part of many CAA systems and\nrequires significant resources in terms of skills and time (Christie, 2003).\nIn terms of system design, the pressure for greater interoperability may force the\ndevelopment of standards for test interoperability beyond items, and current de facto\nstandards such as Questionmark Markup Language are increasingly being supple-\nmented by the development of \u2018open\u2019 standards such as the IMS Consortium\u2019s QTI\nspecification (IMS, 2003). Sclater and Howie\u2019s (2003) \u2018ideal\u2019 CAA system is meant\nto fulfil institutional needs that current \u2018off the shelf\u2019 commercial CAA system argu-\nably do not. They identify 21 possible roles that a full-scale CAA system might\nrequire: six functional roles (authors, viewers and validators of questions, test\nauthors, viewers and validators); three associated with the sale of items or assess-\nments; three administrative roles; six associated with processing results; and three to\ndo with delivery (test-taker, timetabler and invigilator).\nConclusion\nThis paper has provided a review of current activities in the design, delivery and anal-\nysis of online assessment. Many of the barriers and enablers to effective implementa-\ntion mirror those found in the uptake and use of other learning technologies; however,\nCAA differs because it is perceived as more high risk and central to the learning and\nteaching process. Much of the research raises fundamental questions about the whole\nlearning and teaching process; in particular, raising complex pedagogical questions\nabout the role of assessment and its relationship to learning.\nThe role of technology and how it might impact on assessment is still in its infancy\nand we need to develop new models for exploring this. Similarly we need to understand\n28 G. Conole and B. Warburton\nmore about the barriers and enablers to using these tools effectively and mechanisms\nfor addressing these. In light of this there are real issues in terms of what we might\nexpect or want students to learn. In addition there is a general shift across education\nfrom assessment of products or outputs to assessing the processes of learning.\nTherefore we need to consider what we want to be assessing and how best to do this.\nSimilarly, issues arise given the new forms of communication and collaborations that\nare now possible. How can we measure the interactions that occur within an online\ndiscussion forum and how can we attribute this in terms of student learning? What\nabout those who do not contribute\u2014\u2019the lurkers\u2019\u2014are they opting out or learning\ndifferently (e.g. vicariously by reading and reflection on the postings of others)? Conole\n(2004b) lists a number of questions CAA researcher need to address, including explo-\nration of which new forms of assessment might arise as a result of the impact of tech-\nnologies. CAA raises fundamental questions about the whole learning and teaching\nprocess that we need to research and address if we can achieve the desired goal of maxi-\nmising the potential technologies offer to improve learning, teaching and assessment.\nReferences\nAERA (1999) Standards for educational and psychological testing (Washington, DC, AERA).\nALT (2003) The future of higher education: the Association for Learning Technologies response\nto the white paper. Available online at: http:\/\/www.alt.ac.uk\/docs\/he_wp_20030429_final.doc\n(accessed 19 December 2003).\nATP (2003) Guidelines for computer-based testing (Washington, DC, ATP).\nAnderson, L., Krathwohl, D. & Bloom, B. (2001) A taxonomy for learning, teaching, and assessing: a\nrevision of Bloom\u2019s taxonomy of educational objectives (New York, Longman).\nAshton, H. S., Schofield, D. K. & Woodgar, S. C. (2003) Piloting summative web assessment in\nsecondary education, in: J. Christie (Ed.) Proceedings of the 7th International Computer\u2013Assisted\nAssessment Conference, Loughborough University, 8\u20139 July 2003.\nAshton, H. S., Beavers, C. E., Schofield, D. K. & Youngson, M. A. (2004) Informative reports\u2014\nexperiences from the Pass-IT project, in: M. Ashby and R. Wilson (Eds) Proceedings of the 8th\nInternational Computer\u2013Assisted Assessment Conference, Loughborough University, 6\u20137 July\n2004.\nBarnett, R. (1997) Higher education: a critical business (Buckingham, Society for Research into\nHigher Education and the Open University Press).\nBaty, P. (2004, March 12) Litigation fees top \u00a315m as academic disputes grow, Times Higher\nEducation Supplement, 2004, p. 7.\nBennett, R. E. (2002a) Inexorable and inevitable: the continuing story of technology and\nassessment. Journal of Technology, Learning and Assessment, 1. Available online at: http:\/\/\nwww.bc.edu\/research\/intasc\/jtla\/journal\/pdf\/v1n1_jtla.pdf (accessed 17 July 2004).\nBennett, R. E. (2002b) Using electronic assessment to measure student performance. The State Educa-\ntion Standard, National Association of State Boards of Education.\nBloom, B., Engelhart, M., Furst, E., Hill, W. & Krathwohl, D. (1956) Taxonomy of educational\nobjectives: the classification of educational goals. Handbook 1: cognitive domain (London, Longman).\nBoyle, A., Hutchison, D., O\u2019Hare, D. & Patterson, A. (2002) Item selection and application in\nhigher education, in: M. Danson (Ed.), 6th International CAA Conference (Loughborough,\nLoughborough University).\n Boyle, A. & O\u2019Hare, D. (2003) Assuring quality computer-based assessment development in UK\nhigher education, in: J. Christie (Ed.) 7th International CAA Conference, Loughborough\nUniversity, 8\u20139 July 2004.\nReview of computer-assisted assessment 29\nBrown, G., Bull, J. & Pendlebury, M. (1997) Assessing student learning in higher education (Routledge,\nLondon).\nBSI (2002) Code of practice for the use of information technology (IT) in the delivery of assessments\n(London, BSI).\nBull, J. (1999) Update on the National TLTP3 Project The Implementation and Evaluation of\nComputer-assisted Assessment, in: M. Danson (Ed.), 3rd International CAA Conference,\nLoughborough University, 16\u201317 June 1999.\nBull, J. (2001) TLTP85 implementation and evaluation of computer-assisted assessment: final report.\nAvailable online at: www.caacentre.ac.uk\/dldocs\/final_report.pdf (accessed16 March 2003).\nBull, J. & Hesketh, I. (2001) Computer-assisted assessment centre update, in M. Danson &\nC. Eabry (Eds) 5th International CAA Conference, Loughborough University, 2\u20133 July 2001.\nBull, J. & Mckenna, C. (2000) Computer-assisted assessment centre (TLTP3) update, in:\nM. Danson (Ed.) 4th International CAA Conference, Loughborough University, 21\u201322 June\n2000.\nBull, J. & McKenna, C. (2004) Blueprint for computer-assisted assessment (London, RoutledgeFalmer)\nChristie, J. (2003) Automated essay marking for content\u2014does it work?, in J. Christie. (Ed.) 7th\nInternational CAA Conference, Loughborough, 8\u20139 July 2001.\nConole, G. (2004a) Report on the effectiveness of tools for e-learning, report for the JISC commis-\nsioned Research Study on the Effectiveness of Resources, Tools and Support Services used by\nPractitioners in Designing and Delivering E-Learning Activities.\nConole, G. (2004b) Assessment as a catalyst for innovation, invited keynote and paper for the Quality\nAssurance Agency for Higher Education, Assessment Workshop 5. Available online at: http:\/\/\nwww.qaa.ac.uk\/scottishenhancement\/events\/default.htm (accessed 10 September 2004).\nConole, G. & Dyke, M. (2004) What are the affordances of Information and communication\ntechnologies?, ALT-J, 12(2), 111\u2013123.\nCosemans, D., Van Rentergem, L., Verburgh, A. & Wils, A. (2002) Campus wide setup of ques-\ntion mark perception (V2.5) at the Katholieke Universiteit Leuven (Belgium)\u2014facing a large\nscale implementation, in: M. Danson (Ed.) 5th International CAA Conference, University of\nLoughborough, 2\u20133 July 2001.\nDanson, M. (2003) Implementing online assessment in an emerging MLE: a generic guidance document\nwith practical examples (Bristol, JISC).\nDanson, M., Dawson, B. & Baseley, T. (2001) Large scale implementation of question mark\nperception (V2.5)\u2014experiences at Loughborough University, in: M. Danson (Ed.) 6th\nInternational CAA Conference, Loughborough University, 2\u20133 July 2001.\nDavies, P. (2001) CAA must be more than multiple-choice tests for it to be academically credible?,\nin: M. Danson & C. Eabry (Eds) 5th International CAA Conference, Loughborough University.\nDavies, P. (2002) There\u2019s no confidence in multiple-choice testing, in: M. Danson (Ed.) 6th\nInternational CAA Conference, Loughborough University, 4\u20135 July 2002.\nDuke-Williams, E. & King, T. (2001) Using computer-aided assessment to test higher level\nlearning outcomes, in: M. Danson & C. Eabry (Eds) 5th International CAA Conference,\nLoughborough University, 2\u20133 July 2001.\nFarrer, S. (2002) End short contract outrage, MPs insist, Times Higher Education Supplement.\nFarthing, D. & McPhee, D. (1999) Multiple choice for honours-level students?, in: M. Danson\n(Ed.) 3rd International CAA Conference, Loughborough University, 16\u201317 June 1999.\nFiddes, D. J., Korabinski, A. A., McGuire, G. R., Youngson, M. A. & McMillan, D. (2002) Are\nmathematics exam results affected by the mode of delivery?, ALT-J, 10(6), 1\u20139.\nGardner-Medwin, A. & Gahan, M. (2003) Formative and summative confidence-based assess-\nment, in: J. Christie (Ed.) 7th International CAA Conference, Loughborough University, 8\u20139\nJuly 2003.\n Gitomer, D. H., Curtis, M. E., Glaser, R. & Lensky, D. B. (1987) Processing differences as a\nfunction of item difficulty in verbal analogy performance, Journal of Educational Psychology,\n79, 212\u2013219.\n30 G. Conole and B. Warburton\nGoddard, A. (2002, November 1) Universities blamed for exam fiasco, Times Higher Education\nSupplement, p. 7.\nHambrick, K. (2002) Critical issues in online, large-scale assessment: An exploratory study to identify\nand refine issues. Unpublished Ph.D. thesis, Capella University.\nHarwood, I. (2004) What happens when computer-assisted assessment goes wrong?, British\nJournal of Educational Technology, submitted.\nHarwood, I. & Warburton, W. (2004) Thinking the unthinkable: using project risk management\nwhen introducing computer-assisted assessments, in: M. Ashby & R. Wilson (Eds) 8th\nInternational CAA Conference, Loughborough University, 6\u20137 July 2004.\nImrie, B. (1995) Assessment for learning: quality and taxonomies, Assessment and Evaluation in\nHigher Education, 20, 171\u2013189.\nIMS (2003) QTI Enterprise v1.1 specs, examples and schemas. Available online at: http:\/\/\nwww.imsglobal.org\/specificationdownload.cfm (accessed 2 March 2003).\nKennedy, N. (1998) Experiences of assessing LMU students over the web. Leeds Metropolitan\nUniversity.\nKrathwohl, D., Bloom, B. & Masia, B. (1964) Taxonomy of educational objectives the classification of\neducational goals (London, Longman).\nLaurillard, D. (2002,) Rethinking university teaching a conversational framework for the effective use of\nlearning technologies (2nd edn) (London, RoutledgeFalmer).\nLay, S. & Sclater, N. (2001) Question and test interoperability: an update on national and interna-\ntional developments, in: M. Danson & C. Eabry (Eds) 5th International CAA Conference,\nLoughborough University, 2\u20133 July 2001.\nLilley, M. & Barker, T. (2003) An evaluation of a computer adaptive test in a UK university\ncontext, in: J. Christie (Ed.) 7th International CAA Conference, Loughborough University, 8\u20139\nJuly 2003.\nMason, O. & Grove-Stephensen, I. (2002) Automated free text marking with paperless school, in:\nM. Danson (Ed.) 6th International CAA Conference, Loughborough University, 2\u20133 July 2001.\nMcAlpine, M. (2002a) Principles of assessment (Luton, CAA Centre).\nMcAlpine, M. (2002b) Design requirements of a databank (Luton, CAA Centre).\nMcAlpine, M. (2002c) A summary of methods of item analysis (Luton, CAA Centre).\nMcAlpine, M. & Hesketh, I. (2003) Multiple response questions\u2014allowing for chance in authentic\nassessments, in: J. Christie (Ed.) 7th International CAA Conference, Loughborough University,\n8\u20139 July 2000.\nMcKenna, C. (2001) Academic approaches and attitudes towards CAA: a qualitative study, in: M.\nDanson & C. Eaborg (Eds) 5th International CAA Conference, Loughborough University, 2\u20133\nJuly 2001.\nMcKenzie, D., Hallam, B., Baggott, G. & Potts, J. (2002) TRIADS experiences and Developments,\nin: M. Danson (Ed.) 6th International CAA Conference, Loughborough University, 2\u20133 July 2002.\nMiller, A., Imrie, B. & Cox, K. (1998) Student assessment in higher education: a handbook for assess-\ning performance (London, Kogan Page).\nMills, C., Potenza, M., Fremer, J. & Ward, C. (Eds) (2002) Computer-based testing\u2014building the\nfoundation for future assessment (New York, Lawrence Erlbaum Associates).\nOuttz, J. L. (1998) Testing medium, validity and test performance, in: M. D. Hakel (Ed.) Beyond\nmultiple choice evaluating alternative to traditional testing for selection (New York, Lawrence\nErlbaum Associates).\nO\u2019Reilly, M. & Morgan, C. (1999) Online assessment: creating communities and opportunities,\nin: S. Brown, P. Race & J. Bull (Eds) Computer-assisted assessment in higher education (London,\nKogan Page), 149\u2013161.\nPain, D. & Leheron, K. (2003) WebCT and online assessment: the best thing since SOAP?,\nEducational Technology and Society, 6, 62\u201371.\nPaterson, J. S. (2002) What\u2019s in a name? A new hierarchy for question types, in: M. Danson (Ed.)\n6th International CAA Conference, Loughborough University, 4\u20135 July 2002.\nReview of computer-assisted assessment 31\nPritchett, N. (1999) Effective question design, in: S. Brown, P. Race & J. Bull (Eds) Computer-\nassisted assessment in higher education (London, Kogan Page).\nQAA. (1998) University of Bath quality audit report. Available online at: http:\/\/www.qaa.ac.uk\/\nrevreps\/instrev\/bath\/comms.htm (accessed 4 September 2003).\nQuestion Mark Computing Ltd (2004) Perception: Windows based authoring, Available online at:\nhttp:\/\/www.questionmark.com\/uk\/perception\/authoring_windows.htm (accessed 29 August\n2004).\nSclater, N., Low, B. & Barr, N. (2002) Interoperability with CAA: does it work in practice?, in:\nM. Danson (Ed.) Proceedings of the 6th International Conference on Computer-Assisted Assessment,\nLoughborough University, 4\u20135 July 2002.\nSclater, N. & Howie, K. (2003) User requirements of the ultimate online assessment engine,\nComputers and Education, 40, 285\u2013306.\nSclater, N. (Ed.) (2004) Final report for the Item Banks Infrastructure Study (IBIS) (Bristol, JISC).\nSeale, J. (Ed.) (2003) Learning technology in transition\u2014from individual enthusiasm to institutional\nimplementation (Swets and Zeitlinger, The Netherlands).\nSommerlad, E., Pettigrew, M., Ramsden, C. & Stern, E. (1999) Synthesis of TLTP annual reports\n(London, Tavistock Institute).\nStephens, D. & Mascia, J. (1997) Results of a (1995) Survey into the use of computer-assisted assess-\nment in institutions of higher education in the UK, University of Loughborough.\nStevenson, A., Sweeney, P., Greenan, K. & Alexander, S. (2002) Integrating CAA within the\nUniversity of Ulster, in: M. Danson (Ed.) Proceedings of the 6th International conference on\ncomputer-assisted assessment, University of Loughborough, 4\u20135 July 2002.\nScottish Qualifications Authority (2003) Guidelines for online assessment in further education\n(Glasgow, Scottish Qualifications Agency).\nTOIA (2004) The TOIA Project web site. Available online at: http:\/\/www.toia.ac.uk (accessed 31\nAugust 2004).\nWarburton, W. & Conole, G. (2003) CAA in UK HEIs: the state of the art, in: J. Christie (Ed.)\n7th International CAA Conference, University of Loughborough, 8\u20139 July 2003.\nWarburton, W. & Harwood, I. (2004) Implementing perception across a large university: getting it\nright, Perception European Users Conference (Edinburgh, Question Mark Computing).\nWard, W. C., Frederiksen, N. & Carlson, S. B. (1980) Construct validity of free response and\nmachine-scorable forms of a test, Journal of Educational Measurement, 7(1), 11\u201329.\nZakrzewski, S. & Steven, C. (2000) A model for computer-based assessment: the Catherine wheel\nprinciple, Assessment and Evaluation in Higher Education, 25, 201\u2013215.\n"}