{"doi":"10.1214\/09-AOS720","coreId":"96435","oai":"oai:eprints.lse.ac.uk:31540","identifiers":["oai:eprints.lse.ac.uk:31540","10.1214\/09-AOS720"],"title":"Sparsistency and rates of convergence in large covariance matrix estimation","authors":["Lam, Clifford","Fan, Jianqing"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2009","abstract":"This paper studies the sparsistency and rates of convergence for estimating sparse covariance and precision matrices based on penalized likelihood with nonconvex penalty functions. Here, sparsistency refers to the property that all parameters that are zero are actually estimated as zero with probability tending to one. Depending on the case of applications, sparsity priori may occur on the covariance matrix, its inverse or its Cholesky decomposition. We study these three sparsity exploration problems under a unified framework with a general penalty function. We show that the rates of convergence for these problems under the Frobenius norm are of order (sn log pn\/n)1\/2, where sn is the number of nonzero elements, pn is the size of the covariance matrix and n is the sample size. This explicitly spells out the contribution of high-dimensionality is merely of a logarithmic factor. The conditions on the rate with which the tuning parameter \u03bbn goes to 0 have been made explicit and compared under different penalties. As a result, for the L1-penalty, to guarantee the sparsistency and optimal rate of convergence, the number of nonzero elements should be small: at most, among  parameters, for estimating sparse covariance or correlation matrix, sparse precision or inverse correlation matrix or sparse Cholesky factor, where  is the number of the nonzero elements on the off-diagonal entries. On the other hand, using the SCAD or hard-thresholding penalty functions, there is no such a restriction","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/96435.pdf","fullTextIdentifier":"http:\/\/eprints.lse.ac.uk\/31540\/1\/Sparsistency_and_Rates_of_Convergence_%28author%29.pdf","pdfHashValue":"f4c300ec07d785702b655144af1a4ee101d047f0","publisher":"Institute of Mathematical Statistics","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lse.ac.uk:31540<\/identifier><datestamp>\n      2017-10-26T10:39:29Z<\/datestamp><setSpec>\n      74797065733D4445505453:4C53452D5354<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lse.ac.uk\/31540\/<\/dc:relation><dc:title>\n        Sparsistency and rates of convergence in large covariance matrix estimation<\/dc:title><dc:creator>\n        Lam, Clifford<\/dc:creator><dc:creator>\n        Fan, Jianqing<\/dc:creator><dc:subject>\n        HA Statistics<\/dc:subject><dc:description>\n        This paper studies the sparsistency and rates of convergence for estimating sparse covariance and precision matrices based on penalized likelihood with nonconvex penalty functions. Here, sparsistency refers to the property that all parameters that are zero are actually estimated as zero with probability tending to one. Depending on the case of applications, sparsity priori may occur on the covariance matrix, its inverse or its Cholesky decomposition. We study these three sparsity exploration problems under a unified framework with a general penalty function. We show that the rates of convergence for these problems under the Frobenius norm are of order (sn log pn\/n)1\/2, where sn is the number of nonzero elements, pn is the size of the covariance matrix and n is the sample size. This explicitly spells out the contribution of high-dimensionality is merely of a logarithmic factor. The conditions on the rate with which the tuning parameter \u03bbn goes to 0 have been made explicit and compared under different penalties. As a result, for the L1-penalty, to guarantee the sparsistency and optimal rate of convergence, the number of nonzero elements should be small: at most, among  parameters, for estimating sparse covariance or correlation matrix, sparse precision or inverse correlation matrix or sparse Cholesky factor, where  is the number of the nonzero elements on the off-diagonal entries. On the other hand, using the SCAD or hard-thresholding penalty functions, there is no such a restriction.<\/dc:description><dc:publisher>\n        Institute of Mathematical Statistics<\/dc:publisher><dc:date>\n        2009<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lse.ac.uk\/31540\/1\/Sparsistency_and_Rates_of_Convergence_%28author%29.pdf<\/dc:identifier><dc:identifier>\n          Lam, Clifford and Fan, Jianqing  (2009) Sparsistency and rates of convergence in large covariance matrix estimation.  Annals of Statistics, 37 (6B).  pp. 4254-4278.  ISSN 0090-5364     <\/dc:identifier><dc:relation>\n        http:\/\/www.imstat.org\/aos\/<\/dc:relation><dc:relation>\n        10.1214\/09-AOS720<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lse.ac.uk\/31540\/","http:\/\/www.imstat.org\/aos\/","10.1214\/09-AOS720"],"year":2009,"topics":["HA Statistics"],"subject":["Article","PeerReviewed"],"fullText":"Sparsistency and Rates of Convergence in\nLarge Covariance Matrices Estimation \u2217\nBy Clifford Lam, Jianqing Fan\nLondon School of Economics, Princeton university\nThis paper studies the sparsistency and rates of convergence for estimat-\ning sparse covariance and precision matrices based on penalized likelihood\nwith non-concave penalty functions. Here, sparsistency refers to the prop-\nerty that all parameters that are zero are actually estimated as zero with\nprobability tending to one. Depending on the case of applications, spar-\nsity priori may occur on the covariance matrix, its inverse or its Cholesky\ndecomposition. We study these three sparsity exploration problems under\na unified framework with a general penalty function. We show that the\nrates of convergence for these problems under the Frobenius norm are of\norder (sn log pn\/n)\n1\/2, where sn is the number of non-sparse elements, pn is\nthe size of the covariance matrix and n is the sample size. This explicitly\nspells out the contribution of high-dimensionality is merely of a logarithmic\nfactor. The biases of the estimators using different penalty functions are\nexplicitly obtained. As a result, for the L1-penalty, to guarantee the spar-\nsistency and optimal rate of convergence, the non-sparsity rates should be\nlow: s\u2032n = O(pn) at most, among O(p\n2\nn) parameters, for estimating sparse\ncovariance or correlation matrix, sparse precision or inverse correlation ma-\ntrix or sparse Cholesky factor, where s\u2032n is the number of the non-sparse\nelements on the off-diagonal entries. On the other hand, using the SCAD\nor hard-thresholding penalty functions, there is no such a restriction.\nShort Title: Covariance Estimation with Penalization.\n\u2217Clifford Lam is Lecturer, Department of Statistics, London School of Economics and Political\nScience, London, WC2A 2AE (email: C.Lam2@lse.ac.uk); Jianqing Fan is Professor, Department of\nOperation Research and Financial Engineering, Princeton University, Princeton, NJ 08544 (email:\njqfan@princeton.edu). Financial support from the NSF grant DMS-0354223, DMS-0704337 and NIH\ngrant R01-GM072611 is gratefully acknowledged.\n1\nAMS 2000 subject classifications. Primary 62F12; secondary 62J07.\nKey words and phrases. Covariance matrix, high dimensionality, consistency, non-\nconcave penalized likelihood, sparsistency, asymptotic normality.\n1 Introduction\nCovariance matrix estimation is a common statistical problem that arises in many\nscientific applications. For example, in financial risk assessment or longitudinal study,\nan input of covariance matrix \u03a3 is needed, whereas an inverse of the covariance matrix,\nthe precision matrix\u03a3\u22121, is required for optimal portfolio selection, linear discriminant\nanalysis or graphical network models. Yet, the number of parameters in the covariance\nmatrix grows quickly with dimensionality. Depending on the applications, the sparsity\nof the covariance matrix or precision matrix are frequently imposed to strike a balance\nbetween biases and variances. For example, in longitudinal data analysis (see e.g.\nDiggle and Verbyla (1998), or Bickel and Levina (2008b)), it is reasonable to assume\nthat remote data in time are weakly correlated, whereas in Gaussian graphical models,\nthe sparsity of the precision matrix is a reasonable assumption (Dempster (1972)).\nThis initiates a series of researches focusing on the parsimony of a covariance ma-\ntrix. Smith and Kohn (2002) used priors which admit zeros on the off-diagonal elements\nof the Cholesky factor of the precision matrix \u2126 = \u03a3\u22121, while Wong, Carter and Kohn\n(2003) used zero-admitting prior directly on the off-diagonal elements of \u2126 to achieve\nparsimony. Wu and Pourahmadi (2003) used the Modified Cholesky Decomposition\n(MCD) to nonparametrically find a banded structure for \u2126 for longitudinal data while\npreserving positive definiteness of the resulting estimator. Bickel and Levina (2008b)\ndeveloped consistency theories on banding methods for longitudinal data, both for \u03a3\n2\nand \u2126.\nPenalized likelihood methods are used by various authors to achieve parsimony\non covariance selection. Fan and Peng (2004) has laid down a general framework for\npenalized likelihood with diverging dimensionality, with general conditions for oracle\nproperty stated and proved. However, it is not clear whether it is applicable to the\nspecific case of covariance matrix estimation. In particular, they did not link the\ndimensionality pn with the non-sparsity size sn, which is the number of non-zero ele-\nments in the true covariance matrix \u03a30, or precision matrix \u21260. A direct application\nof their results to our setting can only handle a relatively small covariance matrix of\nsize pn = o(n\n1\/10), which behaves like a constant pn.\nRecently, there is a surge of interest on the estimation of sparse covariance ma-\ntrix or precision matrix using penalized likelihood method. Huang, Liu, Pourahmadi\nand Liu (2006) used the LASSO on the off-diagonal elements of the Cholesky factor\nfrom MCD, while Meinshausen and Bu\u00a8hlmann (2006), d\u2019Aspremont, Banerjee, and\nEl Ghaoui (2008) and Yuan and Lin (2007) use different LASSO algorithms to select\nsparse elements in the precision matrix. A novel penalty called the nested Lasso was\nconstructed in Levina, Rothman and Zhu (2008) to penalize on these off-diagonal el-\nements. Thresholding the sample covariance matrix in high-dimensional setting was\nthoroughly studied by El Karoui (2007) and Bickel and Levina (2008a) with remarkable\nresults for high dimensional applications. However, it is not directly applicable to esti-\nmating sparse precision matrix when the dimensionality pn is greater than the sample\nsize n. Wagaman and Levina (2007) proposed an Isomap method for discovering mean-\ningful orderings of variables based on their correlations that result in block-diagonal\nor banded correlation structure, resulting in an Isoband estimator. A permutation\ninvariant estimator, called SPICE, was proposed in Rothman, Bickel, Levina and Zhu\n3\n(2007) based on penalized likelihood with L1-penalty on the off-diagonal elements for\nthe precision matrix. They obtained remarkable results on the rates of convergence.\nThe rate for estimating \u2126 under the Frobenius norm is of order (sn log pn\/n)\n1\/2, with\ndimensionality cost only a logarithmic factor in the overall mean-square error, where\nsn = pn + sn1, pn is the number of the diagonal elements and sn1 is the number of the\nnon-sparse off-diagonal entries. However, such rate of convergence does not address\nexplicitly the sparsistency such as those in Fan and Li (2001) and Zhao and Yu (2006),\nthe bias issues of the L1-penalty nor the sampling distribution of nonsparse elements.\nThese are the core issues of the study. By sparsistency, we mean the property that\nall parameters that are zero are actually estimated as zero with probability tending\nto one, a more loose definition than that of Ravikumar, Lafferty, Liu and Wasserman\n(2008).\nIn this paper, we investigate the aforementioned problems using penalized likeli-\nhood method. Assume a normal random sample {yi}1\u2264i\u2264n with mean zero and co-\nvariance matrix \u03a30. The sparsity of the true precision matrix \u21260 can be explored by\nminimizing the penalized negative normal likelihood:\nq1(\u2126) = tr(S\u2126)\u2212 log |\u2126|+\n\u2211\ni 6=j\np\u03bbn1(|\u03c9ij|), (1.1)\nwhere S = n\u22121\n\u2211n\ni=1 yiy\nT\ni is the sample covariance matrix, with \u2126 = (\u03c9ij), and\np\u03bbn1(\u00b7) is a penalty function, depending on a regularization parameter \u03bbn1, which can\nbe nonconvex. For instance, the L1-penalty p\u03bb(\u03b8) = \u03bb|\u03b8| is convex, while the hard-\nthresholding penalty defined by p\u03bb(\u03b8) = \u03bb\n2\u2212 (|\u03b8| \u2212\u03bb)21{|\u03b8|<\u03bb}, and the SCAD penalty\ndefined by\np\u2032\u03bb(\u03b8) = \u03bb1{\u03b8\u2264\u03bb} + (a\u03bb\u2212 \u03b8)+1{\u03b8>\u03bb}\/(a\u2212 1), for some a > 2, (1.2)\n4\nare nonconvex. Nonconvex penalty is introduced to reduce bias when the true pa-\nrameter has a relatively large magnitude. For example, the SCAD penalty remains\nconstant when \u03b8 is large, while the L1-penalty grows linearly with \u03b8. See Fan and Li\n(2001) for a detailed account of this and other advantages of such a penalty function.\nSimilarly, the sparsity of the true covariance matrix \u03a30 can be explored by mini-\nmizing\nq2(\u03a3) = tr(S\u03a3\n\u22121) + log |\u03a3|+\n\u2211\ni6=j\np\u03bbn2(|\u03c3ij|), (1.3)\nwhere \u03a3 = (\u03c3ij). Note that we only penalize the off-diagonal elements of \u03a3 or \u2126 in\nthe aforementioned two methods, since the diagonal elements of \u03a30 and \u21260 do not\nvanish.\nThe computation of the non-concave maximum likelihood problems can be solved\nby a sequence of L1-penalized likelihood problems via local linear approximation (Zou\nand Li (2008)). For example, given the current estimate \u2126k = (\u03c9ij,k), by the local\nlinear approximation to the penalty function,\nq1(\u2126) \u2248tr(S\u2126)\u2212 log |\u2126|\n+\n\u2211\ni6=j\n[p\u03bbn1(|\u03c9ij,k|) + p\u2032\u03bbn1(|\u03c9ij,k|)(|\u03c9ij| \u2212 |\u03c9ij,k|)].\n(1.4)\nHence, \u2126k+1 should be taken to maximize the right-hand side of (1.4):\n\u2126k+1 = argmax\u2126\n[\ntr(S\u2126)\u2212 log |\u2126|+\n\u2211\ni 6=j\np\u2032\u03bbn1(|\u03c9ij,k|)|\u03c9ij|\n]\n, (1.5)\nafter ignoring the two constant terms. Problem (1.5) is the penalized L1-likelihood.\nIn particular, if we take the most primitive initial value \u21260 = 0, then\n\u21261 = argmax\u2126\n[\ntr(S\u2126)\u2212 log |\u2126|+ \u03bbn1\n\u2211\ni6=j\n|\u03c9ij|\n]\n,\n5\nis already a good estimator. Iterations of (1.5) reduces the biases of the estimator.\nIn fact, in a different setup, Zou and Li (2008) shows that one iteration of such a\nprocedure suffices as long as the initial values are good enough. See Fan, Feng and Wu\n(2008) for detailed implementations on the estimation of precision matrices. See also\nZhang (2007) for a general solution to the nonconvex penalized least-squares problem.\nIn studying sparse covariance or precision matrix, it is important to distinguish\nbetween the diagonal and off-diagonal elements, since the diagonal elements are always\npositive and contribute to the overall mean-squares errors. For example, the true\ncorrelation matrix, denoted by \u03930, has the same sparsity structure as \u03a30 without the\nneed to estimate its diagonal elements. In view of this fact, we introduce a revised\nmethod (3.2) to take this advantage. It turns out that the correlation matrix can be\nestimated with a faster rate of convergence, with rate (sn1 log pn\/n)\n1\/2 instead of ((pn+\nsn1) log pn\/n)\n1\/2, where sn1 is the number of non-vanishing correlation coefficients.\nSimilar advantages can be taken on the estimation of the true inverse correlation\nmatrix, denoted by\u03a80. See Section 2.2. This is an extension of the work of Rothman et\nal. (2007) using the L1-penalty. Such an extension is important since the non-concave\npenalized likelihood ameliorates the bias problem of the L1-penalized likelihood.\nThe bias issues of the commonly used L1-penalty, or LASSO, can be seen from\nour theoretical results. In fact, it is not always possible to choose the regularization\nparameters \u03bbni in the problems (1.3) and (1.1) to satisfy both consistency and spar-\nsistency properties. This is in fact one of the motivations for introducing nonconvex\npenalty functions in Fan and Li (2001) and Fan and Peng (2004), but we state and\nprove the explicit rates in the current context. In particular, we demonstrate that\nL1-penalized likelihood can achieve simultaneously the optimal rate and sparsistency\nfor estimation of \u03a30 or \u21260 only when the number of nonsparse elements in off-diagonal\n6\nentries are no larger than O(pn). On the other hand, using the nonconvex penalty like\nSCAD or hard-thresholding penalty, such an extra restriction is not needed.\nIn this paper, we also compare two different formulations of penalized likelihood\nusing the modified Cholesky decomposition, exploring their respective rates of conver-\ngence and sparsity properties.\nThroughout this paper, we use \u03bbmin(A), \u03bbmax(A), and tr(A) to denote the minimum\neigenvalue, maximum eigenvalue, and trace of a symmetric matrix A, respectively. For\na matrix B, we define the operator norm and the Frobenius norm, respectively, as\n\u2016B\u2016 = \u03bb1\/2max(BTB) and \u2016B\u2016F = tr1\/2(BTB).\n2 Estimation of sparse precision matrix\nIn this section we present the analysis of (1.1) for estimating sparse precision matrix.\nBefore stating and proving the rate of convergence and sparsistency of the resulting\nestimator, we introduce some notations and present regularity conditions concerning\nthe penalty function p\u03bb(\u00b7) and the precision matrix \u21260.\nLet S1 = {(i, j) : \u03c90ij 6= 0}, where \u21260 = (\u03c90ij). Denote sn1 = |S1| \u2212 pn, which is the\nnumber of non-zero elements in the off-diagonal entries of \u21260. Define\nan1 = max\n(i,j)\u2208S1\np\u2032\u03bbn1(|\u03c90ij|), bn1 = max(i,j)\u2208S1 p\n\u2032\u2032\n\u03bbn1\n(|\u03c90ij|).\nThe term an1 is related to the biases of the penalized likelihood estimate due to pe-\nnalization. Note that for L1-penalty, an1 = \u03bbn and bn1 = 0, whereas for SCAD,\nan1 = bn1 = 0 for sufficiently large n under the last assumption of condition (B) below.\nWe assume the following regularity conditions:\n7\n(A) There exists constants \u03c41 and \u03c42 such that\n0 < \u03c41 < \u03bbmin(\u03a30) \u2264 \u03bbmax(\u03a30) < \u03c42 <\u221e for all n.\n(B) an1 = O({1 + pn\/(sn1 + 1)}(log pn\/n)1\/2), bn1 = o(1), and\nmin(i,j)\u2208S1 |\u03c90ij|\/\u03bbn1 \u2192\u221e as n\u2192\u221e.\n(C) The penalty p\u03bb(\u00b7) is singular at the origin, with limt\u21930 p\u03bb(t)\/(\u03bbt) = k > 0.\n(D) There are constants C andD such that, when \u03b81, \u03b82 > C\u03bbn1, |p\u2032\u2032\u03bbn1(\u03b81)\u2212p\u2032\u2032\u03bbn1(\u03b82)| \u2264\nD|\u03b81 \u2212 \u03b82|.\nCondition (A) bounds uniformly the eigenvalues of \u03a30, which facilitates the proof\nof consistency. It also includes a wide class of covariance matrices as noted in Bickel\nand Levina (2008b). The rates an1 and bn1 in condition (B) are also needed for proving\nconsistency. If they are too large, the penalty term can dominate the likelihood term,\nresulting in poor estimates.\nThe last requirement in condition (B) states the rate at which the non-zero pa-\nrameters can be distinguished from zero asymptotically. It is not explicitly needed\nin the proofs, but for asymptotically unbiased penalty functions, this is a necessary\ncondition so that an1 and bn1 are converging to zero fast enough as needed in the first\npart of condition (B). In particular, for the SCAD and hard-thresholding penalties,\nthis condition implies that an1 = bn1 = 0 exactly for sufficiently large n, thus allowing\na flexible choice of \u03bbn1. For the SCAD penalty (1.2), the condition can be relaxed as\nmin(i,j)\u2208S1 |\u03c90ij|\/\u03bbn1 > a.\nSingularity of the origin in condition (C) allows for sparse estimates (Fan and Li\n(2001)). Finally, condition (D) is a smoothing condition for the penalty function, and\n8\nis needed in proving asymptotic normality. The SCAD penalty, for instance, satisfies\nthis condition by choosing the constant D, independent of n, to be large enough.\n2.1 Properties of sparse precision matrix estimation\nMinimizing (1.1) involves nonconvex minimization, and we need to prove that there\nexists a local minimizer \u2126\u02c6 for the minimization problem. We give the rate of conver-\ngence under Frobenius norm. The proof is given in section 5. It is close to the one\ngiven in Rothman et al. (2007), but we now allow for a nonconvex penalty.\nTheorem 1 (Rate of convergence). Under regularity conditions (A)-(D), if (sn1 +\n1) log pn\/n = O(\u03bb\n2\nn1) and (pn+sn1) log pn\/n = o(1), then there exists a local minimizer\n\u2126\u02c6 such that \u2016\u2126\u02c6\u2212\u21260\u20162F = OP{(pn + sn1) log pn\/n}.\nTheorem 1 states explicitly how the non-sparsity size and dimensionality affect the\nrate of convergence. Since there are (pn+sn1) non-zero elements and each of them can\nbe estimated at best with rate O(n\u22121\/2), the total square errors are at least of rate\n(pn + sn1)\/n. The price that we pay for high-dimensionality is merely a logarithmic\nfactor log pn.\nTheorem 1 is also applicable to the L1-penalty function, where the condition for\n\u03bbn1 can be relaxed to log pn\/n = O(\u03bb\n2\nn1). In this case, the local minimizer becomes the\nglobal minimizer. The bias of the L1-penalized estimate an1 \u00b3 \u03bbn1 is controlled via con-\ndition (B), which entails an upper bound on \u03bbn1 = O((1+ pn\/(sn1+1))(log pn\/n)\n1\/2).\nNext we show the sparsistency of the penalized covariance estimator (1.1). We use\nSc to denote the complement of a set S.\nTheorem 2 (Sparsistency). Under regularity conditions (A), (C) and (D), for any\nlocal minimizer of (1.1) satisfying \u2016\u2126\u02c6 \u2212 \u21260\u20162F = OP{(pn + sn1) log pn\/n} and \u2016\u2126\u02c6 \u2212\n9\n\u21260\u20162 = OP (\u03b7n) for a sequence of \u03b7n \u2192 0, if log pn\/n + \u03b7n = O(\u03bb2n1), then with\nprobability tending to 1, \u03c9\u02c6ij = 0 for all (i, j) \u2208 Sc1.\nFirst of all, since \u2016M\u20162 \u2264 \u2016M\u20162F for any matrix M , we can always take \u03b7n =\n(pn + sn1) log pn\/n in Theorem 2, but this will result in more stringent requirement\non the number of sparse elements when L1-penalty is used, as we now explain. The\nsparsistency requires a lower bound on the rate of the regularization parameter \u03bbn1.\nOn the other hand, condition (B) imposes an upper bound on \u03bbn1 when L1-penalty is\nused in order to control the biases. Explicitly, we need, for L1-penalized likelihood,\nlog pn\/n+ \u03b7n = O(\u03bb\n2\nn1) = (1 + pn\/(sn1 + 1))\n2 log pn\/n (2.1)\nfor both consistency and sparsistency to be satisfied. We present two scenarios here for\nthe two bounds to be compatible, making use of the inequalities \u2016M\u20162F\/pn \u2264 \u2016M\u20162 \u2264\n\u2016M\u20162F for a matrix M of size pn.\n1. We always have \u2016\u2126\u02c6 \u2212 \u21260\u2016 \u2264 \u2016\u2126\u02c6 \u2212 \u21260\u2016F . In the worst case scenario where\nthey have the same order, then \u2016\u2126\u02c6 \u2212 \u21260\u20162 = OP ((pn + sn1) log pn\/n) so that\n\u03b7n = (pn + sn1) log pn\/n. It is then easy to see from (2.1) that the two bounds\nare compatible only when sn1 = O(p\n1\/2\nn ).\n2. We also have \u2016\u2126\u02c6\u2212\u21260\u20162F\/pn \u2264 \u2016\u2126\u02c6\u2212\u21260\u20162. In the optimistic scenario where they\nhave the same order,\n\u2016\u2126\u02c6\u2212\u21260\u20162 = OP ((1 + sn1\/pn) log pn\/n),\nwhere 1 + sn1\/pn is the average number of non-zero elements in a row of the\nmatrix \u21260. Hence \u03b7n = (1 + sn1\/pn) log pn\/n, and compatibility of the bounds\nrequires sn1 = O(pn).\n10\nHence even in the optimistic scenario, consistency and sparsistency are guaranteed\nonly when sn1 = O(pn) if the L1-penalty is used, i.e. the precision matrix has to be\nsparse enough.\nHowever, if the penalty function used is unbiased, like the SCAD or the hard-\nthresholding penalties, we do not impose an extra upper bound for \u03bbn1 since its first\nderivative p\u2032\u03bbn1(|\u03b8|) goes to zero fast enough as |\u03b8| increases (exactly equals zero for\nthe SCAD and hard-thresholding penalties, when n is sufficiently large; see condition\n(B) and the explanation thereof). Thus, \u03bbn1 is allowed to decay slower to zero than\nthat for the L1-penalty, allowing even the largest order sn1 = O(p\n2\nn).\nWe remark that asymptotic normality for the estimators of the elements in S1 have\nbeen established in a previous version of this paper. We omit it here for brevity.\n2.2 Properties of sparse inverse correlation matrix estimation\nThe inverse correlation matrix\u03a80 retains the same sparse structure of\u21260. Consistency\nand sparsity results can be achieved with pn as large as log pn = o(n), as long as\n(sn1 + 1) log pn\/n = o(1). We minimize, w.r.t. \u03a8 = (\u03c8ij),\ntr(\u03a8\u0393\u02c6S)\u2212 log |\u03a8|+\n\u2211\ni6=j\np\u03bdn1(|\u03c8ij|), (2.2)\nwhere \u0393\u02c6S = W\u02c6\n\u22121SW\u02c6\u22121 is the sample correlation matrix, with W\u02c62 = DS being the\ndiagonal matrix with diagonal elements of S, and \u03bdn1 is a regularization parameter.\nAfter obtaining \u03a8\u02c6, \u21260 can also be estimated by \u2126\u02dc = W\u02c6\n\u22121\u03a8\u02c6W\u02c6\u22121.\nTo present the rates of convergence for \u03a8\u02c6 and \u2126\u02dc, we define\ncn1 = max\n(i,j)\u2208S1\np\u2032\u03bdn1(|\u03c80ij|), dn1 = max(i,j)\u2208S1 p\n\u2032\u2032\n\u03bdn1\n(|\u03c80ij|),\n11\nwhere \u03a80 = (\u03c8\n0\nij) and modify condition (D) to (D\u2019) with \u03bbn1 there replaced by \u03bdn1,\nand impose\n(B\u2019) cn1 = O({log pn\/n}1\/2), dn1 = o(1). Also, min(i,j)\u2208S1 |\u03c80ij|\/\u03bdn1 \u2192\u221e as n\u2192\u221e.\nTheorem 3 Under regularity conditions (A),(B\u2019),(C) and (D\u2019), if (sn1+1) log pn\/n =\no(1) and (sn1 + 1) log pn\/n = O(\u03bd\n2\nn1), then there exists a local minimizer \u03a8\u02c6 for (2.2)\nsuch that \u2016\u03a8\u02c6 \u2212 \u03a80\u20162F = OP (sn1 log pn\/n) and \u2016\u2126\u02dc \u2212 \u21260\u20162 = OP ((sn1 + 1) log pn\/n)\nunder the operator norm.\nThe proof is sketched in section 5. Note that an order of {pn log pn\/n}1\/2 is re-\nmoved by estimating the inverse correlation rather than the precision matrix, which is\nsomewhat surprising since inverse correlation matrix, unlike correlation matrix, does\nnot have known diagonal elements that contribute no errors to the estimation. This\ncan be explained and proved as follows. If sn1 = O(pn), the result is obvious. When\nsn1 = o(pn), most of off-diagonal elements are zero. Indeed, there are at most O(sn1)\ncolumns of the inverse correlation matrix contain at least one non-zero elements. The\nrest of the columns that have all zero off-diagonal elements must have diagonal entries\n1. These columns represent variables that are actually uncorrelated from the rest.\nNow, it is easy to see from (2.2), that these diagonal elements, which are one, are all\nestimated exactly as one with no estimation error. Hence an order of (pn log pn\/n)\n1\/2\nis not present even in the case of estimating the inverse correlation matrix.\nFor the L1-penalty, our result reduces to that given in Rothman et al. (2007), and\nthe condition for \u03bdn1 can be relaxed to log pn\/n = O(\u03bd\n2\nn1). We offer the sparsistency\nresult as follows.\nTheorem 4 (Sparsistency) Under the conditions given in Theorem 3, for any local\nminimizer of (2.2) satisfying \u2016\u03a8\u02c6\u2212\u03a80\u20162F = OP (sn1 log pn\/n) and \u2016\u03a8\u02c6\u2212\u03a80\u20162 = OP (\u03b7n)\n12\nfor some \u03b7n \u2192 0, if log pn\/n+ \u03b7n = O(\u03bd2n1), then with probability tending to 1, \u03c8\u02c6ij = 0\nfor all (i, j) \u2208 Sc1.\nThe proof follows exactly the same as that for Theorem 2 in section 2.1, and is\nthus omitted.\nFor the L1-penalty, control of biases and sparsistency requires \u03bdn1 to satisfy bounds\nlike (2.1):\nlog pn\/n+ \u03b7n = O(\u03bd\n2\nn1) = log pn\/n. (2.3)\nThis leads to two scenarios:\n1. The worst case scenario has\n\u2016\u03a8\u02c6\u2212\u03a80\u20162 = \u2016\u03a8\u02c6\u2212\u03a80\u20162F = OP (sn1 log pn\/n),\nmeaning \u03b7n = sn1 log pn\/n. Then compatibility of the bounds in (2.3) requires\nsn1 = O(1).\n2. The optimistic scenario has\n\u2016\u03a8\u02c6\u2212\u03a80\u20162 = \u2016\u03a8\u02c6\u2212\u03a80\u20162F\/pn = OP (sn1\/pn \u00b7 log pn\/n),\nmeaning \u03b7n = sn1\/pn\u00b7log pn\/n. Then compatibility of the bounds in (2.3) requires\nsn1 = O(pn).\nOn the other hand, for penalties like the SCAD or the hard-thresholding penalty, we\ndo not need an upper bound for sn1. Hence there is no restriction on the order of\nsn1 as long as sn1 log pn\/n = o(1). It is clear that SCAD results in better sampling\nproperties than the L1-penalized estimator in precision or inverse correlation matrix\nestimation.\n13\n3 Estimation of sparse covariance matrix\nIn this section, we analyze the sparse covariance estimation using penalized likelihood\n(1.3). Then it is modified to estimate the correlation matrix, which improves the rate\nof convergence.\n3.1 Properties of sparse covariance matrix estimation\nLet S2 = {(i, j) : \u03c30ij 6= 0}, where \u03a30 = (\u03c30ij). Denote by sn2 = |S2| \u2212 pn, so that sn2 is\nthe non-sparsity size for \u03a30 on the off-diagonal entries. Put\nan2 = max\n(i,j)\u2208S2\np\u2032\u03bbn2(|\u03c30ij|), bn2 = max(i,j)\u2208S2 p\n\u2032\u2032\n\u03bbn2\n(|\u03c30ij|).\nTechnical conditions in section 2 need some revision. In particular, condition (D)\nnow becomes condition (D2) with \u03bbn1 there replaced by \u03bbn2. Condition (B) should\nnow be\n(B2) an2 = O({1 + pn\/(sn2 + 1)}(log pn\/n)1\/2), bn2 = o(1), and\nmin(i,j)\u2208S2 |\u03c30ij|\/\u03bbn2 \u2192\u221e as n\u2192\u221e.\nTheorem 5 (Rate of convergence). Under regularity conditions (A), (B2), (C) and\n(D2), if (pn+ sn2) log pn\/n = o(1) and (sn2+1) log pn\/n = O(\u03bb\n2\nn2), then there exists a\nlocal minimizer \u03a3\u02c6 such that \u2016\u03a3\u02c6\u2212\u03a30\u20162F = OP{(pn + sn2) log pn\/n).\nThe proof is given in section 5. When the L1-penalty is used, condition for \u03bbn2\nis relaxed to log pn\/n = O(\u03bb\n2\nn2). Like the case for precision matrix estimation, the\ncontrol of the bias term an2 imposes, for the L1-penalty, \u03bbn2 = O((1 + pn\/(sn2 +\n1))2(log pn\/n)\n1\/2).\n14\nTheorem 6 (Sparsistency). Under conditions in Theorem 5, for any local minimizer\n\u03a3\u02c6 of (1.3) satisfying \u2016\u03a3\u02c6\u2212\u03a30\u20162F = OP ((pn + sn2) log pn\/n) and \u2016\u03a3\u02c6\u2212\u03a30\u20162 = OP (\u03b7n)\nfor some \u03b7n \u2192 0, if log pn\/n+ \u03b7n = O(\u03bb2n2), then with probability tending to 1, \u03c3\u02c6ij = 0\nfor all (i, j) \u2208 Sc2.\nThe proof is sketched in section 5. For the L1-penalized likelihood, controlling of\nbias for consistency together with sparsistency requires\nlog pn\/n+ \u03b7n = O(\u03bb\n2\nn2) = (1 + pn\/(sn2 + 1))\n2 log pn\/n. (3.1)\nThis is the same condition as (2.1), and hence in the worst case scenario where\n\u2016\u03a3\u02c6\u2212\u03a30\u20162 = \u2016\u03a3\u02c6\u2212\u03a30\u20162F = OP ((pn + sn2) log pn\/n),\nwe need sn2 = O(p\n1\/2\nn ). In the optimistic scenario where\n\u2016\u03a3\u02c6\u2212\u03a30\u20162 = \u2016\u03a3\u02c6\u2212\u03a30\u20162F\/pn,\nwe need sn2 = O(pn). In both cases, the matrix \u03a30 has to be very sparse, but the\nformer is much sparser.\nOn the other hand, if unbiased penalty functions like the SCAD or hard-thresholding\npenalties are used, we do not need an upper bound on \u03bbn2 since the bias an2 = 0 for\nsufficiently large n. This allows for more flexibility on the order of sn2.\nSimilar to section 2, asymptotic normality for the estimators of the elements in S2\ncan be proved under certain assumptions.\n3.2 Properties of sparse correlation matrix estimation\nThe correlation matrix \u03930 retains the same sparse structure of \u03a30 with known diagonal\nelements. This special structure allows us to estimate \u03930 more accurately. To take\n15\nthe advantage of the known diagonal elements, the sparse correlation matrix \u03930 is\nestimated by minimizing w.r.t. \u0393 = (\u03b3ij),\ntr(\u0393\u22121\u0393\u02c6S) + log |\u0393|+\n\u2211\ni6=j\np\u03bdn2(|\u03b3ij|), (3.2)\nwhere \u03bdn2 is a regularization parameter. After obtaining \u0393\u02c6, \u03a30 can be estimated by\n\u03a3\u02dc = W\u02c6\u0393\u02c6W\u02c6.\nTo present the rates of convergence for \u0393\u02c6 and \u03a3\u02dc, we define\ncn2 = max\n(i,j)\u2208S2\np\u2032\u03bdn2(|\u03b30ij|), dn2 = max(i,j)\u2208S2 p\n\u2032\u2032\n\u03bdn2\n(|\u03b30ij|),\nwhere \u03930 = (\u03b3\n0\nij). We adapt the condition (D) to (D2\u2019) with \u03bbn2 there replaced by \u03bdn2,\nand (B) to (B2\u2019) as follows:\n(B2\u2019) cn2 = O({log pn\/n}1\/2), dn2 = o(1), and min(i,j)\u2208S2 |\u03b30ij|\/\u03bdn2 \u2192\u221e as n\u2192\u221e.\nTheorem 7 Under regularity conditions (A),(B2\u2019),(C) and (D2\u2019), if sn2 log pn\/n =\no(1) and (sn2 + 1) log pn\/n = O(\u03bd\n2\nn2), then there exists a local minimizer \u0393\u02c6 for (3.2)\nsuch that\n\u2016\u0393\u02c6\u2212 \u03930\u20162F = OP (sn2 log pn\/n).\nIn addition, for the operator norm, we have\n\u2016\u03a3\u02dc\u2212\u03a30\u20162 = OP{(sn2 + 1) log pn\/n}.\nThe proof is sketched in section 5. The condition (sn2 + 1) log pn\/n = O(\u03bd\n2\nn2) can\nbe relaxed to log pn\/n = O(\u03bd\n2\nn2) when the L1-penalty is used. This theorem shows\nthat the correlation matrix, like the inverse correlation matrix, can be estimated more\naccurately, since diagonal elements are known to be one.\n16\nTheorem 8 (Sparsistency). Under conditions in Theorem 7, for any local minimizer\n\u0393\u02c6 of (3.2) satisfying \u2016\u0393\u02c6\u2212\u03930\u20162F = OP (sn2 log pn\/n) and \u2016\u0393\u02c6\u2212\u03930\u20162 = OP (\u03b7n) for some\n\u03b7n \u2192 0 , if log pn\/n + \u03b7n = O(\u03bd2n2), then with probability tending to 1, \u03b3\u02c6ij = 0 for all\n(i, j) \u2208 Sc2.\nThe proof follows exactly the same as that for Theorem 6 in section 5, and is\nomitted. For the L1-penalized likelihood, controlling of bias and sparsistency requires\nlog pn\/n+ \u03b7n = O(\u03bd\n2\nn2) = log pn\/n. (3.3)\nThis is the same condition as (2.3), hence in the worst scenario where\n\u2016\u0393\u02c6\u2212 \u03930\u20162 = \u2016\u0393\u02c6\u2212 \u03930\u20162F = OP (sn2 log pn\/n),\nwe need sn2 = O(1). In the optimistic scenario where\n\u2016\u0393\u02c6\u2212 \u03930\u20162 = \u2016\u0393\u02c6\u2212 \u03930\u20162F\/pn = OP (sn2\/pn \u00b7 log pn\/n),\nwe need sn2 = O(pn).\nThe use of unbiased penalties like the SCAD or hard-thresholding penalties, like\nresults in previous sections, does not impose an upper bound on the regularization\nparameter since bias cn2 = 0 for sufficiently large n. This gives more flexibility to the\norder of sn2 allowed.\n4 Extension to sparse Cholesky decomposition\nPourahmadi (1999) proposed the modified Cholesky decomposition (MCD) which fa-\ncilitates the sparse estimation of \u2126 through penalization. The idea is to represent\n17\nzero-mean data y = (y1, \u00b7 \u00b7 \u00b7 , ypn)T using autoregressive models:\nyi =\ni\u22121\u2211\nj=1\n\u03c6ijyj + \u00b2i, and T\u03a3T\nT = D, (4.1)\nwhere T is the unique unit lower triangular matrix with ones on its diagonal and\n(i, j)th element \u2212\u03c6ij for j < i, and D is diagonal with ith element \u03c32i = var(\u00b2i). The\noptimization problem is unconstrained (since the \u03c6ij\u2019s are free variables), and the\nestimate for \u2126 is always positive-definite.\nHuang et al. (2006) and Levina et al. (2008) both used the MCD for estimation\nof \u21260. The former maximized the log-likelihood (ML) over T and D simultaneously,\nwhile the latter suggested also a least square version (LS), with D being first set to\nthe identity matrix and then minimizing over T to obtain T\u02c6. The latter corresponds\nto the original Cholesky decomposition. The sparse Cholesky factor can be estimated\nthrough\n(ML) : q3(T,D) = tr(T\nTD\u22121TS) + log |D|+ 2\n\u2211\ni<j\np\u03bbn3(|tij|). (4.2)\nThis is indeed the same as (1.1) with the substitution of \u2126 = TTD\u22121T and penaliza-\ntion parameter \u03bbn3. Noticing that (4.1) can be written as Ty = \u03b5, the least square\nversion is to minimize tr(\u03b5\u03b5T ) = tr(TTTyyT ) in the matrix notation. Aggregating n\nobservations and adding sparsity penalties, the least-square criterion is to minimize\n(LS) : q4(T) = tr(T\nTTS) + 2\n\u2211\ni<j\np\u03bbn4(|tij|). (4.3)\nIn view of the results in sections 2.2 and 3.2, we can also write the covariance in (4.2)\nas S = W\u02c6\u0393\u02c6SW\u02c6 and then replace D\n\u22121\/2TW\u02c6 by T, resulting in the normalized (NL)\nversion as follows:\n(NL) : q5(T) = tr(T\nTT\u0393\u02c6S)\u2212 2 log |T|+ 2\n\u2211\ni<j\np\u03bbn5(|tij|). (4.4)\n18\n4.1 Properties of sparse Cholesky factor estimation\nSince all theT\u2019s introduced in the three models above have the same sparsity structure,\nlet S and sn3 be the non-sparsity set and non-sparsity size associated with each T\nabove. Define\nan3 = max\n(i,j)\u2208S\np\u2032\u03bbn3(|t0ij|), bn3 = max(i,j)\u2208S p\n\u2032\u2032\n\u03bbn3\n(|t0ij|).\nFor (ML), condition (D) is adapted to (D3) with \u03bbn1 there replaced by \u03bbn3. Condition\n(B) is modified as\n(B3) an3 = O({1 + pn\/(sn3 + 1)}(log pn\/n)1\/2), bn3 = o(1) and\nmin(i,j)\u2208S |\u03c60ij|\/\u03bbn3 \u2192\u221e as n\u2192\u221e.\nAfter obtaining T\u02c6 and D\u02c6 from minimizing (ML), we set \u2126\u02c6 = T\u02c6T D\u02c6\u22121T\u02c6.\nTheorem 9 Under regularity conditions (A),(B3),(C),(D3), if (pn + sn3) log pn\/n =\no(1) and (sn3 + 1) log pn\/n = O(\u03bb\n2\nn3), then there exists a local minimizer T\u02c6 and D\u02c6 for\n(ML) such that \u2016T\u02c6 \u2212 T0\u20162F = OP (sn3 log pn\/n), \u2016D\u02c6 \u2212 D0\u20162F = OP (pn log pn\/n) and\n\u2016\u2126\u02c6\u2212\u21260\u20162F = OP{(pn + sn3) log pn\/n}.\nThe proof is similar to those of Theorems 5 and 7 and is omitted. The Cholesky\nfactor T has ones on its main diagonal without the need for estimation. Hence, the\nrate of convergence is faster than \u2126\u02c6. If the L1-penalty is used, condition for \u03bbn3 can\nbe relaxed to log pn\/n = O(\u03bb\n2\nn3).\nTheorem 10 (Sparsistency). Under the conditions in Theorem 9, for any local min-\nimizer T\u02c6, D\u02c6 of (4.2) satisfying \u2016T\u02c6 \u2212 T0\u20162F = OP (sn3 log pn\/n) and \u2016D\u02c6 \u2212 D0\u20162F =\nOP (pn log pn\/n), if log pn\/n + \u03b7n + \u03b6n = O(\u03bb\n2\nn3), then sparsistency holds for T\u02c6, pro-\nvided that \u2016T\u02c6\u2212T0\u20162 = OP (\u03b7n) and \u2016D\u02c6\u2212D0\u20162 = OP (\u03b6n), for some \u03b7n, \u03b6n \u2192 0.\n19\nThe proof is in section 5. For the L1-penalized likelihood, control of bias and\nsparsistency impose the following:\nlog pn\/n+ \u03b7n + \u03b6n = O(\u03bb\n2\nn3) = (1 + pn\/(sn3 + 1))\n2 log pn\/n. (4.5)\nThe worst scenario corresponds to \u03b7n = sn3 log pn\/n and \u03b6n = pn log pn\/n, so that we\nneed sn3 = O(p\n1\/2\nn ). The optimistic scenario corresponds to \u03b7n = sn3\/pn \u00b7 log pn\/n and\n\u03b6n = log pn\/n, so that we need sn3 = O(pn).\nOn the other hand, such a restriction is not needed for unbiased penalties like\nSCAD or hard-thresholding, which gives more flexibility on the order of sn3.\n4.2 Properties of sparse normalized Cholesky factor estima-\ntion\nWe now turn to analyzing the normalized penalized likelihood (4.4). With T = (tij)\nin (NL) which is lower triangular, define\nan5 = max\n(i,j)\u2208S\np\u2032\u03bbn5(|t0ij|), bn5 = max(i,j)\u2208S p\n\u2032\u2032\n\u03bbn5\n(|t0ij|).\nCondition (D) is now changed to (D5) with \u03bbn1 there replaced by \u03bbn5. Condition (B)\nis now substituted by\n(B5) a2n5 = O(log pn\/n), bn5 = o(1), min(i,j)\u2208S |t0ij|\/\u03bbn5 \u2192\u221e as n\u2192\u221e.\nTheorem 11 (Rate of convergence) Under regularity conditions (A),(B5),(C) and\n(D5), if sn3 log pn\/n = o(1) and (sn3 + 1) log pn\/n = O(\u03bb\n2\nn5), then there exists a local\nminimizer T\u02c6 for (NL) such that \u2016T\u02c6\u2212T0\u20162F = OP (sn3 log pn\/n) and rate of convergence\nin the Frobenius norm\n\u2016\u2126\u02c6\u2212\u21260\u20162F = OP{(pn + sn3) log pn\/n},\n20\nand in the operator norm, it is improved to\n\u2016\u2126\u02c6\u2212\u21260\u20162 = OP{(sn3 + 1) log pn\/n)}.\nThe proof is similar to that of Theorems 5 and 7 and is omitted. The condition\nfor \u03bbn3 can be relaxed to log pn\/n = O(\u03bb\n2\nn3) when the L1-penalty is used. Similar to\nTheorem 3, log pn can also be as large as o(n), as long as sn3 log pn\/n = o(1). It is\nevident that normalizing with W\u02c6 results in an improvement in the rate of convergence\nin operator norm.\nTheorem 12 (Sparsistency). Under the conditions in Theorem 11, for any local min-\nimizer T\u02c6 of (4.4) satisfying \u2016T\u02c6\u2212T0\u20162F = OP (sn3 log pn\/n) if log pn\/n+ \u03b7n = O(\u03bb2n5),\nthen sparsistency holds for T\u02c6, provided that \u2016T\u02c6\u2212T0\u20162 = O(\u03b7n) for some \u03b7n \u2192 0.\nProof is omitted since it goes exactly the same as that of Theorem 10. The above\nresults apply also to the L1-penalty. For simultaneous persistency and optimal rate of\nconvergence using L1-penalty, the biases inherent in L1-penalty induce the restriction\nsn3 = O(1) in the worst scenario where \u03b7\n2\nn = sn3 log pn\/n, and sn3 = O(pn) in the\noptimistic scenario where \u03b72n = sn3\/pn \u00b7 log pn\/n. This restriction does not apply to\nthe SCAD and other asymptotically unbiased penalty functions.\n5 Proofs\nWe first prove two lemmas. The first one concerns with inequalities involving operator\nand Frobenius norms. The other one concerns with order estimation for elements\nin a matrix of the form A(S \u2212 \u03a30)B, which is useful in proving results concerning\nsparsistency.\n21\nLemma 1 Let A and B be real matrices such that the product AB is defined. Then,\ndefining \u2016A\u20162min = \u03bbmin(ATA), we have\n\u2016A\u2016min\u2016B\u2016F \u2264 \u2016AB\u2016F \u2264 \u2016A\u2016\u2016B\u2016F . (5.1)\nIn particular, if A = (aij), then |aij| \u2264 \u2016A\u2016 for each i, j.\nProof of Lemma 1. Write B = (b1, \u00b7 \u00b7 \u00b7 ,bq), where bi is the i-th column vector in\nB. Then\n\u2016AB\u20162F = tr(BTATAB) =\nq\u2211\ni=1\nbTi A\nTAbi \u2264 \u03bbmax(ATA)\nq\u2211\ni=1\n\u2016bi\u20162\n= \u2016A\u20162\u2016B\u20162F .\nSimilarly,\n\u2016AB\u20162F =\nq\u2211\ni=1\nbTi A\nTAbi \u2265 \u03bbmin(ATA)\nq\u2211\ni=1\n\u2016bi\u20162\n= \u2016A\u20162min\u2016B\u20162F ,\nwhich completes the proof of (5.1). To prove |aij| \u2264 \u2016A\u2016, note that aij = eTi Aej,\nwhere ei is the unit column vector with one at the i-th position, and zero elsewhere.\nHence using (5.1),\n|aij| = |eTi Aej| \u2264 \u2016Aej\u2016F \u2264 \u2016A\u2016 \u00b7 \u2016ej\u2016F = \u2016A\u2016,\nand this completes the proof of the lemma. \u00a4\nLemma 2 Let S be a sample covariance matrix of a random sample {yi}1\u2264i\u2264n with\nyi \u223c N(0,\u03a30). Assume pn\/n = o(1), \u03a30 has eigenvalues uniformly bounded above as\nn\u2192\u221e, and A = A0+\u22061, B = B0+\u22062 are matrices such that the constant matrices\n\u2016A0\u2016 = O(1) and \u2016B0\u2016 = O(1) independent of the data, with \u2016\u22061\u2016, \u2016\u22062\u2016 = oP (1).\nThen maxi,j |(A(S\u2212\u03a30)B)ij| = OP ({log pn\/n}1\/2).\n22\nProof of Lemma 2. We first prove the lemma with A and B independent of the\ndata. Let xi = Ayi and wi = B\nTyi. Define ui = (x\nT\ni ,w\nT\ni )\nT , with covariance matrix\n\u03a3u = var(ui) =\n(\nA\u03a30A\nT A\u03a30B\nBT\u03a30A\nT BT\u03a30B\n)\n.\nSince \u2016(AT B)T\u2016 \u2264 (\u2016A\u20162 + \u2016B\u20162)1\/2 = O(1) and \u2016\u03a30\u2016 = O(1) uniformly, we\nhave \u2016\u03a3u\u2016 = O(1) uniformly. Then, with Su = n\u22121\n\u2211n\ni=1 uiu\nT\ni , which is the sample\ncovariance matrix for the random sample {ui}1\u2264i\u2264n, by lemma 3 of , we have\nmax\ni,j\n|(Su \u2212\u03a3u)ij| = OP ({log pn\/n}1\/2).\nIn particular, it means that\nmax\ni,j\n|(A(S\u2212\u03a30)B)ij| =\n(\nn\u22121\nn\u2211\nr=1\nxrw\nT\nr \u2212A\u03a30B\n)\nij\n= OP ({log pn\/n}1\/2),\nwhich completes the proof for A and B independent of the data.\nNow consider A = A0+\u22061, B = B0+\u22062 as in the statement of the lemma. Then\nA(S\u2212\u03a30)B = K1 +K2 +K3 +K4, (5.2)\nwhere K1 = A0(S \u2212 \u03a30)B0, K2 = \u22061(S \u2212 \u03a30)B0, K3 = A0(S \u2212 \u03a30)\u22062 and K4 =\n\u22061(S \u2212 \u03a30)\u22062. Now maxi,j |(K1)ij| = OP ({log pn\/n}1\/2) as proved before. Consider\nK2. Suppose the maximum element of the matrix is at the (i, j)-th position. Then we\ncan set\n\u22061 = cnB\nT\n0 (S\u2212\u03a30)T = cnBT0 (S\u2212\u03a30),\nwhere c2n = o(n\/pn) to keep \u2016\u22061\u2016 = oP (1) since \u2016B0\u2016 = O(1) and \u2016S \u2212 \u03a30\u20162 =\nOP (pn\/n) (see chapter 2 of Bai and Silverstein (2006)), so the maximum element is\nnow on the diagonal, with\nmax\ni,j\n|(\u22061(S\u2212\u03a30)B0)ij| \u2264 cnmax\nk\n|(BT0 (S\u2212\u03a30)2B0)kk|. (5.3)\n23\nSince S \u2212\u03a30 is symmetric, we can find a rotation matrix Q (i.e. QTQ = QQT = I)\nso that\nS\u2212\u03a30 = Q\u039bQT ,\nwhere \u039b is a diagonal matrix with real entries. Then since cn\u2016\u039b\u20162 = oP ({pn\/n}1\/2)\nbut \u2016\u039b\u2016 = OP ({pn\/n}1\/2), we have\ncnmax\nk\n|(BT0 (S\u2212\u03a30)2B0)kk| = max\nk\n|(BT0Qcn\u039b2QTB0)kk|\n\u2264 max\nk\n|(BT0Q\u039bQTB0)kk|\n= max\nk\n|(BT0 (S\u2212\u03a30)B0)kk| = OP ({log pn\/n}1\/2),\nwhere the last line used the previous proof for constant matrix B0. Then combining\nwith (5.3), we have maxi,j |(K2)ij| = OP ({log pn\/n}1\/2). Similar arguments goes for\nK3. For K4, similar arguments hold and we will end up setting\n\u22061 = cn(S\u2212\u03a30), \u22062 = dn(S\u2212\u03a30)2,\nwhere c2n = o(n\/pn) and dn = o(n\/pn) to keep \u2016\u22061\u2016, \u2016\u22062\u2016 = oP (1). Then we have\ncndn\u2016\u039b\u20164 = oP ({pn\/n}1\/2), and\nmax\ni,j\n|(K4)ij| \u2264 cndnmax\nk\n|[(S\u2212\u03a30)4]k| = max\nk\n|[Qcndn\u039b4QT ]k|\n\u2264 max\nk\n|(Q\u039bQT )k| = max\nk\n|(S\u2212\u03a30)k|\n= OP ({log pn\/n}1\/2).\nThis completes the proof of the lemma. \u00a4\nProof of Theorem 1. Let U be a symmetric matrix of size pn, DU be its diagonal\nmatrix and RU = U \u2212DU be its off-diagonal matrix. Set \u2206U = \u03b1nRU + \u03b2nDU . We\n24\nwould like to show that, for \u03b1n = (sn1 log pn\/n)\n1\/2 and \u03b2n = (pn log pn\/n)\n1\/2, and for\na set A defined as A = {U : \u2016RU\u2016F = C1, \u2016DU\u2016F = C2},\nP\n(\ninf\nU\u2208A\nq1(\u21260 +\u2206U) > q1(\u21260)\n)\n\u2192 1,\nfor sufficiently large constants C1 and C2. This implies that there is a local minimizer\nin {\u21260 +\u2206U : \u2016RU\u2016F = C1, \u2016DU\u2016F = C2} such that \u2016\u2126\u02c6\u2212\u21260\u2016F = OP (\u03b1n + \u03b2n).\nConsider, for \u03a3 = \u03a30 +\u2206U , the difference\nq1(\u2126)\u2212 q1(\u21260) = I1 + I2 + I3,\nwhere\nI1 = tr(S\u2126)\u2212 log |\u2126| \u2212 (tr(S\u21260)\u2212 log |\u21260|),\nI2 =\n\u2211\n(i,j)\u2208Sc1\n(p\u03bbn1(|\u03c9ij|)\u2212 p\u03bbn1(|\u03c90ij|)),\nI3 =\n\u2211\n(i,j)\u2208S1,i 6=j\n(p\u03bbn1(|\u03c9ij|)\u2212 p\u03bbn1(|\u03c90ij|)).\nIt suffice to show that the difference is positive asymptotically with probability tending\nto 1. Using Taylor\u2019s expansion with the integral remainder, we have I1 = K1 + K2,\nwhere\nK1 = tr((S\u2212\u03a30)\u2206U),\nK2 = vec(\u2206U)\nT\n{\u222b 1\n0\ng(v,\u2126v)(1\u2212 v)dv\n}\nvec(\u2206U), (5.4)\nwith the definitions \u2126v = \u21260 + v\u2206U , and g(v,\u2126v) = \u2126\n\u22121\nv \u2297\u2126\u22121v . Now\nK2 \u2265\n\u222b 1\n0\n(1\u2212 v) min\n0\u2264v\u22641\n\u03bbmin(\u2126\n\u22121\nv \u2297\u2126\u22121v )dv \u00b7 \u2016vec(\u2206U)\u20162\n= \u2016vec(\u2206U)\u20162\/2 \u00b7 min\n0\u2264v\u22641\n\u03bb\u22122max(\u2126v) \u2265 \u2016vec(\u2206U)\u20162\/2 \u00b7 (\u2016\u21260\u2016+ \u2016\u2206U\u2016)\u22122\n\u2265 (C21\u03b12n + C22\u03b22n)\/2 \u00b7 (\u03c4\u221211 + o(1))\u22122,\n25\nwhere we used \u2016\u2206U\u2016 = o(1).\nConsider K1. It is clear that |K1| \u2264 L1 + L2, where\nL1 =\n\u2223\u2223\u2223\u2223 \u2211\n(i,j)\u2208S1\n(S\u2212\u03a30)ij(\u2206U)ij\n\u2223\u2223\u2223\u2223,\nL2 =\n\u2223\u2223\u2223\u2223 \u2211\n(i,j)\u2208Sc1\n(S\u2212\u03a30)ij(\u2206U)ij\n\u2223\u2223\u2223\u2223.\nUsing Lemma 1 and 2, we have\nL1 \u2264 (sn1 + pn)1\/2max\ni,j\n|(S\u2212\u03a30)ij| \u00b7 \u2016\u2206U\u2016F\n\u2264 OP (\u03b1n + \u03b2n) \u00b7 \u2016\u2206U\u2016F\n= OP (C1\u03b1\n2\nn + C2\u03b2\n2\nn),\nThis is dominated by K2 when C1 and C2 are sufficiently large.\nNow, consider I2\u2212L2. Since we assumed (sn1+1) log pn\/n = O(\u03bb2n1), by condition\n(C), when n is sufficiently large, we have \u03b1n = O(\u03bbn1) and p\u03bbn1(|\u03b1nuij|) \u2265 \u03bbn1k1|\u03b1nuij|\nfor some positive constant k1. Using p\u03bbn1(0) = 0, we then have\nI2 =\n\u2211\n(i,j)\u2208Sc1\np\u03bbn1(|\u03b1nuij|) \u2265 k1\u03bbn1\u03b1n\n\u2211\n(i,j)\u2208Sc1\n|uij|.\nHence\nI2 \u2212 L2 \u2265\n\u2211\n(i,j)\u2208Sc1\n{\n\u03bbn1k1|\u03b1nuij| \u2212 |(S\u2212\u03a30)ij| \u00b7 |\u03b1nuij|\n}\n\u2265\n\u2211\n(i,j)\u2208Sc1\n[\n\u03bbn1k1 \u2212OP ({log pn\/n}1\/2)\n] \u00b7 |\u03b1nuij|\n= \u03bbn1\u03b1n\n\u2211\n(i,j)\u2208Sc1\n[\nk1 \u2212OP (\u03bb\u22121n1 {log pn\/n}1\/2)\n] \u00b7 |uij|.\nWith the assumption that (sn1 + 1) log pn\/n = O(\u03bb\n2\nn1), we see from the above that\nI2 \u2212 L2 \u2265 0 since OP (\u03bb\u22121n1 {log pn\/n}1\/2) = oP (1).\n26\nNow, with L1 dominated by K2 and I2\u2212L2 \u2265 0, the proof completes if we can show\nthat I3 is also dominated by K2, since we have proved that K2 > 0. Using Taylor\u2019s\nexpansion, we can arrive at\n|I3| \u2264 C1\u03b1ns1\/2n1 an1 + C21bn1\u03b12n\/2 \u00b7 (1 + o(1)).\nBy condition (B), we have\n|I3| = C \u00b7O(\u03b12n + \u03b22n) + C2 \u00b7 o(\u03b12n),\nwhich is dominated by K2 with large enough constants C1 and C2. This completes the\nproof of the theorem. \u00a4\nProof of Theorem 2. For \u2126 a minimizer of (1.1), the derivative for q1(\u2126) w.r.t. \u03c9ij\nfor (i, j) \u2208 Sc2 is\n\u2202q1(\u2126)\n\u2202\u03c9ij\n= 2(sij \u2212 \u03c3ij + p\u2032\u03bbn1(|\u03c9ij|)sgn(\u03c9ij)),\nwhere sgn(a) denotes the sign of a. We need to estimate the order of sij \u2212 \u03c3ij inde-\npendent of i and j.\nDecompose sij \u2212 \u03c3ij = I1 + I2, where\nI1 = sij \u2212 \u03c30ij, I2 = \u03c30ij \u2212 \u03c3ij.\nBy Lemma 2 or Lemma 3 of Bickel and Levina (2006), it follows that maxi,j |I1| =\nOP ({log pn\/n}1\/2). It remains to estimate the order of I2.\nBy Lemma 1, |\u03c3ij \u2212 \u03c30ij| \u2264 \u2016\u03a3\u2212\u03a30\u2016, which has order\n\u2016\u03a3\u2212\u03a30\u2016 = \u2016\u03a3(\u2126\u2212\u21260)\u03a30\u2016\n\u2264 \u2016\u03a3\u2016 \u00b7 \u2016\u2126\u2212\u21260\u2016 \u00b7 \u2016\u03a30\u2016\n= O(\u2016\u2126\u2212\u21260\u2016),\n27\nwhere we used Condition (A) to get \u2016\u03a30\u2016 = O(1), and using \u03b7n \u2192 0 so that \u03bbmin(\u2126\u2212\n\u21260) = o(1) for \u2016\u2126\u2212\u21260\u2016 = O(\u03b71\/2n ),\n\u2016\u03a3\u2016 = \u03bb\u22121min(\u2126) \u2264 (\u03bbmin(\u21260) + \u03bbmin(\u2126\u2212\u21260))\u22121\n= (O(1) + o(1))\u22121 = O(1).\nHence \u2016\u2126\u2212\u21260\u2016 = O(\u03b71\/2n ) implies |I2| = O(\u03b71\/2n ).\nCombining the last two results yields that\nmax\ni,j\n|sij \u2212 \u03c3ij| = OP (|sij \u2212 \u03c30ij|+ \u03b71\/2n )\n= OP ({log pn\/n}1\/2 + \u03b71\/2n ).\nBy conditions (C) and (D), we have\np\u2032\u03bbn1(|\u03c9ij|) = C3\u03bbn1\nfor \u03c9ij in a small neighborhood of 0 (excluding 0 itself) and some positive constant C3.\nHence if \u03c9ij lies in a small neighborhood of 0, we need to have log pn\/n+ \u03b7n = O(\u03bb\n2\nn1)\nin order to have the sign of \u2202q1(\u2126)\/\u2202\u03c9ij depends on sgn(\u03c9ij) only with probability\ntending to 1. The proof of the theorem is completed. \u00a4\nProof of Theorem 3. Because of the similarity between equations (2.2) and (1.1),\nthe Frobenius norm result has nearly identical proof as Theorem 1, except that we\nnow set \u2206U = \u03b1nU . For the operator norm result, we refer readers to the proof of\nTheorem 2 of Rothman et al. (2007). \u00a4\nProof of Theorem 5. The proof is similar to that of Theorem 1. We only sketch\nbriefly the proof, pointing out the important differences.\n28\nLet \u03b1n = (sn2 log pn\/n)\n1\/2 and \u03b2n = (pn log pn\/n)\n1\/2, and defineA = {U : \u2016RU\u2016F =\nC1, \u2016DU\u2016F = C2}. Want to show\nP\n(\ninf\nU\u2208A\nq2(\u03a30 +\u2206U) > q2(\u03a30)\n)\n\u2192 1,\nfor sufficiently large constants C1 and C2.\nFor \u03a3 = \u03a30 +\u2206U , the difference\nq2(\u03a3)\u2212 q2(\u03a30) = I1 + I2 + I3,\nwhere\nI1 = tr(S\u2126) + log |\u03a3| \u2212 (tr(S\u21260) + log |\u03a30|),\nI2 =\n\u2211\n(i,j)\u2208Sc2\n(p\u03bbn2(|\u03c3ij|)\u2212 p\u03bbn2(|\u03c30ij|)),\nI3 =\n\u2211\n(i,j)\u2208S2,i 6=j\n(p\u03bbn2(|\u03c3ij|)\u2212 p\u03bbn2(|\u03c30ij|)),\nwith I1 = K1 +K2, where\nK1 = \u2212tr((S\u2212\u03a30)\u21260\u2206U\u21260) = \u2212tr((S\u21260 \u2212\u21260)\u2206U),\nK2 = vec(\u2206U)\nT\n{\u222b 1\n0\ng(v,\u03a3v)(1\u2212 v)dv,\n}\nvec(\u2206U), (5.5)\nand\u03a3v = \u03a30+v\u2206U , S\u21260 is the sample covariance matrix of a random sample {xi}1\u2264i\u2264n\nhaving xi \u223c N(0,\u21260). Also,\ng(v,\u03a3v) = \u03a3\n\u22121\nv \u2297\u03a3\u22121v S\u03a3\u22121v +\u03a3\u22121v S\u03a3\u22121v \u2297\u03a3\u22121v \u2212\u03a3\u22121v \u2297\u03a3\u22121v . (5.6)\nThe treatment of K2 is different from that in Theorem 1. By condition (A), we\nhave\n\u2016v\u2206U\u21260\u2016 \u2264 \u2016\u2206U\u2016\u2016\u21260\u2016 \u2264 \u03c4\u221211 (C1\u03b1n + C2\u03b2n) = o(1).\n29\nThus, we can use the Neumann series expansion to arrive at\n\u03a3\u22121v = \u21260(I + v\u2206U\u21260)\n\u22121 = \u21260(I \u2212 v\u2206U\u21260 + o(1)).\nThat is, \u03a3\u22121v = \u21260 + OP (\u03b1n + \u03b2n), and \u2016\u03a3\u22121v \u2016 = \u03c4\u221211 + OP (\u03b1n + \u03b2n). With SI\ndefined as the sample covariance matrix formed from a random sample {xi}1\u2264i\u2264n\nhaving xi \u223c N(0, I),\n\u2016S\u2212\u03a30\u2016 = OP (\u2016SI \u2212 I\u2016) = OP ({pn\/n}1\/2)\n(see e.g. chapter 2 of Bai and Silverstein (2006)). These entail\nS\u03a3\u22121v = (S\u2212\u03a30)\u03a3\u22121v +\u03a30\u03a3\u22121v\n= OP ({pn\/n}1\/2) + I +OP (\u03b1n + \u03b2n)\n= I + oP (1).\nCombining these results, we have\ng(v,\u03a3v) = \u21260 \u2297\u21260 +OP (\u03b1n + \u03b2n).\nConsequently,\nK2 = vec(\u2206U)\nT\n{\u222b 1\n0\n\u21260 \u2297\u21260(1 + oP (1))(1\u2212 v)dv\n}\nvec(\u2206U)\n\u2265 \u03bbmin(\u21260 \u2297\u21260)\u2016vec(\u2206U)\u20162\/2 \u00b7 (1 + oP (1))\n= \u03c4\u221221 (C\n2\n1\u03b1\n2\nn + C\n2\n2\u03b2\n2\nn)\/2 \u00b7 (1 + oP (1)).\nAll other terms are dealt with similarly as in the proof of Theorem 1, and hence\nwe omit them. \u00a4\nProof of Theorem 6. The proof is similar to that of Theorem 2. We only show the\nmain differences.\n30\nIt is easy to show\n\u2202q2(\u03a3)\n\u2202\u03c3ij\n= 2(\u2212(\u2126S\u2126)ij + \u03c9ij + p\u2032\u03bbn(|\u03c3ij|)sgn(\u03c3ij)).\nOur aim is to estimate the order of |(\u2212\u2126S\u2126+\u2126)ij|, finding an upper bound which is\nindependent of both i and j.\nWrite\n\u2212\u2126S\u2126+\u2126 = I1 + I2,\nwhere I1 = \u2212\u2126(S\u2212\u03a30)\u2126 and I2 = \u2126(\u03a3\u2212\u03a30)\u2126. Since\n\u2016\u2126\u2016 = \u03bb\u22121min(\u03a3) \u2264 (\u03bbmin(\u03a30) + \u03bbmin(\u03a3\u2212\u03a30))\u22121\n= \u03c4\u221211 + o(1),\nwe have\n\u2126 = \u21260 + (\u2126\u2212\u21260) = \u21260 \u2212\u2126(\u03a3\u2212\u03a30)\u21260 = \u21260 +\u2206,\nwhere \u2016\u2206\u2016 \u2264 \u2016\u2126\u2016 \u00b7 \u2016\u03a3 \u2212 \u03a30\u2016 \u00b7 \u2016\u21260\u2016 = O(\u03b71\/2n ) = o(1) by Lemma 1, with \u2016\u03a3 \u2212\n\u03a30\u20162 = O(\u03b7n) . Hence we can apply Lemma 2 and conclude that maxi,j |(I1)ij| =\nOP ({log pn\/n}1\/2).\nFor I2, we have\nmax\ni,j\n|(I2)ij| \u2264 \u2016\u2126\u2016 \u00b7 \u2016\u03a3\u2212\u03a30\u2016 \u00b7 \u2016\u2126\u2016 = O(\u2016\u03a3\u2212\u03a30\u2016) = O(\u03b71\/2n ).\nHence we have\nmax\ni,j\n|(\u2212\u2126S\u2126+\u2126)ij| = O({log pn\/n}1\/2 + \u03b71\/2n ).\nThe rest goes similar to the proof of Theorem 2, and is omitted. \u00a4\n31\nProof of Theorem 7. The proof is nearly identical to that of Theorem 5, except\nthat we now set \u2206U = \u03b1nU . The fact that (\u0393\u02c6S)ii = 1 = \u03b3\n0\nii has no estimation\nerror eliminates an order (pn log pn\/n)\n1\/2 that contributes from estimating tr((\u0393\u02c6S \u2212\n\u03930)\u03a80\u2206U\u03a80) for (3.2). This is why we can estimate more accurately for the sparse\ncorrelation.\nFor the operator norm result, we refer readers to the proof of Theorem 2 of Rothman\net al. (2007). \u00a4\nProof of Theorem 10. For (T,D) a minimizer of (4.2), the derivative for q3(T,D)\nw.r.t. tij for (i, j) \u2208 Sc3 is\n\u2202q3(T,D)\n\u2202tij\n= 2((STTD\u22121)ji + p\u2032\u03bbn3(|tij|)sgn(tij)).\nNow STTD\u22121 = I1 + I2 + I3 + I4, where\nI1 = (S\u2212\u03a30)TTD\u22121, I2 = \u03a30(T\u2212T0)TD\u22121,\nI3 = \u03a30T\nT\n0 (D\n\u22121 \u2212D\u221210 ), I4 = \u03a30TT0D\u221210 .\nBy the MCD (4.1), I4 = T\n\u22121\n0 . Since i > j for (i, j) \u2208 Sc3, we must have (T\u221210 )ji = 0.\nHence we can ignore I4.\nSince \u2016T \u2212 T0\u20162 = O(\u03b7n) and \u2016D \u2212 D0\u20162 = O(\u03b6n) with \u03b7n, \u03b6n = o(1), and by\ncondition (A) we can easily show \u2016D\u22121 \u2212D\u221210 \u2016 = O(\u2016D\u2212D0\u2016) = O(\u03b61\/2n ). Then we\ncan apply Lemma 2 to show that maxij |(I1)ij| = (log pn\/n)1\/2.\nFor I2, we have maxij |(I2)ij| \u2264 \u2016\u03a30\u2016 \u00b7 \u2016T \u2212 T0\u2016 \u00b7 \u2016D\u22121\u2016 = O(\u03b71\/2n ). And finally,\nmaxij |(I3)ij| \u2264 \u2016\u03a30\u2016 \u00b7 \u2016T0\u2016 \u00b7 \u2016D\u22121 \u2212D\u221210 \u2016 = O(\u03b61\/2n ).\nWith all these, we have max(i,j)\u2208Sc3 |(STTD\u22121)ji|2 = log pn\/n+ \u03b7n+ \u03b6n. The rest of\nthe proof goes like that of Theorem 2 or 6. \u00a4\n32\nReferences\n[1] Bai, Z. and Silverstein, J.W. (2006), Spectral Analysis of Large Dimensional Ran-\ndom Matrices, Science Press, Beijing.\n[2] Bickel, P.J. and Levina, E. (2008), Covariance Regularization by Thresholding, to\nappear in Ann. Statist.\n[3] Bickel, P.J. and Levina, E. (2008), Regularized Estimation of Large Covariance\nMatrices, Ann. Statist., 36(1), 199\u2013227.\n[4] d\u2019Aspremont, A., Banerjee, O. and El Ghaoui, L. (2008), First-order Methods For\nSparse Covariance Selection, SIAM. J. Matrix Anal. and Appl., 30(1), 56\u201366.\n[5] Dempster, A.P. (1972), Covariance Selection, Biometrics, 28, 157\u2013175.\n[6] Diggle, P. and Verbyla, A. (1998), Nonparametric Estimation of Covariance Struc-\nture in Longitudinal Data, Biometrics, 54(2), 401\u2013415.\n[7] El Karoui, N. (2007). Operator Norm Consistent Estimation of a Large Dimensional\nSparse Covariance Matrices. Technical report 734, Department of Statistics, UC-\nBerkeley.\n[8] Fan, J., Feng, Y. and Wu, Y. (2008). Network Exploration via the Adaptive LASSO\nand SCAD Penalties. Manuscript.\n[9] Fan, J. and Li, R. (2001), Variable Selection via Nonconcave Penalized Likelihood\nand its Oracle Properties, J. Amer. Statist. Assoc., 96, 1348\u20131360.\n[10] Fan, J. and Peng, H. (2004), Nonconcave Penalized Likelihood With a Diverging\nNumber of Parameters, Ann. Statist., 32, 928\u2013961.\n33\n[11] Huang, J., Liu, N., Pourahmadi, M. and Liu, L. (2006), Covariance Matrix Selec-\ntion and Estimation via Penalised Normal Likelihood, Biometrika, 93(1), 85\u201398.\n[12] Levina, E., Rothman, A.J. and Zhu, J. (2008), Sparse Estimation of Large Covari-\nance Matrices via a Nested Lasso Penalty, Ann. Applied Statist., 2(1), 245\u2013263.\n[13] Meinshausen, N. and Bu\u00a8hlmann, P. (2006). High dimensional graphs and variable\nselection with the Lasso. Ann. Statist., 34, 1436\u20131462.\n[14] Pourahmadi, M. (1999), Joint Mean-Covariance Models with Applications to Lon-\ngitudinal Data: Unconstrained Parameterisation, Biometrika, 86, 677\u2013690.\n[15] Smith, M. and Kohn, R. (2002), Parsimonious Covariance Matrix Estimation for\nLongitudinal Data, J. Amer. Statist. Assoc., 97(460), 1141\u20131153.\n[16] Ravikumar, P., Lafferty, J., Liu, H. and Wasserman, L. (2008). Sparse additive\nmodels. Manuscript.\n[17] Rothman, A.J., Bickel, P.J., Levina, E. and Zhu, J. (2007), Sparse Permutation\nInvariant Covariance Estimation, Technical report No. 467, Dept. of Statistics,\nUniv. of Michigan.\n[18] Wagaman, A.S. and Levina, E. (2007). Discovering sparse covariance structures\nwith the Isomap, to appear in the Journal of Computational and Graphical Statis-\ntics.\n[19] Wong, F., Carter, C. and Kohn, R. (2003). Efficient Estimation of Covariance\nSelection Models, Biometrika, 90, 809\u2013830.\n[20] Wu, W.B. and Pourahmadi, M. (2003), Nonparametric Estimation of Large Co-\nvariance Matrices of Longitudinal Data, Biometrika, 94, 1\u201317.\n34\n[21] Yuan, M. and Lin, Y. (2007). Model Selection and Estimation in the Gaussian\nGraphical Model, Biometrika, 90, 831\u2013844.\n[22] Zhang, C.H. (2007). Penalized Linear Unbiased Selection. Manuscript.\n[23] Zhao, P. and Yu, B. (2006), On Model Selection Consistency of Lasso, Technical\nReport, Statistics Department, UC-Berkeley.\n[24] Zou, H. and Li, R. (2008). One-step Sparse Estimates in Nonconcave Penalized\nLikelihood Models (With Discussion). Ann. Statist., 36(4), 1509\u20131533.\n35\n"}