{"doi":"10.3115\/1117729.1117730","coreId":"70504","oai":"oai:eprints.lancs.ac.uk:11882","identifiers":["oai:eprints.lancs.ac.uk:11882","10.3115\/1117729.1117730"],"title":"Comparing Corpora Using Frequency Profiling","authors":["Rayson, P.","Garside, R."],"enrichments":{"references":[{"id":16304398,"title":"100 million words of English: a description of the background, nature and prospects of the British National Corpus project.","authors":[],"date":"1993","doi":null,"raw":"Leech, G. (1993). 100 million words of English: a description of the background, nature and prospects of the British National Corpus project. English Today 33, Vol. 9, No. 1, Cambridge University Press.","cites":null},{"id":16304392,"title":"A Hybrid Grammatical Tagger: CLAWS4,","authors":[],"date":"1997","doi":null,"raw":"Garside, R. and Smith, N. (1997).  A Hybrid Grammatical Tagger: CLAWS4, in Garside, R., Leech, G., and McEnery, A. (eds.) Corpus Annotation: Linguistic Information from Computer Text Corpora, Longman, London.","cites":null},{"id":16304391,"title":"Accurate Methods for the Statistics of Surprise and Coincidence.","authors":[],"date":"1993","doi":null,"raw":"Dunning, T. (1993).  Accurate Methods for the Statistics of Surprise and Coincidence.","cites":null},{"id":16304406,"title":"Assisting requirements engineering with semantic document analysis.","authors":[],"date":"2000","doi":"10.1007\/978-1-4471-0457-5_20","raw":"Rayson, P., Garside, R., and Sawyer, P. (2000). Assisting requirements engineering with semantic document analysis. In Proceedings of RIAO 2000 (Recherche d'Informations Assistie par Ordinateur, Computer-Assisted Information Retrieval) International Conference, Coll\u00e8ge de France, Paris, France, April 12-14, 2000. C.I.D., Paris, pp. 1363 -1371.","cites":null},{"id":16304393,"title":"Automatic profiling of learner texts.","authors":[],"date":"1998","doi":null,"raw":"Granger, S. and Rayson, P. (1998).  Automatic profiling of learner texts. In S. Granger (ed.) Learner English on Computer. Longman, London and New York, pp. 119-131.","cites":null},{"id":16304369,"title":"Corpus sampling.","authors":[],"date":"1993","doi":"10.1093\/llc\/8.4.243","raw":"Biber, D. (1993).  Representativeness in Corpus Design. Literary and Linguistic Computing, 8, Issue 4, Oxford University Press, pp. 243-257.Clear, J. (1992). Corpus sampling. In G. Leitner (ed.) New directions in English language corpora. Mouton-de-Gruyter, Berlin, pp. 21 - 31.","cites":null},{"id":16304365,"title":"Ethnographically-informed systems design for air traffic control,","authors":[],"date":"1992","doi":"10.1145\/143457.143470","raw":"Bentley R., Rodden T., Sawyer P., Sommerville I, Hughes J., Randall D.,  Shapiro D. (1992). Ethnographically-informed systems design for air traffic control, In Proceedings of ComputerSupported  Cooperative Work (CSCW) '92, Toronto, November 1992.","cites":null},{"id":16304408,"title":"Goodness-of-fit statistics for discrete multivariate data. Springer series in statistics.","authors":[],"date":"1988","doi":"10.1007\/978-1-4612-4578-0_3","raw":"Read, T. R. C. and Cressie, N. A. C. (1988). Goodness-of-fit statistics for discrete multivariate data. Springer series in statistics. Springer-Verlag, New York.","cites":null},{"id":16304397,"title":"Measures for corpus similarity and homogeneity.","authors":[],"date":"1998","doi":null,"raw":"Kilgarriff, A. and Rose, T. (1998).  Measures for corpus similarity and homogeneity. In proceedings of the 3 rd conference on Empirical Methods in Natural Language Processing, Granada, Spain, pp. 46 - 52.","cites":null},{"id":16304371,"title":"Multinomial Goodness-of-Fit Tests.","authors":[],"date":"1984","doi":"10.1007\/978-1-4612-4578-0_5","raw":"Cressie, N. and Read, T. R. C.  (1984) Multinomial Goodness-of-Fit Tests.  Journal of the Royal Statistical Society. Series B (Methodological), Vol. 46, No. 3, pp. 440 - 464.","cites":null},{"id":16304375,"title":"Pearson\u2019s X and the Loglikelihood Ratio Statistic G 2: A comparative review.","authors":[],"date":"1989","doi":"10.2307\/1403582","raw":"Cressie, N. and Read, T. R. C. (1989). Pearson\u2019s X and the Loglikelihood Ratio Statistic G 2: A comparative review. International Statistical Review, 57, 1, Belfast University Press, N.I., pp. 19-43.","cites":null},{"id":16304404,"title":"Social differentiation in the use of English vocabulary: some analyses of the conversational component of the British National Corpus.","authors":[],"date":"1997","doi":"10.1075\/ijcl.2.1.07ray","raw":"Rayson, P., Leech, G., and Hodges, M. (1997). Social differentiation in the use of English vocabulary: some analyses of the conversational component of the British National Corpus. International Journal of Corpus Linguistics. 2 (1). pp. 133 - 152. John Benjamins, Amsterdam\/Philadelphia.","cites":null},{"id":16304400,"title":"Statistics for Corpus Linguistics.","authors":[],"date":"1998","doi":"10.1076\/jqul.6.3.269.6160","raw":"Oakes, M. P. (1998).  Statistics for Corpus Linguistics. Edinburgh University Press, Edinburgh.","cites":null},{"id":16304402,"title":"The ACAMRIT semantic tagging system: progress report, In","authors":[],"date":"1996","doi":null,"raw":"Rayson, P., and Wilson, A. (1996). The ACAMRIT semantic tagging system: progress report, In L. J. Evett, and T. G. Rose (eds.) Language Engineering for Document Analysis and Recognition, LEDAR, AISB96 Workshop proceedings, pp 13-20. Brighton, England.","cites":null},{"id":16304410,"title":"The Statistical Study of Literary Vocabulary.","authors":[],"date":"1944","doi":"10.2307\/2280636","raw":"Yule, G. (1944).  The Statistical Study of Literary Vocabulary. Cambridge University Press.","cites":null},{"id":16304396,"title":"Using word frequency lists to measure corpus homogeneity and similarity between corpora.","authors":[],"date":"1997","doi":null,"raw":"Kilgarriff, A. (1997). Using word frequency lists to measure corpus homogeneity and similarity between corpora. Proceedings 5th ACL workshop on very large corpora. Beijing and Hong Kong.","cites":null},{"id":16304395,"title":"Why chi-square doesn't work, and an improved LOB-Brown comparison.","authors":[],"date":"1996","doi":null,"raw":"Kilgarriff, A. (1996)  Why chi-square doesn't work, and an improved LOB-Brown comparison. ALLCACH Conference, June 1996, Bergen, Norway.","cites":null},{"id":16304394,"title":"Word frequencies in British and American English. The Norwegian Computing Centre for the Humanities,","authors":[],"date":"1982","doi":null,"raw":"Hofland, K. and Johansson, S. (1982).  Word frequencies in British and American English. The Norwegian Computing Centre for the Humanities, Bergen, Norway.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2000-10","abstract":null,"downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/70504.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/11882\/1\/rg_acl2000.pdf","pdfHashValue":"a290f9b683b52988ecdd20ddbf37acbc6c3b4040","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:11882<\/identifier><datestamp>\n      2017-11-09T00:05:13Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D626F6F6B5F73656374696F6E<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Comparing Corpora Using Frequency Profiling<\/dc:title><dc:creator>\n        Rayson, P.<\/dc:creator><dc:creator>\n        Garside, R.<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:date>\n        2000-10<\/dc:date><dc:type>\n        Contribution in Book\/Report\/Proceedings<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/11882\/1\/rg_acl2000.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.3115\/1117729.1117730<\/dc:relation><dc:identifier>\n        Rayson, P. and Garside, R. (2000) Comparing Corpora Using Frequency Profiling. In: WCC '00 Proceedings of the workshop on Comparing corpora. , pp. 1-6.<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/11882\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.3115\/1117729.1117730","http:\/\/eprints.lancs.ac.uk\/11882\/"],"year":2000,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution in Book\/Report\/Proceedings","NonPeerReviewed"],"fullText":"Comparing Corpora using Frequency Profiling\nPaul RAYSON\nComputing Department,\nLancaster University\nLancaster, UK,\npaul@comp.lancs.ac.uk\nRoger GARSIDE\nComputing Department,\nLancaster University\nLancaster, UK,\nrgg@comp.lancs.ac.uk\nAbstract\nThis paper describes a method of comparing\ncorpora which uses frequency profiling. The\nmethod can be used to discover key words in\nthe corpora which differentiate one corpus\nfrom another. Using annotated corpora, it\ncan be applied to discover key grammatical\nor word-sense categories. This can be used\nas a quick way in to find the differences\nbetween the corpora and is shown to have\napplications in the study of social\ndifferentiation in the use of English\nvocabulary, profiling of learner English and\ndocument analysis in the software\nengineering process.\n1 Introduction\nCorpus-based techniques have increasingly been\nused to compare language usage in recent years.\nOne of the largest early studies was the\ncomparison of one million words of American\nEnglish (the Brown corpus) with one million\nwords of British English (the LOB corpus) by\nHofland and Johansson (1982). A difference\ncoefficient defined by Yule (1944) showed the\nrelative frequency of a word in the two corpora.\nA statistical goodness-of-fit test, the Chi-squared\ntest, was also used to compare word frequencies\nacross the two corpora. They noted any resulting\nchi-squared values which indicated that a\nstatistically significant difference at the 5%, 1%,\nor 0.1% level had been detected between the\nfrequency of a word in American English and in\nBritish English. The null hypothesis of the test is\nthat there is no difference between the observed\nfrequencies.\nMore recently, this size of corpus comparison\nis becoming the standard even for postgraduate\nstudies with the increasing availability of\ncorpora and reasoning that one million words\ngives sufficient evidence for higher frequency\nwords. However, with the production of large\ncorpora such as the British National Corpus\n(BNC) containing one hundred million words\n(Aston & Burnard, 1998), frequency\ncomparisons are available across millions of\nwords of text. There are two main types of\ncorpus comparison:\n\u00b7 comparison of a sample corpus to a large(r)\ncorpus\n\u00b7 comparison of two (roughly-) equal sized\ncorpora\nIn the first type, we refer to the large(r) corpus\nas a \u2018normative\u2019 corpus since it provides a text\nnorm (or standard) against which we can\ncompare. These two main types of comparison\ncan be extended to the comparison of more than\ntwo corpora. For example, we may compare one\nnormative corpus to several smaller corpora at\nthe same time, or compare three or more equal\nsized corpora to each other. In general, however,\nthis makes the results more difficult to interpret.\nThere are also a number of issues which need\nto be considered when comparing two (or more)\ncorpora:\n\u00b7 representativeness\n\u00b7 homogeneity within the corpora\n\u00b7 comparability of the corpora\n\u00b7 reliability of statistical tests (for different sized\ncorpora and other factors)\nRepresentativeness (Biber, 1993) is a\nparticularly important attribute for a normative\ncorpus when comparing a sample corpus to a\nlarge normative corpus (such as the BNC) which\ncontains sections from many different text types\nand domains. To be representative a corpus\nshould contain samples of all major text types\n(Leech, 1993) and if possible in some way\nproportional to their usage in \u2018every day\nlanguage\u2019 (Clear, 1992). This first type of\ncomparison is intended to discover features in\nthe sample corpus with significantly different\nusage (i.e. frequency) to that found in \u2018general\u2019\nlanguage.\nThe second type of comparison is one that\nviews corpora as equals (as in the Brown and\nLOB comparison). It aims to discover features in\nthe corpora that distinguish one from another.\nHomogeneity within each of the corpora is\nimportant here since we may find that the results\nreflect sections within one of the corpora which\nare unlike other sections in either of the corpora\nunder consideration (Kilgarriff 1997).\nComparability is of interest too, since the\ncorpora should have been sampled for in the\nsame way. In other words, the corpora should\nhave been built using the same stratified\nsampling method and with, if possible,\nrandomised methods of sample selection. This is\nthe case with Brown and LOB, since LOB was\ndesigned to be comparable to the Brown corpus.\nThe final issue, which has been addressed\nelsewhere, is the one regarding the reliability of\nthe statistical tests in relation to the size of the\ncorpora under consideration. Kilgarriff (1996)\npoints out that in the Brown versus LOB\ncomparison many common words are marked as\nhaving significant chi-squared values, and that\nbecause words are not selected at random in\nlanguage we will always see a large number of\ndifferences in two such text collections. He\nselects the Mann-Whitney test that uses ranks of\nfrequency data rather than the frequency values\nthemselves to compute the statistic. However, he\nobserves that even with the new test 60% of\nwords are marked as significant. Ignoring the\nactual frequency of occurrence as in the Mann-\nWhitney test discards most of the evidence we\nhave about the distribution of words. The test is\noften used when comparing ordinal rating scales\n(Oakes 1998: 17).\nDunning (1993) reports that we should not rely\non the assumption of a normal distribution when\nperforming statistical text analysis and suggests\nthat parametric analysis based on the binomial or\nmultinomial distributions is a better alternative\nfor smaller texts. The chi-squared value becomes\nunreliable when the expected frequency is less\nthan 5 and possibly overestimates with high\nfrequency words and when comparing a\nrelatively small corpus to a much larger one. He\nproposes the log-likelihood ratio as an\nalternative to Pearson\u2019s chi-squared test. For this\nreason, we chose to use the log-likelihood ratio\nin our work as described in the next section. In\nfact, Cressie and Read (1984) show that\nPearson\u2019s X2 (chi-squared) and the likelihood\nratio G2 (Dunning\u2019s log-likelihood) are two\nstatistics in a continuum defined by the power-\ndivergence family of statistics. They go on to\ndescribe this family in later work (1988, 1989)\nwhere they also make reference to the long and\ncontinuing discussion of the normal and chi-\nsquared approximations for X2 and G2.\nWe have applied the goodness-of-fit test for\ncomparison of linguistically annotated corpora.\nThe frequency distributions of part-of-speech\nand semantic tags are sharply different to words.\nIn these comparisons, we are unlikely to observe\nrare events such as tags occurring once.\nHowever, much higher frequencies will occur\nand so the log-likelihood test is less likely to\noverestimate significance in these cases.\n2 Methodology\nThe method is fairly simple and straightforward\nto apply. Given two corpora we wish to\ncompare, we produce a frequency list for each\ncorpus. Normally, this would be a word\nfrequency list, but as described above and as\nwith examples in the following application\nsection, it can be a part-of-speech (POS) or\nsemantic tag frequency list. However, let us\nassume for now that we are performing a\ncomparison at the word level1. For each word in\nthe two frequency lists we calculate the log-\nlikelihood (henceforth LL) statistic. This is\nperformed by constructing a contingency table\nas in Table 1.\n                                                     \n1 The application of this technique to POS or\nsemantic tag frequency lists is achieved by\nconstructing the contingency table with tag rather\nthan word frequencies.\nTable 1 Contigency table for word frequencies\nCORPUS\nONE\nCORPUS\nTWO\nTOTAL\nFreq\nof word\na b a+b\nFreq\nof other\nwords\nc-a d-b c+d-a-b\nTOTAL c d c+d\nNote that the value \u2018c\u2019 corresponds to the\nnumber of words in corpus one, and \u2018d\u2019\ncorresponds to the number of words in corpus\ntwo (N values). The values \u2018a\u2019 and \u2018b\u2019 are called\nthe observed values (O). We need to calculate\nthe expected values (E) according to the\nfollowing formula:\nIn our case N1 = c, and N2 = d. So, for this\nword, E1 = c*(a+b) \/ (c+d) and E2 = d*(a+b) \/\n(c+d). The calculation for the expected values\ntakes account of the size of the two corpora, so\nwe do not need to normalise the figures before\napplying the formula. We can then calculate the\nlog-likehood value according to this formula:\nThis equates to calculating LL as follows:\nLL = 2*((a*log (a\/E1)) + (b*log (b\/E2)))\nThe word frequency list is then sorted by the\nresulting LL values. This gives the effect of\nplacing the largest LL value at the top of the list\nrepresenting the word which has the most\nsignificant relative frequency difference between\nthe two corpora. In this way, we can see the\nwords most indicative (or characteristic) of one\ncorpus, as compared to the other corpus, at the\ntop of the list. The words which appear with\nroughly similar relative frequencies in the two\ncorpora appear lower down the list. Note that we\ndo not use the hypothesis-test by comparing the\nLL values to a chi-squared distribution table. As\nKilgarriff & Rose (1998) note, even Pearson\u2019s\nX2 is suitable without the \u2018hypothesis-testing\nlink\u2019. Given the non-random nature of words in\na text, we are always likely to find frequencies\nof words which differ across any two texts, and\nthe higher the frequencies, the more information\nthe statistical test has to work with. Hence, it is\nat this point that the researcher must intervene\nand qualitatively examine examples of the\nsignificant words highlighted by this technique.\nWe are not proposing a completely automated\napproach.\n3 Applications\nThis method has already been applied to study\nsocial differentiation in the use of English\nvocabulary and profiling of learner English. In\nRayson et al (1997), selective quantitative\nanalyses of the demographically sampled spoken\nEnglish component of the BNC were carried out.\nThis is a subcorpus of circa 4.5 million words, in\nwhich speakers and respondents are identified\nby such factors as gender, age, social group and\ngeographical region. Using the method, a\ncomparison was performed of the vocabulary of\nspeakers, highlighting those differences which\nare marked by a very high value of significant\ndifference between different sectors of the\ncorpus according to gender, age and social\ngroup.\nIn Granger and Rayson (1998), two similar-\nsized corpora of native and non-native writing\nwere compared at the lexical level. The corpora\nwere analysed by a part-of-speech tagger, and\nthis permitted a comparison at the major word-\nclass level. The patterns of significant overuse\nand underuse for POS categories demonstrated\nthat the learner data displayed many of the\nstylistic features of spoken rather than written\nEnglish.\nThe same technique has more recently been\napplied to compare corpora analysed at the\nsemantic level in a systems engineering domain\nand this is the main focus of this section. The\nmotivation for this work is that despite natural\nlanguage's well-documented shortcomings as a\nmedium for precise technical description, its use\nin software-intensive systems engineering\nremains inescapable. This poses many problems\nfor engineers who must derive problem\nunderstanding and synthesise precise solution\ndescriptions from free text. This is true both for\nthe largely unstructured textual descriptions\nfrom which system requirements are derived,\nand for more formal documents, such as\nstandards, which impose requirements on system\ndevelopment processes. We describe an\nexperiment that has been carried out in the\nREVERE project (Rayson et al, 2000) to\ninvestigate the use of probabilistic natural\nlanguage processing techniques to provide\nsystems engineering support.\nThe target documents are field reports of a\nseries of ethnographic studies at an air traffic\ncontrol (ATC) centre. This formed part of a\nstudy of ATC as an example of a system that\nsupports collaborative user tasks (Bentley et al,\n1992). The documents consist of both the\nverbatim transcripts of the ethnographer\u2019s\nobservations and interviews with controllers,\nand of reports compiled by the ethnographer for\nlater analysis by a multi-disciplinary team of\nsocial scientists and systems engineers. The field\nreports form an interesting study because they\nexhibit many characteristics typical of\ndocuments seen by a systems engineer. The\nvolume of the information is fairly high (103\npages) and the documents are not structured in a\nway designed to help the extraction of\nrequirements (say around business processes or\nsystem architecture).\nThe text is analysed by a part-of-speech tagger,\nCLAWS (Garside and Smith, 1997), and a\nsemantic analyser (Rayson and Wilson, 1996)\nwhich assigns semantic tags that represent the\nsemantic field (word-sense) of words from a\nlexicon of single words and an idiom list of\nmulti-word combinations (e.g. \u2018as a rule\u2019). These\nresources contain approximately 52,000 words\nand idioms.\nThe normative corpus that we used was a 2.3\nmillion-word subset of the BNC derived from\nthe transcripts of spoken English. Using this\ncorpus, the most over-represented semantic\ncategories in the ATC field reports are shown in\nTable 2. The log-likelihood test is applied as\ndescribed in the previous section and represents\nthe semantic tag's frequency deviation from the\nnormative corpus. The higher the figure, the\ngreater the deviation.\nTable 2. Over-represented categories in ATC\nfield reports\nLog-\nlikelihood\nTag Word sense (examples\nfrom the text)\n3366 S7.1 power, organising\n(\u2018controller\u2019, \u2018chief\u2019)\n2578 M5 flying (\u2018plane\u2019, \u2018flight\u2019,\n\u2018airport\u2019)\n988 O2 general objects (\u2018strip\u2019,\n\u2018holder\u2019, \u2018rack\u2019)\n643 O3 electrical equipment\n(\u2018radar\u2019, \u2018blip\u2019)\n535 Y1 science and technology\n(\u2018PH\u2019)\n449 W3 geographical terms\n(\u2018Pole Hill\u2019, \u2018Dish Sea\u2019)\n432 Q1.2 paper documents and\nwriting (\u2018writing\u2019,\n\u2018written\u2019, \u2018notes\u2019)\n372 N3.7 measurement (\u2018length\u2019,\n\u2018height\u2019, \u2018distance\u2019,\n\u2018levels\u2019, \u20181000ft\u2019)\n318 L1 life and living things\n(\u2018live\u2019)\n310 A10 indicating actions\n(\u2018pointing\u2019, \u2018indicating\u2019,\n\u2018display\u2019)\n306 X4.2 mental objects\n(\u2018systems\u2019, \u2018approach\u2019,\n\u2018mode\u2019, \u2018tactical\u2019,\n\u2018procedure\u2019)\n290 A4.1 kinds, groups (\u2018sector\u2019,\n\u2018sectors\u2019)\nWith the exception of Y1 (an anomaly caused\nby an interviewee\u2019s initials being mistaken for\nthe PH unit of acidity), all of these semantic\ncategories include important objects, roles,\nfunctions, etc. in the ATC domain. The\nfrequency with which some of these occur, such\nas M5 (flying), are unsurprising. Others are\nmore revealing about the domain of ATC.\nFigure 1 shows some of the occurrences of the\nsemantic category O2 (general objects). The\nimportant information extracted here is the\nimportance of \u2018strips\u2019 (formally, \u2018flight strips\u2019).\nThese are small pieces of cardboard with printed\nflight details that are the most fundamental\nartefact used by the air traffic controllers to\nmanage their air space.  Examination of other\nwords in this category also shows that flight\nFigure 1. Browsing the semantic category O2\nstrips are held in \u2018racks\u2019 to organise them\naccording to (for example) aircraft time-of-\narrival.\nSimilarly, browsing the context for Q1.2\n(paper documents and writing) would allow us\nto discover that controllers annotate flight strips\nto record deviations from flight plans, and L1\n(life, living things) would reveal that some strips\nare \u2018live\u2019, that is, they refer to aircraft currently\ntraversing the controller's sector. Notice also that\nthe semantic categories' deviation from the\nnormative corpus can also be expected to reveal\ndomain roles (actors). In this example, the\nfrequency of S7.1 (power, organising) shows the\nimportance of the roles of \u2018controllers\u2019 and\n\u2018chiefs\u2019.\nUsing the frequency profiling method does not\nautomate the task of identifying abstractions,\nmuch less does it produce fully formed\nrequirements that can be pasted into a\nspecification document. Instead, it helps the\nengineer quickly isolate potentially significant\ndomain abstractions that require closer analysis.\n4 Conclusions\nThis paper has described a method of comparing\ncorpora which uses frequency profiling. The\nmethod has been shown to discover key items in\nthe corpora which differentiate one corpus from\nanother. It has been applied at the word level,\npart-of-speech tag level, and semantic tag level.\nIt can be used as a quick way in to find the\ndifferences between the corpora and is shown to\nhave applications in the study of social\ndifferentiation in the use of English vocabulary,\nprofiling of learner English and document\nanalysis in the software engineering process.\nFuture directions in which we aim to research\ninclude a more precise specification of the\nreliability of the statistical tests (LL, Pearson\u2019s\nX2 and others) under the effects of corpus size,\nratio of the corpora being compared and word\n(or tag) frequency.\nWe do not propose a completely automated\napproach. The tools suggest a group of key items\nby decreasing order of significance which\ndistinguish one corpus from another. It is then\nthat the researcher should investigate\noccurrences of the significant items in the\ncorpora using standard corpus techniques such\nas KWIC (key-word in context). The reasons\nbehind their significance can be discovered and\nexplanations sought for the patterns displayed.\nBy this process, we can compare the corpora\nunder investigation and make hypotheses about\nthe language use they represent.\nAcknowledgements\nOur thanks go to Geoffrey Leech and the\nanonymous reviewers who commented on\nearlier versions of this paper. The REVERE\nproject is supported under the EPSRC Systems\nEngineering for Business Process Change\n(SEBPC) programme, project number\nGR\/MO4846.\nReferences\nAston, G. and Burnard, L. (1998). The BNC\nHandbook: Exploring the British National Corpus\nwith SARA, Edinburgh University Press.\nBentley R., Rodden T., Sawyer P., Sommerville I,\nHughes J., Randall D., Shapiro D. (1992).\nEthnographically-informed systems design for air\ntraffic control, In Proceedings of Computer-\nSupported Cooperative Work (CSCW) '92,\nToronto, November 1992.\nBiber, D. (1993). Representativeness in Corpus\nDesign. Literary and Linguistic Computing, 8,\nIssue 4, Oxford University Press, pp. 243-257.\nClear, J. (1992). Corpus sampling. In G. Leitner (ed.)\nNew directions in English language corpora.\nMouton-de-Gruyter, Berlin, pp. 21 - 31.\nCressie, N. and Read, T. R. C.  (1984) Multinomial\nGoodness-of-Fit Tests. Journal of the Royal\nStatistical Society. Series B (Methodological), Vol.\n46, No. 3, pp. 440 - 464.\nCressie, N. and Read, T. R. C. (1989). Pearson\u2019s X2\nand the Loglikelihood Ratio Statistic G2: A\ncomparative review. International Statistical\nReview, 57, 1, Belfast University Press, N.I., pp.\n19-43.\nDunning, T. (1993). Accurate Methods for the\nStatistics of Surprise and Coincidence.\nComputational Linguistics, 19, 1, March 1993, pp.\n61-74.\nGarside, R. and Smith, N. (1997). A Hybrid\nGrammatical Tagger: CLAWS4, in Garside, R.,\nLeech, G., and McEnery, A. (eds.) Corpus\nAnnotation: Linguistic Information from Computer\nText Corpora, Longman, London.\nGranger, S. and Rayson, P. (1998). Automatic\nprofiling of learner texts. In S. Granger (ed.)\nLearner English on Computer. Longman, London\nand New York, pp. 119-131.\nHofland, K. and Johansson, S. (1982). Word\nfrequencies in British and American English. The\nNorwegian Computing Centre for the Humanities,\nBergen, Norway.\nKilgarriff, A. (1996) Why chi-square doesn't work,\nand an improved LOB-Brown comparison. ALLC-\nACH Conference, June 1996, Bergen, Norway.\nKilgarriff, A. (1997). Using word frequency lists to\nmeasure corpus homogeneity and similarity\nbetween corpora. Proceedings 5th ACL workshop\non very large corpora. Beijing and Hong Kong.\nKilgarriff, A. and Rose, T. (1998). Measures for\ncorpus similarity and homogeneity. In proceedings\nof the 3rd conference on Empirical Methods in\nNatural Language Processing, Granada, Spain, pp.\n46 - 52.\nLeech, G. (1993). 100 million words of English: a\ndescription of the background, nature and\nprospects of the British National Corpus project.\nEnglish Today 33, Vol. 9, No. 1, Cambridge\nUniversity Press.\nOakes, M. P. (1998). Statistics for Corpus\nLinguistics. Edinburgh University Press,\nEdinburgh.\nRayson, P., and Wilson, A. (1996). The ACAMRIT\nsemantic tagging system: progress report, In L. J.\nEvett, and T. G. Rose (eds.) Language Engineering\nfor Document Analysis and Recognition, LEDAR,\nAISB96 Workshop proceedings, pp 13-20.\nBrighton, England.\nRayson, P., Leech, G., and Hodges, M. (1997). Social\ndifferentiation in the use of English vocabulary:\nsome analyses of the conversational component of\nthe British National Corpus. International Journal\nof Corpus Linguistics. 2 (1). pp. 133 - 152. John\nBenjamins, Amsterdam\/Philadelphia.\nRayson, P., Garside, R., and Sawyer, P. (2000).\nAssisting requirements engineering with semantic\ndocument analysis. In Proceedings of RIAO 2000\n(Recherche d'Informations Assistie par Ordinateur,\nComputer-Assisted Information Retrieval)\nInternational Conference, Coll\u00e8ge de France, Paris,\nFrance, April 12-14, 2000. C.I.D., Paris, pp. 1363 -\n1371.\nRead, T. R. C. and Cressie, N. A. C. (1988).\nGoodness-of-fit statistics for discrete multivariate\ndata. Springer series in statistics. Springer-Verlag,\nNew York.\nYule, G. (1944). The Statistical Study of Literary\nVocabulary. Cambridge University Press.\n"}