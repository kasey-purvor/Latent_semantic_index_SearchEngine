{"doi":"10.1007\/978-3-642-04843-2_49","coreId":"65354","oai":"oai:dro.dur.ac.uk.OAI2:6136","identifiers":["oai:dro.dur.ac.uk.OAI2:6136","10.1007\/978-3-642-04843-2_49"],"title":"Parallel hybrid particle swarm optimization and applications in geotechnical engineering.","authors":["Zhang, Y.","Gallipoli, D.","Augarde, C. E."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":["Cai, Z.","Li, Z.","Kang, Z.","Liu, Y."],"datePublished":"2009-09-01","abstract":"A novel parallel hybrid particle swarm optimization algorithm named hmPSO is presented. The new algorithm combines particle swarm optimization (PSO) with a local search method which aims to accelerate the rate of convergence. The PSO provides initial guesses to the local search method and the local search accelerates PSO with its solutions. The hybrid global optimization algorithm adjusts its searching space through the local search results. Parallelization is based on the client-server model, which is ideal for asynchronous distributed computations. The server, the center of data exchange, manages requests and coordinates the time-consuming objective function computations undertaken by individual clients which locate in separate processors. A case study in geotechnical engineering demonstrates the effectiveness and efficiency of the proposed algorithm","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/65354.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/6136\/1\/6136.pdf","pdfHashValue":"32ca653245f533276df85a79059f114ec8ea758e","publisher":"Springer","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:6136<\/identifier><datestamp>\n      2015-03-31T11:44:08Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Parallel hybrid particle swarm optimization and applications in geotechnical engineering.<\/dc:title><dc:creator>\n        Zhang, Y.<\/dc:creator><dc:creator>\n        Gallipoli, D.<\/dc:creator><dc:creator>\n        Augarde, C. E.<\/dc:creator><dc:description>\n        A novel parallel hybrid particle swarm optimization algorithm named hmPSO is presented. The new algorithm combines particle swarm optimization (PSO) with a local search method which aims to accelerate the rate of convergence. The PSO provides initial guesses to the local search method and the local search accelerates PSO with its solutions. The hybrid global optimization algorithm adjusts its searching space through the local search results. Parallelization is based on the client-server model, which is ideal for asynchronous distributed computations. The server, the center of data exchange, manages requests and coordinates the time-consuming objective function computations undertaken by individual clients which locate in separate processors. A case study in geotechnical engineering demonstrates the effectiveness and efficiency of the proposed algorithm.<\/dc:description><dc:subject>\n        Particle swarm optimization<\/dc:subject><dc:subject>\n         Asynchronous parallel computation<\/dc:subject><dc:subject>\n         Server-client model<\/dc:subject><dc:subject>\n         hmPSO. <\/dc:subject><dc:publisher>\n        Springer<\/dc:publisher><dc:source>\n        Cai, Z.  & Li, Z. & Kang, Z. & Liu, Y. (Eds.). (2009). Advances in computation and intelligence. Berlin: Springer, pp. 466-475, Lecture notes in computer science(5821)<\/dc:source><dc:contributor>\n        Cai, Z. <\/dc:contributor><dc:contributor>\n        Li, Z.<\/dc:contributor><dc:contributor>\n        Kang, Z.<\/dc:contributor><dc:contributor>\n        Liu, Y.<\/dc:contributor><dc:date>\n        2009-09-01<\/dc:date><dc:type>\n        Book chapter<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:6136<\/dc:identifier><dc:identifier>\n        issn:0302-9743<\/dc:identifier><dc:identifier>\n        issn: 1611-3349<\/dc:identifier><dc:identifier>\n        doi:10.1007\/978-3-642-04843-2_49<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/6136\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1007\/978-3-642-04843-2_49<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/6136\/1\/6136.pdf<\/dc:identifier><dc:rights>\n        The final publication is available at Springer via http:\/\/dx.doi.org\/10.1007\/978-3-642-04843-2_49<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["0302-9743"," 1611-3349","issn: 1611-3349","issn:0302-9743"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2009,"topics":["Particle swarm optimization","Asynchronous parallel computation","Server-client model","hmPSO."],"subject":["Book chapter","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n10 November 2009\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nZhang, Y. and Gallipoli, D. and Augarde, C. E. (2009) \u2019Parallel hybrid particle swarm optimization and\napplications in geotechnical engineering.\u2019, in Advances in computation and intelligence. Heidelberg: Springer\nBerlin , pp. 466-475. Lecture notes in computer science. (5821).\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1007\/978-3-642-04843-249\nPublisher\u2019s copyright statement:\nAdditional information:\nThe original publication is available at www.springerlink.com\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\n  \nDurham Research Online \n \nDeposited in DRO: \n10 November 2009 \n \nPeer-review status: \nPeer-reviewed \n \nPublication status of attached file: \nAccepted for publication version \n \nCitation for published item: \nZhang, Y. and Gallipoli, D. and Augarde, C. E. (2009) 'Parallel hybrid particle swarm \noptimization and applications in geotechnical engineering.', in Advances in computation and \nintelligence. Heidelberg: Springer Berlin , pp. 466-475. Lecture notes in computer science. \n(5821). \n \nFurther information on publisher\u2019s website: \nhttp:\/\/dx.doi.org\/10.1007\/978-3-642-04843-2_49 \n \nPublisher\u2019s statement: \nThe original publication is available at www.springerlink.com \n \n \n \n \n \n \n \n \n \n \n \nUse policy \n \nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior \npermission or charge, for personal research or study, educational, or not-for-profit purposes provided that : \n \n\uf0a7 a full bibliographic reference is made to the original source \n\uf0a7 a link is made to the metadata record in DRO \n\uf0a7 the full-text is not changed in any way \n \nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders. \n \nPlease consult the full DRO policy for further details. \n \nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom \nTel : +44 (0)191 334 2975 | Fax : +44 (0)191 334 2971 \nhttp:\/\/dro.dur.ac.uk \nParallel Hybrid Particle Swarm Optimization and \nApplications in Geotechnical Engineering \nYouliang Zhang1, Domenico Gallipoli2, Charles Augarde3 \n \n1 State Key Laboratory for GeoMechanics and Deep Underground Engineering, China \nUniversity of Mining & Technology , Xuzhou, 221008, P.R. China \nylzhang@whrsm.ac.cn \n2 Department of Civil Engineering, University of Glasgow, Glasgow G12 8LT, UK \ngallipoli@civil.gla.ac.uk \n3 School of Engineering, Durham University, South Road, Durham DH1 3LE, UK \nCharles.augarde@dur.ac.uk \nAbstract. A novel parallel hybrid particle swarm optimization algorithm named \nhmPSO is presented. The new algorithm combines particle swarm optimization \nwith a local search method which aims to accelerate the rate of convergence. \nThe hybrid global optimization algorithm adjusts its searching space through \nthe local search results. Parallelization is based on the client-server model, \nwhich is ideal for asynchronous distributed computations. The server is the \ncenter of data exchange, which manages requests and coordinates the time-\nconsuming computations undertaken by individual clients. A case study in \ngeotechnical engineering demonstrates the effectiveness and efficiency of the \nproposed algorithm.  \nKeywords: particle swarm optimization; asynchronous parallel computation; \nserver-client model; hmPSO. \n1   Introduction \nGlobal optimization algorithms are important tools for parameter identification in \ngeotechnical engineering. However, due to the lack of analytical solutions for most \ngeotechnical problems, evaluation of the objective function f(x) during optimization is \nusually achieved by performing numerical (e.g. finite element) simulations of the \nrelevant boundary value problem. This approach, which is often referred to as \n\u201csimulation-based optimization\u201d might produce multi-modal forms of the objective \nfunction due to numerical fluctuations and is generally computationally expensive.  \nParticle Swarm Optimization (PSO) proposed by Eberhart and Kennedy [1, 2] is a \nnew evolutionary algorithm inspired by social behavior of bird flocks. Recently, PSO \nhas been receiving increasing attention due to its effectiveness, efficiency, and \nsimplicity in achieving global optimization. In particular, PSO is capable of capturing \nthe global optimum for a wide range of multi-modal problems (i.e. those problems \nwhere the objective function might have many local minima). However, for some \ncomplex problems, PSO could suffer from premature convergence towards a \nsuboptimal solution, just like other evolutionary algorithms. Another weakness of \nPSO is the slow rate of convergence, especially in the later stage of the search [3, 4].  \nA modified parallel PSO is here proposed to address shortfalls such as premature \nidentification of suboptimal solutions, slow convergence rate and high computational \ncosts. In particular, a hybridization technique is introduced where a local search is run \nin parallel with the main PSO algorithm to carry out quick and efficient explorations \naround the current optimum at a much lower computational cost. The search space for \nthe PSO algorithm is centered on the latest solution from the local search routine and \nis progressively narrowed as the algorithm progresses. Hybridization of a global \noptimization method such as PSO with a local search method has proved to be very \neffective in solving a range of problems [5-7]. In addition, an asynchronous parallel \nversion of the hybrid PSO algorithm is here proposed to reduce computational time. \nThe main parts of the method were presented in a previous paper [8]. The \nimplementation details and an application to a typical geotechnical problem are \ninstead the focus of this paper. \n2   Parallel hybrid particle swarm optimization \nPSO is advantageous for global exploration of the search space while local search \nmethods offer fast convergence rates when good starting points are provided. The \nhybrid PSO algorithm presented here aims to combine the strengths from both these \napproaches. The proposed parallel hybrid PSO algorithm consists of the basic PSO, a \nlocal search method and an asynchronous parallel strategy. The Message Passing \nInterface (MPI) [9] is chosen as the parallel programming library. \n2.1   Basic PSO \nThe PSO algorithm was proposed as a population-based stochastic global \noptimization method by Kennedy & Eberhart [1, 2]. The population, which is called \n\u201cswarm\u201d in PSO, is made up of \u201cparticles\u201d. The ith particle has such properties as \nfitness fi, position xi, and velocity vi. The PSO starts by initializing the particle \npositions xi and velocities vi and computes the corresponding fitness fi . Then particles \n\u201cfly\u201d across the search space, which means that a series of iteration is performed \nwhere, for each iteration, the fitness of all particles is updated according to the \nfollowing rules \n( ) ( )igiikikki rcrcw xPxPvv \u2212+\u2212+=+ 22111  . (1) \n11 ++ += kikiki vxx  . (2) \nwhere the superscripts  k, k+1 are iteration numbers; r1 and  r2 are two random factors \nin the interval [0,1]; wk is the inertia weight and c1 and c2 are constants representing \nthe \"cognitive\" and \"social\" components of knowledge, respectively. Each particle \nremembers its best position (cognitive knowledge), which is denoted as Pi. In \naddition, knowledge of the best position ever achieved across the whole swarm, \nwhich is denoted as Pg, is shared between particles (social knowledge). Updating \nrules drive particles towards the optimal region and, as the algorithm progresses, all \nparticles tend to group around the global optimum until a solution is finally found.  \n2.2   Local search method \nLocal search methods are a class of methods that hunt for the optimum solution in the \nvicinity of a given set of starting points. The particular choice of these starting points \nstrongly affects the efficiency of the local search. The Nelder-Mead simplex \nalgorithm [10] is used herein mainly due to its simplicity and conformity with PSO. \nBoth Nelder-Mead and PSO methods require only evaluation of the objective function \nat given points in the search space and no gradient information is needed, which \nmakes the two methods easy to combine. The simplex method needs n + 1 points as \nstarting points, where n is the number of unknown parameters to be determined.   \n2.3 Asynchronous parallel hybrid PSO  \nThe objective of the hybrid method described here is to make best use of the strengths \nof the global and the local search methods. PSO is powerful for global exploration of \nthe search space while the Nelder-Mead simplex algorithm is efficient for local \nexploration. The combination of the two methods is achieved by taking the n+1 best \nsolutions from the PSO algorithm as starting points of the Nelder-Mead simplex \nalgorithm. The solution from the local search is then fed back into the PSO as a \ncandidate global optimum. At the same time, a sub-swarm is allocated to explore a \nsmaller region centered on the solution obtained from the local search. In the \nproposed algorithm, which is named hmPSO (hybrid moving-boundary Particle \nSwarm Optimization), the whole swarm is therefore divided into two sub-swarms. \nThe first \u201cglobal\u201d sub-swarm searches in the original space while the second \u201clocal\u201d \nsub-swarm searches in a smaller space that is continuously updated to coincide with \nthe region around the latest solution from the local search.   \nIn the parallel implementation of the hmPSO algorithm each particle is assigned to \na different processor. Parallelization can be achieved by using either a synchronous or \nasynchronous, model. In the synchronous model, PSO moves to the next iteration \nonly when all particles have completed their current iteration, i.e. when all particles \nhave terminated the numerical simulation corresponding to their current position in \nthe search space. A drawback of the synchronous model is that the slowest particle \ndetermines the speed of the algorithm and, in non-linear simulation-based \noptimization, the computational time can vary significantly between particles. In \ncontrast, in the asynchronous model, each particle moves to the next iteration \nimmediately after it finishes the current one without having to wait for other particles. \nThis feature is particularly advantageous when load unbalances between particles are \nvery large as no processor remains idle while other processors are busy. The use of \nthe asynchronous model is also essential in distributed parallel computing where \ncomputing power of different processors vary significantly due to hardware \nconfigurations.  \nThe client-server model is adopted for the implementation of the asynchronous \nmodel as shown in Fig. 1. The client-server model consists of three parts: the server, \nthe particle clients and the local search clients. The server resides on one processor, \nwhich is the centre for data sharing and system management. The server stores and \nsorts the best positions of individual particles and updates the swarm\u2019s best position. \nThe clients are instead responsible for undertaking the actual numerical simulations \ncorresponding to the current position of their respective particles and for running the \nNelder-Mead simplex local searches. Each client communicates with the server, and \nthere is no communication between particle clients. There is only one local search \nclient when a serial local search algorithm is used. Otherwise, if a parallel local search \nalgorithm is employed, there is more than one local search client. In this case a \nmaster-slave model is adopted for the parallel local search where the master of the \nlocal search group coordinates and manages computations undertaken by the slaves. \nWhen a master-slave model is adopted, the server only communicates with the master \nof the local search group. The number of clients depends on the dimension of the \nproblem and the specific algorithm used. \n \nClient (particle 1)\nServer\nClient (particle 2) Client (particle n)\nLocal search process 1\nmaster\nLocal search process 2\nslave\nLocal search process m\nslave\nParticle swarm group (clients)\nApproximate local search group (clients)\nData back\nRequest for serviceRequest for service\nData back\nRequest for service\nData back\n \nFig. 1. Client-server model for parallel hmPSO \n \nThe client's responsibility is to perform the actual numerical simulations for \nevaluating the objective function and to undertake the local searches. The server's \nresponsibility is to store and manage data shared by clients, listen to clients' queries \nfor information or data, process the queries and return data or commands back to the \nclients. The server is also in charge of termination of all clients on receipt of stop \ninformation either from the local search client or from a particle client. It is obvious \nthat different processors run different programs using different data, so the paradigm \nof the parallel hmPSO is MPMD (Multiple Programs Multiple Data). \n3 Parallel implementation of hmPSO \nNon-blocking MPI communication functions such as MPI_Isend, MPI_Irecv, and \nMPI_Test are used for the implementation of the asynchronous parallel hmPSO. By \nusing non-blocking communication which allows both communications and \ncomputations to proceed concurrently, the performance of the parallel program can be \nimproved. For example, MPI_Isend starts a send operation then returns immediately \nwithout waiting for the communication to complete. Similarly, MPI_Irecv starts a \nreceive operation and returns before the data has been received. The structure for a \ncommunication overlapping with a computation is, \n MPI_Isend \n Do some computation \n MPI_Test  \nChecking for the completion of non-blocking send and receive operations is carried \nout by MPI_Test. This check should be performed before any data being exchanged \nbetween processors is used. MPI_Test waits until a non-blocking communication has \nfinished but it does not block the algorithm so to avoid any deadlock. More details on \nhow to use these functions can be found in [9].  \nThe three parts of the algorithm, i.e. the server program, the particle client program, \nand the Nelder-Mead simplex program, are listed below. In the parallel algorithm, \neach processor hosts only one client.  \nparallel hmPSO  (program resides on the server) \n  repeat \n  MPI_Irecv (receiving data from all clients ) \n  MPI_Test \n  switch according to client data\/request  \n    if ( data is particle best solution ) \n      update swarm best \n      send swarm best back to the particle \n      If (FES>startFES) \n      send starting points to clients of local search \n    else if ( data is local search solution ) \n      update swarm best \n      sort particles by their best fitness \n      send starting points to local search \n      restart a sub-swarm search on a smaller search \nspace based on the local search solution \n    else if ( data is stop message ) \n      send stop message to all clients \n      terminate server \n    end if \n    check stopping criterion \n    if (stopping criterion is met ) \n      send stop message to all clients \n      terminate server \n    end if \n  }while ( stopping criterion is not met ) \nThe server program keeps on receiving data or requests from all clients, and \nresponds accordingly. The server also stores the best position of the whole swarm and \nthe best positions of individual particles. The swarm best position is sent back to \nparticle clients when a request is made.  \nAfter sorting the best positions of individual particles, the top n+1 particles are \ntaken as the starting points for the Nelder-Mead simplex local search. After \ntermination of the Nelder-Mead simplex local search, a sub-swarm is allocated to \nexplore a smaller search space centered on the solution from the local search. The \nserver is also in charge of the termination of all clients by sending a stop message.  \nparallel hmPSO  (program resides on particle clients) \n  initialize position xi \n  initialize velocity vi \n  evaluate objective function f(xi)  \n  update particle best Pi  \n  MPI_Isend (send particle best Pi to server)  \n  repeat  \n    MPI_Irecv (receive data\/command from server ) \n    MPI_Test \n    update velocity vi  \n    update position xi  \n    evaluate objective function f(xi)  \n    switch according to the received data from service \n      if ( data is stop message ) \n        terminate this client process \n      else if ( data is swarm best position Pg ) \n        update Pg for this particle \n      else if ( data is to restart sub-swarm) \n        reset particle\u2019s new search space \n        initialize position xi \n        initialize velocity vi \n      end if \n    check stopping criterion \n    if (stopping criterion is met ) \n      send stop message to the server \n    else \n      MPI_Isend (send particle best Pi to server ) \n    end if \n  }while ( stopping criterion is not met ) \nParticle clients update their respective positions and velocity by performing the \nrelevant numerical simulation and evaluate the objective function. These are the most \ncomputationally expensive operations of the whole optimization process. The \nparticles receive the swarm best position Pg from the server and, after completing the \nnumerical simulation, return the particle best position Pi to the server. When the \nconvergence criterion is met, the clients are commanded by the server to terminate \ntheir processes.  \n \nparallel hmPSO  (program resides on local search client) \n  MPI_Irecv (receive data\/command from server ) \n  MPI_Test \n  repeat  \n    switch according to the received data from service \n      if ( data is stop message ) \n        terminate this client process \n      else if ( data is on starting points ) \n        perform a local search \n      end if \n    check stopping criterion \n    if (stopping criterion is met ) \n      send stop message to the server \n    else \n      MPI_Isend (send local search solution to server ) \n    end if \n  }while ( stopping criterion is not met ) \n \nThe local search client starts a new run of the Nelder-Mead simplex algorithm as \nsoon as it receives a fresh set of n+1 starting points from the server. The solution from \nthe local search is then sent back to the server to update the current best position of \nthe whole swarm. When the convergence criterion is met, the local search client sends \na message to the server.  \n4 Applications to geotechnical engineering \nThe above parallel hmPSO algorithm is here used for selecting parameter values in \nthe Barcelona Basic Model (BBM) [11] based on the results from pressuremeter tests. \nBBM .is one of the best known constitutive models for unsaturated soils and is \nformulated in terms of 9 independent parameters. In this work, however, only a sub-\nset of 6 parameter values is determined. This follows a sensitivity analysis that has \nshown the dominant influence of these 6 parameters in governing the soil response \nduring simulations of pressuremeter tests. \nPressuremeter testing is a widely used in-situ technique for characterizing soil \nproperties. The technique consists in the application of an increasing pressure to the \nwalls of a cylindrical borehole while measuring the corresponding radial strains. \nCavity expansion curves showing applied pressure versus radial expansion are then \nanalyzed to infer soil properties.  Due to the nonlinearity of BBM, no closed-form \nanalytical solution exists to predict soil behaviour during a pressuremeter test and, \nhence finite element simulations are here used.  \nIn order to validate the proposed algorithm, three simulated constant suction \npressuremeter tests corresponding to a known set of parameter values in BBM (see \nFig. 2) were taken as the \u201cexperimental\u201d data. The optimization algorithm was then \ntested to check whether the same set of parameter values could be found. The \nobjective function is given in Eq. 3 and is defined in such a way that the three \npressuremeter tests at different suctions have to be simultaneously matched \n( )\u2211\n=\n\u2212=\nN\ni\nm\nic\ns\nicf\n1\n2\n,, \u03b5\u03b5  . \n(3) \nwhere (\u03b5sc,i - \u03b5mc,i) is the difference between the \u201cexperimental\u201d and simulated cavity \nstrains at the same value of cavity pressure, and N is the total number of \n\u201cexperimental\u201d points on the three curves. \nThe range of parameter values defining the entire 6-dimensional search space are \ngiven in Table 1. The optimum solution in this search space is x=(M, k, \u03ba, r, \u03b2, \npc)=(0.9, 0.5, 0.025, 1.5, 1.0e-5Pa-1, 2.0e6Pa), corresponding to the set of BBM \nparameter values used to generate the curves shown in Fig. 2. The other three BBM \nparameters that were not included in the optimization process were taken equal to (G, \n\u03bb(0), po*)=(3.0e+3 kPa, 0.13, 9.18 kPa) in all simulations. Readers can refer to [11] \nfor the physical meaning of the individual model parameters.  \nPressuremeter tests were simulated by a 2D axisymmetric FE model using eight-\nnoded quadrilateral elements with pore water pressure and pore air pressure on the \ncorner nodes. The cavity pressure was applied incrementally in steps of 10 kPa. \nIn the PSO algorithm, the swarm size was 35, the constants c1 and c2 were both \ntaken equal to 2.0, the convergence tolerance for the objective function was equal to \n1.0e-5 and the inertia weight wk decreased linearly with the number of iterations from \n0.9 to 0.4.   \nThe parallel computer \u201cHamilton\u201d was used for the analysis. This is a Linux cluster \nhosted at Durham University consisting of 96 dual-processor dual-core Opteron 2.2 \nGHz nodes with 8 GBytes of memory and a Myrinet fast interconnect for running \nMPI code, and 135 dual-processor Opteron 2 GHz nodes with 4 GBytes of memory \nand a Gigabit interconnect. The system has 3.5 Terabytes of disk storage. The \noperating system is SuSE Linux 10.0 (64-bit). \nTable 1.  BBM parameters and their ranges.  \nparameter Minimum value Maximum value \nM 0.1 1.4 \nk 0.1 0.7 \n\u03ba 0.01 0.1 \nr 1.05 1.8 \n\u03b2 1.0e-6 kPa-1 1.0e-4 kPa-1 \npc 1.0e4 kPa 1.0e7 kPa \n \n020\n40\n60\n80\n100\n120\n140\n160\n0 0.1 0.2 0.3 0.4 0.5 0.6\nCavity strain\nCa\nvi\nty\n p\nre\nss\nur\ne \n(k\nPa\n)\ns=0kPa \ns=100kPa \ns=200kPa \n \nFig. 2. Curves generated by the chosen parameter set \n \nResults show that the algorithm was capable to find the target 6 parameter values \nwith very high accuracy. The optimum parameter values were found at the end of a \nlocal search and it took 4 local searches to converge to the solution. As an example, \nthe first 7 rows in Table 2 show the initial values at the 7 corners of the simplex for \nthe last local search while the bottom row shows the final optimum solution. The \nhmPSO algorithm took a total number of 34884 objective function evaluations to \nconverge towards the solution. This corresponded to a computational time of 191.6 \nhours for the asynchronous parallel implementation. The average time of a single \nobjective function evaluation is 122.2 seconds. So it can be estimated that if a serial \nalgorithm is used, the computation time could be over 1184.2 hours. This confirms \nthe advantage of using a parallel implementation rather than a serial one. \nTable 2.  Results from the last local search.  \nobjective \nfunction value M k \u03ba r \u03b2(kPa-1) pc (kPa) \n2.71E-05 9.00E-01 5.00E-01 2.50E-02 1.50E+00 9.99E-06 2.01E+06 \n3.04E-03 9.14E-01 4.54E-01 2.12E-02 1.32E+00 1.14E-05 1.62E+06 \n3.22E-03 9.14E-01 4.56E-01 2.14E-02 1.33E+00 1.17E-05 1.59E+06 \n3.24E-03 9.13E-01 4.53E-01 2.16E-02 1.32E+00 1.13E-05 1.61E+06 \n3.38E-03 9.11E-01 4.54E-01 2.12E-02 1.33E+00 1.12E-05 1.90E+06 \n3.41E-03 9.17E-01 4.54E-01 2.18E-02 1.32E+00 1.12E-05 1.30E+06 \n3.54E-03 9.12E-01 4.54E-01 2.18E-02 1.33E+00 1.14E-05 1.68E+06 \n2.0618E-06 9.00E-01 5.00E-01 2.50E-02 1.50E+00 1.00E-05 2.00E+06 \n5 Conclusions \nThe paper presents the implementation and application of a parallel hybrid moving-\nboundary Particle Swarm Optimization algorithm (hmPSO). The algorithm originates \nfrom the hybridization of the basic particle swarm optimization algorithm with a \nNelder-Mead simplex local search. A client-server model is used for the \nasynchronous parallel implementation of the algorithm. Using the proposed \nmethodology, 6 parameter values of a nonlinear constitutive unsaturated soil model \nwere simultaneously identified by means of back analysis of pressuremeter tests. \nComputational time was reduced significantly by parallelization of the algorithm on \nthe computer cluster \u201cHamilton\u201d at Durham University, UK.  \n \nAcknowledgements  \nThe authors gratefully acknowledge support from U.K. EPSRC (grant ref. \nEP\/C526627\/1) and State Key Laboratory for GeoMechanics and Deep Underground \nEngineering (grant SKLGDUE08003X).  \nReferences \n1. Kennedy J. and Eberhart R. Particle swarm optimization. In: IEEE, NeuralNetworks Council \nStaff, IEEE Neural Networks Council editor. Proc. IEEE International Conference on Neural \nNetworks. IEEE.1942\u20131948 (1995).  \n2. Eberhart R. and Kennedy J. A new optimizer using particle swarm theory. In: Proceedings of \nthe Sixth International Symposium on Micro Machine and Human Science, Nagoya Japan, \n39\u201343 (1995). \n3. Xie X., Zhang W. and Yang Z. A dissipative particle swarm optimization. in: Proceedings of \nthe 2002 Congress on Evolutionary Computation (CEC\u201902), Hawaii, USA,1456\u20131461(2002). \n4. Zhang W., Liu M. and Clerc Y. An adaptive pso algorithm for reactive power optimization. \nIn: Sixth international conference on advances in power system control, operation and \nmanagement (APSCOM) Hong Kong, China, 302-307 (2003).   \n5. Renders J. and Flasse S. Hybrid methods using genetic algorithms for global optimization. \nIEEE Trans Syst Man Cybern B Cybern. 26(2), 243\u2013258 (1996).  \n6. Yen R., Liao J., Lee B. and Randolph D. A hybrid approach to modeling metabolic systems \nusing a genetic algorithm and Simplex method. IEEE Transactions on Systems, Man and \nCybernetics Part-B, 28(2),173\u2013191 (1998).  \n7. Fan S., Liang Y. and Zahara E. Hybrid simplex search and particle swarm optimization for \nthe global optimization of multimodal functions. Engineering Optimization. 36, 401\u2013418 \n(2004). \n8. Zhang Y., Gallipoli D. and Augarde C.E. Simulation-based calibration of geotechnical \nparameters using parallel hybrid moving boundary particle swarm optimization. Computers \nand Geotechnics. 36 (4), 604-615 (2009). \n9. Snir M., Otto S., Huss-Lederman S., Walker D. and Dongarra J. MPI: The Complete \nReference. MIT Press (1996). \n10. Nelder J. and Mead R. A simplex method for function minimization. The Computer Journal. \n7, 308\u2013313 (1965). \n11. Alonso E.E., Gens A. and Josa A. A constitutive model for partially saturated soils. \nG\u00e9otechnique. 40(3), 405-430 (1990). \n"}