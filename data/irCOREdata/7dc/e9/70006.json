{"doi":"10.1017\/S0267190500200093","coreId":"70006","oai":"oai:eprints.lancs.ac.uk:14331","identifiers":["oai:eprints.lancs.ac.uk:14331","10.1017\/S0267190500200093"],"title":"Assessment and testing.","authors":["Clapham, Caroline"],"enrichments":{"references":[{"id":16321630,"title":"1999. Validity in language assessment.","authors":[],"date":null,"doi":"10.1017\/s0267190599190135","raw":"__________ 1999. Validity in language assessment. In W. Grabe, et al. (eds.) Annual Review of Applied Linguistics, 19. Survey of applied linguistics. New York: Cambridge University Press. 254\u2013272.","cites":null},{"id":16321722,"title":"Accountability in language Assessment.","authors":[],"date":"1997","doi":"10.1007\/978-1-4020-4489-2_28","raw":"Norton, B. 1997. Accountability in language Assessment. In C. Clapham and D. Corson (eds.) Language testing and assessment, Vol. 7. The encyclopedia of language and education. Dordrecht, Holland: Kluwer Academic. 313\u2013322.","cites":null},{"id":16321708,"title":"An introduction to structural equation modeling for language assessment research. Language Testing. 15.295\u2013332. ___________ (ed.) In press. Fairness and validation in language assessment. Cambridge:","authors":[],"date":"1998","doi":"10.1177\/026553229801500302","raw":"Kunnan, A. J. 1998. An introduction to structural equation modeling for language assessment research. Language Testing. 15.295\u2013332. ___________ (ed.) In press. Fairness and validation in language assessment. Cambridge: Cambridge University Press.","cites":null},{"id":16321619,"title":"Applied linguistics and language testing: A case study of the ELTS test. Applied Linguistics.","authors":[],"date":"1992","doi":"10.1093\/applin\/13.2.149","raw":"____________ and C. Clapham. 1992. Applied linguistics and language testing: A case study of the ELTS test. Applied Linguistics. 13.149\u2013167.","cites":null},{"id":16321683,"title":"Applying ethical standards to portfolio assessment of writing in English as a second language.","authors":[],"date":"1996","doi":null,"raw":"Hamp-Lyons, L. 1996. Applying ethical standards to portfolio assessment of writing in English as a second language. In M. Milanovic and N. Saville (eds.) Performance testing, cognition and assessment. Cambridge: Cambridge University Press. 151\u2013162.","cites":null},{"id":16321680,"title":"Approaches to alternative assessment.","authors":[],"date":"1995","doi":"10.1017\/s0267190500002695","raw":"Hamayan, E. 1995. Approaches to alternative assessment. In W. Grabe, et al. (eds.) Annual Review of Applied Linguistics, 15. Survey of applied linguistics. New York: Cambridge University Press. 212\u2013226.","cites":null},{"id":16321623,"title":"Automated essay scoring for non-native English speakers. Paper presented at the Association of Computational Linguistics and the International Association of Language Learning Technologies. [Available from http:\/\/www.ets.org\/research\/erater.html] _________","authors":[],"date":"1999","doi":null,"raw":"Burstein, J. and M. Chodorow. 1999. Automated essay scoring for non-native English speakers. Paper presented at the Association of Computational Linguistics and the International Association of Language Learning Technologies. [Available from http:\/\/www.ets.org\/research\/erater.html] _________ T. Frase, A. Ginther and L. Grant. 1996 Technologies for language assessment. In W. Grabe, et al. (eds.) Annual Review of Applied Linguistics, 16. Language and technology. New York: Cambridge University Press. 240\u2013260.","cites":null},{"id":16321716,"title":"Can there be validity without reliability? Educational Researcher.","authors":[],"date":"1994","doi":"10.3102\/0013189x023002005","raw":"Moss, P. 1994. Can there be validity without reliability? Educational Researcher. 23.8.5\u201312.","cites":null},{"id":16321677,"title":"Classroom-based evaluation in second language education. Cambridge:","authors":[],"date":"1996","doi":"10.1017\/s0272263198214057","raw":"Genesee, F. and J. Upshur. 1996. Classroom-based evaluation in second language education. Cambridge: Cambridge University Press.","cites":null},{"id":16321626,"title":"Computer adaptive testing in second language contexts.","authors":[],"date":"1999","doi":"10.1017\/s0267190599190147","raw":"__________________ and C. Deville. 1999. Computer adaptive testing in second language contexts. In W. Grabe, et al. (eds.) Annual Review of Applied Linguistics, 19. Survey of applied linguistics. New York: Cambridge University Press. 273\u2013299.","cites":null},{"id":16321628,"title":"Construct definition and validity enquiry in SLA research.","authors":[],"date":"1998","doi":"10.1017\/cbo9781139524711.004","raw":"Chapelle, C. 1998. Construct definition and validity enquiry in SLA research. In L. F. Bachman and A. Cohen (eds.) Interfaces between second language acquisition and language testing research. Cambridge: Cambridge University Press. 32\u201370.","cites":null},{"id":16321753,"title":"Developments in foreign language testing and instruction: A national perspective.","authors":[],"date":"1994","doi":null,"raw":"Stansfield, C. 1994. Developments in foreign language testing and instruction: A national perspective. In C. Hancock (ed.) Teaching, testing and assessment: Making the connection. Lincolnwood, IL: National Textbook Company. 43\u201367.","cites":null},{"id":16321698,"title":"E-mail message to the Linguistics Association of Great Britain (LAGB) listserve. [See http:\/\/www.phon.ucl.ac.uk\/home\/dick\/AL.html] Huerta-Macias.","authors":[],"date":"1999","doi":null,"raw":"Hudson, R. 1999. E-mail message to the Linguistics Association of Great Britain (LAGB) listserve. [See http:\/\/www.phon.ucl.ac.uk\/home\/dick\/AL.html] Huerta-Macias. 1995. Alternative assessment \u2013 Responses to commonly asked questions. TESOL Journal. 5.8\u201311.","cites":null},{"id":16321701,"title":"Encyclopedic dictionary of applied linguistics.","authors":[],"date":"1998","doi":"10.1111\/b.9780631214823.1999.x","raw":"Johnson, K. and H. Johnson (eds.) 1998. Encyclopedic dictionary of applied linguistics. Malden, MA: Blackwell. Johnstone, R. In Press. Context-sensitive assessment of modern languages in primary and early secondary education: Scotland and the European experience. Language Testing. 17.","cites":null},{"id":16321686,"title":"Ethics in language testing.","authors":[],"date":"1997","doi":"10.1177\/026553229701400301","raw":"_____________ 1997. Ethics in language testing. In C. Clapham and D. Corson (eds.) Language testing and assessment, Vol. 7. The encyclopedia of language and education. Dordrecht, Holland: Kluwer Academic. 323\u2013333.","cites":null},{"id":16321640,"title":"Ethics in language testing. [Special issue of Language Testing. 14.3.] DIALANG.","authors":[],"date":"1997","doi":"10.1177\/026553229701400301","raw":"Davies, A. (ed.) 1997. Ethics in language testing. [Special issue of Language Testing. 14.3.] DIALANG. 1997. \u2018DIALANG: A new European system for diagnostic language assessment.\u2019 Language Testing Update. 21.38\u201339. [http:\/\/www.jyu.fi\/DIALANG] Drasgow, F. and J. Olson-Buchanan (eds.) 1998. Innovations in computerized assessment. Mahwah, NJ: L. Erlbaum.","cites":null},{"id":16321695,"title":"From testing to assessment.","authors":[],"date":"1994","doi":"10.1017\/s0142716400008237","raw":"Hill, C. and K. Parry. 1994. From testing to assessment. London: Longman.","cites":null},{"id":16321622,"title":"How applied linguistics is the same as any other science. Applied Linguistics.","authors":[],"date":"1997","doi":"10.1111\/j.1473-4192.1997.tb00107.x","raw":"Brumfit, C. 1997. How applied linguistics is the same as any other science. Applied Linguistics. 7.86\u201394.","cites":null},{"id":16321743,"title":"How can language testing and SLA benefit from each other? The case of discourse.","authors":[],"date":"1998","doi":"10.1017\/cbo9781139524711.009","raw":"__________ 1998. How can language testing and SLA benefit from each other? The case of discourse. In L. F. Bachman and A. Cohen. (eds.) Interfaces between second language acquisition and language testing research. Cambridge: Cambridge University Press. 156\u2013176.","cites":null},{"id":16321633,"title":"How does washback influence teaching? Implications for Hong Kong. Language Education.","authors":[],"date":"1997","doi":"10.1080\/09500789708666717","raw":"Cheng, L. 1997. How does washback influence teaching? Implications for Hong Kong. Language Education. 11.38\u201354.","cites":null},{"id":16321760,"title":"Impact and washback in language. In","authors":[],"date":"1997","doi":"10.1007\/978-1-4020-4489-2_26","raw":"Wall, D. 1997. Impact and washback in language. In C. Clapham and D. Corson (eds.) Language testing and assessment, Vol. 7. The encyclopedia of language and education. Dordrecht, Holland: Kluwer Academic. 291\u2013302.","cites":null},{"id":16321735,"title":"Individual differences and fundamental similarities of implicit and explicit adult second language learning.","authors":[],"date":"1997","doi":"10.1111\/0023-8333.21997002","raw":"Robinson, P. 1997. Individual differences and fundamental similarities of implicit and explicit adult second language learning. Language Learning. 47.45\u201399.","cites":null},{"id":16321638,"title":"Language testing standards.","authors":[],"date":"1997","doi":"10.1007\/978-1-4020-4489-2_27","raw":"Davidson, F., C. E. Turner and A. Huhta. 1997. Language testing standards. In C. Clapham and D. Corson (eds.) Language testing and assessment, Vol. 7.ASSESSMENT AND TESTING 159 The encyclopedia of language and education. Dordrecht, Holland: Kluwer Academic. 303\u2013311.","cites":null},{"id":16321751,"title":"Measured words.","authors":[],"date":"1995","doi":"10.1177\/026553229601300108","raw":"Spolsky, B. 1995. Measured words. Oxford: Oxford University Press.","cites":null},{"id":16321763,"title":"Models and fictions. Applied Linguistics.","authors":[],"date":"1980","doi":"10.1093\/applin\/1.2.165","raw":"Widdowson, H. G. 1980. Models and fictions. Applied Linguistics. 1.165\u2013170.","cites":null},{"id":16321738,"title":"Performance assessment in language testing.","authors":[],"date":"1995","doi":"10.1017\/s0267190500002683","raw":"Shohamy, E. 1995. Performance assessment in language testing. In W. Grabe, et al. (eds.) Annual Review of Applied Linguistics, 15. Survey of applied linguistics. New York: Cambridge University Press. 188\u2013211.","cites":null},{"id":16321724,"title":"PhonePass test validation report. Menlo Park,","authors":[],"date":"1998","doi":null,"raw":"Ordinate Corporation. 1998. PhonePass test validation report. Menlo Park, CA: Ordinate.","cites":null},{"id":16321714,"title":"Policy and social considerations in language assessment.","authors":[],"date":"1998","doi":"10.1017\/s0267190500003603","raw":"____________ 1998. Policy and social considerations in language assessment. In W. Grabe, et al. (eds.) Annual Review of Applied Linguistics, 18. Foundations of second language teaching. 18.304\u2013319.","cites":null},{"id":16321690,"title":"Positivistic versus alternative perspectives on validity within the LTRC.","authors":[],"date":"1998","doi":null,"raw":"_____________ and B. Lynch. 1998. Positivistic versus alternative perspectives on validity within the LTRC. Unpublished manuscript.","cites":null},{"id":16321730,"title":"Retuning in applied linguistics. Applied Linguistics. 7.3\u201325. Rea-Dickens, P. In press. Snares and silver bullets: Disentangling the construct of formative assessment.","authors":[],"date":"1997","doi":null,"raw":"Rampton, B. 1997. Retuning in applied linguistics. Applied Linguistics. 7.3\u201325. Rea-Dickens, P. In press. Snares and silver bullets: Disentangling the construct of formative assessment. Language Testing. 17.","cites":null},{"id":16321635,"title":"Review of Hill,","authors":[],"date":"1997","doi":null,"raw":"Clapham, C. 1997. Review of Hill, C. and Parry, K. (eds.) From testing to assessment. 1994. Language and Education. 11.222\u2013223. __________ To appear. Assessment. In M. Byram (ed.) Encyclopedia of Language Teaching and Learning. London: Routledge.","cites":null},{"id":16321719,"title":"Scaling descriptors for language proficiency scales. Language Testing.","authors":[],"date":"1998","doi":"10.1177\/026553229801500204","raw":"North, B. and G. Schneider. 1998. Scaling descriptors for language proficiency scales. Language Testing. 15.217\u2013262.","cites":null},{"id":16321705,"title":"Simulating conversations in oral proficiency assessment: A conversation analysis of role play and non-scripted interviews in language exams. Language Testing.","authors":[],"date":"1999","doi":"10.1177\/026553229901600203","raw":"Kormos, J. 1999. Simulating conversations in oral proficiency assessment: A conversation analysis of role play and non-scripted interviews in language exams. Language Testing. 16.163\u2013188.","cites":null},{"id":16321692,"title":"Small words and valid testing.","authors":[],"date":"1998","doi":null,"raw":"Hasselgren, A. 1998. Small words and valid testing. Bergen, Norway: University of Bergen. Unpublished Ph. D. diss.","cites":null},{"id":16321756,"title":"Systematic effects in the rating of secondlanguage speaking ability: Test method and learner discourse. Language Testing.","authors":[],"date":"1998","doi":"10.1177\/026553229901600105","raw":"Upshur, J. and C. Turner. 1998. Systematic effects in the rating of secondlanguage speaking ability: Test method and learner discourse. Language Testing. 16.82\u2013111.ASSESSMENT AND TESTING 161 Valette, R. 1994. Teaching, testing and assessment: Conceptualizing the relationship. In C. Hancock (ed.) Teaching, testing and assessment: Making the connection. Lincolnwood, IL: National Textbook Company. 1\u201342.","cites":null},{"id":16321745,"title":"Task-based instruction. In","authors":[],"date":"1998","doi":"10.1017\/s0267190500003585","raw":"Skehan, P. 1998. Task-based instruction. In W. Grabe, et al. (eds.) Annual Review of Applied Linguistics, 18. Foundations of second language teaching. New York: Cambridge University Press. 304\u2013319.","cites":null},{"id":16321727,"title":"The effect of topic variation in performance testing: The case of the chemistry TEACH test for international teaching assistants. Language Testing.","authors":[],"date":"1999","doi":"10.1177\/026553229901600104","raw":"Papajohn, D. 1999. The effect of topic variation in performance testing: The case of the chemistry TEACH test for international teaching assistants. Language Testing. 16.52\u201381.","cites":null},{"id":16321748,"title":"The influence of task structure and processing conditions on narrative retellings. Language Learning.","authors":[],"date":"1999","doi":"10.1111\/1467-9922.00071","raw":"_________ and P. Foster. 1999. The influence of task structure and processing conditions on narrative retellings. Language Learning. 49.93\u2013120.","cites":null},{"id":16321624,"title":"Theoretical bases of communicative approaches to second language teaching and testing. Applied Linguistics.","authors":[],"date":"1980","doi":"10.1093\/applin\/i.1.1","raw":"Canale, M. and M. Swain. 1980. Theoretical bases of communicative approaches to second language teaching and testing. Applied Linguistics. 1.1\u201347. Chalhoub-Deville, M. (ed.) In press. Issues in the computer adaptive testing of reading proficiency. Cambridge: Cambridge University Press.","cites":null},{"id":16321711,"title":"Using G-theory and many-facet Rasch measurement in the development of performance assessments of the ESL speaking skills of immigrants. Language Testing.","authors":[],"date":"1998","doi":"10.1177\/026553229801500202","raw":"Lynch, B. and T. McNamara. 1998. Using G-theory and many-facet Rasch measurement in the development of performance assessments of the ESL speaking skills of immigrants. Language Testing. 15.158\u2013180.160 CAROLINE CLAPHAM McNamara, T. 1997. Performance testing. In C. Clapham and D. Corson (eds.) Language testing and assessment, Vol. 7. The encyclopedia of language and education. Dordrecht, Holland: Kluwer Academic. 131\u2013139.","cites":null},{"id":16321621,"title":"Washback in testing. [Special issue of Language Testing.","authors":[],"date":"1996","doi":"10.1177\/026553229601300303","raw":"____________ and D. Wall (eds.) 1996. Washback in testing. [Special issue of Language Testing. 13.1.] Bachman, L. F. 1990. Fundamental considerations in language testing. Oxford: Oxford University Press. Brindley, G. In press. Assessment. In R. Carter and D. Nunan (eds.) The Cambridge ELT companion. Cambridge: Cambridge University Press.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2000-01","abstract":"In this brief article, I discuss the relationship between language testing and the other sub-disciplines of applied linguistics and also the relationship, as I see it, between testing and assessment. The article starts with a brief exploration of the term \u2018applied linguistics\u2019 and then goes on to discuss the role of language testing within this discipline, the relationship between testing and teaching, and the relationship between testing and assessment. The second part of the article mentions some areas of current concern to testers and discusses in more detail recent advances in the areas of performance testing, alternative assessment, and computer assessment. One of my aims in this article is to argue that the skills involved in language testing are necessary not only for those constructing all kinds of language proficiency assessments, but also for those other applied linguists who use tests or other elicitation techniques to help them gather language data for research","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/70006.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/14331\/1\/download.pdf","pdfHashValue":"91fb766f94875b20bb98268a2db37e4042cbdec8","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:14331<\/identifier><datestamp>\n      2018-01-24T00:04:57Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D50:5031<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Assessment and testing.<\/dc:title><dc:creator>\n        Clapham, Caroline<\/dc:creator><dc:subject>\n        P Philology. Linguistics<\/dc:subject><dc:description>\n        In this brief article, I discuss the relationship between language testing and the other sub-disciplines of applied linguistics and also the relationship, as I see it, between testing and assessment. The article starts with a brief exploration of the term \u2018applied linguistics\u2019 and then goes on to discuss the role of language testing within this discipline, the relationship between testing and teaching, and the relationship between testing and assessment. The second part of the article mentions some areas of current concern to testers and discusses in more detail recent advances in the areas of performance testing, alternative assessment, and computer assessment. One of my aims in this article is to argue that the skills involved in language testing are necessary not only for those constructing all kinds of language proficiency assessments, but also for those other applied linguists who use tests or other elicitation techniques to help them gather language data for research.<\/dc:description><dc:date>\n        2000-01<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1017\/S0267190500200093<\/dc:relation><dc:identifier>\n        Clapham, Caroline (2000) Assessment and testing. Annual Review of Applied Linguistics, 20. pp. 147-161. ISSN 1471-6356<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/14331\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1017\/S0267190500200093","http:\/\/eprints.lancs.ac.uk\/14331\/"],"year":2000,"topics":["P Philology. Linguistics"],"subject":["Journal Article","PeerReviewed"],"fullText":"147\nAnnual Review of Applied Linguistics (2000) 20, 147\u2013161. Printed in the USA.\nCopyright \u00a9 2000 Cambridge University Press 0267-1905\/00 $9.50\nASSESSMENT AND TESTING\nCaroline Clapham\nINTRODUCTION\nIn this brief article, I discuss the relationship between language testing and\nthe other sub-disciplines of applied linguistics and also the relationship, as I see it,\nbetween testing and assessment.  The article starts with a brief exploration of the\nterm \u2018applied linguistics\u2019 and then goes on to discuss the role of language testing\nwithin this discipline, the relationship between testing and teaching, and the\nrelationship between testing and assessment.  The second part of the article \nmentions some areas of current concern to testers and discusses in more detail\nrecent advances in the areas of performance testing, alternative assessment, and\ncomputer assessment.  One of my aims in this article is to argue that the skills\ninvolved in language testing are necessary not only for those constructing all kinds\nof language proficiency assessments, but also for those other applied linguists who\nuse tests or other elicitation techniques to help them gather language data for\nresearch.\nAPPLIED LINGUISTICS AND THE ROLE OF ASSESSMENT\nIt is usually the case with new disciplines that they go through periods of\nadjustment as the limits of the discipline are realigned.  Applied linguistics is going\nthrough such a stage at present as its scope widens and its subfields start to impinge\non those of other disciplines.\nThe term \u2018applied linguistics\u2019 appears first to have been used in the late\n1940s when the discipline embraced the teaching and learning of second and\nforeign languages (Johnson and Johnson 1998), but since then the discipline has\nexpanded to cover a wider range of sub-disciplines or \u2018subfields\u2019 as they are called\nby Bachman and Cohen (1998).  In 1980, Henry Widdowson said, \u201c\u2026applied\nlinguistics yields descriptions which are projections of actual language which\nexplore linguistic theory as illumination\u2026\u201d (p. 169), and in 1997, Chris Brumfit\n148 CAROLINE CLAPHAM\ndefined applied linguistics as \u201c\u2026the theoretical and empirical investigation of real-\nworld problems in which language is the central issue\u201d (p. 93).  (See also Ben\nRampton\u2019s [1997] introduction to the special issue of Applied Linguistics in which\nBrumfit\u2019s article appeared; this issue of Applied Linguistics focused on the concept\nof \u2018applied linguistics.\u2019)\nIn 1999, Richard Hudson, in an e-mail to the LAGB (Linguistics\nAssociation of Great Britain) listserve, said:\nThe main distinguishing characteristic of AL (Applied Linguistics) is its\nconcern with professional activities whose aim is to solve \u2018real-world\u2019\nlanguage-based problems, which means that research touches on a\nparticularly wide range of issues\u2014psychological, pedagogical, social,\npolitical and economic as well as linguistic.  As a consequence, AL\nresearch tends to be interdisciplinary (Hudson 1999).\nLanguage assessment plays a pivotal role in applied linguistics,\noperationalizing its theories and supplying its researchers with data for their\nanalysis of language knowledge or use.  It has itself become a sub-discipline of\napplied linguistics, which is in some ways unfortunate, since it is has tended to\nbecome compartmentalized (Bachman and Cohen 1998) and does not interact as\nmuch as it should with the other sub-disciplines.  Bachman and Cohen (1998) decry\nthe compartmentalization of second language acquisition (SLA) and language\ntesting, saying that most mainstream researchers in the two sub-disciplines are\nunaware of research taking place in the other.  However, they hope that this lack of\nawareness may now be changing.  (For other discussions of the relationship\nbetween testing and SLA, see Shohamy 1998, Upshur and Turner 1998.)\nA key point, which perhaps not all applied linguists appreciate, is that\nlanguage testing is by no means limited to assessing the linguistic proficiency of L2\nstudents.  Many areas of linguistic research use elicitation instruments to gather\ndata, and these instruments often take the form of tests or tasks (see, for example,\nRobinson 1997, Skehan and Foster 1999).  If the results of such data elicitation\ntechniques are to be credible, they need to be prepared with as much rigor as\nproficiency tests, and they therefore have to be valid and reliable (Alderson and\nBanerjee in press).  Crudely, a valid elicitation technique is one that accurately\nelicits what it is intended to elicit, and a reliable technique is one that produces\nconsistent results.  (For discussions of validity, see Chapelle 1999, Messick 1989;\n1996, Shepard 1993; and for comments on the relationship between validity and\nreliability, see Moss 1994.)  Although Messick (1989) subsumes reliability under\nvalidity, since any valid measure must, by definition, be reliable, it is useful here to\ndistinguish between validity and reliability since the assessment of an instru-ment\u2019s\nreliability is often neglected in elicitation procedures.  If research is to have\ncredibility, data gathering instruments must not only be carefully designed to ensure\nthat they will elicit the type of language required, they must also be pre-tested to\ncheck that the measures do indeed elicit such language practice and that any rating\nASSESSMENT AND TESTING 149\nor coding system is workable and capable of producing consistent results (see North\nand Schneider 1998).  Since it is generally expected that subjects in an investigation\nproduce similar kinds of language regardless of when the task is done, and that this\nlanguage can be analyzed in a similar manner regardless of when and by whom it is\nassessed or coded, the reliability of the elicitation techniques must be given careful\nconsideration.\nASSESSMENT AND LANGUAGE TEACHING\nThere has been much discussion about how language testing fits into\napplied linguistics and how it relates to language teaching (see, for example,\nBachman and Palmer 1996).  In general, it seems clear that \u201c\u2026language testing\nbenefits from insights from applied linguistics as a discipline\u2026\u201d (Alderson and\nClapham 1992:164) but that it is sometimes necessary for testing to lead the way:\nWe believe that language testers can serve linguistic theory by examining\nthe way in which their tests work, how their different components\ninterrelate, and what they reveal about candidates\u2019 language proficiency. \nInsights from such an analysis of test results should contribute to the\ndevelopment of a better understanding of what is involved in knowing and\nusing language (1992:164).\nIt seems, indeed, that each affects the other: Methods of assessment may affect\nteaching in the classroom (Cheng 1997, Wall 1996; 1997), while new theories of\nlanguage learning and teaching lead to changes in testing practices (Spolsky 1995). \nTHEORIES OF LANGUAGE TESTING\nWith the advent of communicative teaching in the late 1970s, there was a\nneed for testers to devise new theories of language testing.  Canale and Swain\n(1980), whose model applied to both teaching and testing second and foreign\nlanguages, included grammatical, sociolinguistic, and strategic competence in their\ndescription of the domains of language use.  In 1990, Bachman added psychophy-\nsiological mechanisms and proposed four components in his model: grammatical,\ntextual, illocutionary, and sociolinguistic competence.  Bachman and Palmer (1996)\nelaborated on this model further to include both affective and metacognitive factors. \nBachman and Palmer\u2019s model of communicative language ability is used as the\ntheoretical basis for tests such as the International English Language Testing\nSystem (IELTS) test, and it provides the basis for many current research projects\n(e.g., Hasselgren 1998).  (See McNamara 1996 for a discussion of language testing\nmodels.)\n150 CAROLINE CLAPHAM\nASSESSMENT AND TESTING\nThe term \u2018assessment\u2019 is used both as a general umbrella term to cover all\nmethods of testing and assessment, and as a term to distinguish \u2018alternative\nassessment\u2019 from \u2018testing.\u2019  Some applied linguists use the term \u2018testing\u2019 to apply to\nthe construction and administration of formal or standardized tests such as the Test\nof English as a Foreign Language (TOEFL) and \u2018assessment\u2019 to refer to more\ninformal methods such as those listed below under the heading \u2018alternative\nassessment.\u2019  For example, Valette (1994) says that \u2018tests\u2019 are large-scale\nproficiency tests and that \u2018assessments\u2019 are school-based tests.  Intriguingly, some\ntesters are now using the term \u2018assessment\u2019 where they might in the past have used\nthe term \u2018test\u2019 (see, for example, Kunnan 1998).  There seems, indeed, to have\nbeen a shift in many language testers\u2019 perceptions so that they, perhaps subcon-\nsciously, may be starting to think of testing solely in relation to standardized, large-\nscale tests.  They therefore use the term \u2018assessment\u2019 as the wider, more acceptable\nterm.\nSince, for the remainder of this article, I wish to comment on differing\nattitudes between \u2018testers\u2019 and \u2018assessors,\u2019 I shall use the term \u2018testers\u2019 for those\nwho concern themselves with requirements of validity and reliability, and\n\u2018assessors\u2019 for those who are not consciously guided by such constraints.  I must\nemphasize, though, that while I am giving the two terms distinct meanings, I do not\nthink that there is a fundamental difference between them, and in other publications\n(e.g., Clapham 1997; to appear), I use the two terms interchangeably. \nUnfortunately, although \u2018assessors\u2019 and \u2018testers\u2019 have the same aims, there\nis less dialogue than there should be between them, possibly because many of them\ntend to think of \u2018testing\u2019 and \u2018assessment\u2019 as being categorically different (see Hill\nand Parry 1994) instead of being on a continuum with at one end those \u2018testers\u2019\nwho deliver carefully validated multiple choice tests, and at the other end,\n\u2018assessors\u2019 who prepare real-life tasks for their students and candidates but who do\nnot concern themselves with how well these tasks actually work.  \u2018Assessors\u2019\nappear to distrust extreme \u2018testers\u2019 because they feel that these \u2018testers\u2019 are so\nwedded to the numerical analysis of data that they are not sufficiently concerned\nwith the content and administration of their tests.  Such \u2018assessors\u2019 tend to be\nconcerned that these \u2018tests\u2019 are not \u2018communicative,\u2019 and that they may lead to\nnegative washback (Brown and Hudson 1998).  In contrast, many \u2018testers\u2019 are\nconcerned with the fact that, although the \u2018assessors\u2019\u2019 methods of assessment may\nbe novel and interesting, the tasks are not pre-tested to see whether they work as\nintended or whether the assessments can be delivered and marked in a consistent\nmanner.  In short, they distrust \u2018assessors\u2019 because \u2018assessors\u2019 do not appreciate the\nimportance of investigating the validity and reliability of their instruments.  Brown\nand Hudson (1998) quote Huerta-Macias (1995) who says that it is unnecessary to\nevaluate the validity and reliability of methods of alternative assessment because\nthey are already built into the assessment process.  Brown and Hudson make the\npoint that it is not enough to build validity and reliability into the measures; the\nASSESSMENT AND TESTING 151\nmeasures must also be trialed to see whether or not they are valid and reliable in\npractice.  (See also Johnstone [in press] and Rea-Dickens [in press].)\nAnother source of distinction between \u2018tests\u2019 and \u2018assessments\u2019 is that some\neducators and applied linguists feel that \u2018high stakes\u2019 tests, which have a direct\nbearing on students\u2019 immediate futures, need to have validity and reliability built\ninto them, but that \u2018low stakes\u2019 tests such as classroom tests, which do not have\nsuch an obvious impact on students\u2019 futures, do not (Davidson, et al. 1997).  This\ndivision of tests into high- and low-stakes types seems to me to be misguided: Tests\ndo not fall neatly into one or the other category.  Whether a test is \u2018high-\u2019 or \u2018low-\nstakes\u2019 is surely a question of degree; a test may be deemed to be more \u2018high-\nstakes\u2019 than another if students\u2019 futures are more clearly at stake, but all tests\nwhich assess students\u2019 proficiency levels, whether in the examination hall or the\nclassroom, are in reality high-stakes.  Even if the results of a classroom test do not\naffect a student\u2019s immediate future, the results may become self fulfilling; for\nexample, a student with a low score may be considered by both teacher and student\nto be a poor language learner, and this may have a damaging effect on the student\u2019s\nfuture performance.  If students are to have an accurate idea of their proficiency,\nthey should, where possible, be given tests which are valid and reliable.  Even\nclassroom tests, therefore, should, at least from time to time, be checked to see\nwhether the skills being assessed are those intended, whether the marking scheme is\nappropriate and can be used consistently, and whether the results tally with other\nviews of the student\u2019s proficiency.  \nThis apparent dichotomy between \u2018testers\u2019 and \u2018assessors\u2019 has, however,\nbecome less marked as the move towards \u2018authenticity\u2019 (see Bachman 2000) in text\nand task has led \u2018testers\u2019 towards the increased use of performance tests.  In\naddition, there is a trend at present, perhaps partly due to the influence of Messick\n(1989), for \u2018testers\u2019 to be more concerned with the construct validity than simply\nwith the reliability of their measures.  It is also the case, possibly due to the\ninfluence of post modernism, that many \u2018testers\u2019 are rejecting the positivist\nprinciple that there is an \u201c\u2026independently existing reality that can be discovered\n(or measured) using objective, scientific methods\u2026\u201d (Hamp-Lyons and Lynch\n1998).  The desire to question former \u2018truths\u2019 has led many \u2018testers\u2019 to trust\nstandard statistical procedures less than they used to.  Some \u2018testers\u2019 are also now\nexpressing concerns about the ethicality of testing (see Kunnan in press).\nCURRENT AREAS OF CONCERN IN TESTING\nAreas that are attracting attention in the testing literature at present have, in\na number of cases, been the subject of recent ARAL reviews.  Performance testing\nwas covered by Shohamy (1995), alternative assessment by Hamayan (1995),\nadvances in the use of the computer for testing by Chalhoub-Deville and Deville\n(1999) and the interface between tasks and assessment by Skehan (1998) and\nMcNamara (1998).  Other areas of current interest include test washback (Alderson\nand Wall 1993; 1996, Wall 1997) and the ethics of language testing (Davies 1997,\n152 CAROLINE CLAPHAM\nHamp-Lyons 1997, Kunnan in press, Norton 1997).  (For more about these and\nother areas of current concern, see Bachman 2000, Brindley in press; see also\nClapham and Corson 1997.)  In the remainder of this article, I will update the\nARAL articles on performance testing, alternative assessment, and the use of\ncomputers for testing. I will, at the same time, relate these three areas to real or\nimaginary differences between \u2018testers\u2019 and \u2018assessors.\u2019\n1.  Performance testing\nAs Shohamy (1995) points out, alternative assessment and performance\ntesting have much in common.  Indeed, the major difference between the two\nseems to be that performance testers agonize about the validity and reliability of\ntheir instruments while alternative assessors do not (Hamayan 1995).  Both,\nhowever, are concerned with asking students to create or produce something, and\nboth focus on eliciting samples of language which are as close to real life as\npossible (see Kormos 1999, Lynch and McNamara 1998, Papajohn 1999, Upshur\nand Turner 1998).  McNamara (1996) states that a defining characteristic of\nperformance testing is that \u201cthe assessment of the actual performances of relevant\ntasks are required of candidates, rather than the more abstract demonstration of\nknowledge, often by means of paper-and-pencil tests\u201d (McNamara 1996:6; see also\nMcNamara 1997).  The same could well be said of methods of alternative\nassessment.\n2.  Alternative assessment\n\u2018Alternative assessment\u2019 is one of the terms used to refer to informal\nassessment procedures such as those often used in the classroom.  Typical examples\nof such methods involve portfolios (Hamp-Lyons 1996), learner diaries or journals\n(Genesee and Upshur 1996), and interviews with teachers (Genesee and Upshur\n1996).  Such procedures may be more time-consuming and difficult for the teacher\nto administer than \u2018paper-and-pencil\u2019 tests, but they have many advantages.  They\nproduce information that is easy for administrators, teachers, and students to\nunderstand; they tend to be integrated; and they can reflect the more holistic\nteaching methods being used in the classroom (Hamayan 1995).  A problem with\nmethods of alternative assessment, however, lies with their validity and reliability:\nTasks are often not tried out to see whether they produce the desired linguistic\ninformation; marking criteria are not investigated to see whether they \u2018work\u2019; and\nraters are often not trained to give consistent marks (see Brown and Hudson 1998). \nAs Hamayan (1995) says, such alternative methods of assessment will not be\nconsidered to be part of the mainstream of language assessment until they can be\nshown to be both valid and reliable. \nMethods of alternative assessment are not, of course, solely used for\nclassroom purposes.  Many alternative assessment schemes are initiated by local or\nnational government agencies (see Brindley 1998, Stansfield 1994) with aims such\nas comparing students\u2019 levels of linguistic ability or comparing levels of instruction\nASSESSMENT AND TESTING 153\nacross institutions.  Governments (sometimes in a hurry) wish to use the results for\ntheir own aims (Brindley 1998).  They do not appreciate the need for careful\ntrialing, and therefore they do not always allow researchers the time to try out the\nassessment instruments.  Some types of assessment used in these circumstances\nmay have high face validity\u2014they may look excellent to the uninformed\u2014but they\nmay be marred by inappropriate marking schemes and rating inconsistencies. \nTrialing such measures is essential if the final tools are to be valid and reliable, but\nunfortunately such pre-testing and subsequent editing of materials takes time, and\ntime is often in short supply.  Inevitably, the ensuing tasks and marking schemes\nare not valid and reliable (Brindley 1998), and such schemes, therefore, launched\nwith much fanfare, may produce invalid results which are unfair to students and\nteachers alike.  If all assessors appreciated the importance of trialing in the\nconstruction of all assessment procedures, they might be able to work together to\ntell governments and other funding bodies that test development requires more time\nif assessors are to devise satisfactory assessment instruments.\nThe present move towards the inclusion of more performance testing in\nexaminations (see for example the University of Cambridge Local Examinations\nSyndicate examinations) is also likely to bring \u2018testing\u2019 and \u2018assessment\u2019 closer\ntogether, but, unfortunately, because of financial and practical constraints, some\nlarge-scale tests are likely to remain much as they are, with uncontextualized,\nmultiple choice items.  There is a danger that the authors of such large-scale tests\nwill be marginalized, and that their thorough work on pre-testing their items will\nnot be appreciated because of perceived weaknesses in test content.  Indeed, it is\npossible to envisage a day when performance testers and alternative assessors align\nthemselves against the producers of large-scale tests.  Such a division must not be\nallowed to occur.\n3.  Advances in computer assessment\nIt is probably too early to say whether advances in the use of computer\ntechnology for testing will have a positive or a negative effect on testing and\nwhether computer administered tests will be distrusted by \u2018assessors.\u2019  Until\nrecently, computer testing tended to fossilize existing objective testing methods\nbecause objectively marked items such as multiple-choice questions and gap-filling\ntasks were straightforward to answer on the computer and were easy to mark\nmechanically.  Any attempts to introduce interesting new methods of assessment\nand testing were foiled by limitations in the memory size and processing speed of\nthe computers.  The move of the TOEFL towards computer based testing too has,\nat least in the short run, extended the use of multiple choice and other easy-to-mark\nobjective items in computer tests.  However, it seems that the promised testing\nrevolution may at last be on its way.  The expanding use of video conferencing and\nvideo interviews, the comparative ease with which videos and listening extracts can\nnow be downloaded from the Web, the improvement in the computer\u2019s ability to\nrecognize sounds and letters, the expanding uses of language corpora for teaching\nand testing, and the increasing and more rapid storage capacity of modern\n154 CAROLINE CLAPHAM\ncomputers are all widening the scope of computer administered tests (see Burstein,\net al. 1996, Drasgow and Olson-Buchanan 1998, Ordinate Corporation 1998). \nThese advances will not only increase the efficiency of standardized tests, but will\nincrease the scope of other elicitation techniques.  \nOne project that has the potential to produce interesting and yet easy-to-\ndeliver-and-mark tests is DIALANG, which aims to produce diagnostic tests in 14\ndifferent European foreign languages (DIALANG 1997).  The tests will be\ndelivered on the world wide web; they will be computer adaptive (see Chalhoub-\nDeville and Deville 1999); and students, after taking their chosen test, will receive\ninstant diagnostic information about the strengths and weaknesses of their\nperformance.  At present, compositions written for DIALANG will be marked by\nhand, but this may not always be the case as there are now research projects\nlooking into the computer marking of tests of language production (see Burstein and\nChodorow 1999).  Although it is hard to imagine computers ever replacing human\nmarkers, and we might not wish them to, it is possible that they will take some of\nthe drudgery away from subjective marking and will thus make the marking task\neasier and more interesting for the raters.\nThere will thus, in the near future, be great advances in computer testing. \nHowever, we cannot yet know whether or not there is still a danger that the use of\ncomputers will limit rather than expand the kinds of tests that will be used.\nCONCLUSION\nOne of my aims in this article has been to discuss the need to bring\n\u2018testers\u2019 and \u2018assessors\u2019 closer together, and I expect that over the next decade any\ndifferences between them will become less and less marked.  \u2018Testers,\u2019 I hope, will\nbecome more open to ideas of different kinds of assessment, and \u2018assessors\u2019 will be\nmore willing to accept that even the most carefully designed task or set of marking\ncriteria needs to be trialed.  If \u2018testers\u2019 and \u2018assessors\u2019 can each see that they are\naiming for the same goal, they will perhaps start a dialogue which might transform\nboth tests and assessments.  Similarly, on a larger scale, the basic issues of testing\nand assessment are important in all areas of applied linguistics that call on the use\nof elicitation techniques to collect data.  All such elicitation techniques should be\nreliable and valid.  Understanding this connection, however, will only be possible if\nall putative applied linguists (and all potential language teachers) are introduced\nduring their training to the vital tenets of testing so that they can be more critical of\nthe elicitation techniques they use.\nASSESSMENT AND TESTING 155\nANNOTATED BIBLIOGRAPHY\nAlderson, J. C. and D. Wall. 1993. Does washback exist? Applied Linguistics.\n14.115\u2013129.\nThis article opened language testers\u2019 eyes to the fact that, although the\nexpression \u2018washback\u2019 was freely used in language teaching and testing\ncommunities, and although there was plenty of anecdotal evidence as to its\nexistence, there had been few serious attempts to define it or to investigate\nits existence and possible effects.  The authors reported on the few existing\nstudies of language testing washback and listed fifteen washback hypo-\ntheses which, they said, should be investigated.  This article led to a flurry\nof investigations into test washback, and the results of the first of these\nstudies are now being published. \nBachman, L. F. 2000. Modern language testing at the turn of the century: Assuring\nthat what we count counts. Language Testing. 17.1.\nThis is a state-of-the-art article on language testing in the year 2000.  After\na brief overview of testing in the 1980s, Bachman comments on some of\nthe main areas of interest in the 1990s: criterion referenced measurement,\ngeneralizability theory, item response theory, structural equation modeling,\nqualitative research approaches, testing cross-cultural pragmatics, testing\nlanguage for specific purposes, testing vocabulary, computer based\nassessment, and research into factors that affect performance.  He also\ndiscusses recent theories of language testing and the concept of\ncommunicative testing.  He concludes his overview of the 1990s with\ndiscussions of test washback and test ethics, and then turns to what lies\nahead.\nBachman, L. F. and A. Cohen. 1998. Language testing\u2014SLA interfaces: An\nupdate. In L. F. Bachman and A. Cohen (eds.) Interfaces between second\nlanguage acquisition and language testing research. Cambridge:\nCambridge University Press. 1\u201331.\nThis chapter serves as a useful introduction to the book, Interfaces between\nsecond language acquisition and language testing research.  It discusses\nthe reasons why SLA and language testing were for some time viewed as\ntotally distinct, and it gives reasons why in recent years the two fields seem\nto have moved closer together.  Bachman and Cohen describe areas of\ncommon interest between SLA and language testing and make\nrecommendations for future joint areas of research.\n156 CAROLINE CLAPHAM\nBachman, L. F. and A. Palmer. 1996. Language testing in practice. Oxford:\nOxford University Press.\nThis book, which is a key textbook in language testing at present, is\ncomposed of three parts: Part 1 discusses the conceptual basis of test\ndevelopment and includes the Communicative Language Ability model,\nwhich is used as the basis of much current research in language testing;\nPart 2 describes the stages of language test development; and Part 3\ndescribes a set of ten illustrative test development projects ranging from a\nplacement test used in a U.S. university to a syllabus based test for primary\nschool children.\nBrindley, G. 1998. Outcomes-based assessment and reporting in language learning\nprogrammes: A review of the issues. Language Testing. 15.45\u201385.\nIn this article, Brindley discusses national standards, frameworks, and\nbenchmarks of various kinds.  He shows that the introduction of such\nsystems has sometimes been problematic because of political, practical, and\ntechnical factors.  Brindley discusses what the problems are and makes\nsuggestions for how future systems might be more successfully\nimplemented.\nBrown, J. D. and T. Hudson. 1998. The alternatives in language assessment.\nTESOL Quarterly. 32.653\u2013675.\nThe main purpose of Brown and Hudson\u2019s article is to help language\nteachers decide what types of language tests they should use in the\nclassroom.  The article discusses the concepts of validity and reliability,\nand lists the different kinds of tests that teachers might use.  The authors\ngroup tests under various headings such as \u2018selected response\u2019 and\n\u2018performance assessments\u2019 and describe the advantages and disadvantages\nof each type.\nClapham, C. and D. Corson (eds.) 1997. Language testing and assessment, Vol. 7.\nThe encyclopedia of language and education. Dordrecht, Holland: Kluwer\nAcademic.\nThis volume contains 29 chapters on different aspects of first and second\nlanguage testing and assessment.  Each chapter presents a state-of-the-art\ndescription of one aspect of language assessment and provides a\nbibliography of about 30 references for future researchers in the field.  The\nbook is divided into four sections covering the testing of individual skills,\nmethods of assessment, quantitative and qualitative approaches to test\nvalidation, and the ethics and effects of testing and assessment.\nASSESSMENT AND TESTING 157\nMcNamara, T. 1996. Measuring second language performance. London:\nLongman.\nThis highly readable book starts with a discussion of communicative testing\nand defines what McNamara means by performance testing.  This\ndiscussion is followed by a valuable discussion of the influential models of\ncommunicative language testing that have been devised since Dell Hymes\nintroduced his theory of communicative competence in 1972.  In the\nsecond part of the book, McNamara starts by describing a performance\ntest, the Occupational English Test, and then devotes most of the rest of\nthe book to a discussion of the use of Rasch multi-faceted measurement for\nresearch into the assessment of second language performance.\n Messick, S. 1996. Validity and washback in language testing. Language Testing.\n13.241\u2013256.\nThis article focuses on washback and the consequential aspects of construct\nvalidity.  Messick relates washback to his overall conception of validity\nand, in doing so, explains his ideas about validity more clearly and\nconcisely than he did in his seminal 1989 article.\nShepard, L. 1993. Evaluating test validity. Review of Research in Education.\n19.405\u2013450.\nThis article discusses validity in educational measurement as a whole. \nShepard starts by describing the evolution of the concept of validity over\ntime and then devotes the second half of the article to a very clear\nexplanation of Messick\u2019s theory of validity.  She concludes the chapter\nwith comments on the implications of this reconceptualizing of the term for\nnew test standards.\nWall, D. 1996. Introducing new tests into traditional systems: Insights from general\neducation and from innovation theory. Language Testing. 13.334\u2013354.\nThis article describes several key concepts in educational innovation.  The\nauthor applies these concepts to the teaching of English as a foreign or\nsecond language and relates them to a study she carried out into the\nwashback of a new school examination in Sri Lanka.  She shows how the\nbelief that assessment and the curriculum would together affect teaching in\nthe classroom turned out to be misplaced, partly because of discrepancies\nbetween the curriculum and the examination, and partly because of a lack\nof teacher training in the new \u2018communicative\u2019 methodology.  In her\nconclusion, she makes suggestions as to how future investigations into\nwashback should be carried out and how innovations in the classroom\nmight be brought about more successfully.\n158 CAROLINE CLAPHAM\nUNANNOTATED BIBLIOGRAPHY\nAlderson, J. C. and J. Banerjee. In press. Impact and washback research in\nlanguage testing. In C. Elder, et al. (eds.) Experimenting with uncertainty:\nEssays in honor of Alan Davies. Cambridge: Cambridge University Press.\n____________ and C. Clapham. 1992. Applied linguistics and language testing: A\ncase study of the ELTS test. Applied Linguistics. 13.149\u2013167.\n____________ and D. Wall (eds.) 1996. Washback in testing. [Special issue of\nLanguage Testing. 13.1.]\nBachman, L. F. 1990. Fundamental considerations in language testing. Oxford:\nOxford University Press.\nBrindley, G. In press. Assessment. In R. Carter and D. Nunan (eds.) The\nCambridge ELT companion. Cambridge: Cambridge University Press.\nBrumfit, C. 1997. How applied linguistics is the same as any other science. Applied\nLinguistics. 7.86\u201394.\nBurstein, J. and M. Chodorow. 1999. Automated essay scoring for non-native\nEnglish speakers. Paper presented at the Association of Computational\nLinguistics and the International Association of Language Learning\nTechnologies. [Available from http:\/\/www.ets.org\/research\/erater.html]\n_________ T. Frase, A. Ginther and L. Grant. 1996 Technologies for language\nassessment. In W. Grabe, et al. (eds.) Annual Review of Applied\nLinguistics, 16. Language and technology. New York: Cambridge\nUniversity Press. 240\u2013260.\nCanale, M. and M. Swain. 1980. Theoretical bases of communicative approaches\nto second language teaching and testing. Applied Linguistics. 1.1\u201347.\nChalhoub-Deville, M. (ed.) In press. Issues in the computer adaptive testing of\nreading proficiency. Cambridge: Cambridge University Press.\n__________________ and C. Deville. 1999. Computer adaptive testing in second\nlanguage contexts. In W. Grabe, et al. (eds.) Annual Review of Applied\nLinguistics, 19. Survey of applied linguistics. New York: Cambridge\nUniversity Press. 273\u2013299.\nChapelle, C. 1998. Construct definition and validity enquiry in SLA research. In L.\nF. Bachman and A. Cohen (eds.) Interfaces between second language\nacquisition and language testing research. Cambridge: Cambridge\nUniversity Press. 32\u201370.\n__________ 1999. Validity in language assessment. In W. Grabe, et al. (eds.)\nAnnual Review of Applied Linguistics, 19. Survey of applied linguistics.\nNew York: Cambridge University Press. 254\u2013272.\nCheng, L. 1997. How does washback influence teaching? Implications for Hong\nKong. Language Education. 11.38\u201354.\nClapham, C. 1997. Review of Hill, C. and Parry, K. (eds.) From testing to\nassessment. 1994. Language and Education. 11.222\u2013223.\n__________ To appear. Assessment. In M. Byram (ed.) Encyclopedia of Language\nTeaching and Learning. London: Routledge.\nDavidson, F., C. E. Turner and A. Huhta. 1997. Language testing standards. In C.\nClapham and D. Corson (eds.) Language testing and assessment, Vol. 7.\nASSESSMENT AND TESTING 159\nThe encyclopedia of language and education. Dordrecht, Holland: Kluwer\nAcademic. 303\u2013311.\nDavies, A. (ed.) 1997. Ethics in language testing. [Special issue of Language\nTesting. 14.3.]\nDIALANG. 1997. \u2018DIALANG: A new European system for diagnostic language\nassessment.\u2019 Language Testing Update. 21.38\u201339.\n[http:\/\/www.jyu.fi\/DIALANG]\nDrasgow, F. and J. Olson-Buchanan (eds.) 1998. Innovations in computerized\nassessment. Mahwah, NJ: L. Erlbaum.\nGenesee, F. and J. Upshur. 1996. Classroom-based evaluation in second language\neducation. Cambridge: Cambridge University Press.\nHamayan, E. 1995. Approaches to alternative assessment. In W. Grabe, et al.\n(eds.) Annual Review of Applied Linguistics, 15. Survey of applied\nlinguistics. New York: Cambridge University Press. 212\u2013226. \nHamp-Lyons, L. 1996. Applying ethical standards to portfolio assessment of\nwriting in English as a second language. In M. Milanovic and N. Saville\n(eds.) Performance testing, cognition and assessment. Cambridge:\nCambridge University Press. 151\u2013162.\n_____________ 1997. Ethics in language testing. In C. Clapham and D. Corson\n(eds.) Language testing and assessment, Vol. 7. The encyclopedia of\nlanguage and education. Dordrecht, Holland: Kluwer Academic. 323\u2013333.\n_____________ and B. Lynch. 1998. Positivistic versus alternative perspectives on\nvalidity within the LTRC. Unpublished manuscript.\nHasselgren, A. 1998. Small words and valid testing. Bergen, Norway: University\nof Bergen. Unpublished Ph. D. diss. \nHill, C. and K. Parry. 1994. From testing to assessment. London: Longman.\nHudson, R. 1999. E-mail message to the Linguistics Association of Great Britain\n(LAGB) listserve. [See http:\/\/www.phon.ucl.ac.uk\/home\/dick\/AL.html]\nHuerta-Macias. 1995. Alternative assessment \u2013 Responses to commonly asked\nquestions. TESOL Journal. 5.8\u201311.\nJohnson, K. and H. Johnson (eds.) 1998. Encyclopedic dictionary of applied\nlinguistics. Malden, MA: Blackwell.\nJohnstone, R. In Press. Context-sensitive assessment of modern languages in\nprimary and early secondary education: Scotland and the European\nexperience. Language Testing. 17.\nKormos, J. 1999. Simulating conversations in oral proficiency assessment: A\nconversation analysis of role play and non-scripted interviews in language\nexams. Language Testing. 16.163\u2013188.\nKunnan, A. J. 1998. An introduction to structural equation modeling for language\nassessment research. Language Testing. 15.295\u2013332.\n___________ (ed.) In press. Fairness and validation in language assessment.\nCambridge: Cambridge University Press.\nLynch, B. and T. McNamara. 1998. Using G-theory and many-facet Rasch\nmeasurement in the development of performance assessments of the ESL\nspeaking skills of immigrants. Language Testing. 15.158\u2013180.\n160 CAROLINE CLAPHAM\nMcNamara, T. 1997. Performance testing. In C. Clapham and D. Corson (eds.)\nLanguage testing and assessment, Vol. 7. The encyclopedia of language\nand education. Dordrecht, Holland: Kluwer Academic. 131\u2013139.\n____________ 1998. Policy and social considerations in language assessment. In\nW. Grabe, et al. (eds.) Annual Review of Applied Linguistics, 18.\nFoundations of second language teaching. 18.304\u2013319.\nMessick, S. 1989. Validity. In R. L. Linn (ed.) Educational measurement. New\nYork: American Council of Education\/Macmillan. 13\u2013103.\nMoss, P. 1994. Can there be validity without reliability? Educational Researcher.\n23.8.5\u201312.\nNorth, B. and G. Schneider. 1998. Scaling descriptors for language proficiency\nscales. Language Testing. 15.217\u2013262.\nNorton, B. 1997. Accountability in language Assessment. In C. Clapham and D.\nCorson (eds.) Language testing and assessment, Vol. 7. The encyclopedia of\nlanguage and education. Dordrecht, Holland: Kluwer Academic. 313\u2013322.\nOrdinate Corporation. 1998. PhonePass test validation report. Menlo Park, CA:\nOrdinate.\nPapajohn, D. 1999. The effect of topic variation in performance testing: The case\nof the chemistry TEACH test for international teaching assistants.\nLanguage Testing. 16.52\u201381.\nRampton, B. 1997. Retuning in applied linguistics. Applied Linguistics. 7.3\u201325.\nRea-Dickens, P. In press. Snares and silver bullets: Disentangling the construct of\nformative assessment. Language Testing. 17.\nRobinson, P. 1997. Individual differences and fundamental similarities of implicit\nand explicit adult second language learning. Language Learning. 47.45\u201399.\nShohamy, E. 1995. Performance assessment in language testing. In W. Grabe, et\nal. (eds.) Annual Review of Applied Linguistics, 15. Survey of applied\nlinguistics. New York: Cambridge University Press. 188\u2013211.\n__________ 1998. How can language testing and SLA benefit from each other?\nThe case of discourse. In L. F. Bachman and A. Cohen. (eds.) Interfaces\nbetween second language acquisition and language testing research.\nCambridge: Cambridge University Press. 156\u2013176.\nSkehan, P. 1998. Task-based instruction. In W. Grabe, et al. (eds.) Annual Review\nof Applied Linguistics, 18. Foundations of second language teaching. New\nYork: Cambridge University Press. 304\u2013319.\n_________ and P. Foster. 1999. The influence of task structure and processing\nconditions on narrative retellings. Language Learning. 49.93\u2013120.\nSpolsky, B. 1995. Measured words. Oxford: Oxford University Press.\nStansfield, C. 1994. Developments in foreign language testing and instruction: A\nnational perspective. In C. Hancock (ed.) Teaching, testing and\nassessment: Making the connection. Lincolnwood, IL: National Textbook\nCompany. 43\u201367.\nUpshur, J. and C. Turner. 1998. Systematic effects in the rating of second-\nlanguage speaking ability: Test method and learner discourse. Language\nTesting. 16.82\u2013111.\nASSESSMENT AND TESTING 161\nValette, R. 1994. Teaching, testing and assessment: Conceptualizing the\nrelationship. In C. Hancock (ed.) Teaching, testing and assessment:\nMaking the connection. Lincolnwood, IL: National Textbook Company.\n1\u201342.\nWall, D. 1997. Impact and washback in language. In C. Clapham and D. Corson\n(eds.) Language testing and assessment, Vol. 7. The encyclopedia of\nlanguage and education. Dordrecht, Holland: Kluwer Academic. 291\u2013302.\nWiddowson, H. G. 1980. Models and fictions. Applied Linguistics. 1.165\u2013170.\n"}