{"doi":"10.1007\/s10514-009-9167-2","coreId":"55021","oai":"oai:eprints.lincoln.ac.uk:2286","identifiers":["oai:eprints.lincoln.ac.uk:2286","10.1007\/s10514-009-9167-2"],"title":"Computationally efficient solutions for tracking people with a mobile robot: an experimental evaluation of Bayesian filters","authors":["Bellotto, Nicola","Hu, Huosheng"],"enrichments":{"references":[{"id":18444073,"title":"A new approach to linear \ufb01ltering and prediction problems.","authors":[],"date":"1960","doi":"10.1115\/1.3662552","raw":"Kalman, R. (1960). A new approach to linear \ufb01ltering and prediction problems. Trans of the ASME - Journal of Basic Eng., 82:35\u201345.","cites":null},{"id":18444071,"title":"A New Extension of the Kalman Filter to Nonlinear Systems.","authors":[],"date":"1997","doi":"10.1117\/12.280797","raw":"Julier, S. J. and Uhlmann, J. K. (1997). A New Extension of the Kalman Filter to Nonlinear Systems. In Proc. of SPIE AeroSense Symposium, pages 182\u2013193, FL, USA.","cites":null},{"id":18444072,"title":"A new method for the nonlinear transformation of means and covariances in \ufb01lters and estimators.","authors":[],"date":"2000","doi":"10.1109\/9.847726","raw":"Julier, S. J., Uhlmann, J. K., and Durrant-Whyte, H. F. (2000). A new method for the nonlinear transformation of means and covariances in \ufb01lters and estimators. IEEE Trans. on Automatic Control, 45(3):477\u2013482.","cites":null},{"id":18444057,"title":"A tutorial on particle \ufb01lters for online nonlinear\/non-Gaussian Bayesian tracking.","authors":[],"date":"2002","doi":"10.1109\/78.978374","raw":"Arulampalam, M., Maskell, S., Gordon, N., and Clapp, T. (2002). A tutorial on particle \ufb01lters for online nonlinear\/non-Gaussian Bayesian tracking. IEEE Trans. on Signal Processing, 50(2):174\u2013188.","cites":null},{"id":18444083,"title":"Active people recognition using thermal and grey images on a mobile security robot.","authors":[],"date":"2005","doi":"10.1109\/iros.2005.1545530","raw":"Treptow, A., Cielniak, G., and Duckett, T. (2005). Active people recognition using thermal and grey images on a mobile security robot. In Proc. of IEEE\/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), pages 2103\u20132108, Canada.","cites":null},{"id":18444088,"title":"An Introduction to the Kalman Filter.","authors":[],"date":"2004","doi":null,"raw":"Welch, G. and Bishop, G. (2004). An Introduction to the Kalman Filter. Technical Report 95-041, University of North Carolina.","cites":null},{"id":18444059,"title":"Bayesian estimation and the kalman \ufb01lter.","authors":[],"date":"1994","doi":"10.1016\/0898-1221(95)00156-s","raw":"Barker, A. L., Brown, D. E., and Martin, W. N. (1994). Bayesian estimation and the kalman \ufb01lter. Technical Report IPC-TR-94-002, Institute of Parallel Computing, School of Engineering and Applied Science, University of Virginia.","cites":null},{"id":18444079,"title":"Beyond the Kalman \ufb01lter: particle \ufb01lters for tracking applications.","authors":[],"date":"2004","doi":"10.1155\/s1110865704405095","raw":"Ristic, B., Arulampalam, S., and Gordon, N. (2004). Beyond the Kalman \ufb01lter: particle \ufb01lters for tracking applications. Artech House.","cites":null},{"id":18444078,"title":"Conditional particle \ufb01lters for simultaneous mobile robot localization and people-tracking.","authors":[],"date":"2002","doi":"10.1109\/robot.2002.1013439","raw":"Montemerlo, M., Whittaker, W., and Thrun, S. (2002). Conditional particle \ufb01lters for simultaneous mobile robot localization and people-tracking. In Proc. of IEEE Int. Conf. on Robotics and Automation (ICRA), pages 695\u2013 701, Washington DC, USA.","cites":null},{"id":18444069,"title":"editors (2001). Sequential Monte Carlo Methods in Practice.","authors":[],"date":null,"doi":"10.1007\/978-1-4757-3437-9","raw":"Doucet, A., de Freitas, N., and Gordon, N., editors (2001). Sequential Monte Carlo Methods in Practice. Springer.","cites":null},{"id":18444075,"title":"iBotGuard: an internet-based intelligent robot security system using invariant face recognition against intruder.","authors":[],"date":"2005","doi":"10.1109\/tsmcc.2004.840051","raw":"Liu, J. N. K., Wang, M., and Feng, B. (2005). iBotGuard: an internet-based intelligent robot security system using invariant face recognition against intruder. IEEE Trans. on Systems, Man, and Cybernetics (Part C), 35(1):97\u2013105.","cites":null},{"id":18444084,"title":"Introductions to the algorithmics of data association in multiple-target tracking. In","authors":[],"date":"2001","doi":"10.1201\/9781420038545.ch3","raw":"Uhlmann, J. K. (2001). Introductions to the algorithmics of data association in multiple-target tracking. In Hall, D. L. and Llinas, J., editors, Handbook of multisensor data fusion. CRC Press.","cites":null},{"id":18444064,"title":"Laser motion detection and hypothesis tracking from a mobile platform.","authors":[],"date":"2004","doi":null,"raw":"Bobruk, J. and Austin, D. (2004). Laser motion detection and hypothesis tracking from a mobile platform. In Proc.","cites":null},{"id":18444065,"title":"Learning-based computer vision with Intel\u2019s open source computer vision library.","authors":[],"date":"2005","doi":"10.1535\/itj.0902.03","raw":"Bradski, G., Kaehler, A., and Pisarevsky, V. (2005). Learning-based computer vision with Intel\u2019s open source computer vision library. Intel Technology Journal, 09(02):119\u2013130.","cites":null},{"id":18444060,"title":"Multisensor integration for human-robot interaction.","authors":[],"date":"2005","doi":null,"raw":"Bellotto, N. and Hu, H. (2005). Multisensor integration for human-robot interaction. The IEEE Journal of Intelligent Cybernetic Systems, 1.","cites":null},{"id":18444062,"title":"Multisensor-based human detection and tracking for mobile service robots.","authors":[],"date":"2009","doi":"10.1109\/tsmcb.2008.2004050","raw":"Bellotto, N. and Hu, H. (2009). Multisensor-based human detection and tracking for mobile service robots. IEEE Trans. on Systems, Man, and Cybernetics \u2013 Part B, 39(1):167\u2013181.","cites":null},{"id":18444058,"title":"MultitargetMultisensor Tracking: Principles","authors":[],"date":"1995","doi":null,"raw":"Bar-Shalom, Y. and Li, X. R. (1995). MultitargetMultisensor Tracking: Principles and Techniques. Y. BarShalom.","cites":null},{"id":18444070,"title":"Novelapproachtononlinear\/non-Gaussian Bayesianstate estimation.","authors":[],"date":"1993","doi":null,"raw":"Gordon, N. J., Salmond, D. J., and Smith, A. F. M. (1993). Novelapproachtononlinear\/non-Gaussian Bayesianstate estimation. IEE Proc. of Radar and Signal Processing, 140(2):107\u2013113.","cites":null},{"id":18444067,"title":"Panoramic vision and laser range \ufb01nder fusion for multiple person tracking.","authors":[],"date":"2006","doi":"10.1109\/iros.2006.282149","raw":"Chakravarty, P. and Jarvis, R. (2006). Panoramic vision and laser range \ufb01nder fusion for multiple person tracking. In Proc. of IEEE\/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), pages 2949\u20132954, Beijing, China.","cites":null},{"id":18444081,"title":"People Tracking with Anonymous and ID-Sensors Using RaoBlackwellised Particle Filters.","authors":[],"date":"2003","doi":null,"raw":"Schulz, D., Fox, D., and Hightower, J. (2003b). People Tracking with Anonymous and ID-Sensors Using RaoBlackwellised Particle Filters. In Proc. of the Int. Joint Conf. on Arti\ufb01cial Intelligence (IJCAI), pages 921\u2013926, Acapulco, Mexico.","cites":null},{"id":18444080,"title":"People Tracking with Mobile Robots Using Sample-based Joint Probabilistic Data Association Filters.","authors":[],"date":"2003","doi":"10.1177\/0278364903022002002","raw":"Schulz, D., Burgard, W., Fox, D., and Cremers, A. B. (2003a). People Tracking with Mobile Robots Using Sample-based Joint Probabilistic Data Association Filters. Int. Journal of Robotics Research, 22(2):99\u2013116.","cites":null},{"id":18444085,"title":"Rapid object detection using a boosted cascade of simple features.","authors":[],"date":"2001","doi":"10.1109\/cvpr.2001.990517","raw":"Viola, P. and Jones, M. J. (2001). Rapid object detection using a boosted cascade of simple features. In IEEE Conf. on Computer Vision and Pattern Recognition, pages 511\u2013 518, Kauai, HI, USA.","cites":null},{"id":18444074,"title":"Real-time particle \ufb01lters.","authors":[],"date":"2004","doi":"10.1109\/jproc.2003.823144","raw":"Kwok, C., Fox, D., and Meil\u02d8 a, M. (2004). Real-time particle \ufb01lters. Proc. of the IEEE, 92(3):469\u2013484.","cites":null},{"id":18444082,"title":"Socially assistive robotics.","authors":[],"date":"2007","doi":"10.1109\/mra.2007.339605","raw":"Tapus, A., Mataric, M. J., and Scasselati, B. (2007). Socially assistive robotics. IEEE Robotics and Automation Magazine, 14(1):35\u201342.","cites":null},{"id":18444086,"title":"Ten Books on Architecture. Project Gutenberg. English translation by","authors":[],"date":"1914","doi":"10.4159\/harvard.9780674435247","raw":"Vitruvius (1914). Ten Books on Architecture. Project Gutenberg. English translation by M. H. Morgan.","cites":null},{"id":18444076,"title":"The unscented particle \ufb01lter. CUED\/F-INFENG TR 380,","authors":[],"date":"2000","doi":null,"raw":"Merwe, R. V. D., Doucet, A., Freitas, N. D., and Wan, E. (2000). The unscented particle \ufb01lter. CUED\/F-INFENG TR 380, Cambridge University Engineering Department.","cites":null},{"id":18444066,"title":"TOURBOT13 and WebFAIR: Web-Operated Mobile Robots for TelePresence in Populated Exhibitions.","authors":[],"date":"2002","doi":"10.1109\/mra.2005.1458329","raw":"Burgard, W., Trahanias, P., H\u00a8 ahnel, D., Moors, M., Schulz, D., Baltzakis, H., and A., A. (2002). TOURBOT13 and WebFAIR: Web-Operated Mobile Robots for TelePresence in Populated Exhibitions. In Proc. of the IROS 2002 Workshop on Robots in Exhibitions.","cites":null},{"id":18444063,"title":"Tracking people from a mobile platform.","authors":[],"date":"2001","doi":"10.1007\/3-540-36268-1_20","raw":"Beymer, D. and Konolige, K. (2001). Tracking people from a mobile platform. In IJCAI Workshop on Reasoning with Uncertainty in Robotics, Seattle, WA, USA.","cites":null},{"id":18444068,"title":"Tracking system performance assessment.","authors":[],"date":"2003","doi":"10.1109\/radar.2003.1278802","raw":"Colegrove, S., Cheung, B., and Davey, S. (2003). Tracking system performance assessment. In Proc. of the 6th Int. Conf. on Information Fusion, pages 926\u2013933, Cairns, Australia.","cites":null},{"id":18444061,"title":"Vision and laser data fusion for tracking people with a mobile robot.","authors":[],"date":"2006","doi":"10.1109\/robio.2006.340251","raw":"Bellotto, N. and Hu, H. (2006). Vision and laser data fusion for tracking people with a mobile robot. In Proc. of IEEE Int. Conf. on Robotics and Biomimetics (ROBIO), pages 7\u201312, Kunming, China.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-05","abstract":"Modern service robots will soon become an essential part of modern society. As they have to move and act in human environments, it is essential for them to be provided with a fast and reliable tracking system that localizes people in the neighbourhood. It is therefore important to select the most appropriate filter to estimate the position of these persons.\\ud\nThis paper presents three efficient implementations of multisensor-human tracking based on different Bayesian estimators: Extended Kalman Filter (EKF), Unscented Kalman Filter (UKF) and Sampling Importance Resampling (SIR) particle filter. The system implemented on a mobile robot is explained, introducing the methods used to detect and estimate the position of multiple people. Then, the solutions based on the three filters are discussed in detail. Several real experiments are conducted to evaluate their performance, which is compared in terms of accuracy, robustness and execution time of the estimation. The results show that a solution based on the UKF can perform as good as particle filters and can be often a better choice when computational efficiency is a key issue","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/55021.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/2286\/1\/Bellotto2010.pdf","pdfHashValue":"3bcc1264d8ebb515d4d268d9b09bf4e833fa7b30","publisher":"Springer","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:2286<\/identifier><datestamp>\n      2013-11-18T16:00:54Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F48:6A6163735F48363730<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/2286\/<\/dc:relation><dc:title>\n        Computationally efficient solutions for tracking people with a mobile robot: an experimental evaluation of Bayesian filters<\/dc:title><dc:creator>\n        Bellotto, Nicola<\/dc:creator><dc:creator>\n        Hu, Huosheng<\/dc:creator><dc:subject>\n        H670 Robotics and Cybernetics<\/dc:subject><dc:description>\n        Modern service robots will soon become an essential part of modern society. As they have to move and act in human environments, it is essential for them to be provided with a fast and reliable tracking system that localizes people in the neighbourhood. It is therefore important to select the most appropriate filter to estimate the position of these persons.\\ud\nThis paper presents three efficient implementations of multisensor-human tracking based on different Bayesian estimators: Extended Kalman Filter (EKF), Unscented Kalman Filter (UKF) and Sampling Importance Resampling (SIR) particle filter. The system implemented on a mobile robot is explained, introducing the methods used to detect and estimate the position of multiple people. Then, the solutions based on the three filters are discussed in detail. Several real experiments are conducted to evaluate their performance, which is compared in terms of accuracy, robustness and execution time of the estimation. The results show that a solution based on the UKF can perform as good as particle filters and can be often a better choice when computational efficiency is a key issue.<\/dc:description><dc:publisher>\n        Springer<\/dc:publisher><dc:date>\n        2010-05<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/2286\/1\/Bellotto2010.pdf<\/dc:identifier><dc:identifier>\n          Bellotto, Nicola and Hu, Huosheng  (2010) Computationally efficient solutions for tracking people with a mobile robot: an experimental evaluation of Bayesian filters.  Autonomous Robots, 28  (4).   pp. 425-438.  ISSN 0929-5593  <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1007\/s10514-009-9167-2<\/dc:relation><dc:relation>\n        10.1007\/s10514-009-9167-2<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/2286\/","http:\/\/dx.doi.org\/10.1007\/s10514-009-9167-2","10.1007\/s10514-009-9167-2"],"year":2010,"topics":["H670 Robotics and Cybernetics"],"subject":["Article","PeerReviewed"],"fullText":"Autonomous Robots manuscript No.\n(will be inserted by the editor)\nComputationally Efficient Solutions for Tracking People with a Mobile\nRobot: an Experimental Evaluation of Bayesian Filters\nNicola Bellotto \u00b7 Huosheng Hu\nReceived: date \/ Accepted: date\nAbstract Modern service robots will soon become an es-\nsential part of modern society. As they have to move and\nact in human environments, it is essential for them to be\nprovided with a fast and reliable tracking system that lo-\ncalizes people in the neighbourhood. It is therefore impor-\ntant to select the most appropriate filter to estimate the posi-\ntion of these persons. This paper presents three efficient im-\nplementations of multisensor-human tracking based on dif-\nferent Bayesian estimators: Extended Kalman Filter (EKF),\nUnscented Kalman Filter (UKF) and Sampling Importance\nResampling (SIR) particle filter. The system implemented\non a mobile robot is explained, introducing the methods used\nto detect and estimate the position of multiple people. Then,\nthe solutions based on the three filters are discussed in de-\ntail. Several real experiments are conducted to evaluate their\nperformance, which is compared in terms of accuracy, ro-\nbustness and execution time of the estimation. The results\nshow that a solution based on the UKF can perform as good\nas particle filters and can be often a better choice when com-\nputational efficiency is a key issue.\nKeywords People Tracking \u00b7 Mobile Robot \u00b7 Kalman\nFilter \u00b7 Particle Filter \u00b7 Multisensor Fusion\n1 Introduction\nIn the last decade, several mobile robots have been employed\nin exhibitions and public places to entertain visitors, inter-\nN. Bellotto\nSchool of Computer Science, University of Lincoln\nBrayford Pool, Lincoln LN6 7TS, United Kingdom\nTel.: +44-(0)522-886080\nE-mail: nbellotto@lincoln.ac.uk\nH. Hu\nE-mail: hhu@essex.ac.uk\nacting with them and providing useful information. The tour-\nguide robot in (Burgard et al. 2002), for example, has been\nworking in a museum to accompany visitors and provide\nthem with information about the different exhibits. The robot\nwas equipped with a laser-based tracking to create maps of\nthe environments discarding human occlusions, and to adapt\nits velocity to the visitors\u2019 motion. Another case was the in-\nteractive mobile robot described in (Bellotto and Hu 2005),\nwhich integrated laser and visual data to detect human legs\nand faces, moving towards visitors to interact with them by\nuse of synthesized speech and a touch-screen interface.\nAnother field of application for people tracking is au-\ntomatic or remote surveillance with mobile security robots,\nwhich can be used to monitor wide areas of interest oth-\nerwise difficult to cover with fixed sensors. These robots\nshould be able to detect and track people in restricted zones,\nsignaling, for examples, the presence of intruders to the se-\ncurity personnel. Such a task was accomplished by an internet-\nbased mobile robot in (Liu et al. 2005), which used a PTZ\ncamera to detect and recognize human faces. The security\nrobot in (Treptow et al. 2005), instead, combined thermal\nvision, to detect and track people, with a normal camera, to\ntrack and recognize faces. Human tracking is also very im-\nportant in the new research area of socially assistive robotics\n(Tapus et al. 2007) to maintain an appropriate spatial dis-\ntance between people and robots, when these are engaged in\nsocial interactions.\nTo achieve full autonomy in mobile robotics, no exter-\nnal sensors or computers should be used, otherwise the sys-\ntem performance is completely dependent on the working\nenvironment. With these constraints, people tracking is par-\nticularly challenging and becomes even more difficult if the\nhardware resources are limited. Therefore, the computational\nefficiency of the tracking system has also to be carefully\nconsidered during software design.\n2The main contribution of this paper is an experimen-\ntal comparison of different Bayesian estimators, which is\nimportant to select the most appropriate solution for track-\ning people with a fully autonomous mobile robot. Although\nsome previous work already analyzed the performance of\nKalman and particle filters (Merwe et al. 2000), results have\nbeen obtained only in simulation with synthetic models, or\nfrom batch estimations on limited set of data. These situa-\ntions are significantly different from the case here consid-\nered, which deals instead with the difficult problem of real-\ntime target tracking under computational constraints. The\nperformance evaluation of these Bayesian estimators, con-\nsidering also hardware and software limitations, is of funda-\nmental importance for practical applications of modern ser-\nvice robots.\nThree classic approaches are examined: Extended Kal-\nman Filter (EKF), Unscented Kalman Filter (UKF) and Sam-\npling Importance Resampling (SIR) particle filter. While the\nfirst one is a well known technique developed long time ago\n(Kalman 1960), the last two solutions have been proposed\nmore recently and extensively used only in the last decade\n(Julier and Uhlmann 1997; Gordon et al. 1993). The choice\nof these particular filters is due to the fact that all of them\nhave been already applied, somehow, to people tracking with\nmobile robots. In this context, the performance of each in-\ndividual technique has been already described, but not yet\ncompared, in previous robotics literature.\nThe EKF has been implemented for tracking humans\nwith mobile robots in the works of (Beymer and Konolige\n2001) and (Bobruk and Austin 2004), using visual or laser\ndata respectively. Both the devices have been used in (Bel-\nlotto and Hu 2009) applying sensor fusion techniques and\nUKF estimation to perform people tracking in typical office\nenvironments. Several other approaches have been proposed\nusing particle filters with laser data and\/or vision (Schulz\net al. 2003a; Chakravarty and Jarvis 2006).\nTo evaluate and compare the effectiveness of each tech-\nnique, a common framework has to be set up, on which\nquantitative and qualitative experiments can be conducted.\nTherefore, in the following sections, a general probabilis-\ntic approach for tracking people with a mobile robot is in-\ntroduced. This solution integrates legs and face detections,\nobtained from robot\u2019s laser and camera respectively, which\nare fused using a sequential Bayesian filter. Since the com-\nparison focuses on multi-target (people) tracking, the same\ndata association algorithm is applied to all the filtering tech-\nniques under consideration.\nThe choice of the best estimator to use for human track-\ning depends on several factors, among which the following\nimportant ones: linearity\/non-linearity of the system, prob-\nability distribution of the uncertainty and, last but not least,\ncomputational efficiency. Whose familiar with the subject\nalready know that Kalman filters are the most computational\nefficient, while particle filters are the most accurate. The\nchallenge however lies on the design of meaningful exper-\niments so that known facts can be proved on the base of\nsolid quantitative data. In this paper, accuracy, robustness\nand execution time of the three Bayesian filters are analyzed,\nshowing that a solution based on the UKF not only performs\nbetter than the EKF, but can also be a valid alternative to\nparticle filters when used for tracking people with a mobile\nplatform.\nThe remainder of the paper is organized as follows. Sec-\ntion 2 introduces the system designed to track people with a\nmobile robot. Sections 3, 4 and 5 describe respectively the\nimplementation of the EKF, UKF and SIR particle filters.\nSeveral experiments are illustrated in Section 6 to compare\nthe performance of the different solutions in real scenarios.\nFinally, conclusions and future work are discussed in Sec-\ntion 7.\n2 People Tracking with a Mobile Robot\nIn general, tracking is a problem of estimating the position\nof a target from noisy sensor measurements. In the pres-\nence of multiple targets, which is the case for people track-\ning, each measurement has also to be assigned to the proper\ntrack. This section introduces the solutions adopted for hu-\nman detection, tracking and data association with a mobile\nrobot. The system used was a Pioneer platform, shown in\nFig. 1, equipped with a SICK laser and a PTZ camera, which\nprovided data respectively at 5Hz and 10fps. The on-board\nPC of the robot was a Pentium III 850MHz with 128MB of\nRAM, running Linux OS.\n2.1 Human Detection\nTwo kind of sensors, cameras and laser range finders, are\nthe most commonly used for tracking people with a mobile\nplatform (Beymer and Konolige 2001; Schulz et al. 2003a;\nChakravarty and Jarvis 2006). The robot employed in the\ncurrent research makes use of both the sensors to recognize\nhuman legs and faces. The detection algorithms, and the ad-\nvantage of combining laser and visual information, are de-\nscribed in detail in (Bellotto and Hu 2009).\nThe legs detection algorithm is able to recognize differ-\nent legs postures on a 180\u25e6 laser horizontal scan, with a res-\nolution of 0.5\u25e6, returning their direction and distance. The\nalgorithm starts with a smoothing process of the laser read-\nings, and then detects all the radial edges on the directions\nof the laser beam. Groups of adjacent edges, possibly gen-\nerated by human legs, are extracted using simple geomet-\nric relations and spatial constraints. The mid-points of these\ngroups, corresponding to the 2D location of the legs, are fi-\nnally computed.\n3Fig. 1 Robot with laser and camera used for legs and face detection.\nSince legs are detected in real-time from a single laser\nscan, the algorithm does not need to compensate for the dy-\nnamics of the robot, as other motion-based techniques do.\nThe method is also quite robust to cluttered environments\nand showed to perform well compared to other laser-based\ndetection techniques (Bellotto and Hu 2009). An example of\ndetection is illustrated in Fig. 1, which shows a typical laser\nscan from the robot with two lines pointing to the human\nlegs mid-points.\nWhen in proximity of a person, vision improves human\ntracking thanks to face detection. This is based on a popular\nalgorithm (Viola and Jones 2001) available on the OpenCV\nlibrary (Bradski et al. 2005). The solution works in real-\ntime on a single camera\u2019s frame, 320 \u00d7 240, and is color-\nindependent, which makes it more robust to lighting varia-\ntions.\nThe method is based on a cascade of (weak) classifiers\nusing particular visual features. Each classifier is trained to\ndetect faces, from sub-regions of the image, with a high hit\nrate. A sub-region can be rejected by the current classifier or\npassed to the following one. For a certain number of trained\nclassifiers, the final false alarm will be therefore very low,\nyet keeping a total hit rate close to 100%. Using a pin-hole\ncamera model, the direction of the face is finally calculated\nand used for human tracking, as discussed in Section 2.4.\n2.2 Bayesian Estimation\nThe most popular methods for dynamic state estimation be-\nlong to the family of recursive Bayesian estimators, which\ninclude Kalman filters (Welch and Bishop 2004; Julier and\nUhlmann 1997) and sequential Monte Carlo estimators (Aru-\nlampalam et al. 2002), also known as particle filters. These\nestimate the target position recursively, combining the ex-\npected state information with the current observations from\nthe sensors.\nIn the discrete-time domain, for a general tracking appli-\ncation, the evolution of the target state can be described by\nthe following general model:\nxk = f(xk\u22121,wk\u22121) (1)\nwhere xk is the state vector at the current time step k and\nwk\u22121 is white noise. The relative observations are generally\ndescribed by another model with additive noise:\nzk = h(xk) + vk (2)\nwhere zk is the observation vector and vk is white noise,\nmutually independent from wk\u22121. The functions f and h\ncan be non-linear.\nIf Zk = {z1, . . . , zk} is the set of observations up to\ntime k, the prior probability density p(xk|Zk\u22121) can be ex-\npressed as follows:\np(xk|Zk\u22121) =\n\u222b\np(xk|xk\u22121,Zk\u22121) p(xk\u22121|Zk\u22121) dxk\u22121\n=\n\u222b\np(xk|xk\u22121) p(xk\u22121|Zk\u22121) dxk\u22121 (3)\nwhere the transitional p(xk|xk\u22121,Zk\u22121) = p(xk|xk\u22121) is\ndetermined by the Markovian prediction model in (1). Then,\napplying Bayes\u2019 rule, the posterior density is given by the\nfollowing equation:\np(xk|Zk) = p(xk|zk,Zk\u22121)\n=\np(zk|xk,Zk\u22121) p(xk|Zk\u22121)\np(zk|Zk\u22121)\n=\np(zk|xk) p(xk|Zk\u22121)\np(zk|Zk\u22121)\n(4)\nNote that, in the numerator term, p(zk|xk,Zk\u22121) = p(zk|xk)\nbecause zk is completely described by the observation model\nin (2), which depends only on the current state xk and the\nnoise vk. The denominator is just a normalization factor cal-\nculated as follows:\np(zk|Zk\u22121) =\n\u222b\np(zk|xk) p(xk|Zk\u22121) dxk (5)\nEquations (3) and (4) are called, respectively, prediction\nand update, or correction, of the recursive Bayesian estima-\ntion. The desired estimate is usually obtained, at the end\nof every predict-update iteration, by the minimum mean-\nsquare error value, i.e. the conditional mean x\u02c6k , E[xk|Zk].\n42.3 Prediction\nA common solution to approximate human motion, while\nwalking at a normal speed, is the constant velocity model.\nThe version here considered is an extension of the latter, al-\nready introduced in (Bellotto and Hu 2006), which includes\na state vector formed by the position (xk, yk) and the height\nzk of the human subject, plus the relative orientation \u03c6k and\nvelocity vk. The equations of the model are the following:\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nxk = xk\u22121 + vk\u22121\u03b4k cos \u03c6k\u22121\nyk = yk\u22121 + vk\u22121\u03b4k sin \u03c6k\u22121\nzk = zk\u22121 + n\nz\nk\u22121\n\u03c6k = \u03c6k\u22121 + n\n\u03c6\nk\u22121\nvk = |vk\u22121|+ n\nv\nk\u22121\n(6)\nwhere \u03b4k = tk\u2212 tk\u22121 is the time interval, while nzk\u22121, n\n\u03c6\nk\u22121\nand nvk\u22121 are noises. These latter are assumed to be zero-\nmean Gaussians with \u03c3z = 0.01m, \u03c3\u03c6 = pi6 rad and \u03c3v =\n0.1m\/s respectively. The motion model in (6) is used for the\nprediction step of the Bayesian filter, as illustrated in Fig. 3\n2.4 Sequential Update\nThe observation models described next take into account the\n2D location and orientation of the robot given by the odom-\netry. Its cumulative error is not an issue in the current appli-\ncation, since the objective of the system is to track humans\nrelatively to the current robot\u2019s position. The odometry er-\nror between two consecutive estimations is also very small,\nand can be safely included in the noise of the observation\nmodels.\nGiven the location (xRk, yRk) and heading \u03c6Rk of the robot,\nthe absolute position (xLk, yLk) and orientation \u03c6Lk of the laser\nare calculated as follows:\nxLk = x\nR\nk + Lx cos \u03c6\nR\nk\nyLk = y\nR\nk + Lx sin \u03c6\nR\nk\n\u03c6Lk = \u03c6\nR\nk\n(7)\nwhere the constant Lx is the horizontal distance of the laser\nfrom the robot\u2019s centre (Ly is zero). Using the quantities in\n(7), the observation model for the bearing bk and the dis-\ntance rk of the detected legs can be written as follows:\uf8f1\uf8f4\uf8f2\n\uf8f4\uf8f3\nbk = tan\n\u22121\n(\nyk \u2212 y\nL\nk\nxk \u2212 xLk\n)\n\u2212 \u03c6Lk + n\nb\nk\nrk =\n\u221a\n(xk \u2212 xLk)\n2\n+ (yk \u2212 yLk)\n2\n+ nrk\n(8)\nwhere the noises nbk and nrk are zero-mean Gaussians with\nstandard deviations \u03c3b = pi60 rad and \u03c3r = 0.1m.\nSimilarly, the absolute position (xCk, yCk, zCk) and orienta-\ntion (\u03c6Ck, \u03b8Ck) of the camera take into account the horizontal\ndistance Cx from the robot\u2019s centre (Cy is zero), the height\n\u03b3\n\u03b2\n\u03b1\nh\n3\nh\nFig. 2 Face observation angles, including chin, measured from the\ncamera.\nCz , the pan C\u03c6 and the tilt C\u03b8. Combining the odometry\ninformation, these can be calculated as follows:\nxCk=x\nR\nk + Cx cos \u03c6\nR\nk\nyCk=y\nR\nk + Cx sin \u03c6\nR\nk\nzCk=Cz\n\u03c6Ck=\u03c6\nR\nk + C\u03c6\n\u03b8Ck=C\u03b8\n(9)\nThe next observation model is relative to the bearing \u03b1k and\nthe elevation \u03b2k of the face\u2019s centre, plus the elevation \u03b3k of\nits chin. The latter is relative to the size of the face and is\nuseful to discriminate false positives or facilitate data asso-\nciation in case of multiple faces. The equations of the model\nare the following:\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u03b1k = tan\n\u22121\n(\nyk \u2212 yCk\nxk \u2212 xCk\n)\n\u2212 \u03c6Ck + n\n\u03b1\nk\n\u03b2k = \u2212 tan\n\u22121\n\uf8ee\n\uf8f0 zk\u2212zCk\u221a\n(xk\u2212xCk)\n2\n+(yk\u2212yCk)\n2\n\uf8f9\n\uf8fb\u2212\u03b8C+n\u03b2k\n\u03b3k = \u2212 tan\n\u22121\n\uf8ee\n\uf8f0 \u00b5 zk\u2212zCk\u221a\n(xk\u2212xCk)\n2\n+(yk\u2212yCk)\n2\n\uf8f9\n\uf8fb\u2212\u03b8C+n\u03b3k\n(10)\nThe noises n\u03b1k , n\n\u03b2\nk and n\n\u03b3\nk are zero-mean Gaussians with\n\u03c3\u03b1 = \u03c3\u03b2 =\npi\n45\nrad and \u03c3\u03b3 = pi30 rad. Note that, in the third\nmember of (10), the constant \u00b5 is chosen so that the product\n\u00b5 zk corresponds to the height of the lower face\u2019s bound, i.e.\napproximately the chin. For the latter, the \u201ccanon of propor-\ntion\u201d of the human figure, as described in (Vitruvius 1914),\nhas been adopted. This considers the average height of a per-\nson being 8 times his head, and the distance from the chin to\nthe nose is 1\/3 of the head\u2019s length h, as illustrated in Fig.\n2. Since the face detection is centred on the nose, a value\n\u00b5 ' 0.955 can be easily derived.\nThe independent measurements provided by legs and face\ndetection are finally used for a sequential update of the es-\ntimation (Bar-Shalom and Li 1995). As shown by the di-\nagram in Fig. 3, which illustrates a single iteration of the\nfilter, legs measurements are the first to be considered, since\nmore accurate, and then faces. If any of the two observations\nis missing, the estimate is updated only by one sensor.\n5no\nyes\nyes\nno\npredict\nlegs\ndetected?\nface\ndetected?\nupdate\nupdate\nxk\nxk\u22121\nlegs data\nface data\nFig. 3 Sensor data fusion with sequential estimation.\n2.5 Data Association\nIn the current system, a gating procedure is first applied\nusing a validation region for each predicted observation z\u02c6i\n(Bar-Shalom and Li 1995), relative to the ith target, so that\na real measurement zj is accepted only if it satisfies the fol-\nlowing condition:\n(z\u02c6i \u2212 zj)\nT\nS\u22121ij (z\u02c6i \u2212 zj) < \u03bb\n2 (11)\nwhere Sij is the covariance matrix of the difference z\u02c6i\u2212 zj .\nThe constant \u03bb is chosen from tables of the chi-square dis-\ntribution for a probability PG of the correct measurements\nto fall within the validation region. This value depends on\nthe size of the observation vector and is set to 3.03 for legs\ndetections and 3.37 for faces.\nInstead of solutions like JPDA and MHT, powerful but\ncomputationally expensive, an efficient algorithm based on\nnearest-neighbour data association is adopted (Bar-Shalom\nand Li 1995). This showed to be a good compromise be-\ntween performances and computational cost in case the set\nof subjects to track is not too dense (Montemerlo et al. 2002;\nBellotto and Hu 2006), in particular for autonomous robots\nwith limited processing power. At every time step, two as-\nsociation matrices are created, one for the laser and another\nfor the camera information. The elements of these matrices\ncontain the following similarity measure (Uhlmann 2001):\ndij =\n1\u221a\n(2pi)\nn |Sij |\nexp\n[\n\u2212\n1\n2\n(z\u02c6i \u2212 zj)\nT S\u22121ij (z\u02c6i \u2212 zj)\n]\n(12)\nwhere n is the size of the observation vector, i.e. 2 for legs\ndetection and 3 for faces.\n2.6 Creating and Removing Tracks\nNew tracks are created from the sensor readings discarded\nduring the validation gate procedure or the data association.\nInitially, a candidate track is generated by a sequence of\nmeasurements falling inside a certain region, calculated ac-\ncording to the maximum distance a person can cover at the\nmaximum speed of 1.5m\/s. The candidate is promoted to\nhuman track if there are at least 3 readings falling within\nthis region, each one of which must be received not later\nthan 0.5s from the previous one, otherwise the candidate is\nremoved. Tracks are eventually deleted from the database if\nnot updated for more than 2s or if the uncertainty of their 2D\nposition is too big, i.e. the sum of the variances in x and y is\ngreater than 2m2.\nNote that the procedure for tracks creation is indepen-\ndent from the particular Bayesian estimator used, therefore\nits parameters, equally set for EKF, UKF or SIR filter, do\nnot influence the experimental comparison. The deletion cri-\nteria, instead, is based on time but also on the estimated\ncovariance of the track, which might therefore be different\ndepending on the filter used. In practice however, the un-\ncertainty\u2019s threshold works only as a precaution, and tracks\nare usually removed because they exceed the time condition,\nwhich is the same for all the estimators.\n3 EKF Implementation\nThe Kalman filter was initially proposed in (Kalman 1960)\nand, although originally not formulated as such, it has been\nlater shown to belong to the more general class of Bayes-\nian estimators (Barker et al. 1994). It was also proved to be\noptimal in case of linear systems with Gaussian noises, for\nwhich the posterior in (4) becomes the following:\np(xk|Zk) = N (xk; x\u02c6k,Pk)\n= |2piPk|\n\u22121\/2\nexp\n[\n\u2212\n1\n2\n(xk\u2212x\u02c6k)\nT\nP\u22121k (xk\u2212x\u02c6k)\n]\n(13)\nwhere x\u02c6k and Pk are, respectively, the estimated mean and\ncovariance of xk.\nIn case of non-linearities, the EKF provides an approx-\nimated solution, applying the same equations to linearized\nsystem models. This can give good results if the lineariza-\ntion is sufficiently accurate to describe the system, but fails\nbadly if it is not.\nGiven the state vector xk = [xk, yk, zk, \u03c6k, vk]T and\nthe relative noise wk = [0, 0, nzk, n\n\u03c6\nk , n\nv\nk]\nT\n, the components\n6of which have already been defined in Section 2.3, the lin-\nearized version of the prediction model in (6) has the form:\nxk = Fkxk\u22121 + wk\u22121 (14)\nThe Jacobian Fk is calculated as follows:\nFk =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1 0 0 \u2212vk\u22121\u2206tk sin \u03c6k\u22121 \u2206tk cos \u03c6k\u22121\n0 1 0 vk\u22121\u2206tk cos \u03c6k\u22121 \u2206tk sin \u03c6k\u22121\n0 0 1 0 0\n0 0 0 1 0\n0 0 0 0 sgn (vk\u22121)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb (15)\nwhere \u2206tk = tk \u2212 tk\u22121 is the time interval and sgn(vk\u22121)\nis the algebraic sign of vk\u22121.\nThe prediction stage consists in calculating the a-priori\nestimate x\u02c6\u2212k and the covariance matrix P\n\u2212\nk of its error:\nx\u02c6\u2212k = f(x\u02c6k\u22121, 0) (16)\nP\u2212k = FkPk\u22121F\nT\nk + Q (17)\nwhere Q is the covariance of the (additive) process noise:\nQ =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0 0 0 0 0\n0 0 0 0 0\n0 0 \u03c32z 0 0\n0 0 0 \u03c32\u03c6 0\n0 0 0 0 \u03c32v\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0 0 0 0 0\n0 0 0 0 0\n0 0 10\u22124 0 0\n0 0 0 pi\n2\n81\n0\n0 0 0 0 10\u22122\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb (18)\nThe observation models described in Section 2.4 are lin-\nearized as follows:\nzk = Hkxk + vk (19)\nwhere Hk is the Jacobian of the laser or camera observa-\ntion, with relative noise vectors vk \u2261 [nbk, nrk]T or vk \u2261\n[n\u03b1k , n\n\u03b2\nk , n\n\u03b3\nk ]\nT\n. In the first case, given the observation vector\n[bk, rk]\nT and the quantities defined in (8), Hk is defined as\nfollows:\nHk \u2261 H\nL\nk =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u2212\nyk \u2212 yLk\nd2k\nxk \u2212 xLk\nd2k\n0 0 0\nxk \u2212 xLk\ndk\nyk \u2212 yLk\ndk\n0 0 0\n\uf8f9\n\uf8fa\uf8fa\uf8fb\nwith d2k = (xk \u2212 xLk)\n2\n+ (yk \u2212 y\nL\nk)\n2\n(20)\nFor the second one, given the vector [\u03b1k, \u03b2k, \u03b3k]T , the Jaco-\nbian matrix of the model in (10) is the following:\nHk \u2261 HCk =\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u2212\nyk \u2212 y\nC\nk\nd2k\nxk \u2212 x\nC\nk\nd2k\n0 0 0\n(xk \u2212 xCk) (zk \u2212 z\nC\nk)\nr2k dk\n(yk \u2212 yCk) (zk \u2212 z\nC\nk)\nr2k dk\n\u2212\ndk\nr2k\n0 0\n(xk \u2212 x\nC\nk) (\u00b5 zk \u2212 z\nC\nk)\nl2k dk\n(yk \u2212 y\nC\nk) (\u00b5 zk \u2212 z\nC\nk)\nl2k dk\n\u2212\ndk\nl2k\n0 0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n(21)\nwith d2k = (xk \u2212 xCk)\n2\n+ (yk \u2212 y\nC\nk)\n2\nr2k = d\n2\nk + (zk \u2212 z\nC\nk)\n2\nl2k = d\n2\nk + (\u00b5 zk \u2212 z\nC\nk)\n2\nThe update part includes the calculation of the following\nKalman gain Kk:\nSk = HkP\n\u2212\nk H\nT\nk + R (22)\nKk = P\n\u2212\nk H\nT\nk S\n\u22121\nk (23)\nThe quantity Sk in (22) is the innovation covariance and R\nis the covariance matrix of the observation noise vk. In case\nof laser readings, the latter is set as follows:\nR \u2261 RL =\n[\n\u03c32b 0\n0 \u03c32r\n]\n=\n[\npi2\n3600\n0\n0 10\u22122\n]\n(24)\ninstead for the camera the following matrix is used:\nR \u2261 RC =\n\uf8ee\n\uf8f0 \u03c32\u03b1 0 00 \u03c32\u03b2 0\n0 0 \u03c32\u03b3\n\uf8f9\n\uf8fb =\n\uf8ee\n\uf8ef\uf8f0\npi2\n2025\n0 0\n0 pi\n2\n2025\n0\n0 0 pi\n2\n900\n\uf8f9\n\uf8fa\uf8fb (25)\nFinally, the a-posteriori estimate x\u02c6k and the relative error\ncovariance Pk are computed as follows:\nx\u02c6k = x\u02c6\n\u2212\nk + Kk (zk \u2212 z\u02c6k) (26)\nPk = P\n\u2212\nk \u2212KkSkK\nT\nk (27)\nwhere the term (zk \u2212 z\u02c6k), with z\u02c6k = h(x\u02c6\u2212k ), is the differ-\nence between real and predicted measurements, also called\ninnovation.\n4 UKF Implementation\nTo overcome the problem of the linearization, which could\nintroduce large errors and require the computation of big\nJacobian matrices, the UKF makes use of another approxi-\nmation, called the Unscented Transformation (UT). This is\nbased on the idea that it is generally easier and more accu-\nrate to approximate probability distributions than non-linear\nfunctions. The UT captures mean and covariance of a prob-\nability distribution with carefully chosen weighted points,\ncalled sigma points. These differ from the points of particles\nfilters in that they are not randomly sampled and do not have\nto lie in the interval [0, 1].\nFrom the state x of size n, and its error covariance P,\nthe 2n + 1 sigma points X i and associated weights Wi of\nthe UT are calculated using the following equations (Julier\nand Uhlmann 1997):\nX 0 = x W0 = \u03c1\/(n + \u03c1)\nX i = x +\n[\u221a\n(n + \u03c1)P\n]\ni\nWi = [2 (n + \u03c1)]\n\u22121\nX i+n = x\u2212\n[\u221a\n(n + \u03c1)P\n]\ni\nWi+n = [2 (n + \u03c1)]\n\u22121\n(28)\n7where i = 1, . . . , n. The term\n[\u221a\n(n + \u03c1)P\n]\ni\nis the ith col-\numn or row of the matrix square root of P, and \u03c1 is a param-\neter for tuning the higher order moments of the approxima-\ntion (n + \u03c1 = 3 for Gaussian distributions).\nMean and covariance of a generic non-linear transfor-\nmation y = g(x) are calculated using the sigma points as\nfollows:\nY i = g (X i) (29)\ny =\n2n\u2211\ni=0\nWiY i (30)\nPyy =\n2n\u2211\ni=0\nWi [Y i \u2212 y] [Y i \u2212 y]\nT (31)\nThese equations yield to a projected mean and covariance\nthat are correct up to the second order, giving better results\nthan the EKF\u2019s linearization, yet keeping the same compu-\ntational complexity.\nGiven the state vector xk = [xk, yk, zk, \u03c6k, vk]T of size\nn = 5, the estimation procedure of the UKF consists ini-\ntially in an UT. This takes the last estimate x\u02c6k\u22121 and its\nrelative covariance Pk\u22121 to generate, using (28), the 2n +\n1 = 11 sigma points X ik\u22121 . Note that, in this case, the tun-\ning parameter assumes a negative value \u03c1 = 3\u2212 n = \u22122.\nIn (Julier et al. 2000), it is shown that \u03c1 < 0 can lead to a\nnon-positive semidefinite matrix when the state covariance\nis calculated with (31). In order to solve this problem, the\nauthors suggest to simply add a term [Y 0 \u2212 y\u02c6] [Y 0 \u2212 y\u02c6]T to\nthe sum in (31).\nUsing the prediction model f(xk\u22121) defined in (6), the\na-priori estimate x\u02c6\u2212k and covariance P\n\u2212\nk are computed as\nfollows:\nx\u02c6k\u22121\nUT\n\u2212\u2192\n{\nX ik\u22121\n}10\ni=0\n(32)\nX \u2212ik = f\n(\nX ik\u22121\n)\nfor i = 0, . . . , 10 (33)\nx\u02c6\u2212k =\n10\u2211\ni=0\nWiX\n\u2212\nik\n(34)\nP\u2212k =\n10\u2211\ni=0\nWi\n[\nX \u2212ik \u2212 x\u02c6\n\u2212\nk\n] [\nX \u2212ik \u2212 x\u02c6\n\u2212\nk\n]T\n+\n[\nX \u22120k \u2212 x\u02c6\n\u2212\nk\n] [\nX \u22120k \u2212 x\u02c6\n\u2212\nk\n]T\n+ Q (35)\nwhere Q is the covariance of the process noise defined in\n(18).\nThe expected observations for the legs and face detec-\ntions are generated using the observation model h(xk), de-\nfined respectively in (8) and (10), applied to the sigma points\nin (33) as follows:\nZ ik = h\n(\nX \u2212ik\n)\nfor i = 0, . . . , 10 (36)\nz\u02c6k =\n10\u2211\ni=0\nWiZ ik (37)\nSk =\n10\u2211\ni=0\nWi [Z ik \u2212 z\u02c6k] [Z ik \u2212 z\u02c6k]\nT\n+\n[Z 0k \u2212 z\u02c6k] [Z 0k \u2212 z\u02c6k]\nT\n+ R (38)\nwhere z\u02c6k is the predicted observation, Sk is the innovation\ncovariance and R is the covariance of the observation noise,\ndefined in (24) for the laser and in (25) for the camera.\nThe cross-correlation Ck and the gain Kk are computed\nusing the following formulas:\nCk =\n10\u2211\ni=0\nWi\n[\nX \u2212ik \u2212 x\u02c6\n\u2212\nk\n]\n[Z ik \u2212 z\u02c6k]\nT (39)\nKk = CkS\n\u22121\nk (40)\nFinally, the a-posteriori estimate x\u02c6k and relative covari-\nance Pk are determined applying the same equations (26)\nand (27) previously used for the EKF.\n5 SIR Implementation\nParticle filters are recursive Bayesian estimators that make\nuse of Monte Carlo methods to approximate and transform\nprobability distributions (Doucet et al. 2001; Arulampalam\net al. 2002; Ristic et al. 2004). The major advantages of such\nfilters are their independence from the non-linearities of a\nsystem and capability to approximate any kind of probability\ndistribution, including multimodal cases. The drawback is\nthat a large number of particles is normally required for a\ngood estimation, with a consequent negative effect on the\ncomputational cost.\nIn particle filters, the posterior of the state, introduced in\n(4), is approximated by the weighted sum of N samples xik:\np(xk|Zk) \u2248\nN\u2211\ni=1\nwik\u03b4(xk \u2212 x\ni\nk) (41)\nwhere \u03b4(\u00b7) is the Dirac delta measure. The samples xik are\ndrawn from a known importance density q(xik|xik\u22121, zk),\nand their weights are calculated recursively as follows:\nwik \u221d w\ni\nk\u22121\np(zk|x\ni\nk)p(x\ni\nk|x\ni\nk\u22121)\nq(xik|x\ni\nk\u22121, zk)\n(42)\nIt can be proved that for N \u2192\u221e the approximation in (41)\ntends to the true posterior p(xk|Zk).\nThere are many different implementations of particle fil-\nters, however the SIR algorithm is probably the most popu-\nlar, due to its simplicity. This estimator, originally proposed\n8in (Gordon et al. 1993) with the name of \u201cbootstrap\u201d filter,\nmakes use of the transitional prior as importance density:\nq(xik|x\ni\nk\u22121, zk) = p(x\ni\nk|x\ni\nk\u22121) (43)\nLike for the previous ones, the SIR estimation has an\niterative predict-update sequence. The prediction part gen-\nerates new particles, from the previous ones, using (1) and\nsamples drawn from the probability distribution of the state\nnoise. In the current implementation, the number of samples\nused was 1000, similar to other existing solutions (Chakrava-\nrty and Jarvis 2006; Schulz et al. 2003a), and also 500, which\nreduces the computational burden but is still sufficient to\ntrack humans correctly. The prior distribution p(xk|xk\u22121)\nis a GaussianN [xk; f(xk\u22121),Q], where f(xk\u22121) is the pre-\ndiction model defined in (6) and Q is the same covariance\nmatrix in (18).\nThen, as soon as a new measurement is available, the\nupdate is performed calculating the new weights of the sam-\nples. The choice of the importance density in (43) simplifies\nthe calculus of the weights, which are given by the following\nformula:\nwik \u221d w\ni\nk\u22121 p(zk|x\ni\nk) (44)\nThe likelihood p(zk|xk) is a GaussianN [zk;h(xk),R] that\ndepends on the observation models h(xk) defined in (8) and\n(10), for laser and camera respectively, and on the relative\nnoise covariance R illustrated in (24) and (25).\nWeighted samples are finally used to calculate an ap-\nproximated posterior with (41). At the end of each iteration,\nthe SIR algorithm performs also a resample step that elimi-\nnates all the particles with very small weights and, from the\nremaining ones, generates new samples equally weighted. A\ndetailed explanation of SIR and other particle filters is given\nin (Arulampalam et al. 2002; Ristic et al. 2004).\n6 Experimental Results\nThe effectiveness of the tracking system has been tested,\nwith several experiments in a real environment, comparing\nthe three solutions based on EKF, UKF and SIR filters. To\nachieve maximum performances, the code has been writ-\nten in C\/C++ making use of highly optimized libraries for\nimage processing1 and estimation2. When running in real-\ntime on the robot, the maximum update frequency of the\nprogram was approximately 4Hz, but it could decrease in\ncase particle filters were used. The test scenario was the in-\ndoor environment illustrated in Fig. 4, which includes sev-\neral offices, connected by a corridor to a laboratory and a\nrobot arena. Data have been collected tracking 7 different\n1 Intel IPP \u2013 http:\/\/developer.intel.com\/software\/products\/ipp\n2 Bayes++ \u2013 http:\/\/bayesclasses.sourceforge.net\nOFFICE 2\nLIFT\nROBOT ARENA\nOFFICE 1\nLABORATORY\nCO\nRR\nID\nO\nR\nFig. 4 Floor plan of the environment used for the experiments.\nsubjects who were moving in this environment. The results\nhave been compared in terms of accuracy, robustness and\ncomputational efficiency.\n6.1 Tracking Accuracy\nThe accuracy of the estimations has been determined us-\ning the ground-truth position measured in the robot arena.\nThis is equipped with a marker-based tracking system us-\ning a camera mounted on the ceiling, calibrated to provide\nthe ground-truth position of the robot and the people around\nit. A bird-eye view from the ceiling camera and the relative\nobservation from the robot are shown in Fig. 5.\nExperimental data have been recorded during four simi-\nlar trials, for a total length of approximately 5 minutes (1200\ntime steps). These covered various cases in which a sin-\ngle person or multiple people were tracked, either with the\nrobot static or in motion. An example is represented in Fig.\n6, which illustrates the trajectory of the robot, moving at\n0.4m\/s, and the random paths of three people wandering\naround it. The data collected from the robot and from the\nglobal tracking system have been used for an off-line com-\nparison of the accuracy, where the tracking error was given\nby the Euclidean distance between the estimated human po-\nsition, (x\u02c6k, y\u02c6k), and the relative ground-truth, (x\u2217k, y\u2217k). The\nlatter was obtained tracking the robot with the ceiling cam-\nera, together with the human targets. Then, at every time\nstep, their absolute position was transformed to the robot\u2019s\nframe of reference.\nThe results of the experiments are summarized in Ta-\nble 1, which reports the root mean square (RMS) of the 2D\nposition error ek, calculated over all the M tracking steps,\n9(a) Bird-eye view from the ceiling camera of the robot arena. Each\ntarget has a color marker (one more for the robot to get its orientation).\n(b) Same situation as observed by the robot. Face and legs detection\nare shown on the left. The robot R and the (true) position of the hu-\nmans, A,B, and C, are shown on the right.\nFig. 5 Example of people tracked in the robot arena.\nand the relative mean e\u00af, the standard deviation (SD) and the\nmaximum value. The position error is defined as follows:\nek =\n\u221a\n(x\u02c6k \u2212 x\u2217k)\n2 + (y\u02c6k \u2212 y\u2217k)\n2 (45)\nThe number M is the sum of the duration, in time steps, of\nall the human tracks created during these experiments. The\nRMS, the mean and the SD were calculated as follows:\nRMS =\n\u221a\u2211M\nk=1 e\n2\nk\nM\n(46)\ne\u00af =\n\u2211M\nk=1 ek\nM\n(47)\nSD =\n\u221a\u221a\u221a\u221a 1\nM \u2212 1\nM\u2211\nk=1\n(ek \u2212 e\u00af)2 (48)\nThe results in Table 1 show that the performances of the\ntwo SIR filters were almost identical, despite the different\nnumber of particles used. Note also that the UKF\u2019s tracking\nFig. 6 Paths of robot (thick line) and three persons: A (thin line), B\n(thin dashed line) and C (thick dashed line).\nTable 1 Tracking error\nEKF UKF SIR(500) SIR(1000)\nRMS [m] 0.439 0.317 0.285 0.280\nMean [m] 0.325 0.261 0.248 0.244\nSD [m] 0.296 0.180 0.141 0.138\nMax [m] 2.084 1.680 1.291 1.267\naccuracy, besides being better than the EKF, was also very\nclose to that one obtained with particle filters.\nThis is also confirmed by the graphs of the cumulative\ndistribution function (CDF) for the RMS and the SD of the\nerror, shown respectively in Fig. 7 and Fig. 8. Using the so-\nlution proposed in Colegrove et al. (2003), which defines\na practical method to evaluate the performance of tracking\nsystems from real data, RMS and SD values are adopted\nas comparison metrics. These are calculated for the whole\nlength Mt of each human track t created during the exper-\niments. The relative RMSt and SDt are reported in the ab-\nscissa of the graphs, each one corresponding to an increment\n1\/Mt of the probability in the ordinate. Since the lower the\nmetric value the better the performance, the CDFs in Fig.\n7 and Fig. 8 show that the tracking system based on UKF\nis better than the EKF\u2019s one, and is comparable to the SIR\nsolutions.\n6.2 Tracking Robustness\nWhen comparing different tracking solutions, another essen-\ntial factor to be considered is robustness. To evaluate this,\ntwo important parameters are considered here: the number\nof tracking errors and the total amount of tracks generated\nby the different systems. In addition to the previous data\nrecorded in the robot arena, several other experiments have\n10\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.10\n0.2\n0.4\n0.6\n0.8\n1\nRMS [m]\nCD\nF\n \n \nEKF\nUKF\nSIR 500\nSIR 1000\nFig. 7 Cumulative distribution function of the root mean square error.\n0 0.1 0.2 0.3 0.4 0.5 0.60\n0.2\n0.4\n0.6\n0.8\n1\nSD [m]\nCD\nF\n \n \nEKF\nUKF\nSIR 500\nSIR 1000\nFig. 8 Cumulative distribution function of the error standard deviation.\nbeen carried out tracking people who were moving between\nthe rooms in Fig. 4. Totally, more than 10 minutes of data\nhave been collected, and all the generated tracks have been\nmanually labeled.\nThe number of tracking errors was evaluated considering\nonly the 2D position for sake of simplicity. Each one of the\nfollowing situations was counted as an error: a) the track\ndeviates from the correct trajectory of the human target and\nis eventually deleted by the system; b) the track \u201cjumps\u201d to\na static object, adjacent to the path of the person, due to a\nfalse positive (gating error); c) the track switches to another\nperson close to the original one (data association error). All\nthese cases are strictly related to the estimate of the filter and\nto the distribution of its uncertainty.\nAlthough this work does not include an exhaustive eval-\nuation of the tracking performance under varying sensing\nconditions (false positives, occlusions, etc.), intuitively these\nwill be better handled by the UKF and the SIR particle filter,\nrather than the EKF, due to their ability to better model the\npropagated probability functions. This is shown, for exam-\nple, in Fig. 9 and Fig. 10, where a couple of EKF\u2019s errors\noccurred while the robot was following some persons be-\ntween different rooms. The correct path of the UKF track\n(identical to the SIR case) and the wrong one generated by\nthe EKF are shown on the left of the figures, together with\nthe robot\u2019s trajectory. A moment of the wrong tracking with\nthe EKF is shown in the middle, while the correct estimation\nof the UKF is illustrated on the right. In Fig. 9, the tracking\nerror in the office was caused by the curvilinear trajectory\nof the human and the simultaneous motion of the robot, as\nFig. 11 Number of tracking errors with different filters.\nshown also in Video 1. In Fig. 10 and relative Video 2, the\nEKF failed between the laboratory and the arena as a conse-\nquence of a false positive on the legs detection, generated by\na column. These situations were correctly handled instead\nby the UKF and the SIR tracking systems.\nThe chart in Fig. 11, showing the total amount of track-\ning errors, illustrates clearly that the results obtained with\nthe UKF and the particle filters were much better than the\nEKF-based tracking. The non-linearity of the system, in-\ndeed, made the EKF fail in several occasions, in particu-\nlar when both the robot and the person being tracked were\nmoving. The performance of the UKF was generally simi-\nlar to the SIR tracking in terms of the number of errors, but\ndiffered on the type. Despite occasional errors due to some\nfalse positives, the major accuracy of particle filters in rep-\nresenting the probability distribution of the estimate seemed\nto be an advantage for data association. However, as will\nbe shown in Section 6.3, a solution based on particle filters\nwas not feasible for real-time tracking with the current robot\nplatform.\nThe previous results were also confirmed by the total\nnumber of tracks generated for each system implementation,\nas reported in Fig. 12. Indeed, the more robust and stable is\nthe estimation, the less likely is the tracking to fail, and con-\nsequently the smaller is the number of tracks generated by\nthe system. Although the whole tracking length was about\nthe same for all the solutions (approximately 2750 estima-\ntion steps), the chart shows that the number of tracks was\nhigher using the EKF. Instead, even in this case, the values\nrelative to UKF and SIR particle filters were very close.\n11\nFig. 9 Human tracking in Office 1.\nFig. 10 Human tracking in the laboratory.\nFig. 12 Total amount of tracks generated during the experiments.\n6.3 Computational Efficiency\nIt is known that, in general, particle filters are computation-\nally much more demanding than Kalman filters, and that the\ntime needed for the estimation increases with the number\nof samples used. For many applications, this does not rep-\nresent a problem, because the number of actual estimations\nis limited (e.g. single target tracking) or simply because the\nhardware is powerful enough. It might pose a serious con-\nstraint, however, in case of frequent estimations and limited\ncomputing resources, e.g. for the system currently studied.\nIn this experiment, the execution time needed by each\nfilter to perform an estimation (i.e. single iteration of the\nprocess in Fig. 3) was compared while tracking one or more\npersons. For simplicity, only legs detections were used to up-\ndate the track estimates. The detections have been simulated\nwith static laser data hard-coded in the software, generating\nfrom one to four pairs of legs observable at the same time.\nThis permitted to have the same inputs constantly available\nto the tracking system for all the estimators under compari-\nson.\nThe graph in Fig. 13 shows the average times needed\nfor an update iteration run on the Pioneer robot, which in-\ncludes only the prediction and one filter update, i.e. the first\ntwo blocks in the diagram of Fig. 3. The time spent for legs\ndetection and additional routines (tracks handling, data as-\nsociation, logging, etc.) has not been counted. The results\nhave been obtained averaging the total estimation time on\n100 consecutive time steps, tracking up to 4 target simulta-\nneously. The estimation processing used approximately 70%\nof the CPU and, as expected, had different time durations,\ndepending on the filter adopted for tracking. The graph shows\nthat the time increased almost linearly with the number of\ntracked persons and, for the SIR filter, with the number of\nparticles.\nIt is important to note that, while the EKF and UKF so-\nlutions were very fast, the ones based on particle filters were\nmuch slower and, in some case, their tracking performance\n12\n1 2 3 4\n10\u22121\n100\n101\n102\n103\nNumber of persons\nEs\ntim\nat\nio\nn \ntim\ne \n[m\ns]\n \n \nEKF\nUKF\nSIR 500\nSIR 1000\nLaser\nFig. 13 Estimation time, in logarithmic scale, as function of the num-\nber of persons being tracked. The laser scans period (200ms) is shown\nfor reference.\nwas drastically limited. For example, during the tracking of\nthree or more people, the execution time of the SIR with\n500 particles was close to 200ms (i.e. the period of a laser\nscan). Since normally there are other tasks to be executed\nin addition to the estimation, the SIR tracking system can-\nnot process the sensor information as fast as it should and,\nvery often, it is not able to work properly. Indeed, in this ex-\nperiment, the measurement of the execution time was possi-\nble only because the targets were static, otherwise the SIR-\nbased tracking would have failed because of the low update\nfrequency (i.e. it could not work in real-time). Same consid-\nerations can be done for the SIR with 1000 particles, or con-\nsidering a larger number of people. Although other particle\nfilters computationally more efficient (Schulz et al. 2003b;\nKwok et al. 2004) should be considered in future compar-\nisons, the UKF remains generally a faster solution.\n7 Conclusions and Future Work\nThis paper presented an experimental comparison of peo-\nple tracking systems based on three different Bayesian es-\ntimators, namely EKF, UKF and SIR particle filter. These\nsolution makes use of probabilistic sensor fusion techniques\nto integrate laser and visual data. Their implementation on\na mobile robot have been described in detail. With several\nexperiments in real situations, the systems have been com-\npared in terms of accuracy, robustness and computational\nefficiency.\nOn the specific task of real-time people tracking with\nmobile robots, the results showed that a UKF solution could\nperform as good as particle filters. Furthermore, analyzing\nthe estimation time, the UKF proved to be a better choice\nfor the current application, in particular when hardware re-\nsources are limited. An approach based on this filter could\nbe generally preferred for autonomous robots with low pro-\ncessing power, for which the computational efficiency is a\nkey issue.\nIn the future, it would be interesting to extend this com-\nparison to include more recent and efficient particle filters,\npossibly using different mobile platforms as well. Their per-\nformance could also be evaluated using different data asso-\nciation algorithms to see how these influence people track-\ning. The results of this research are also important for the\nauthors\u2019 implementation of an interactive robot performing\nsimultaneous people tracking and recognition, for which an\naccurate and robust real-time estimation is a fundamental\nrequirement.\nReferences\nArulampalam, M., Maskell, S., Gordon, N., and Clapp,\nT. (2002). A tutorial on particle filters for online\nnonlinear\/non-Gaussian Bayesian tracking. IEEE Trans.\non Signal Processing, 50(2):174\u2013188.\nBar-Shalom, Y. and Li, X. R. (1995). Multitarget-\nMultisensor Tracking: Principles and Techniques. Y. Bar-\nShalom.\nBarker, A. L., Brown, D. E., and Martin, W. N. (1994).\nBayesian estimation and the kalman filter. Technical\nReport IPC-TR-94-002, Institute of Parallel Computing,\nSchool of Engineering and Applied Science, University\nof Virginia.\nBellotto, N. and Hu, H. (2005). Multisensor integration for\nhuman-robot interaction. The IEEE Journal of Intelligent\nCybernetic Systems, 1.\nBellotto, N. and Hu, H. (2006). Vision and laser data fusion\nfor tracking people with a mobile robot. In Proc. of IEEE\nInt. Conf. on Robotics and Biomimetics (ROBIO), pages\n7\u201312, Kunming, China.\nBellotto, N. and Hu, H. (2009). Multisensor-based hu-\nman detection and tracking for mobile service robots.\nIEEE Trans. on Systems, Man, and Cybernetics \u2013 Part B,\n39(1):167\u2013181.\nBeymer, D. and Konolige, K. (2001). Tracking people from\na mobile platform. In IJCAI Workshop on Reasoning with\nUncertainty in Robotics, Seattle, WA, USA.\nBobruk, J. and Austin, D. (2004). Laser motion detection\nand hypothesis tracking from a mobile platform. In Proc.\nof the 2004 Australian Conference on Robotics & Au-\ntomation, Canberra, Australia.\nBradski, G., Kaehler, A., and Pisarevsky, V. (2005).\nLearning-based computer vision with Intel\u2019s open source\ncomputer vision library. Intel Technology Journal,\n09(02):119\u2013130.\nBurgard, W., Trahanias, P., Ha\u00a8hnel, D., Moors, M., Schulz,\nD., Baltzakis, H., and A., A. (2002). TOURBOT\n13\nand WebFAIR: Web-Operated Mobile Robots for Tele-\nPresence in Populated Exhibitions. In Proc. of the IROS\n2002 Workshop on Robots in Exhibitions.\nChakravarty, P. and Jarvis, R. (2006). Panoramic vision and\nlaser range finder fusion for multiple person tracking. In\nProc. of IEEE\/RSJ Int. Conf. on Intelligent Robots and\nSystems (IROS), pages 2949\u20132954, Beijing, China.\nColegrove, S., Cheung, B., and Davey, S. (2003). Track-\ning system performance assessment. In Proc. of the 6th\nInt. Conf. on Information Fusion, pages 926\u2013933, Cairns,\nAustralia.\nDoucet, A., de Freitas, N., and Gordon, N., editors (2001).\nSequential Monte Carlo Methods in Practice. Springer.\nGordon, N. J., Salmond, D. J., and Smith, A. F. M. (1993).\nNovel approach to nonlinear\/non-Gaussian Bayesian state\nestimation. IEE Proc. of Radar and Signal Processing,\n140(2):107\u2013113.\nJulier, S. J. and Uhlmann, J. K. (1997). A New Extension of\nthe Kalman Filter to Nonlinear Systems. In Proc. of SPIE\nAeroSense Symposium, pages 182\u2013193, FL, USA.\nJulier, S. J., Uhlmann, J. K., and Durrant-Whyte, H. F.\n(2000). A new method for the nonlinear transformation\nof means and covariances in filters and estimators. IEEE\nTrans. on Automatic Control, 45(3):477\u2013482.\nKalman, R. (1960). A new approach to linear filtering and\nprediction problems. Trans of the ASME - Journal of Ba-\nsic Eng., 82:35\u201345.\nKwok, C., Fox, D., and Meila\u02d8, M. (2004). Real-time particle\nfilters. Proc. of the IEEE, 92(3):469\u2013484.\nLiu, J. N. K., Wang, M., and Feng, B. (2005). iBotGuard: an\ninternet-based intelligent robot security system using in-\nvariant face recognition against intruder. IEEE Trans. on\nSystems, Man, and Cybernetics (Part C), 35(1):97\u2013105.\nMerwe, R. V. D., Doucet, A., Freitas, N. D., and Wan, E.\n(2000). The unscented particle filter. CUED\/F-INFENG\nTR 380, Cambridge University Engineering Department.\nMontemerlo, M., Whittaker, W., and Thrun, S. (2002). Con-\nditional particle filters for simultaneous mobile robot lo-\ncalization and people-tracking. In Proc. of IEEE Int.\nConf. on Robotics and Automation (ICRA), pages 695\u2013\n701, Washington DC, USA.\nRistic, B., Arulampalam, S., and Gordon, N. (2004). Be-\nyond the Kalman filter: particle filters for tracking appli-\ncations. Artech House.\nSchulz, D., Burgard, W., Fox, D., and Cremers, A. B.\n(2003a). People Tracking with Mobile Robots Using\nSample-based Joint Probabilistic Data Association Fil-\nters. Int. Journal of Robotics Research, 22(2):99\u2013116.\nSchulz, D., Fox, D., and Hightower, J. (2003b). People\nTracking with Anonymous and ID-Sensors Using Rao-\nBlackwellised Particle Filters. In Proc. of the Int. Joint\nConf. on Artificial Intelligence (IJCAI), pages 921\u2013926,\nAcapulco, Mexico.\nTapus, A., Mataric, M. J., and Scasselati, B. (2007). So-\ncially assistive robotics. IEEE Robotics and Automation\nMagazine, 14(1):35\u201342.\nTreptow, A., Cielniak, G., and Duckett, T. (2005). Active\npeople recognition using thermal and grey images on a\nmobile security robot. In Proc. of IEEE\/RSJ Int. Conf. on\nIntelligent Robots and Systems (IROS), pages 2103\u20132108,\nCanada.\nUhlmann, J. K. (2001). Introductions to the algorithmics of\ndata association in multiple-target tracking. In Hall, D. L.\nand Llinas, J., editors, Handbook of multisensor data fu-\nsion. CRC Press.\nViola, P. and Jones, M. J. (2001). Rapid object detection\nusing a boosted cascade of simple features. In IEEE Conf.\non Computer Vision and Pattern Recognition, pages 511\u2013\n518, Kauai, HI, USA.\nVitruvius (1914). Ten Books on Architecture. Project Guten-\nberg. English translation by M. H. Morgan.\nWelch, G. and Bishop, G. (2004). An Introduction to the\nKalman Filter. Technical Report 95-041, University of\nNorth Carolina.\n"}