{"doi":"10.1109\/FUZZY.2010.5584280","coreId":"68013","oai":"oai:eprints.lancs.ac.uk:33927","identifiers":["oai:eprints.lancs.ac.uk:33927","10.1109\/FUZZY.2010.5584280"],"title":"Real-time human activity recognition from wireless sensors using evolving fuzzy systems.","authors":["Andreu, Javier","Angelov, Plamen"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-07","abstract":"A new approach to real-time knowledge extraction from streaming data generated by wearable wireless accelerometers based on self-learning evolving fuzzy rule-based classifier is proposed and evaluated in this paper. Based on experiments with real subjects we collected data from 18 different classifieds activities. After preprocessing and classifying data depending on the sequence of activities regarding time, we achieved up to 99.81% of accuracy in recognizing a sequence of activities. This technique allows re-training the system as long as the application is running on the wearable intelligent\/smart sensor, getting a better classification rate throughout the time without an increase of the delay in performance. (c) IEEE Pres","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:33927<\/identifier><datestamp>\n      2018-01-24T02:05:28Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D626F6F6B5F73656374696F6E<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Real-time human activity recognition from wireless sensors using evolving fuzzy systems.<\/dc:title><dc:creator>\n        Andreu, Javier<\/dc:creator><dc:creator>\n        Angelov, Plamen<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        A new approach to real-time knowledge extraction from streaming data generated by wearable wireless accelerometers based on self-learning evolving fuzzy rule-based classifier is proposed and evaluated in this paper. Based on experiments with real subjects we collected data from 18 different classifieds activities. After preprocessing and classifying data depending on the sequence of activities regarding time, we achieved up to 99.81% of accuracy in recognizing a sequence of activities. This technique allows re-training the system as long as the application is running on the wearable intelligent\/smart sensor, getting a better classification rate throughout the time without an increase of the delay in performance. (c) IEEE Press<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2010-07<\/dc:date><dc:type>\n        Contribution in Book\/Report\/Proceedings<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/ 10.1109\/FUZZY.2010.5584280<\/dc:relation><dc:identifier>\n        Andreu, Javier and Angelov, Plamen (2010) Real-time human activity recognition from wireless sensors using evolving fuzzy systems. In: IEEE International Conference on Fuzzy Systems (FUZZ), 2010. IEEE, pp. 2652-2659. ISBN 978-1-4244-6919-2<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/33927\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":null,"relations":["http:\/\/dx.doi.org\/ 10.1109\/FUZZY.2010.5584280","http:\/\/eprints.lancs.ac.uk\/33927\/"],"year":2010,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution in Book\/Report\/Proceedings","NonPeerReviewed"],"fullText":"  \nAbstract\u2014 A new approach to real-time knowledge extraction \nfrom streaming data generated by wearable wireless \naccelerometers based on self-learning evolving fuzzy rule-based \nclassifier is proposed and evaluated in this paper. Based on \nexperiments with real subjects we collected data from 18 \ndifferent classifieds activities. After preprocessing and classifying \ndata depending on the sequence of activities regarding time, we \nachieved up to 99.81% of accuracy in recognizing a sequence of \nactivities. This technique allows re-training the system as long as \nthe application is running on the wearable intelligent\/smart \nsensor, getting a better classification rate throughout the time \nwithout an increase of the delay in performance. \nI. INTRODUCTION \nHE use of ubiquitous systems which monitor individuals is \nprojected to increase rapidly in the next decade. The \ncomputer of the past was in a form of a rack, later the \n\u2018personal computer\u2019 was hailed as a great step in the \nminiaturization and personalization indeed, then laptops, \nPDAs and palm held computers become the next step in the \nsame direction. The future computers will be micro and even \nnano devices that are literally embedded in various objects of \nthe everyday life, including sports cloths, equipment used by \nelderly etc. All this miniaturized devises will surround us and \nwill be able to monitor our movements, temperature or \ndistance in a passive manner. MEMS and nano-electronics \nmarked a significant advance in the increase of the processing \ncapacities of these devices during the last two decades [1].  \nIn the case of Wireless Sensors which are more suitable for \nmonitoring purposes than mobile phones, sensory capacities \nhave increased including several sensors that gather data in a \nsynchronized fashion by comprising a powerful and very \ninformative sensor array [2]. As a result of severe power \nconsumption constraints, these wireless sensors limited in the \npast to merely gather the data from transducers and to forward \nit to the back-end. Nowadays, they are able to process \ncomplex algorithms, to compress, process the gathered data \non-line. 1 \nThis offers the possibility to develop principally new types \nof applications that are relatively more computationally \ncomplex but lead to the increase of the level of intelligence \nmachine quotient (MIQ) of these devices and the systems and \nnetworks that include them. One such possibility to make use \nof the increased computational capacity of these devices for \n \nBoth  authors  are  with  the  Intelligent  Systems  Research  Laboratory, \nDepartment of Communication Systems, InfoLab21, Lancaster University, \nLancaster,  LA1  4WA,  UK;  e-mail: j.andreu@lancaster.ac.uk \nthe development of an intelligent application is presented in \nthis paper. It should be noted that despite the increased \ncomputational capacities the practical limitations that still \nexists as well as the requirements for real-time mode of \noperation require the algorithms and applications that are \nbeing developed to be implemented and uploaded on board \nsuch devices to be computationally lean, recursive and fast. \nWe have chosen as a case study of this generic approach \nactivity recognition and alert generation task that is applicable \nto sportsmen elderly, soldiers etc. Activity recognition based \non motion data is a very attractive field of study and \nexperimentation for sport and healthcare applications. \nMonitoring wireless wearable devices can be embedded on \neveryday accessories such as bracelets or wrist watches which \nwould be able to track all data at the same time offering a \nsupposed high level meaning of the perceived information to a \nback-end center. At a (usually remote) centre\/back-end \ncoaches or doctors would be able to know the activity the \nsubjects they are monitoring are most likely to be performing \n(involved in) as well as to receive an alert in case of an \nabnormal behavior. This information is the most important and \ncan increase the efficiency of the monitoring process, save \nbandwidth and energy of transmission. Instead of sending the \nraw data and overloading the communication channel, \nmonitors and the power requirements of the micro- or nano-\ndevice the high level knowledge will be transmitted in a more \naggregated (granulated) form. The knowledge includes besides \nthe type of the activity being recognized on-line also the \n(fuzzy) rules based on which this recognition process has \ntaken place as well as the recursive parameters based on which \nthe on-line learning process can continue with new data. \n    In a real life experiment the training data is obtained from a \nsingle subject who is monitored in an automated manner. On \nthe other hand, when several subjects are monitored, these \nexperiments are carried out following some pre-defined \ninstructions which make the recognition very accurate but they \nare based on restricted gesture models [3]. A real life \nexperiment must allow a complete freedom of movement of \nthe individual subjects of the experiment. In the experiments \nthat are described in this paper both of these points of view are \ntaken into account, because they have different domains of \napplications. In the former case, it could be useful for assisted \nlive applications, for disabled people which perform similar \ngesture models. Alternatively, in the latter case, it is intended \nthat a unique application is able to recognize a common \npattern from different characters, which makes the definition \nReal-Time Human Activity Recognition from \nWireless Sensors using Evolving Fuzzy Systems \nJavier Andreu, Student Member, IEEE, Plamen Angelov, Senior Member, IEEE \nT\nWCCI 2010 IEEE World Congress on Computational Intelligence \nJuly, 18-23, 2010 - CCIB, Barcelona, Spain FUZZ-IEEE\n978-1-4244-8126-2\/10\/$26.00 c\u00a92010 IEEE 2652\n of the pattern extremely complex. Moreover, we have to \nconsider that our aim is to carry out this inference in real-time, \nwhich makes practically impossible to build a classification \nmodel a priori. Therefore, the approach that is taken is based \non signal processing techniques which are adaptable and \nversatile so that recognition is based on non-fixed (evolving) \nmodels. In this study we develop and study the application of \na recently introduced autonomous machine learning technique \ncalled evolving (fuzzy) rule-based systems [4,5,6] that provide \nthe ability to take into account the dynamics and progressive \nevolution which the living subjects exhibit. \nThe paper is organized as follows. Section II describes our \nprevious studies and other experimental works on this matter. \nSection III we explain the accomplished experiment and \nintroduce the used techniques.  Section IV explains with detail \ndataset collected and Section V pre-processing of data and \nfeature extraction. Section VI elaborates on applied Evolving \nFuzzy Systems (EFS). Section VII shows obtained results, \nVIII gives some details about alert recognition using same \ntechniques and finally IX conclusions. \nII. BACKGROUND AND RELATED WORK \nIn previous studies we worked on Ambient Assisted Living \n(AAL) application using wireless sensors network (WSN) for \nelderly monitoring [7]. Across our projects we employed a \ngreat variety of heterogeneous sensors such as RFID, bio-\nsensors, accelerometers, gyroscopes etc. We organized them \nin a cooperative framework [8]. The applications that were \ndeveloped took a form of services. One of them was a fall \ndetector that generates an alarm that is triggered when pre-\ndefined thresholds are surpassed. By testing the fall detector \nprototype we realized that responses from the system and \nalarm generated were not effective when falls happened in an \nunusual manner, position or speed-up. As a result of this \nshortcoming we had to reconfigure related parameters on \nmany occasions. This emphasizes the importance of \nalgorithms and methods that are generic in nature and do not \nrequire user- or problem-specific thresholds, tuning but can \ninstead adapt to the changing\/evolving behavior of the \nsubjects that are being monitored.  \nOur research was therefore directed towards study and \ndevelopment of generic in nature activity recognition methods \nand algorithms that are dynamically evolving, self-learning \nand adapting (as opposed to being pre-fixed and off-line). \nThere is a wide range of publications that report human \nactivities recognition using wearable devices, e.g. using \nmagnetic red switches [9], RFID Readers [10] and so on. \nHowever, the accelerometer attached to the body has been the \npredominantly used hardware. Some articles are focused on \ndealing with acceleration data such as [2] but they use several \nhoarders which were uncomfortable for monitored \ncharacters\/subjects and even further away of a suitable \nprototype to be offered on the market.  \nIn this paper we do compare the proposed approach and \nresults of the study with the other published approaches which \nconcern achieving an accurate activity recognition using just a \nsingle sensor. On this basis, an accurate activity classification \ntechnique using a single wrist-worn accelerometer has been \ndeveloped [11]. In [12] a decision tree algorithm built from \nclusters created beforehand has been used which assumes an \noff-line experimental setting and pre-fixing the structure of the \nclassifier which also implies that any future changes or \nevolution of the behavior by the subjects being monitored will \nnot be correctly classified. Therefore, this algorithm is not \nsuitable for real-time application. \nIII. THE PROPOSED APPROACH \nThis section introduces the proposed approach for adaptive, \nreal-time activity recognition from wearable wireless sensors. \nIt comprise of several stages which are closely related. \nSuccess of recognition depends strongly on every single layer \nof the system. In what follows we will describe all stages that \nare included in the proposed self-learning classifier.   \nWe used a wireless sensor with an accelerometer on board \nas a prototype. This sensor was a SunSPOT mote [13] which \nhas a low-power, three-axial accelerometer that can be set to \nmeasure accelerations over a scale of \u00b16g. Moreover, the radio \nrange is up to 10 meters (USA version allows up to 100m), but \ncan be significantly less depending on other interference \nfactors in the environment. Battery life is entirely dependent \non the duty cycle (when running all the LEDs and the radio at \nfull power will support less than a day (closer to 6 hours) of \nlife; an almost continuous deep sleep can support well over a \nyear). \n \n The core idea of the proposed paradigm is to infer higher \nlevel (human intelligible) knowledge (in the form of fuzzy \nrules) by processing on-line in a computationally efficient \nrecursive manner in real-time streams of raw data locally on \nthese wearable wireless sensors and only transmit the high \nlevel knowledge instead of transmitting the huge amounts of \nraw data directly. The proposed concept is centered at \nembedding local intelligence in wearable sensors borrowing \nsome ideas from robotics. Emerging concept of evolving \n(intelligent) systems [4,5,6] has already been applied \nsuccessfully to intelligent sensors in chemical industry [14], to \nrobotic systems [15], etc. A hardware realization in a system-\non-chip embedded device such as FPGA of the evolving \nclustering was also reported [16]. The proposed paradigm \nallows the personalization aspect to be addressed by extracting \nuser-specific knowledge from the movement pattern extracted \nfrom the low level data streams (acceleration time series).  \n      \nFig. 1.  Smart wireless sensor attached to a subject\u2019s\/character\u2019s arm \n \n2653\n IV. COLLECTING REPRESENTATIVE SAMPLES \n At the beginning of this research we needed to generate an \naccurate representative database for training the initial model. \nIt was obtained from 18 subjects (10 men and 8 women).  \nSupervised annotations were taken on during the experiment. \nAll of experiments had duration between 60 and 90 minutes. \nThe wireless wearable sensor was attached to the upper arm \n(figure 1) and maximum duration of monitoring was 20 \nminutes. Other studies claim that 20Hz of frequency is \nrequired to classify physical actives. In our case this frequency \nis slightly higher reaching 30 Hz of frequency. \nA total of 18 activities were performed: \n  \nTABLE I  LIST OF ACTIVITIES FROM THE EXPERIMENT \n \nThe conductor of the experiment was directing users to \nproceed with the selected activities as well as was taking note \nof the time when a new activity took place. Activities were \nperformed in different order aiming a more realistic scenario \nand an increased complexity.  \nV. PROCESSING RAW DATA \nData collection is important, but processing the raw data is the \nkey of a successful approach. First of all, the data are \ncompressed. Raw data measured by two-dimensional \naccelerometer were used:  \n         1,2,...t)}y ,{(x = S ttt =                  (1) \nOnly the values when the derivative is equal to 0 (inflection \npoint of the signal) are taken into account since they are the \nmost significant points of the signal. This can be denoted as: \n},y                              \n),y yy (y )yy y(y          \n\u00a6)}y ,{{(x = S                             \n1,1-k\nk1kk1-kk1kk1-k\nkkt\ntkk Syy \u2208\n>\u2227>\u2228<\u2227<\n+\n++\n   (2) \nIn order to allow for a fair comparison of the proposed \nalgorithm with other approaches we formed two types of \nexperimental datasets: \n1) Offline Dataset:  This data set is based on the raw data \nfrom the sensors from each one of the characters\/subjects. \nWe gather a big dataset with all the data collected for \neach one of the characters and then we proceed with the \nfeature extraction. In total, we extracted 72 features, \nf1,\u2026,f72 (36 for each of the axes, x and y), namely: \n\u2022 variance;  \n\u2022 kurtosis; \n\u2022 skewness; \n\u2022 30 different percentiles; \n\u2022 correlation between axes data; correlation is defined \nas the ratio of the covariance and the product of the \nstandard deviations for both axes, x and y: \n \nxx\nyxC\n\u03c3\u03c3\n),cov(\n=\n                        (3) \n\u2022 Energy; It is the sum of the squared discrete Fourier \ntransform component magnitude of the signal divided \nby the length of the collected dataset: \n.\n2\n11\nw\nx\nEnergy\nw\ni\u2211 =\n=\n                                    (4) \n2) Online Dataset: In this case features were selected in a \nreal-time fashion i.e. each time a new data was collected \nrespective feature was updated recursively. In this sense, \nsome approaches which are not recursive cannot be \napplied in on-line mode. In the case of Evolving Fuzzy \nSystems (EFS) [4,5] which is fully recursive approach \nthis is the most appropriate mode. In total, we formed 66 \ndifferent features (f1,\u2026,f66); for each of the two \nacceleration axis\u2019s, x and y: \n\u2022 recursive mean: \nttt xtt\nt 11\n1 +\n\u2212\n=\n\u2212\n\u00b5\u00b5 ;        (5) \n\u2022 recursive variance:  \n22\n1\n2 )(\n1\n11\ntttt xtt\nt \u00b5\u03c3\u03c3 \u2212\n\u2212\n+\n\u2212\n=\n\u2212\n;         (6) \n\u2022 30 recursive percentiles: \nn\nc\nP t\nS\nt\nS\nt\n2\n1\n\u03c3\u00b5 \u22c5+=\n\u2212\n;             (7) \nwhere n is the current number of samples, S is the \nrequired percentile. Cs is the value of the required \npercentile in a constant normal distribution. \n\u2022 current value of derivatives: \n       \u0001 \u0002\u0003 \u0004\u0005 \u0006 ln \u0005\t\u0002 .                   (8) \nVI. EVOLVING SELF-LEARNING FUZZY RULE-BASED \nCLASSIFIERS FOR ACTIVITY RECOGNITION  \nA classifier is a mapping from the feature space to the class \nlabel space. The antecedent part of a FRB classifier describes \nthe fuzzy partitioning of the feature space, f\u2208R66 and with the \nconsequent part - the class label, Li, i=[1, K]. The structure of \neClass [17] follows this typical construct of a fuzzy rule-based \nclassifier:  \n( ) ( )\n( )i\niii\nLTHEN\nxaroundisxANDfaroundisfIFR *6666*11 ...:\n (9) \nwhere Tffff ],...,,[ 6621= is the vector of features; iR  \ndenotes the ith fuzzy rule; i=[1, N];N is the number of fuzzy \nrules; ( )*ijj faroundisf  denotes the jth fuzzy set of the ith \nfuzzy rule; j=[1,n]; *if is the focal point of the ith rule \nActivities list \nWalking Carrying things while walking \nWatching TV Running \nWashing Tidying a room \nLying in a bed Brushing teeth \nClimbing stairs Reading \nCycling Vacuum cleaning \nStanding Exercising\/stretching \nEating Relaxing in a chair \nOffice work (in a chair) Others \n \n2654\n antecedent (note that this is a prototype - a real, existing data \nsample not the mean). Li is the label of the class of the ith \nprototype (focal point). \nThe inference in eClass is produced by \u2018winner takes all\u2019 \nrule: \n\n \u0006 \n\u000b\f; \u000e\f \u0006 \u0010argmax \u0016\u0017\u000b\u0018\u000e \u0006 1             (10) \nWhere \u0017\u000b denotes the firing level of (degree of confidence \nin) the ith fuzzy rule, which is determined as a product of the \nmembership values, \u001a\u001b\u000b  of the jth  feature, j=[1,n] to the fuzzy \nset ( )*ijj faroundisf : \u0017\u000b \u0006 \u220f \u00b5\u001e\u001f x\u001e!;    i \u0006 #1, N&'\u001e(\u0002          (11) \nThe membership functions that describe the degree of \nassociation with a specific prototype are usually of Gaussian \nform (characterized by good generalization capabilities and \ncoverage of the whole feature space): \n\u001a\u001b\u000b \u0006 )*\n+,-.\/\n0\n1\/0 2\n,\n;   \u000e \u0006 #1, \u0010&;   3 \u0006 #1, 4&                             (12) \nwhere \u0004\u001b\u000b is th distance between a sample and the prototype \n(focal point) of the ith fuzzy rule; 6\u001b\u000b is the spread of the \nmembership function, which also represents the radius of the \nzone of influence of the fuzzy rule.         \nA. Learning eClass \nTypically, classifiers are trained off-line using evolutionary \nalgorithms or gradient-based schemes such as back-\npropagation when combined with NN. The eClass family, \nhowever, is designed for on-line applications with an evolving \n(self-developing) FRB structure. To achieve this, the \nantecedents of the FRB are formed from the data stream \naround highly descriptive focal points (prototypes) in the \ninput-output space, [ ]TT Lfz ,=  per class.  \nThis on-line algorithm works similarly to adaptive control \nand estimation \u2013 in the period between two samples two \nphases are performed:  \ni) class prediction (classification);  \nii) classifier update or evolution.  \nDuring the first phase the class label is not known and is \nbeing predicted; during the second phase, however, it is \nknown and is used as supervisory information to update the \nclassifier (including its structure evolution as well as its \nparameters update).  \nThe main difference between eClass and conventional \nclassifiers is;  \ni) the open structure of the rule-base (it self-develops \non-line starting \u2018from scratch\u2019 while in a \nconventional classifier it is determined off-line and \nthen fixed);  \nii) the on-line learning mechanism which takes into \naccount this flexible rule-base structure.  \nNote that the overall fuzzy rule base is composed of K sub-\nrule-bases so that in each sub-rule-base the consequents of all \nrules are the same, but the number of rules, N should be no \nless than the number of classes (N\u2265K). That is, every new data \nsample with a class label that has not been previously seen \nbecomes automatically a prototype. Since there is a prototype \nreplacement and removal mechanism this is usually \ntemporarily (this prototype is often later replaced by more \ndescriptive prototypes). In this way, the classifier learns \u2018from \nscratch\u2019 and the number of classes does not need to be known \nin advance. \nThe basic notion of the partitioning algorithm is that of the \npotential or density in the feature space which is defined as a \nCauchy function of the sum of distances between a certain \ndata sample and all other data samples of class L in the feature \nspace [18]:  789 \u0016:8\u0018 \u0006 \u0002;* <\u0002 =\u2211  \u0016?@0 \u0018,AB+\/C+D E \u2211 ?@0  F@C+0AB+\/C+                     (13) G \u0006  2,3, \u2026.               789 \u0016:\u0002\u0018 \u0006 1 \nWhere L8\u000b \u0006 L8*\u0002\u000b M N \u0016?@0 \u0018,\u2211  \u0016?@\/\u0018,AB+\/O+  , L\u0002\u000b \u0006 N \u0016?+\n0 \u0018,\n\u2211  \u0016?+\/\u0018,AB+\/O+   , \u000e \u0006 #1, 4 M 1& and P \u0006  #QR, \n&Ris the input\/output vector. \n \n \nPartitioning using the density\/potential is based on the \nfollowing principle: \u2018the point with the highest density is \nchosen to be the focal point of the antecedent of a fuzzy rule\u2019. \nIn this way fuzzy rules with high descriptive power and \ngeneralization capabilities are generated.      \nEach time a new data sample is read it affects the data \ndensity; therefore the potentials of the existing centers need to \nbe updated. This update can also be done in a recursive way:  \n \n7ST P9\u000b\f! \u0006 UV@W*\u0002XY@C+W U?W0\fXV@W*\u0002Z[ V@W*;!- +\\@C+W U]W0\fX*\u00022Z^U?W0\f ,?@X_\n                  (14) \n \nOnce the density\/potential of the new-coming data sample \nis calculated recursively using (11) and the density\/potential of \neach of the previously existing prototypes is recursively \nupdated using (14) they are compared. If the new data sample \nhas a higher potential than any of the previously existing \nprototypes of that class, L then it is a good candidate to \nbecome a focal point of a new rule in this sub-rule-base \nbecause it has high descriptive power and generalization \npotential: \n( ) ( ) LiLLkkLk NizDzD \u2208\u2200> ** ;   (15) \nForming a new fuzzy rule around the newly added \nprototype leads to a gradual increase of the size of the sub-\nrule-base, which is why the approach is called \u2018evolving\u2019: \n( )\nk\nN zz\nL\n\u2190+ *1\n (16) \nThe potential of the newly generated rule is set to 1 \ntemporarily (it will be updated to take into account the \ninfluence of each new data sample on the generalization \npotential of this particular focal point by (15) with each new \ndata sample being read): \n( )( ) 1*1 \u2190+LNLk zD  (17) \nTo increase the interpretability and update the rule base one \n2655\nneeds to remove previously existing rules that beco\nambiguous after insertion of a new rule. Therefore, each time \na new fuzzy rule is added it is also checked whether any of the \nalready existing prototypes in this sub-rule\nby this rule to a degree higher than e-1: \n)(];,1[, 11 jezNii LNjiL \u2200>=\u2203 \u2212+\u00b5\nIf any of the previously existing prototypes satisfy this \ncondition the rules that correspond to them are being removed \nfrom this sub-rule base (in fact, replaced by the newly formed \nrule). \nThe spreads of the membership functions of the sub\nbase of the respective class are also recursively updated based \non the data distribution: \n],1[;)1( 11 Likikik Nirr =\u2212+= \u2212\u2212 \u03c3\u03c1\u03c1\n \nwhere  \u03c1 is a learning parameter; it determines how quickly the\nspread of the membership functions will converge to the local \nscatter of that cluster; the default value is 0.5\nVII. EXPERIMENTAL RESULTS FOR ACTIVITY \nIn this section the experimental results using the proposed \napproach will be described and compared with other existing \npublished studies and also algorithms used in other \ninvestigations in the activity recognition field. \nWe separated the experimental part in two tests. First, in an \noff-line mode we tried to classify from the \nwhich activity happened in a certain moment \nthe other hand, we used the on-line dataset from just one \ncharacter to recognize the activity (s)he is performing\/ \nengaging in a real time fashion. \nA.  Offline Test \n A dataset from 18 subjects was collected. \ncross validation test was carried out in order to \nperformance of a range of widely used off\ntechniques. This test is used in order to evaluate \nparticular approach will generalize to a\n[18]. \n\u2022 BayesNet: Bayesian Network classifier\nprobabilistic relationship among variables of interest. \nAn estimator which estimates \nprobabilities directly from data was used \nstructure of the Bayesian network \noff line; \n\u2022 ADABoostM1: AdaBoost.M1  access\nlearning  algorithm  (which  the  developers  \ngenerically  title  WeakLearn) that \ndifferent distributions of the training \nWeakLearn calculates a hypothesis, or classifier,   \nthat   attempts   to   correctly   classify   all instances  \nof  the  test  data [20]; \n\u2022 J48: It is a version of an earlier algorithm developed \nby J. R. Quinlan, the very popular C4.5\n\u2022 J48 Rotation Forest: is a method for generating \nclassifier ensembles based on feature extraction [\nTo create the training data for a base\nfeature set is randomly split into \nparameter  of  the  algorithm)  and  Principal  \nme \n-base are described \n],1[, nj =\n (18) \n-rule-\n(19) \n \n  \nRECOGNITION \n \noff-line dataset \nof the past. On \nA standard 10 fold \nassess the \n-line classification \nhow a \n validation data set \ns [19] encode \nconditional \nonce the \nhas been learned \nes  to  a  \ncalls repeatedly \ndata set. \n [21].  \n22]. \n classifier, the \nK subsets (K  is  a  \nComponent  Analysis  (PCA)  is  applied  to each \nsubset. All principal components are retained\nto preserve the variability inf\nThus, K axes rotations take place to form the new \nfeatures for the base classifier. Rotation Forest\nclassification and regression depending on the base \nlearner. In our case,\nC4.5 tree; \n\u2022 Support Vector Classifier:\nsequential minimal optimization algorithm for \ntraining a support vector classifier\ndimensional class problems \nclassification [23]. W\noutputs of the support vector machine\neClass1 first order local linear models [\nmulti-class case the predicted probabilities are \ncoupled using Hastie and Tibshirani's pair\ncoupling method [18\n\u2022 Neural Networks: A \nerror back propagation \ndesigned using a multilayer perceptron framework.\nuses three or more layers of neurons (nodes) with \nnonlinear activation functions\n\u2022 KNN Classifier: It i\nwhich operates on the premise that classification can \nbe done by relating new samples with the older ones\naccording to some \n[19]. \n   In a real life and real time \nrate is not the only factor that is important. The\ntime the classifier requires to be developed or\na paramount importance. Some of the classifiers, for example, \nthe Neural Network classifier and\nthan 1 minute to build the model\/classifier. This constraint \nmakes these algorithms inefficient\nAnother vitally important aspect of an algorithm is the \nmemory requirements. Most of them require significant\nmemory storage (kNN and EFS are the only exceptions). The \niterative nature is also a big restriction that is typical for some \nof the algorithms (most notably ADABoost, NN and SVM) \nwhich also hampers real-time on\nComparison results with all the selected approaches are \ndisplayed in Figure 2. \nFig. 2 Comparative results with different algorithms\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nTotal Classification Rate\n \n in order \normation in the data. \n can do \n the base selected classifier is a \n We tested the John Platt\u2019s \n. It solves multi-\nusing pair-wise \ne used regression models like \n (similar to the \n17]). In the \n-wise \n]; \nclassifier that uses the popular \nlearning technique was \n It \n; \ns an instance-based classifier \n, \ndistance or similarity function \napplications, the classification \n amount of \n trained is also of \n Rotation Forest take more \n for real time purposes. \n \n-line applications. \n \n. \nBayesNet\nAdaBoostM1\nC.45\nC.45 Rotation Forest\nSupport Vector \nClassifier\nNeural Networks\nKNN\nEFZ\n2656\n  \nAlthough EFS is on-line by virtue of its nature, we \ncompared it using off-line mode (assuming we stopped its \nlearning and used classification\/predictions only) for the sake \nof a comparison with other off-line methods. It has to be noted \nthat EFS demonstrated very good results that were pretty close \nto what other robust off-line classifiers like Neural Network \nand Rotation Forest get and even slightly superior to Support \nVector Machine classifiers. This is a very good result having \nin mind that EFS is one pass, recursive, computationally light \nand designed primarily for low constraint devices and real \ntime applications.  \nB. Online Test \n On the off-line test described above we aimed to mainly \nput the proposed technique in the context of other existing \n(primarily off-line) techniques. By having the results that the \nproposed technique is comparable and superior quality than \nother off-line techniques in terms of precision, we \nconcentrated ourselves on the advantages of the on-line \napplication such as computational simplicity (low memory \nrequirements, short time of response, re-learning ability). In \nthis test we will also employ the on-line feature extraction \ncapability of EFS. \nTABLE II \nONLINE DATASET \n \nIt should be noted that an on-line experiment has some \nspecifics that are generic for all approaches that are applied. \nOn-line approaches needs to create some stable patterns (fuzzy \nrules in the case of EFS) for classification which might require \na relatively long period of time if these activities have similar \nmotion patterns. On this basis, to include \u2018Watching TV\u2019 and \n\u2018Sitting in a Chair\u2019, followed in the same action sequence, \nwould result in the classifier not being able to distinguish \nbetween both to a sufficient degree. In order to get significant \nresults in an effective fashion we carried out an experiment \nwith a total length of 6 minutes. In this experiment, 3 \nunrelated activities were performed in the following order: \n\u2018Climbing Stairs\u2019, \u2018Exercising\/Stretching\u2019 and \u2018Walking\u2019. \nBetween the former and the latter there is a similar motion \npattern. For this reason, we included the activity \n\u2018Exercising\/Stretching\u2019 between the two. All experiments \nwere performed ten times and the results were averaged. In the \nfollowing experiment the dataset was composed of data from a \nsingle character\/subject.  \nAccording to real-time and sensors requirements, response \ntime should be the minimum possible in order to flush data \nbuffers as soon as possible. The more the time of operation is \nconstant the more suitable is the approach for a real-time \noperation. This is because we should define operational time \nconstraints that allow us to set up a maximum time of \nresponse. \n \n \nFig. 3 a) top plot - time needed for calculations (vertical) versus the \nactual(real) time of the experiment (both in seconds); b) bottom plot \u2013 \nclassification rate in percentage (vertical) versus actual (real) time of the \nexperiment (in seconds). \n \nFigure 3 a) (top plot) shows the time required for the \nrespective algorithms to calculate the results in seconds (the \nvertical line) versus the actual (real) time of the experiment for \nthe 140th to 360th seconds of the experiments (we registered \nthe data stating form the 140th  second because at this moment \na new activity started). We consider that it is not interesting to \ncarry out the comparison when we were only getting data from \na single activity. Therefore, periods of time displayed \ncorrespond to a transition between different activities as \nfollows: \n\u2022 [140,310], s: Climbing Stairs \u0001 \nExercising\/Stretching \n\u2022 [310, 360], s: Exercising\/Stretching \u0001 Walking \n From Figure 3 b), bottom plot one can see that it takes to \nbuild an ANN classifier 40% more time than the real time of \nthe experiment which makes it prohibitive for a real-time \napplication. Responses of a number of such algorithms come \ndelayed and response time increases exponentially. SVM, \nC4.5 and BayesNet increase the time of their response \ncontinuously. In addition, ANN needs to gather and keep all \nprevious data in order to build a classification model in any \nparticular time instant. As opposed to that, EFS simply uses \nthe last data sample to update the model rather than to re-build \nit each time. The response time of EFS oscillates between 23s \nand 93s, this rate keeps uniformity though. It increases when a \nnew rule\/activity has been identified but immediately \n0\n100\n200\n300\n400\n500\n600\nEFS\nSVM\nC4.5\nBayesNet\nANN\nEFS\nSVM\nC4.5\nBayesNet\nProc. Time\nReal \nTime\nTraining \nSubjects  Activities Total Time Data Samples \n1    3 \u22486 min \u224891,510 \nTesting \nSubjects  Activities Total Time Data Samples \n1    1 \u224820 sec.     \u22485,484  \n \n80\n82\n84\n86\n88\n90\n92\n94\n96\n98\n100\nEFS\nSVM\nC4.5\nBayesNet\nANN\nReal \nTime\nClassification Rate\n2657\n afterwards, for new data samples, time of response becomes \nlower, until a new rule is formed again. \nAll algorithms provide a very good classification rate on the \nfist activity. However, when the second activity takes place \n(around 310th s) some of the algorithms (most notably C4.5 \nand SVM reduced their classification rate significantly, see \nFig. 3 b). BayesNet keeps a similar classification rate as EFS. \nHowever, in on-line mode EFS is fairly superior maintaining a \nclassification rate very close to 100%. It is on condition that \nthis classification rate becomes slower across the time and \nnew activities are detected. However, evolving techniques \nhelp the system to recover its accuracy, as a result of its \nadaptive behavior. \nVIII. ALERT RECOGNITION \n \nFig 4. a) left plot - Raw data for the activity Walking, and b) right plot - \nResultant clusters from processed raw data with EFS. \n \nDuring our study we reckoned that there were various \noutliers identified. They do not form a significant part of the \npattern of the activity, but they are important indicators of \npossible sensor faults or falls of the character\/subject. Thus, \nwe observed the time when they appeared and then we \ndiscovered that it is related to some special situations such as \nsensor failure, disconnection or whatever fatality. \nAccordingly, an important hypothesis was studied aiming to \nmodel such cases by new rules in order to get good event \nrecognition of any non-expected situation that takes place in \nthe observed character\/ subject. This event recognition can be \nvery useful in healthcare and ambient assisted living \napplications as well as in defense and sport. \n \nFig 5. a) left plot - Raw data for the activity \u2018Sitting\u2019; b) right plot - \nResultant clusters from processed raw data with EFS. \n \nFor the following experiment individuals were asked to fall \ndown in a mat after a pre-established circuit. Tree activities \nwere selected to be performed during the supervised circuit. \nThe individual performs 3 falls as long as he was executing \nthe required activity.  \nIn the following graphs (Figures 4-6) we can see the \nevolution of raw data when a fall is performed. Selected \nactivities were \u2018Walking\u2019 (Figure 4), \u2018Sitting\u2019 (Figure 5) and \n\u2018Climbing Stairs\u2019 (Figure 6) respectively. \n \n \n \nFig 6. a) Raw data for the activity \u2018Climbing stairs\u2019; b) right plot - \nResultant clusters from processed raw data with EFS. \n \nAll of these graphs have different waveforms as a result of \ndifferent motion actions performed. The event is easily visible \nin the graphs as a peak (maximum) of the function. \nNevertheless, in a real time application we have to know that \nthose data samples denote a maximum fluctuation in the wave, \nwhich means anomalous event during the course of the \nexperiment. However to achieve this aim, we have to consider \nsome meaningful indicators during the course of the \nmonitoring so that we can figure out rules for this maximum \naccording to an impact value per data sample. It is called \npotential (as mentioned above) which represents the data \ndensity and is the basis of the formation of new rules in the \nmodel\/classifier.  \n Motion events may appear in completely different forms \ndepending on the activity and also depending on the monitored \nsubject. It is important to emphasize those factors such as age, \ngender, life style and so on, influence the appearance of such \nevents. The reason of it is that is difficult or impossible to take \ninto account all such factors; hence the use of adaptive and \nevolving techniques such as EFS is critical for a well designed \napplication. The use of techniques based on fixed threshold \ncan be a weak approach which cannot be extended to real \nactivity conditions because it will be biased towards a specific \nperson or persons and never be generic (it will be subject- and \nproblem-specific).  \nAcceleration waveform usually changes when a fall has \ntaken place, because the physical posture of the monitored \ncharacter obviously changes as well as the normal and \ngravitational acceleration. \n In Figures 4-6 b) (right plots) there are clusters plots \nregarding raw data shown in Figures 4-6 a) (left plots). The \ncolours of each one of the clusters represent different motion \npatterns (the blue colour corresponds to the starting activity; \nred color corresponds to the data samples that are directly \nrelated with the anomalous event; and, finally, violet color \nrepresents the new activity which comes after the event has \nbeen performed).  \n In the case of \u2018Walking\u2019 most of data can be found between \ntwo well delimited cluster centers (see Figure 4b). \nNevertheless, when a fall takes place, data start to come with a \n2658\n slight increase of acceleration. This new data establishes a \nnew rule in the model\/classifier that is interpreted as new \nphysical posture. It is important to note that a new physical \nposture comes after an event. If the change of posture has not \nbeen soft enough and points with considerable potential came \npreviously, it might mean a significant event inside the model.  \n On the other hand, during \u2018Climbing stairs\u2019 the fall that was \nperformed by the character\/subject reached a greater level of \ncomplexity. Therefore, in this case two clusters represent the \nevent (see Figure 6b). \nIn order to reflect the appearance of a new event new rules \nare formed around new prototypes as described above. They \nhelp the overall classifier to be tuned to the new data pattern.  \nIf an outlier data point has been discovered, it could be an \nanomalous behavior. Therefore, we check the local \ndensity\/potential to identify the local significance of this \nchange of the motion data pattern and then to be able to \nidentify a new event or even to classify the data as an outlier \n(possibly due to sensor fault. When the subject is moving the \nwindow of time move through, the local density\/potential \ncalculated can be larger than 1s. In general, it varies between 1 \nand 3s. In this time a new rule is formed as a result of the new \nacceleration scheme induced by the postural change. Having \ninto account the coming out of an outer data point and a rule in \nthe next second, this is enough information to consider that an \nevent such a fall has taken place. \nIX. CONCLUSION \nTo sum it up, real-time evolving classifier (eClass) has been \nused in this research to recognize activities from two-axial \naccelerometer data, collected by smart sensor nodes. Results \nwere very successful, getting almost a 99% by delineating 3 \nunrelated activities. For example regarding Running, Walking, \nRunning, Sitting or Climbing Stairs, acceleration patterns in \nconnection with gravity were easily identifiable after a short \nperiod of pre-training. However, there are complex activities \nsuch as Cycling, Tiding, Cleaning, Exercising etc. which are \ncomprised of several another basic activities that are not easy \nto recognize in real-time after a short pre-training time. We \nalso aim at classifying these complex activities, so next \nresearch steps would be to study in depth the transitions \nbetween different activities in order to work out delimited \nstates that determine those complex activities. Another \ndirection of future research is linked to collaborative \nclassifiers in a network of diversified smart sensors. \nREFERENCES \n[1] M. Kim \u201cADL Monitoring System Using FSR Arrays and Optional 3-\nAxix Accelerometer\u201d in Proc. 5597 Ambient Assistive Health and \nWellness Management in the Heart of the City. pp. 217-224, 2009. \n[2] L. Bao and S.S. Intille, \u201cActivity Recognition from user-annoted \nacceleration data\u201d Pervasive, pp.1-17, 2004. \n[3] J. Mantyjarvi, J. Himberg and T. Seppanen, \u201cRecognizing human \nmotion with multiple acceleration sensors\u201d IEEE Intern. Conf. on \nSystems, Man, and Cybernetics, pp. 747-752, 2001. \n[4] P. Angelov, \u201cEvolving Rule-based Models: A Tool for Intelligent \nAdaptation\u201d, Proc. 9th IFSA World Congress, Vancouver, BC, Canada, \n25-28, pp.1062-1067, July 2001. \n[5] P. Angelov, \u201cAn Approach for Fuzzy Rule-base Adaptation using On-\nline Clustering\u201d, International Journal of Approximate Reasoning, vol. \n35 , No 3, pp. 275-289, March 2004. \n[6] P. Angelov and D. Filev, \u201cAn Approach to On-line Identification of \nTakagi-Sugeno Fuzzy Models\u201d,  IEEE Trans. on  System, Man, and \nCybernetics, part B - Cybernetics, vol.34, No. 1, pp. 484-498, 2004. \n[7] J. Andreu, J. Vi\u00fadez and J.A. Holgado, \u201cAn Ambient Assisted-Living \nArchitecture based on Wireless Sensor Networks.\u201d Advances in Soft \nComputing, Vol 51, pp. 234-248, 2008. \n[8] J. Andreu, J.A. \u00c1lvarez, A. Fern\u00e1ndez-Montes and J. A. Ortega, \n\u201cService-Oriented Device Integration for Ubiquitous Ambient Assisted \nLiving Environments\u201d, International Work-Conference on Artificial \nNeural Networks IWANN. No. 10, pp. 843-850, 2009. \n[9] B. Logan, J. Healey, M. Philipose, E. Munguia Tapia and S.S. Intille: \u201cA \nLong-Term Evaluation of Sensing Modalities for Activity Recognition\u201d. \nUbicomp, pp. 483-500, 2007. \n[10] M. Buettner and R. Prasad \u201cRecognizing daily activities with RFID-\nbased sensors\u201d. Proc.of the 11th international conference on Ubiquitous \ncomputing, pp. 51-60, 2009.  \n[11] E. Haapalainen, P. Laurinen  and P. Siirtola, \u201cExercise energy \nexpenditure estimation based on acceleration data using the linear mixed \nmodel\u201d. Information Reuse and Integration,  pp. 131-136, 2008.  \n[12] P Siirtola, \u201cClustering-based activity classification with a wrist-worn \naccelerometer using basic features\u201d. Proc. IEEE Symposium on \nComputational Intelligence and Data Mining. pp. 95-100, 2005. \n[13] Sun Microsystem's SunSPOT. Available: http:\/\/www.sunspotworld.com \n[14] P. Angelov and A. Kordon, \u201dAdaptive Inferential Sensors based on \nEvolving Fuzzy Models: An Industrial Case Study\u201d.  IEEE Trans. on \nSystems, Man and Cybernetics \u2013 part B, Cybernetics, 2010. \n[15] P. Angelov and X. Zhou, \u201cEvolving Fuzzy Classifier for Real-time \nNovelty Detection and Landmark Recognition by a Mobile Robot\u201d.  \nStudies in Computational Intelligence Series, pp. 95-124, 2007.  \n[16] M. Everett, P. Angelov, EvoMap: On-Chip Implementation of Intelligent \nInformation Modelling using EVOlving MAPping,  Lancaster, UK: \nLancaster University, 2005. \n[17] P.  Angelov and X.  Zhou, \u201cEvolving Fuzzy-Rule-based Classifiers from \nData Streams\u201d, IEEE Trans. on Fuzzy Systems, vol. 16, No. 6, pp.1462-\n1475, Dec. 2008. \n[18] T. Hastie and R. Tibshirani, and J. Friedman, The Elements of Statistical \nLearning: Data Mining, Inference and Prediction. Ed. Heidelberg, \nGermany: Springer Verlag, 2001.  \n[19] J. Pearl and S. Russell, \"Bayesian Networks\". Handbook of Brain \nTheory and Neural Networks. pp. 157\u2013160. USA: MIT Press, 2001. \n[20] R.E. Schapire. \u201cThe boosting approach to machine learning: an \noverview. Nonlinear Estimation and Classification\u201d, Lecture Notes in \nStatist., Vol. 171, pp 149-171. 2003. \n[21]  J.  Ross, C4.5: Programs for Machine Learning, Morgan Kaufmann, \n1993. \n[22] J.  Rodriguez, \u201cRotation Forest: A New Classifier Ensemble Method\u201d. \nIEEE Transactions on Pattern Analysis & Machine Intelligence Vol. 28, \npp. 1619-1630, 2006. \n[23] T. Hastie, S. Rosset, R. Tibshirani, J. Zhu, \u201cThe Entire Regularization \nPath for the Support Vector Machine\u201d, Journal of Machine Learning \nResearch , pp.1391-1414, 2004. \n2659\n"}