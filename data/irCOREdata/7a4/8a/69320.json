{"doi":"10.1023\/A:1026734014466","coreId":"69320","oai":"oai:eprints.lancs.ac.uk:27226","identifiers":["oai:eprints.lancs.ac.uk:27226","10.1023\/A:1026734014466"],"title":"Network Performance Implications of Variability in Data Traffic.","authors":["Roadknight, Chris","Marshall, Ian W.","Bilchev, George"],"enrichments":{"references":[{"id":989164,"title":"Enhancing the web\u2019s infrastructure: From caching to replication.","authors":[],"date":"1997","doi":null,"raw":"M. Baentsch, L. Baum, G. Molter, S. Rothkugel and P. Sturm.  Enhancing the web\u2019s infrastructure: From caching to replication.  IEEE Internet Computing. March 1997.  P. 18-27.[6]  L Breslau, P Cao, L Fan, G Phillips and S Shenker. \u2019Web Caching and Zipflike Distributions: Evidence and Implications.\u2019  IEEE Infocom \u201999. http:\/\/www.cs.wisc.edu\/~cao\/papers\/zipf-implications.html.","cites":null},{"id":989766,"title":"Estimators for long-range dependance: an empirical study.\u2019 Fractals.","authors":[],"date":"1995","doi":"10.1142\/S0218348X95000692","raw":"M Taqqu, V Teverovsky and W Willinger.  \u2019Estimators for long-range dependance: an empirical study.\u2019  Fractals. Vol 3, No. 4 (1995) 785-788.","cites":null},{"id":990402,"title":"File popularity characterisation\u2019.","authors":[],"date":null,"doi":"10.1145\/346000.346014","raw":"C Roadknight, I Marshall and D Vearer. \u2019File popularity characterisation\u2019. Second Workshop on Internet Server Performance in conjunction with ACM SIGMETRICS 99. http:\/\/www.cc.gatech.edu\/fac\/Ellen.Zegura\/wisp99\/accepted.html","cites":null},{"id":990956,"title":"Generating Representative Web Workloads for Network and Server Performance Evaluation,\u2019","authors":[],"date":null,"doi":"10.1145\/277858.277897","raw":"P. Barford and M. E. Crovella, \u2019Generating Representative Web Workloads for Network and Server Performance Evaluation,\u2019 in Proceedings of Performance \u201998\/ACM SIGMETRICS \u201998, pp. 151-160, Madison WI.","cites":null},{"id":988603,"title":"Linking cache performance to user behaviour\u2019,","authors":[],"date":"1998","doi":null,"raw":"I Marshall, C Roadknight, \u2019Linking cache performance to user behaviour\u2019, \u2019Computer Networks and ISDN systems\u2019 30 (1998), pp.2123-2130.","cites":null},{"id":991241,"title":"Modelling and Performance Analysis of Cache Networks. Fifteenth Annual UK Performance Engineering Workshop.","authors":[],"date":"1999","doi":null,"raw":"G Bilchev, I Marshall, C Roadknight and S Olafsson.  Modelling and Performance Analysis of Cache Networks.  Fifteenth Annual UK Performance Engineering Workshop. 1999. P367-378.","cites":null},{"id":988472,"title":"On the effect of traffic self-similarity on network performance. Performance and Control of Network Systems, The International Society for Network Engineering.","authors":[],"date":"1997","doi":null,"raw":"K Park, G Kim and M Crovella.  On the effect of traffic self-similarity on network performance.  Performance and Control of Network Systems, The International Society for Network Engineering.  (1997)  3231: 296-310.","cites":null},{"id":988249,"title":"Self-Similarity through high variability: Statistical analysis of ethernet LAN traffic at source level.","authors":[],"date":"1995","doi":null,"raw":"W Willinger, M Taqqu, R Sherman and D Wilson.  Self-Similarity through high variability:  Statistical analysis of ethernet LAN traffic at source level. Proceedings of SIGCOMM \u201995 (1995) p. 100-113","cites":null},{"id":990081,"title":"Statistical methods for data with long-range dependence\u2019.","authors":[],"date":"1992","doi":"10.1214\/ss\/1177011122","raw":"J Beran.  \u2019Statistical methods for data with long-range dependence\u2019.  Statistical Sciences 1992, Vol. 7, No. 4, 404-427.","cites":null},{"id":989529,"title":"Web Caching and Zipflike Distributions: Evidence and Implications.\u2019","authors":[],"date":null,"doi":"10.1109\/INFCOM.1999.749260","raw":"L Breslau, P Cao, L Fan, G Phillips and S Shenker. \u2019Web Caching and Zipflike Distributions: Evidence and Implications.\u2019  IEEE Infocom \u201999. http:\/\/www.cs.wisc.edu\/~cao\/papers\/zipf-implications.html.","cites":null},{"id":988881,"title":"Web Proxy Caching: The Devil is","authors":[],"date":"1998","doi":null,"raw":"R Caceres, F Douglis, A Feldmann, G Glass and M Rabinovich.  Web Proxy Caching: The Devil is in the Detail.  1st Workshop on Internet Server Performance in conjunction with ACM SIGMETRICS.  1998.  P111-118","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2000-04","abstract":"World Wide Web (WWW) traffic will dominate network traffic for the foreseeable future. Accurate predictions of network performance can only be achieved if network models reflect WWW traffic statistics. Through analysis of usage logs at a range of caches it is shown that WWW traffic is not a Poisson arrival process, and that it displays significant levels of self-similarity. It is also shown for the first time that the self-similar variability extends to demand for individual pages, and is far more pervasive than previously thought. These measurements are used as the basis for a cache-modelling tool-kit. Using this software the impact of the variability on predictive planning is illustrated. The model predicts that optimisations based on predictive algorithms (such as least recently used discard) are likely to reduce performance very quickly. This means that, far from improving the efficiency of the network, conventional approaches to network planning and engineering will tend to reduce efficiency and increase costs","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/69320.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/27226\/1\/27226.pdf","pdfHashValue":"a6171b5b22179b32b4cc6e9adc812e0a63813b8f","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:27226<\/identifier><datestamp>\n      2018-01-24T02:50:34Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Network Performance Implications of Variability in Data Traffic.<\/dc:title><dc:creator>\n        Roadknight, Chris<\/dc:creator><dc:creator>\n        Marshall, Ian W.<\/dc:creator><dc:creator>\n        Bilchev, George<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        World Wide Web (WWW) traffic will dominate network traffic for the foreseeable future. Accurate predictions of network performance can only be achieved if network models reflect WWW traffic statistics. Through analysis of usage logs at a range of caches it is shown that WWW traffic is not a Poisson arrival process, and that it displays significant levels of self-similarity. It is also shown for the first time that the self-similar variability extends to demand for individual pages, and is far more pervasive than previously thought. These measurements are used as the basis for a cache-modelling tool-kit. Using this software the impact of the variability on predictive planning is illustrated. The model predicts that optimisations based on predictive algorithms (such as least recently used discard) are likely to reduce performance very quickly. This means that, far from improving the efficiency of the network, conventional approaches to network planning and engineering will tend to reduce efficiency and increase costs.<\/dc:description><dc:date>\n        2000-04<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/27226\/1\/27226.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1023\/A:1026734014466<\/dc:relation><dc:identifier>\n        Roadknight, Chris and Marshall, Ian W. and Bilchev, George (2000) Network Performance Implications of Variability in Data Traffic. BT Technology Journal, 18 (2). pp. 151-158. ISSN 1573-1995<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/27226\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1023\/A:1026734014466","http:\/\/eprints.lancs.ac.uk\/27226\/"],"year":2000,"topics":["QA75 Electronic computers. Computer science"],"subject":["Journal Article","PeerReviewed"],"fullText":"Network performance implications of multi-\ndimensional variability in data traffic\nChris Roadknight, Ian Marshall and George Bilchev.\nBT Adastral Park, Martlesham Heath, SUFFOLK, IP5 3RE, UK\n{christopher.roadknight, ian.w.marshall, george.bilchev}@bt.com\nAbstract.\nWWW traffic will dominate network traffic for the foreseeable future.  Accurate\npredictions of network performance can only be achieved if network models reflect\nWWW traffic statistics.  Through analysis of usage logs at a range of caches we\nconfirm that WWW traffic is not a Poisson arrival process, and that it shows\nsignificant levels of self-similarity. We show for the first time that the self-similar\nvariability extends to demand for individual pages, and is far more pervasive than\npreviously thought. These measurements are used as the basis for a cache modelling\ntoolkit. Using this software we illustrate the impact of the variability on predictive\nplanning.  The model predicts that optimisations based on predictive algorithms (such\nas least recently used discard) are likely to reduce performance very quickly.  This\nmeans that far from improving the efficiency of the network, conventional approaches\nto network planning and engineering will tend to reduce efficiency and increase costs.\nKeywords Web traffic, self-similarity\n1 INTRODUCTION\nApproximately half of the traffic carried by the worlds major telecommunication\nnetworks is now data traffic originating from modems and LANs.  Extrapolating\ncurrent growth trends we can predict that this \u2019Internet\u2019 traffic will represent 90% of\nthe total within 5 years.  The World-Wide Web currently generates over 80% of the\n\u2019Internet\u2019 traffic and is likely to remain the dominant traffic source for some time.\nNetwork performance is thus likely to be dominated by web traffic statistics and web\nperformance for much of the coming decade.  Current traffic models are based on\nPoisson arrival processes, which are a good fit to observed telephony traffic, but data\ntraffic does not fit a Poisson arrival process.  Over the last decade evidence has been\naccumulating that data traffic (and web traffic in particular) exhibits a range of long\nrange dependencies [1,2,3], which manifest in the traffic as burstiness that does not\naverage out when the traffic is multiplexed.  This burstiness can persist to extremely\nlong timescales.  To accurately predict quality of service and performance levels in a\nnetwork dominated by data traffic a seismic shift in modelling approaches will\ntherefore be required. Pending development of accurate traffic models a more\nheuristic approach has been applied.\nA large proportion of web objects are static [4], so caching popular files nearer to\nthe users reduces demand on remote network devices, and minimises the probability\nof overload at bottlenecks due to bursts in demand for a particular object. WWW\ncaches have therefore been deployed globally in an effort to decrease the load on\nnetwork and server hardware [5]. Subsequently many efforts have focused on the\neffectiveness of cache replacement algorithms [6], in an effort to minimise the cost of\nthe caches and maximise their scalability. However the studies are based on\nassumptions of independence at large timescales.  To validate or refute the\nassumptions there is a need to understand the long-term temporal characteristics of\nthe traffic load in greater detail.\nFor example, self-similarity is the property of a series of data points to retain a\npattern or appearance regardless of the level of granularity used and is the result of\nlong range dependence (LRD) in the data series. A system is said to be long range\ndependent if the auto-covariance function decays hyperbolically, a memory-less\nprocess decays exponentially.  Fractal patterns are the clearest exhibition of this\nproperty, where regardless of how much one zooms into (or out of) a pattern, the\nbasic appearance remains fundamentally unchanged.  A truly fractal demand pattern\nfor a Web object would imply the possibility of bursts in demand, similar to the\nlargest observed burst, occurring at arbitrarily large time intervals.  It also implies that\nhowever long the measurement interval there is no guarantee that representative burst\nstatistics for an object have been observed.  The implications for predictive planning\nalgorithms (such as cache replacement) are potentially very serious.  Of course real\nworld data cannot be expected to precisely match the mathematical definitions, but if\nself-similarity is observed between time intervals over several orders of magnitude\nthis would be sufficient to significantly impact network performance.\nIn this paper we provide an analysis of the temporal properties of Web traffic and\ntheir performance impacts. In particular we show that Web traffic is self-similar over\na very large timescale range, and that individual page popularity is far more bursty\nthan has previously been assumed.  We have built an analytic model based on our\nobservations, and present some performance predictions of the model that confirm the\nsignificant impact of the observed temporal behaviours.\n2 DETERMINISTIC BEHAVIOUR IN WEB TRAFFIC\nTo obtain meaningful analysis results it was important to use recent data sets of\nhigh quality and covering a significant time and request period for a substantial\nnumber of users.  We were lucky to be able to use details of individual requests from\nseveral user groups, the most frequently used being the request logs from NLANR\n(ircache.nlanr.net\/Cache\/Statistics\/) and EduWeb, an Internet service for use by\nteachers and pupils in the U.K (www.eduweb.co.uk\/).\n2.1 Self-Similarity.\nHit rate, the % of file requests served from the cache, was chosen as a metric of the\ncommunity\u2019s behaviour.  This single metric is useful as it gives some idea of possible\nsimilarities in behaviour and also a very important metric for evaluating the\nperformance of a cache.  Plotting the hit rate variance, Var(x(m)), against the\naggregation level of the signal, m, on a log-log plot will result in points falling on a\nstraight line fit if the process x, under consideration, exhibits the property of self-\nsimilarity, that straight line should decay at a rate slower than m-1, with the decay rate,\nm\n-\u03b2\n (that is, the slope of the straight line, -\u03b2) giving an estimate of the Hurst\nparameter,  H = 1 \u2013 \u03b2\/2.  For completely self-similar processes H equals 1, but a value\nof between 0.5 and 1 points to an increasing amount of underlying self-similarity.\ny  =  5 .7 6 37 x-0 .4 3 5 4\nR 2 =  0 .9 9 8 6\n0 .1\n1\n1 0\n1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0\nA n a ly s is  in te rv a l (m in )\nFigure 1. Aggregate Variance Plot Showing Self-Similarity Over 4 Orders of\nMagnitude.\nNormally, self-similarity is estimated using an aggregate variance method [7,8].\nHowever this technique is only accurate for stationary data sets, and it is known that\ncache hit rates exhibit a diurnal variation.  The data sets were therefore normalised\n(by subtracting a suitable moving average) and also analysed using a wavelet\nestimator [9].  The benefit of wavelet estimators is that they require fewer\nassumptions about the data and are relatively unaffected by periodic behaviour or\nsystematic noise in the data. The method was applied to long term data sets from the\nNLANR-LJ cache and the EduWeb cache.  Fig 1 shows clearly that significant self-\nsimilarity can be observed over at least 4 orders of magnitude at the NLANR-LJ\ncache.  The exponent of -0.435 equates to a Hurst parameter of  ~0.78, which is high\nenough to suggest a large amount of self-similarity is influencing the data [8].  This\nwas confirmed by wavelet estimation giving H = 0.805 \u00b1 0.012.  These results were\nfurther validated by using daily hit rate statistics for an 18 month period at the\nNLANR caches (available at http:\/\/ircache.nlanr.net\/Cache\/ Statistics \/Reports\/) to\ngenerate aggregate variance plots.  This analysis is less accurate, so the predicted\nHurst parameter of up to 0.923 is less reliable.  However, the longer sampling period\nindicated self-similarity extends to periods of at least 54 days.  Similar results were\nobserved at the EduWeb cache, where we observed H = 0.861 \u00b1 0.046, using log file\ndata covering 6 weeks.  It is likely that the self-similarity at EduWeb extends to\nlonger timescales, but insufficient data was available to confirm this.\nGiven that self-similarity is observed in web traffic, what factors could be the\ncauses for this? Long-range dependence can only arise from a process, or set of\nprocesses, which have memory, so that a stimulus that occurs at one moment may\ntrigger another related event at another later time. Several causes have been proposed\nfor this including heavy tailed distributions for session lengths [3], file lengths [4],\nand packetization.  However, a further cause is required to explain the long timescale\ndependency we have observed.  With the notable exceptions of web spiders and robot\nassisted browsing, all web requests emanate from a human user.  We have shown [5]\nthat individual user traces can exhibit long-range dependent behaviour. Figure 2\nshows some examples.  It is thus extremely likely that one of the contributing factors\nis the memory of the users, which clearly persists over the required timescale of\nseveral months and may be aided by memory aids such as bookmarks.\n1 0\n1 0 0\n1 0 0 0\n1 0 0 0 0\n1 0 1 0 0 10 0 0 10 0 0 0\nR e q ue s ts\nB T  C lie n t\nH  =  0 .7\nH it ra te  ~2 4 %\nB U  C lie nt\nH  =  0.5 5\nA u to  h it ra te  ~  65 %\nR M P L C  C lie n t\nH  =  0.6\nH it R a te  ~ 1 6 %\nR M P L C  C lie n t\nH  = 0 .6 6\nH it R a te  ~ 4 0 %\nFigure 2.  Single users also exhibit signs of self-similarity.\n2.2  Temporal Request Dynamics.\nThe observed self-similarity in hit rate also immediately suggests that page\npopularity is highly dynamic, contrary to simplifying assumptions made in some\nmodels (e.g. [6]).  To verify this we have examined the temporal variation in demand\nfor individual pages. Figure 3 shows the popularity of some typical websites (their\nranking on that day) over 3 weeks at an Australian university cache (eg.\nhttp:\/\/squid.latrobe.edu.au\/usage\/days\/hosts\/proxy.895.total-remote-hosts.html).  The\nfigure clearly illustrates the range of variation in demand one can expect for popular\npages over the course of a three week period.\n0\n50\n100\n150\n200\n250\n300\n350\n400\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21\nTime (days)\nwww-aus.cricket.org\nwww.comsec.com.au\nad.au.doubleclick.net\nhome.microsoft.com\nFigure 3.  Examples of Temporal Dynamics at 4 Sites.\nIt should be apparent that the popularity of each site over time varies in different\nways.  See how:\n1.  A cricket site\u2019s popularity seems very erratic, possibly varying with occurrence of\nimportant matches.\n2.  Microsoft\u2019s site seems consistently popular demonstrating its time independent\nnature.\n3.  Doubleclick is an \u2019adbanner\u2019 site where automatic requests are made to from a\nlarge selection of web sites so is stable over time.\n4.  A share dealing site (www.comsec.au.com) is much less popular at weekends\nwhen share dealing is not possible.\nThese are some simple examples.  It is expected that most page dynamics will be\nmuch more complex than this, often with overlays of several influencing factors and\nwith less clear definitions of cause-effect relationships.\nWe automatically classified each file into one of 27 types judged on their degree\n(high, medium, low) of each of the three metrics, over a 25 day period.  The definition\nof high medium and low is presently subjective, but could be classified more\nrigorously given data sets from a wider range of caches.  We do not feel the precise\ncategorisation boundaries significantly affected the results.\nWe performed this analysis, covering the entire cache, based on 3 key time\ndependent metrics over the specified time period:\n1. Presence.  What fraction of days\/hours was a file requested at least once during\nthe period of analysis, typically 25 days. (Pres-low, Pres-med, Pres-high)\n2. Frequency or crossover rate.  How many times did the daily request rate cross\nthe average request rate (+ and - a 10% buffer zone) for the period. (Cross-low, Cross-\nmed, Cross-high)\n3. Amplitude.  This is the variance in daily request rate divided by the average\ndaily request rate for the file in question.  (Amp-low, Amp-med, Amp-high)\nFigure 4. File Grouping Based on 3 Key Temporal Characteristics.\nSampling a range of caches we have derived a mean value for the proportion of\npages in each category, illustrated in figure 4.  It is immediately apparent that the\nmajority of the popular pages (high presence) are not continuously popular as has\nbeen assumed in the past, and a significant proportion of the popular pages are not\nrequested on every day (medium and low presence).\nPres-low pres-med\npres-high\n0\n20\n40\n60\n80\n100\nAmp-low \nPres-low\npres-med\npres-high\nAmp - med \nPres-low\npres-med\npres-high\nCross-low\nCross-med\nCross-high\nAmp - high\nIn addition to analysing the popularity of individual pages we analysed the\nvariability of the popularity curve.  The popularity curve is normally assumed to be a\nZipf curve where normalised probability of a request equals (popularity)\u03b1.  The\nexponent is a number close to -1, which is assumed to be constant.  In practice we\nobserved that \u03b1 varies in a self-similar way with a Hurst parameter of approximately\n0.8 at the caches we analysed. Figure 5 shows the aggregate variance plot for this\nseries of exponents and the calculated Hurst parameter for this plot (0.81), showing\nstrong signs of self-similarity.  This variability must also be factored into more\naccurate model of cache performance.\nN L AN R  - S V\ny =  0 .0006x -0 .3 81 9\nR 2 =  0 .97 58\n0.000 1\n0.001\n1 10 100\nag g regate  o ver x  d ays\nH  =  0 .81\nFigure 5. Self-similarity in the exponent of the popularity ranking curve\n3.  SYSTEMATIC CAUSES FOR DIFFERENCES IN\nPOPULARITY EXPONENT\nCache logs are used to analyse file popularity. A simple, least squares method was\nused to fit power law curves to the popularity curve (rank vs. popularity), and the\nlocality can then be equated to the slope (exponent) of the resulting curve.  The\nquality of the fit can then be checked using the standard R2 test.  This exponent seems\nthe best single metric to encapsulate request behaviour.  A simple examination of file\npopularity exponents at several caches over time reveals that, like hit rates, these\nexponents are not universal, different caches have different exponents and these\ndifferences are significant [9,10].   These exponents are always between -0.5 and -1\nbut have a value that is specific.\nCaches are arranged in hierarchies, some caches receive requests direct from the\nusers (eg, browser caches, some local caches), some receive requests from first level\ncaches and browser caches and so on to very high level caches that only take requests\nfrom other high level caches.  An examination of the exponents at these caches and\nthe physical position in the hierarchy suggests a direct relationship between position\nand exponent [9]: the popularity curve gets steeper (bigger negative exponent) as the\nposition of the cache gets \u2019nearer\u2019 the clients (table 1).  It has been shown that the\nrequest popularity curve for a single user over time, at the source of these requests, is\nclose to -1 [3], so it must be assumed that as intercepting caches are placed on the\nnetwork the popularity curve slope observed at these caches get increasingly shallow.\nCache Position Exponent R squared Error estimate\nNLANR - lj Highest -0.644 0.9897 \u00b10.024\nPISA Local -0.913 0.9807 \u00b10.038\nFUNET National -0.699 0.9883 \u00b10.046\nSPAIN National -0.724 0.9817 \u00b10.045\nRMPLC Local -0.858 0.9795 \u00b10.109\nTable 1.  Cache factors of Interest\nAnother approach was investigated that used data from a cache hierarchy\nvisualization tool called plankton (http:\/\/www.ircache.net\/Cache\/Plankton\/).  This\ntool gives information about the origin of all requests reaching the NLANR servers.\nFrom the number of requests presented on all the links and the topology of the node to\nnode links, it is possible to estimate the depth the NLANR cache over a time period.\nDepth is taken to mean the average number of caches (not including any browser\ncache) a request has passed through before it reaches the NLANR cache.\nFigure 6 shows a clear relationship between depth and exponent.  Only one graph\nis shown, but all best fit analysis from all NLANR caches showed a relationship of\nthe same direction, suggesting that the more intervening caches that a set of requests\ngo through the smaller the curve exponent is.  The simplest explanation for this is that\neach intervening cache is systematically filtering out request so as to make more\npopular files appear to be less popular, thus decreasing the slope of the popularity\ncurve.\nThese results show that locality can be characterized with a single parameter,\nwhich primarily varies with the topological position of the cache.  Accurate cache\nmodels can therefore be built without any need to consider cultural effects that are\nhard to predict.\nNLANR - PB\ny = 1.6916x-0.9895\n2\n2.2\n2.4\n2.6\n2.8\n3\n3.2\n3.4\n0.58 0.59 0.6 0.61 0.62 0.63 0.64 0.65\nPopularity Curve Exponent\nFigure 6.  Relationship Between Popularity Curve Slope and Cache Depth\nIt was also found that curves needed to be fitted to a large number of requests\n(>500 000) this is due to the self-similar nature of requests making conventional\nsampling regimes insufficient.\n4. CACHE MODEL\nThe starting assumption of our model is that users access the WWW via a proxy\ncache, i.e., all the url requests pass through the proxy cache and the proxy either\nprovides the data as a local copy or downloads it from the origin server. We then\ndevelop a model for a user community as seen from the proxy. Analysed log data\nsuggests that there are two types of user activity patterns \u2013 a daily activity pattern and\na stochastic component.\nThe daily activity pattern is modelled as a trend using a \u2018superposition\u2019 of periodic\nfunctions:\n{ })(max)(\n0),2sin(max)(\ntrendtrend\ntrend\ntyty\nd\nT\nt\ncbaty\nii\niiiii\n=\n\uf8fe\uf8fd\n\uf8fc\n\uf8f3\uf8f2\n\uf8f1\n++= pi\nwhere ia  is an amplitude shift, ib  is the amplitude, ic  is the frequency, id  is the\nphase and T is the period during which cyclic patterns are observed. Once the trend\nhas been approximated the stochastic component can be modelled as a Brownian\nmotion:\n\u03b7+\u2212= )1()( BMBM tyty\nSince the number of requested files is always non-negative we have to truncate a\nnegative value of )(BM ty  to zero. Also bursts in positive direction are higher than\nbursts in negative direction and to accommodate for this we define \u03b7 as:\n\uf8f4\uf8f3\n\uf8f4\uf8f2\n\uf8f1 >\n=\notherwise          \u2019\n0\u2019 if         \u2019\n\u03bb\n\u03b7\n\u03b7\u03b7\n\u03b7\nwhere ),0Norm(\u2019 \u03c3\u03b7 \u2208  and \u03bb is a parameter determining the ratio between the\nheights of the positive and negative bursts. The second modification also has the\neffect of reducing the number of times the series has to be truncated due to negative\nvalues.\nSince the stochastic component must be superimposed on the trend, a way of\n\u201cguiding\u201d the random walk of the Brownian motion towards the trend without\ndestroying the above described properties is needed. We suggest using a sequence of\nnon-overlapping random walks each starting from around the trend:\n),0()()( trendtrend \u03c3Normtkytky +\u2206=\u2206\n i.e., at each time step ,...2,1,0, =\u2206 ktk , a Brownian motion process begins for t\u2206\nsteps:\n\u03b7+\u2212+\u2206=+\u2206 )1()( BMBM mtkymtky\nwhere, 1,...,2,1 \u2212\u2206= tm . Then it stops and a new process begins. This completes our\nmodel of the intensity of the http requests. For the popularity distribution of the url\nrequests we have selected to use a Zipf\u2019s-like distribution, which conforms to our real\ndata analysis.\nAfter developing the user community model we proceed with the cache proxy\nmodel. The cache proxy is modelled by Web content expiry statistics and it also\nimplements a simplified caching algorithm. The expiry statistics models both the rate\nof change of Web pages (reflecting server assigned time-to-live or TTL) and cache\npurging due to stale data (i.e., cache assigned TTL). To account for caches of limited\nsize, the model also implements a replacement algorithm, e.g. the least recently used\n(LRU) replacement algorithm.\nThe simulation works as follows. The proxy cache receives requests for\nindividual files. It checks if the requested file has already been registered in the cache\nmodel before. If not, the file is time-stamped, registered in the cache and a miss is\nreported (i.e., this reflects downloading the file from the origin server and caching it).\nIf the requested file has been registered in the cache before, the proxy checks the\nTTL. If the file has expired the proxy verifies whether the file has changed. If so, a\nmiss is reported and the time stamp of the file is reset (this reflects downloading of\nthe new version of the file from the origin server). If the file has not changed, a hit is\nreported and again the time stamp is reset.\nOnce the single proxy has been modelled, we can build meshes of\ninterconnected proxy caches. To achieve this we have developed a simulation toolbox\nthat allows caches to be linked together to form various caching infrastructures.\nIn order to analyse the performance of a cache and to reliably compare it with other\ncaches, the following performance measures have been defined:\n\u2022 Request Hit Rate is a measure of the efficiency of the cache. It does not\ncorrespond one-to-one with the saved bandwidth since the requests are for files\nwith various sizes. The importance of this measure, however, stems from the fact\nthat opening an HTTP connection is a relatively expensive process comparable to\nthe actual time needed for smaller files to be transferred.\n[%]\nRequests of No. Total\nHits of No.\n  RateHit Request =\n\u2022 Byte Hit Rate is also a measure of the efficiency of the cache. It reflects the actual\namount of saved bandwidth.\n[%] \nData Requested Total  theof Volume\nCache in the Found Data Requested of  Volume\n  RateHit  Byte =\n\u2022 Saved Bandwidth is a measure of particular interest to network designers,\nshowing the actual effect of the cache on the network. It is defined as the volume\nof the requested data minus the volume of data not in the cache.\n5. PERFORMANCE IMPLICATIONS\nOur research efforts have shown that WWW traffic is self similar over many orders\nof magnitude.  There are probably several mechanisms causing the self-similarity,\neach of them acting on different time scales. Buffering and queuing behavior cause\nburstiness at very short time scales, less than the buffer residency time.  At timescales\nbetween 10s and an hour the self-similarity is primarily due to the heavy tailed\nproperties of session lengths and file sizes [11].  For longer timescales between an\nhour and 3 months the observed self-similarity probably relates to users revisiting\nuseful links (i.e. it originates in the persistent memory of users). This is shown most\nclearly in the bursty demand for specific pages and in traces of traffic from single\nusers that also exhibit self-similarity.  The self-similarity has important consequences\nfor the design of web servers and networks.  In particular, the advantages of caching\nare much greater than the average reduction in load one might deduce from the hit\nrate.  Distributing web content widely into caches offers significant performance\nimprovements to web users during demand bursts since the target server is rarely\noverloaded.  Caching also protects other network users from performance degradation\ndue to packet losses on a link that is unpredictably saturated by a burst in demand for\na particular web page.  On the other hand, since apparently unpopular pages can\nsuddenly experience a demand burst, the storage capacity of caches needs to be very\nlarge.  Also, as the bursts can be widely separated in time, the current practice of\nminimising disk usage by operating a cache replacement algorithm (such as least\nfrequently used) on an hourly basis leads to a rapid increase in the latency\nexperienced by end users\nWe have created an analytic model, section 4, of cache behaviour [12], that\nreproduces the statistical properties we have observed in real traffic logs.  This model\nsuccessfully predicted the weak dependence of the popularity exponent on\nhierarchical position that we report in section 3.  The model allows us to create a wide\nvariety of network scenarios, with different cache distributions and hierarchies and\ndifferent demand distributions.  We can then evaluate the performance of each\nscenario and predict an optimum hierarchy for a given traffic load.\nThe possible explanations for the observed changes in the popularity curve include:\n\u2666 Merging requests from communities with diverse interests\n\u2666 The filtering effect of lower level caches\n\u2666 Both of the above\nTo test the first possible explanation we have merged 500,000 file requests from\nNLANR and EduWeb and calculated the slope of the new popularity curve. If the\nhypothesis were likely to be true then the merged slope should be less steep than the\nslopes of the two comprising individual logs. The slope of NLANR was \u20130.703 and\nfor EduWeb: -0.858, which averages to \u20130.78. The slope of the merged file request\npopularity was \u20130.794, which is steeper than both NLANR\u2019s slope and the average,\nthereby implying that increasing a cache's geographical, demographic and cultural\ndiversity fails to significantly reduce the slope of the file popularity curve.  We have\nfurther verified these results with other experiments from Funet [http:\/\/www.funet.fi\/]\nand a simulated model [12]. This indicates that there is neither direct nor indirect\nevidence for the first hypothesis to be true.\nThe second hypothesis is more difficult to test with existing log data. Therefore,\nwe have set up a simulation experiment in which five first level caches are connected\nto a common second level cache.\nAll the first level caches exhibit popularity curve slope of \u20130.75. They also have\nthe same cache size, which we can vary during the set of experiments. We start with a\nlarge cache size (10% of the overall simulated domain, i.e. the number of simulated\nstatic web pages) and gradually reduce it. This results in reducing the filtering effect\nof the first level caches.  At cache sizes of 10%, 1%, 0.1% and 0.01% of the domain\nsize, we see exponents of -0.463, -0.602, -0.686 and -0.693.\nThese results show that the less filtering effect the first level caches possess, the\nbigger the slope of the observed popularity curve at the second level cache. This gives\nevidence that the second possible explanation holds true and the observed changes in\nthe popularity slope are largely due to the filtering effect of lower level caches.\nTo illustrate the impact of self-similarity on predictive algorithms we used the\nmodel to predict the performance of a single cache using a simple Least Recently\nUsed (LRU) cache replacement algorithm.  The results shown in figure 7 show how\neven small amounts of purging can have a significant effect of cache performance.\nAssuming a hit rate of 30%, a mean retrieval time from the target server of 4 seconds\nand a mean time to fetch from the cache of 500ms (suggested by end-user logs in our\nlaboratory) a reduction of 1% in hit rate results in an increase of 1.2% in mean\nwaiting time for the user.  We consider that saving a few dollars on disk capacity\nprobably does not justify the reduction in quality of service.\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0 10000 20000 30000 40000 50000\nNumber of files purged down to daily using LRU\nNB.  50 000 = no purging\nFigure 7. Predicted performance impact of predictive cache replacement\nalgorithm.\nThis result is not surprising. Very large sample sizes are needed to make\nreasonable predictions.  The cache we modelled was purging on a daily basis and was\nreceiving requests for about 2% of the total file pool per day.  Some large caches such\nas the NLANR cluster have similar request rates, so even here, predictions made on a\ndaily sample will not be accurate since the observed burstiness extends over a period\nof weeks.  Only a least frequently used algorithm using a sample period of 3 months\nis likely to deliver accurate results, and in this case the reduction in disk usage will be\nsmall.  Other predictive algorithms, such as those used in network dimensioning will\nface similar difficulties.\n5 CONCLUSIONS\nWe have found that many aspects of WWW traffic (the dominant component of\ndata traffic in todays networks) exhibit significant self similarity at large timescales.\nAn important causal factor being repetitive behaviour by users.  Many existing\nmodels make simplifying assumptions that do not fit our observations. We have\ntherefore introduced a new simulation model that attempts to account for all of the\nvariability we have observed.  The model has made accurate predictions of observed\nbehaviour.  The model has also been used to predict the impact of the observed self-\nsimilarity on predictive optimisations.  As might be expected from qualitative\narguments about the properties of fractal traffic, the model has demonstrated that\nalgorithms that predict behaviour in the current time interval by extrapolating from\nprevious time intervals are not useful in the presence of traffic that is self similar at\nthe timescale of the measurement interval.  Failure to recognise this can lead to a\nserious overestimation of the benefits of predictive optimisations and planning\nalgorithms.  Given the very long timescale self-similarity we have observed, the\nmeasurement intervals required to make accurate predictions will often need to be\nlonger than the desired response time of the network.  We therefore suggest that rapid\nnetwork responses should not be based on predictive approaches.  A better approach\nmight be to enable extensive load balancing to respond to the local overloads that\ncannot be predicted.\n6  REFERENCES\n[1]  W Willinger, M Taqqu, R Sherman and D Wilson.  Self-Similarity through\nhigh variability:  Statistical analysis of ethernet LAN traffic at source level.\nProceedings of SIGCOMM \u201995 (1995) p. 100-113\n[2]  K Park, G Kim and M Crovella.  On the effect of traffic self-similarity on\nnetwork performance.  Performance and Control of Network Systems, The\nInternational Society for Network Engineering.  (1997)  3231: 296-310.\n[3]  I Marshall, C Roadknight, \u2019Linking cache performance to user behaviour\u2019,\n\u2019Computer Networks and ISDN systems\u2019 30 (1998), pp.2123-2130.\n[4]   R Caceres, F Douglis, A Feldmann, G Glass and M Rabinovich.  Web Proxy\nCaching: The Devil is in the Detail.  1st Workshop on Internet Server Performance in\nconjunction with ACM SIGMETRICS.  1998.  P111-118\n[5]  M. Baentsch, L. Baum, G. Molter, S. Rothkugel and P. Sturm.  Enhancing the\nweb\u2019s infrastructure: From caching to replication.  IEEE Internet Computing. March\n1997.  P. 18-27.\n[6]  L Breslau, P Cao, L Fan, G Phillips and S Shenker. \u2019Web Caching and Zipf-\nlike Distributions: Evidence and Implications.\u2019  IEEE Infocom \u201999.\nhttp:\/\/www.cs.wisc.edu\/~cao\/papers\/zipf-implications.html.\n[7] M Taqqu, V Teverovsky and W Willinger.  \u2019Estimators for long-range\ndependance: an empirical study.\u2019  Fractals. Vol 3, No. 4 (1995) 785-788.\n[8] J Beran.  \u2019Statistical methods for data with long-range dependence\u2019.  Statistical\nSciences 1992, Vol. 7, No. 4, 404-427.\n[9]  C Roadknight, I Marshall and D Vearer. \u2019File popularity characterisation\u2019.\nSecond Workshop on Internet Server Performance in conjunction with ACM\nSIGMETRICS 99.\nhttp:\/\/www.cc.gatech.edu\/fac\/Ellen.Zegura\/wisp99\/accepted.html\n[10]  L Breslau, P Cao, L Fan, G Phillips and S Shenker. \u2019Web Caching and Zipf-\nlike Distributions: Evidence and Implications.\u2019  IEEE Infocom \u201999.\nhttp:\/\/www.cs.wisc.edu\/~cao\/papers\/zipf-implications.html.\n[11]  P. Barford and M. E. Crovella, \u2019Generating Representative Web Workloads\nfor Network and Server Performance Evaluation,\u2019 in Proceedings of Performance\n\u201998\/ACM SIGMETRICS \u201998, pp. 151-160, Madison WI.\n[12]  G Bilchev, I Marshall, C Roadknight and S Olafsson.  Modelling and\nPerformance Analysis of Cache Networks.  Fifteenth Annual UK Performance\nEngineering Workshop. 1999. P367-378.\n"}