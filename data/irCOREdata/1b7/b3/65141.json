{"doi":"10.1016\/j.ijforecast.2010.05.012","coreId":"65141","oai":"oai:dro.dur.ac.uk.OAI2:6773","identifiers":["oai:dro.dur.ac.uk.OAI2:6773","10.1016\/j.ijforecast.2010.05.012"],"title":"Group-based judgmental forecasting : an integration of extant knowledge and the development of priorities for a new research agenda.","authors":["Wright,  G.","Rowe,  G."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2011-01","abstract":"We review and integrate the extant knowledge on group-based forecasting, paying particular attention to the papers included in this special issue of the International Journal of Forecasting. We focus on the relative merits of different methods of aggregating individual forecasts, the advantages of heterogeneity in group memberships, the impact of others\u2019 opinions on group members, and the importance of perceptions of trust. We conclude that a change of opinion following group-based deliberation is most likely to be appropriate where the group membership is heterogeneous, the minority opinion is protected from pressure to conform, information exchange between group members has been facilitated, and the recipient of the advice is able \u2014 by reasoning processes \u2014 to evaluate the reasoning justifying the proffered advice. Proffered advice is least likely to be accepted where the advisor is not trusted \u2014 an evaluation which is based on the advisor having different perceived values to the recipient and being thought to be self-interested. In contrast, the outcome of a group-based deliberation is most likely to be accepted when there is perceived to be procedural fairness and the participants in the process are perceived to be trustworthy. Finally, we broaden our discussion of group-based forecasting to include a consideration of other group-based methodologies which are aimed at enhancing judgment and decision making. In particular, we discuss the relevance of research on small-group decision making, the nature and quality of the advice, group-based scenario planning, and public engagement processes. From this analysis, we conclude that, for medium- to long-term judgemental forecasting, a variety of non-outcome criteria need to be considered in the evaluation of alternative group-based methods","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/65141.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/6773\/1\/6773.pdf","pdfHashValue":"345f1f0b56a8f85cccf45329ffd50f39c81470d7","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:6773<\/identifier><datestamp>\n      2011-01-18T16:42:16Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Group-based judgmental forecasting : an integration of extant knowledge and the development of priorities for a new research agenda.<\/dc:title><dc:creator>\n        Wright,  G.<\/dc:creator><dc:creator>\n        Rowe,  G.<\/dc:creator><dc:description>\n        We review and integrate the extant knowledge on group-based forecasting, paying particular attention to the papers included in this special issue of the International Journal of Forecasting. We focus on the relative merits of different methods of aggregating individual forecasts, the advantages of heterogeneity in group memberships, the impact of others\u2019 opinions on group members, and the importance of perceptions of trust. We conclude that a change of opinion following group-based deliberation is most likely to be appropriate where the group membership is heterogeneous, the minority opinion is protected from pressure to conform, information exchange between group members has been facilitated, and the recipient of the advice is able \u2014 by reasoning processes \u2014 to evaluate the reasoning justifying the proffered advice. Proffered advice is least likely to be accepted where the advisor is not trusted \u2014 an evaluation which is based on the advisor having different perceived values to the recipient and being thought to be self-interested. In contrast, the outcome of a group-based deliberation is most likely to be accepted when there is perceived to be procedural fairness and the participants in the process are perceived to be trustworthy. Finally, we broaden our discussion of group-based forecasting to include a consideration of other group-based methodologies which are aimed at enhancing judgment and decision making. In particular, we discuss the relevance of research on small-group decision making, the nature and quality of the advice, group-based scenario planning, and public engagement processes. From this analysis, we conclude that, for medium- to long-term judgemental forecasting, a variety of non-outcome criteria need to be considered in the evaluation of alternative group-based methods.<\/dc:description><dc:publisher>\n        Elsevier<\/dc:publisher><dc:source>\n        International journal of forecasting, 2011, Vol.27(1), pp.1-13 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2011-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:6773<\/dc:identifier><dc:identifier>\n        issn:0169-2070<\/dc:identifier><dc:identifier>\n        doi:10.1016\/j.ijforecast.2010.05.012 <\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/6773\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1016\/j.ijforecast.2010.05.012<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/6773\/1\/6773.pdf<\/dc:identifier><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["0169-2070","issn:0169-2070"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2011,"topics":[],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n10 May 2010\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nWright, G. and Rowe, G. (2011) \u2019Group-based judgmental forecasting : an integration of extant knowledge\nand the development of priorities for a new research agenda.\u2019, International journal of forecasting., 27 (1). pp.\n1-13.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1016\/j.ijforecast.2010.05.012\nPublisher\u2019s copyright statement:\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\n1 \n \nGroup-based judgmental forecasting: an integration of extant knowledge and the \ndevelopment of priorities for a new research agenda \n \nGeorge Wright \nDurham Business School \nMill Hill Lane \nDurham  \nDH1 3LB \nUK \ne-mail: george.wright@durham.ac.uk \n \n \n \nGene Rowe \nInstitute of Food Research \nNorwich Research Park \nColney \nNorwich  \nNR4 7UA \nUK \ne-mail: gene.rowe@bbsrc.ac.uk \n \n \n \n \n2 \n \n \nAbstract \n \nWe review and integrate extant knowledge on group-based forecasting, paying special \nattention to the papers included in this special issue of the International Journal of \nForecasting. We focus on the relative merits of different methods for aggregating \nindividual forecasts, the advantages of heterogeneity in group membership, the impact \nof others\u201f opinions on group members, and the importance of perceptions of trust. We \nconclude that opinion change after group-based deliberation is most likely to be \nappropriate where group membership is heterogeneous, minority opinion is protected \nfrom pressure to conform, information exchange between group members has been \nfacilitated, and the recipient of advice is able \u2013 by reasoning processes \u2013 to evaluate \nthe reasoning justifying proffered advice. Proffered advice is least likely to be \naccepted where the advisor is not trusted \u2013 indicated by having different perceived \nvalues to the recipient of the advice and being thought to be self-interested. In \ncontrast, the outcome of a group-based deliberation is most likely to be accepted when \nthere is perceived procedural fairness and the participants in the process are perceived \nas trustworthy. Finally, we broaden our discussion of group-based forecasting to \ninclude consideration of other group-based methodologies aimed at enhancing \njudgment and decision making. In particular, we discuss the relevance of research on \nsmall-group decision making, the nature and quality of advice, group-based scenario \nplanning, and public engagement processes. From this analysis, we conclude that, in \nmedium- to long-term judgemental forecasting, a variety of non-outcome criteria need \nto be considered in the evaluation of alternative group-based methods. \n \n3 \n \n \nGroup-based judgmental forecasting: an analysis of extant knowledge and the \ndevelopment of priorities for a new research agenda \n \nGroup-based forecasting can be achieved in many ways. The simplest is to average \nindividual opinions and take the achieved figure as the forecast. Alternatively, \nindividuals can meet to discuss the issue in groups \u2013 either with or without some \nformal structure to the process. Unstructured group processes are historically typical, \nand seen as a benchmark for judging the performance of structured methods. Research \nhas demonstrated how various social factors can undermine good forecasting (and \ndecision making in general) in unstructured groups, and hence identified the need to \nsomehow control human interaction to pre-empt or ameliorate these factors. Within \none such structured-interaction technique - the Nominal Group Technique (NGT) - \nindividuals first make personal forecasts, then the group members meet to discuss the \nforecast problem, and finally the individuals are given the opportunity to revise their \nearlier forecasts (Van de Ven & Delbecq, 1974). The average of these revised \nopinions can be taken as the group forecast. The structure of the Delphi technique is \nsimilar to that of the NGT \u2013 with the exception that the group members exchange \ntheir initial forecasts anonymously with other group members and are given the \nopportunity to revise their individual forecasts over several Delphi rounds, with the \nfinal round average taken as the group forecast (Rowe, Wright, & Bolger, 1991). \nOther group\u2013based methods include role-playing \u2013 in which individuals simulate the \ninteraction of groups in conflict situations in order to understand\/predict how the \nconflict is likely to resolve itself (Green, 2002). Recently, prediction markets have \n4 \n \nbeen studied to see if the evolving monetary-derived predictions - produced by self-\ninterested participants - are accurate (Berg, Nelson, & Rietz, 2008).  \n \nThe structured methods vary in terms of the amount and type of information that is \nexchanged between group members and the process by which the information \nexchange is managed. The interaction of these aspects ultimately impacts upon the \ndegree to which the \u201cadvice\u201d of others is integrated with the group members\u201f own \nopinions. Another important feature affecting the quality of forecasts produced by the \ndifferent methods is the extent to which group membership is relatively homogeneous \nor heterogeneous. Finally, some group-based methods are more acceptable to group \nmembers than others \u2013 a practical aspect that is often overlooked in choosing which \nforecasting method to use in any particular situation.  \n \nIn this article, we integrate the contributions of the authors in this special issue of \nInternational Journal of Forecasting toward addressing these key themes in group-\nbased forecasting research, before summarising some general issues for future \nresearch to consider. Finally, we contextualise the current forecasting research with \nrespect to other research areas of relevance. We first consider the issue of aggregating \nindividual forecasts. \n \nAggregating individual forecasts \nIn the first article, Kerr & Tindale (2011) focus on how to aggregate individual \nopinions to achieve an accurate group-based judgment. They distinguish between \njudgment in intellective tasks, in which deduction of the (already-existing) truth is the \nfocus of attention, and judgmental forecasting, in which the forecasters can only \n5 \n \nexplain and defend their judgments, since the outcome has not yet occurred. This \nreview suggests that pre-existing majority opinions generally determine group \nconsensus decisions in judgmental forecasting tasks, arguing that only in intellective \ntasks, where those group members who favour the correct answer can explain or \ndemonstrate why they are correct, is a correct minority likely to be persuasive. \nBecause of the disproportionate power of majority opinion to determine the \nsubsequent group position in judgmental forecasting tasks, Kerr and Tindale argue \nthat only those aggregation methods that facilitate information exchange between \ngroup members are likely to be beneficial over-and-above a statistical averaging of \nprior opinions. Such information exchange in both face-to-face groups and in \nstructured group interaction \u2013 such as in Delphi and nominal groups \u2013 provides the \nenabling conditions for group members to recognise errors in justifications of \njudgments. Delphi has the advantage of attenuating any social anxiety about \npublically identifying other group members\u201f errors. However, face-to-face groups \nmay enhance individual task motivation if individual effort and accuracy is \nidentifiable. Both face-to-face groups and Delphi can, under the right circumstances, \nprovide perceived procedural fairness to all group members who participate, whilst \nDelphi can protect a minority from group pressure to conform to the majority \nviewpoint to preserve group harmony \u2013 since, within unstructured groups, social \ngoals, such as maintaining group harmony, may conflict with generating the best \npossible judgmental forecast. \n \nIn the next section, we focus on the usefulness of having heterogeneous group \nmembership. As we shall see, hetrogeneity can be enhanced by role-playing. \n \n6 \n \nThe advantages of heterogeneity in group membership \nThe article by Yaniv (2011) reports an empirical study on susceptibility to framing \neffects as a measure of judgment quality. Yaniv labelled as homogeneous those \ngroups made up of individuals that had been assigned to the same framing \nmanipulation within a version of the Kahneman and Tversky\u201fs classic Asian Disease \nproblem (Kahneman & Tversky, 1984). By contrast, the heterogeneous groups were \nmade up of individuals with a mix of prior frames for a formally identical decision \nproblem. Yaniv demonstrated that subsequent discussion within the two types of \ngroup led to either (i) intensification of framing bias in the homogeneous groupings \nand (ii) attenuation of framing bias in the heterogeneous groupings. He argues that the \ninvalid consensus in homogeneous groupings may serve to increase group-based \nconfidence but decrease judgmental accuracy. From this perspective, assignment of \ndifferent role-playing responsibilities to individuals can artificially create useful \nheterogeneity and thus attenuate conformity to spurious majority opinion.  \n \nOnkal, Lawrence, & Sayim (2011) follow up such a suggestion in their own study of \n\u201cmodified consensus groups\u201d \u2013 where individual group members role-play functional \nspecialists in marketing, production, and forecasting. In their study, these specialist \nrole-holders provided independent individual forecasts that were then statistically \naveraged. Participants were instructed to act out their assigned roles as they believed \nthese would be realised in real-life.  The experimental materials were sets of time-\nseries data for product sales. A pure model-based statistical forecast was also \nprovided to the group for discussion and, finally, the role-players were asked to accept \nor adjust this model-based prediction to reach a finally-agreed group-based forecast. \nOnkal, Lawrence, and Sayim reached the overall conclusion that this group-based \n7 \n \nprocess tended to improve on the accuracy of both (i) the statistical average of the \nindividual-member forecasts and (ii) the pure model-based forecast.  \n \nGreen & Armstrong\u201fs (2011) article studies the worth of inviting individuals to \u201cstand \nin the other person\u201fs shoes\u201d. Would this invective give those individuals useful \ninsights into the quality of their initial intuitive judgments?  Green and Armstrong\u201fs \nfocus was on forecasting the outcomes of conflict situations - with participants \ninstructed to indicate \u201cwhich decision you think that each party in the situation would \nprefer to be made and how likely is it that each party\u201fs decision will actually occur\u201d.  \nBut such invective to engage in \u201crole-thinking\u201d proved no more accurate than \nguessing the outcomes of the conflict situations \u2013 even for a range of non-\nundergraduate (\u201eexpert\u201f) respondents. In contrast, when students were required to \nbecome more engaged with the conflict situations - by \u201crole-playing\u201d simulated \ninteractions between participants in the conflicts \u2013 the predictive accuracy of the post-\nrole participants\u201f judgements reached 90%. \n \nAll three of these studies therefore speak in one way or another to the need for \nheterogeneity in groups to aid forecasting \u2013 and of the value of using role-playing to \ncreate artificial heterogeneity when this does not exist. \n \nIn the next section, we turn our focus to the impact of others\u201f opinions (i.e., advice) \non opinion revision. Most of the research has been conducted within the Judge-\nAdvisor System (hereafter, JAS) paradigm (Sniezek, 1992) which simulates Advisors \nwho give information to others acting as Judges (who are ultimately accountable for a \n8 \n \njudgment or forecast).  Hence, the decision to use or discard the opinions of others is \nthat of the Judge. \n \nThe impact of others\u201f opinions \nSoll & Mannes (2011) note the well-documented finding that judges often overweight \ntheir own opinions relative to the advice of another (when the single advisor offers \nsimple numerical advice) \u2013 so called egocentric discounting. In such instances, the \naveraging of one\u201fs own opinion and that of the advisor would often have led to \ngreater accuracy. However, as Soll and Mannes note, when the experimental task \ninstead focuses on combining the opinions of others, the relative weights attached in \nsuch combination tasks tend to be less biased.  One explanation of the overweighting \nof one\u201fs own opinion is that the reasons for one\u201fs own opinions are, perhaps, richer \nand more salient than those of the advisor \u2013 where the reasons for proffered advice are \nseldom part of the experimental paradigm. For example, in the few JAS studies where \nnon-numerical justification of numerical advice has been made available to \nparticipants, such justification of advice has been impoverished and superficial. In \nVan Swol & Sniezek (2001), advisors gave their recommendations as to which \nanswers to multiple-choice items were correct. The advisors were free to elaborate on \ntheir recommendations by writing comments that would be seen by the judge.  In the \nminority of instances where advisors elaborated upon their recommendations, such \nelaborations amounted to little more than comments such as \u201eI\u201fm definite about this\u201f, \nor \u201eThis is a guess\u201f (p.297).   In their own empirical study, Soll and Mannes \nmanipulated both the task of revising one\u201fs own opinion and the task of combining \nthe opinions of others - within a single experiment. As these authors note, statistical \naveraging performs best when both the probability of detecting someone with true \n9 \n \nexpertise is low, where differences in expertise are also low, and where errors in \njudgments are randomly distributed. Further, in many (laboratory and real) situations \nexperts are not easily identified and so the advantage of averaging advice - over \npicking a single advisor - is increased. In their empirical study, Soll and Mannes \noperationalized \u201cadvice\u201d by presenting numerical values on four cues that were \npurportedly used by an advisor in making judgments of a focal numerical variable. \nHowever, note that Soll and Mannes did not utilize verbal justifications or rationales \nunderpinning these predictions. The authors found that participants approached the \ndual tasks of revision of their own opinions and the combination of the opinions of \nothers in different ways. Their analysis revealed that the obtained egocentric \ndiscounting in opinion revision was not caused by respondents giving more credibility \nto their own opinions than the opinions of others. Additionally, respondents did not \nrate themselves as more accurate in their judgments than their advisors. \n \nVan Swol (2011) focuses on the distinction between intellective and judgmental tasks, \ndiscussed earlier, and argues that people are more likely to accept advice on \nintellective tasks where the true answer is already known. This is because, she argues, \npeople seek out accuracy in answering intellective problems and an advisor may be \nable to demonstrate the correctness of their advice. By contrast, in judgmental tasks \u2013 \nwhere the true outcome or answer is not known at the time of the judgment - the \ndegree of trust placed in the advisor is more important for acceptance of proffered \nadvice. If the advisor is perceived as sharing similar values to oneself, then trust \nincreases - and so does acceptance of advice. By contrast, in intellective tasks, high \nadvisor confidence tends to lead to increasing trust in the proffered advice.  Van Swol \nargues that, in judgmental forecasting tasks, advisors are likely to differ in their \n10 \n \nadvice and so a potentially useful cue that may be used to differentiate advisors is the \ndegree to which an advisor shares the values of the client. In her empirical study, she \nfound that advisors spontaneously provided much additional information in a task \nwhere the advice was focussed on which movie the client would find enjoyable. She \nargues that this unstructured material would likely help establish common values and \nthus increase trust in the proffered advice. \n \nJodlbauer & Jonas (2011) investigate the influence of perceptions of an advisor\u201fs self-\ninterested intentions on the client\u201fs (i.e., advisee\u201fs) own intention to accept proffered \nadvice. As these authors note, the most-studied characteristics of the advisor have \nbeen perceived expertise, reputation, and confidence associated with proffered advice. \nBut clients, these authors argue, know that the advisor has an advantage in knowledge \nbut are also aware that advisors may be self-interested and so untrustworthy. Using a \nsimple experimental manipulation, Jodlbauer and Jonas varied whether an advisor \nintroduced himself as either a representative from a not-for-profit organization or a \nfor-profit organization. The student clients placed less trust in both the ability and the \nintegrity of the for-profit advisor. \n \nIn structured group processes, such as Delphi, the degree to which advisors (i.e. other \ngroup members) share common values and are to be trusted in their opinions has not \nbeen a focus of research. Recall that forecasts and opinions are shared anonymously \nin Delphi applications. However, recently, issues to do with procedural justice and \ntrust have been raised (and see also the studies above) and it is to this issue that we \nturn next. \n \n11 \n \nThe importance of perceptions of trust  \nIn many real-world settings, there are additional complexities to conducting research \n\u2013 and to answering the fundamental question of how to obtain a \u201egood\u201f forecast or \ndecision. Two studies in this Special Issue are noteworthy in their reporting of large-\nscale real-world interventions in organizations with a future-orientated focus, and \nthese exemplify some of the research difficulties, as well as the definitional difficulty \nof identifying a \u201egood\u201f forecast. That is, when one cannot easily control the \nexperimental environment and use forecast accuracy as a criterion measure \u2013 then \nhow can one judge the merits of a group-based process?  \n \nLandeta & Barrutia (2011) utilize a version of the Delphi process within a policy \ndevelopment setting that incorporated specific enhancements to the Delphi technique \nto help achieve a process that was of high-quality and acceptable to the varied \nmembers of a professional bureaucracy. In particular, the authors wished to attenuate \nany potential conflict between nominal group members. Their case study documents a \nprocess intervention that (i) maximises the perceived importance of participation, (ii) \nminimises the possibilities of manipulation of outcomes by powerful individuals, (iii) \nfacilitates the exchange of reasoned justifications for the divergent opinions existing \nin the professional bureaucracy. In this way, the process variation created trust and a \nsense of procedural justice \u2013 such that participants were willing to accept the \nconsequences of the Delphi yield. Interestingly, Landeta and Barrutia\u201fs method for \nthe selection of interest-group representatives for membership of the Delphi panel \nalso establishes that those representatives share similar values to their electors and \nthus, by implication, share the trust of their electors to represent the electors\u201f interests. \n \n12 \n \nKlenk & Hickey (2011) present another variation on the Delphi technique to enhance \ngroup-based deliberations. Their focus is on a method development to aid integration \nof group-based knowledge and to map areas of both consensus and dissent \u2013 whilst \nminimizing any negative group dynamics during deliberations of viewpoints.  Using \n\u201cconcept mapping\u201d techniques, their case study illustrates a practical tool to document \nboth shared and individual viewpoints in both knowledge and values. Such a tool may \nbe particularly useful, given the importance \u2013 previously discussed \u2013 of group \nmembers being able to provide rationales\/justifications of their forecasts in order to \naid other group members in assessing the quality of proffered advice. As with Landeta \nand Barrutia, the merits of this method were largely assessed through the use of post-\nevent questionnaires seeking the opinions of participants about the process. After all, \na process that is unacceptable to its participants is unlikely to have much impact \u2013 \nhence acceptance would seem an important additional criterion for judging forecast \nprocess quality, and acceptance appears to be closely linked to perceived  \u201etrust\u201f in the \npurveyors of advice. \n \nWe next turn to study the adequacy of group-based processes in another \u201cnaturalistic\u201d \ndecision setting \u2013 that of selecting the best research papers and research applications. \n \nNaturalistic decision making in Academia \nBenda & Engles (2011) take an unusual \u2013 yet actually highly salient \u2013 perspective on \ngroup-based forecasting. They focus on the operation of the peer review process in \nboth the selection of academic manuscripts for journal publication and in the selection \nof grant applications for research funding \u2013 arguing that in each case, what \neditors\/reviewers are doing is attempting to forecast the success of a paper or project \n13 \n \n(as might be indicated by, for example, a paper\u201fs future citations). They argue that \ninter-referee agreement is not at all important for valid selection \u2013 again, seeming to \nindicate the importance of heterogenous group membership. The key, to Benda and \nEngles, is that each knowledgeable referee should produce a credible review. Low \ninter-referee agreement can underpin subsequent strong internal validity of the \nreviewing process because a lack of agreement by knowledgeable referees can act to \ndiscourage superficial vote-counting. Dissensus should be resolved by equally \nknowledgeable journal editors and grant-awarding panels, who should seek to \nunderstand why disagreements between credible referees exist. Benda and Engles \nidentify and describe tension in valid peer review processes and go on to show that, \nwithout such tension, genuinely innovative research papers and proposals may be \nrejected. Scientific revolutions and paradigm shifts may encounter resistance when \nresearch papers or research proposals are evaluated by groups - compared to research \nthat presents additional increments to existing paradigm-based knowledge. Thus both \nvote counting and averaging of opinions may attenuate the insights provided by an in-\nthe-minority reviewer. As Benda and Engles point out, groups can be less-than-\noptimal users of information that is not already generally shared amongst the group \nmembership. As a remedy, these authors advocate that individual referees \u2013 who are \ncredible and knowledgeable - should have the occasional power to declare the \nunilateral acceptance of a research-based paper or research proposal. \n \nBut, which of the methods of unstructured and structured group-based judgmental \nforecasting is best? It is to this issue that we next turn. \n \nComparison of group-based forecasting methods \n14 \n \nGraefe & Armstrong (2011) compare the accuracy of unstructured face-to-face groups \nwith three structured methods: (i) nominal groups, (ii) Delphi, and (iii) prediction \nmarkets. Their task was the quantitative estimation of ten almanac quantities, such as \nthe percentage of the US population aged over 65 years in 2000. Overall, they found \nfew differences between the four methods, but all of the three structured group \ninteraction methods improved over the group members\u201f individual prior estimates. \nImportantly, only the Delphi method improved over the statistical averaging of the \ngroup members\u201f prior opinions. However, as Graefe and Armstrong note, their \n\u201cintellective\u201d estimation task was impoverished in that participants would, likely, be \nunable to share information that would be of use to aid improvements in the \njudgments of other group members. Interestingly, the study\u201fs participants expressed \nan attitudinal preference for group processes that involved face-to-face synchronous \ncontact with other group members over participation in either Delphi or prediction \nmarkets. \n \nSummary of key findings in the Special Issue papers \nFrom the papers discussed in the preceding paragraphs we suggest that an individual\u201fs \nopinion change after group deliberation is most likely to be appropriate where: \n \n1. Group membership is heterogeneous. Artificial heterogeneity can and should \nbe achieved by role-playing rather than role-thinking. \n2. Minority opinion is protected from majority pressure to conform \u2013 which \nmight best be achieved through anonymity of participants\u201f judgments. \n3. Information exchange between group members has been facilitated such that \nerrors in opinions can be recognised as such. The addition of novel approaches \n15 \n \nsuch as concept mapping (Klenk & Hickey, 2011) might help with this \nexplication. \n4.  The advisee is able \u2013 by reasoning processes - to evaluate the reasoning \nunderpinning the proffered advice (again highlighting the possible value of \nreason-decomposition approaches - such as concept mapping). \n \nProffered advice is least likely to be accepted (whether that advice is appropriate or \nnot) when: \n \n1. The advisor is perceived to have different values to the advisee. \n2. The advisor is thought to be self-interested. \n3.  The advisor is not trusted (which is liable to be related to a degree to having \ndifferent values and being self-interested). \n4. The advisor(s) are in the minority. \n5. The advisor(s) are not able to justify recommendations made \u2013 such as when \nadvice is given in numerical form only. \n6. Advisors express little confidence in their opinions. \n \nIn contrast, the outcome of a group-based deliberation is most likely to be accepted \nwhen: \n \n1. There is perceived procedural fairness to the group-based process. \n2. The participants in a group-based process are perceived to be trustworthy \u2013 as \nindicated by a commonality of values with the advisees and a lack of self-\ninterest in the advice proffered. \n16 \n \n3. There is a sizable majority favouring the prediction or outcome. \n \nIn the next sections, we broaden our discussion of group-based forecasting to evaluate \nother group-based methodologies to enhance judgment and decision making. We \ndraw out implications for improving group-based forecasting. \n \nGroup decision making \nSchweiger, Sandberg, & Ragan (1986) discuss approaches to engender debate and \nevaluation of decisions in management teams. They differentiate (i) dialectical inquiry \nand (ii) devil\u201fs advocacy. Both methods systematically introduce conflict and debate \nby using sub-groups that role-play. In dialectical inquiry, the subgroups develop \nopposing alternatives and then come together to debate their assumptions and \nrecommendations. In devil\u201fs advocacy, one subgroup offers a proposal, while the \nother plays devil\u201fs advocate, critically probing all elements and recommendations in \nthe proposal. Both methods encourage groups to generate alternative courses of action \nand minimise tendencies towards premature agreement or convergence on a single \nalternative. Both methods also lead to a more critical evaluation of assumptions by \nproviding mechanisms for encouraging dissent whilst at the same time fostering a \nhigh-level of understanding of the final group decision. Nevertheless, these role-\nplayed, conflict-enhancing, interventions for improving decision making need to be \nfocussed on factual information because personalities can, inappropriately, become \nthe focus of discussion. Schweiger, Sandberg, & Rechner (1989) compared both \ntechniques to a non-adversarial approach where decisions were simply discussed with \nthe aim of achieving a consensus amongst group members. Questionnaire ratings by \ngroup participants found that the two conflict-based approaches were rated higher in \n17 \n \nterms of producing better recommendations and better questioning of assumptions.  \nFormalizing and legitimizing conflict can thus enhance perceptions of the quality of \nthe outcome of group decision making. However, whilst conflict can improve \nperceived decision quality, it may weaken the ability of the group to work together in \nthe future if the role-playing is not sensitively managed.  Also, as Nemeth, Brown, & \nRogers (2001) document, authentic minority dissent, when correctly managed, is \nsuperior to role-playing interventions in stimulating a greater search for information \non all sides of an issue. But, generally, the authentic dissenter is disliked even when \nshe\/he has been shown to stimulate better thought processes. However, Nemeth & \nChiles (1998) showed that the persistent authentic dissenter, while not liked, can be \nadmired and respected. Also, it must be recognised that implementation of decisions \nrests on securing the subsequent cooperation of involved parties (as highlighted by \nseveral of the papers in this special issue \u2013 e.g. Landeta & Barrutia, 2011) and so \naffective personal criticism invoked in the prior critical debate will be dysfunctional. \n \nAs we have discussed, in a forecasting context, an understanding of the likely \noutcomes of conflicts can be invoked by the use of role-playing the interactions of the \nconflicted parties to a dispute (Green & Armstrong, 2011). However, the focus on \ncritique and debate that are entailed in dialectical inquiry and devil\u201fs advocacy has not \nbeen a prior topic of research in group-based forecasting. For example, the work of \nYaniv (2011) and Onkal et al (2011) promotes the inclusion of heterogeneity of \nopinions rather than the structuring of dissent. We advocate that methods which \ninvoke critique and dissent should now become such a research focus. \n \n18 \n \nThe next section of this paper reviews research on the acceptability of advice in \ncontexts other than JAS or Delphi. As we shall see, justifications for proffered advice \nhave been shown to be a crucial component of the advice\u201fs subsequent acceptability. \n \nAcceptability of advice  \nExpert systems capture the reasoning of experts within computer systems that can \nthen act to replicate the expert\u201fs decision making. Often such systems are used by \nless-expert decision makers as an aid to decision making. Arnold, Clark, Collier, \nLeech, & Sutton (2006) found that novice users of expert systems tend to accept \nsystems\u201f recommendations, while more-expert users have a stronger interest in \nexamining the explanations that the systems generate for particular recommendations. \nAs such, expert users are interested in comparing the recommendations and \nunderpinning reasoning of the systems with their own judgment. In fact, this focus on \nevaluation and verification may be a precondition for acceptance of systems by more-\nexpert users. \n \nApart from the use of expert systems, statistical models can be used to automate \ndecision making, or aid decision makers to make decisions. In the USA, a quarter of a \nmillion people are admitted unnecessarily to hospital each year with suspected heart \nfailure. Yet, using seven predictive indicators (four based on quantifications of a \npatient\u201fs medical history and three being summary measures of electrocardiogram \ntests), Corey & Merenstein (1987) developed a quantitative linear regression model \nthat was correct 85% of the time. Because of its predictive success, use of this \ndecision aid was made mandatory for physicians in one major hospital. However, \nafter some time, its use was made a voluntary choice. From then on, the linear model \n19 \n \nwas used to aid diagnosis of only 3% of patients with suspected heart failure. Why did \nthis very effective decision aid not get consulted more often?  \n \nSeick & Arkes (2005), in a study of decision aid neglect, found that decision-makers \nwho had access to the statistical equation underpinning an effective decision aid often \ndidn\u201ft bother to examine the workings of the method. The tendency was for the \ndecision-makers to rely on their own judgment and, later, report that they performed \nbetter on the prediction task than the advice offered by the decision aid \u2013 although the \ndecision aid actually outperformed their intuitive judgments. One participant in the \nstudy commented: \u201e\u2026 the statistical equation gave me more confidence if it was \nsimilar to my original guess. If it was different, I went along with my gut instinct \nrather than use the equation. If I had absolutely no idea, I went with what the equation \ngave me\u201f. \n \nIn fact, people are much more likely to follow a recommendation that comes from an \nexpert, for example a physician, rather than one that comes from a statistical model. \nExpert systems that provide the user with explanations of the advice given are more \nlikely to be heeded than the unexplained, although accurate, predictions of linear \nmodels. This result bears comparison with the previously discussed research in \nforecasting (Kerr & Tindale, 2011) that emphasizes that the  justifications for \nproffered advice from other group members needs to be made as explicit as possible, \nin order to have any influence on opinion change. We develop discussion of this issue \nin the next section. \n \n \n20 \n \nExtending study of the reasons underpinning proffered advice \nWe have had a longstanding concern with the nature of information exchange \nbetween participants in nominal groups doing forecasting tasks (see Rowe & Wright, \n1996), and in particular, the need to understand the processes leading to \njudgment\/forecast change in individuals within such groups (and interacting groups \nmore widely) (Rowe, Wright &  Bolger, 1991). That is, what factors are responsible \nfor leading participants to accept others\u201f judgments or forecasts and amend their own \nto some degree? Some of the papers in this special issue have provided further \nevidence of a need for the information being exchanged to possess certain qualities \u2013 \nincluding indicating shared values between information provider and recipient. \nInformation that leads a recipient to trust the information provider, or that indicates \ntheir expertise (at least in certain tasks \u2013 perhaps intellective moreso than judgmental) \nis also important. However, what is it about the content and type of forecast \njustifications and challenges that induces participants to change their positions, and \nthus potentially improve forecasts? We had hoped that this special issue would flush \nout a number of empirical papers addressing this topic, but that has not been the case - \nand this research area remains under-explored. \n \n \nHowever, some early research spoke to this issue, and, we propose, ought to be \nrevisited by modern researchers. For example, Brockriede & Ehninger (1960) have \nshown that only a limited number of argument types are, in principle, available to \npeople advocating specific propositions or claims \u2013 arguments of parallel case, \nanalogy, motivation, and authority:    \n \n21 \n \n \nIn Analogous reasoning, the reason given makes use of our general knowledge of \nrelationships between two events in dissimilar situations. For example, if someone is \ntrying to estimate the time it will take to drive to a nearby airport, an advisor may \nreason that, \u201cthe airport is roughly the same distance away as the shopping mall. \nTherefore, the time it will take to get to the airport will be approximately the same as \nit is to travel to the shopping mall \u2013 about 30 minutes\u201d.  \n \nParallel case reasoning involves making use of our knowledge of a previous \nexperience of a near identical situation. For example, if someone is trying to estimate \nthe time it will take to drive to a nearby airport an advisor may reason that, \u201cit will \ntake about 30 minutes to drive to the airport because it took me 30 minutes at the \nsame time of day last month\u201d.  \n \nAuthoritative reasoning involves making use of substantive knowledge. For example, \n\u201cthe radio announcer has said that traffic to the airport is heavy today and so I \nestimate that you should add 20 minutes to your journey time\u201d.  \n \nMotivational reasoning involves making use of specific insights about people\u201fs \nmotivations or desires. For example, \u201csince you will be in a hurry, then I reckon that \nyou can cut five minutes off your usual journey time\u201d. \n \n \nImportantly, whilst reasoning by analogy, parallel case, authority, and motivation are \navailable justifications for advice in judgmental tasks, only justification by authority, \n22 \n \nor expertise, is available as a justification for advice in intellective tasks. Thus, many \ncrucial questions remain to be explored and answered. For example, what components \nof advice-giving cause opinion change in the judgmental forecasting of individual \nexperts? How is advice evaluated and under what conditions will advice be \nassimilated or discounted? When one expert defers in his or her own opinion to the \nwell-argued opinion, or challenge, of another, is this an indicator of the presence of \nvalid advice that will improve validity in (the revised) judgmental prediction?  \n \n \nIn the next section, we focus on another technique that is used by decision makers to \nanticipate the future \u2013 scenario planning. We show that stakeholder analysis \u2013 akin to \nrole-thinking (as discussed earlier - see Green & Armstrong, 2011) - is a component \nof current scenario development practice. \n \nScenario planning and stakeholder perspectives \nThe scenario method explores the complex relationship between social, economic, \ntechnological, environmental and political factors from multiple perspectives, enables \nsense making of their interactions, and provides a vehicle for the development of \nplausible futures that may impact on the focal organization. \n \nThe approach entails some consideration of stakeholder values and actions to add \nrealism to already-constructed scenarios. In practice, stakeholder analysis is an \noptional addition to the 'mix' of ingredients; as 'a tool to be used in parallel with the \nscenario process, as and when members of the scenario team find it useful' [van der \n23 \n \nHeijden, Bradfield, Burt, Cairns, & Wright, 2002, p.219]. Stakeholders include the \nfocal organization\u201fs competitors, customers, regulators, etc.  \n \nWright & Goodwin (2009) have argued for a more intense focus on stakeholder \nanalysis within the scenario development process - as the likely actions of \nstakeholders to enhance and preserve their own interests in a particular unfolding \nscenario are thought-through. Such role-thinking assumes that scenario participants \nwill be able to put themselves in the shoes of each particular stakeholder grouping \nwhen this does not involve actual interactions with representatives of such groupings. \nAt the same time, stakeholder interests and values may be more subtle than those that \nare obvious on the surface. However, as we have discussed, there is, by contrast, \nevidence that role-playing unfamiliar roles can lead to insights. Green (2002) first \nshowed that when university students were asked to role-play the participants in six \nheterogeneous conflict situations, their subsequent group-based resolutions of the \nconflict \u2013 or the group-based forecasts of the outcomes of these conflicts \u2013 were \naccurate. Intuitively, it would seem that one\u201fs own experiences of the past resolution \nof conflicts \u2013 perhaps as recalled or previously experienced, and including both \npersonal and non-personal conflicts \u2013 offer a strong guide to the prediction\/resolution \nof the outcomes of novel conflicts. In other words, if the resolutions of conflicts are, \ngenerally, the result of the operation of basic human motivations and value systems, \nthen the conditions for reasoning by analogy \u2013 and acting by analogy - are favourable \n(Wright, 2002). Our current analysis now leads us to advocate that scenario \ndevelopment should incorporate role\u2013playing of stakeholders - rather than use less-\nexperiential role-thinking activities. \n \n24 \n \nFurther, Cairns, Sliwa, & Wright (in press) advocate the interrogation of the scenario \nstories from the perspective of the full range of involved and affected actors through \napplication of Flyvbjerg\u201fs question framework for phonetic inquiry. They advocate \nstructuring this analysis using a matrix that lists the developed scenarios along the top \nrow of a matrix (Where are we going?) and the range of identified actor groups down \nthe left-hand column. Within each box that marks an intersection of a scenario and an \nactor group, they suggest considering two issues: (i) the impact of the unfolding future \non the actor group's interests and values (Is this development desirable?) and (ii) the \nlikely action\/reaction of the group to the particular unfolding future (What, if \nanything, should we do about it? where \u201ewe\u201f is the particular actor group). Use of \nrole-playing would enable participants to become more sensitised to the plight of each \nof the groups of actors and become aware of the degree of power of action that each \nof them has to preserve or enhance their own interests as a particular scenario unfolds.  \n \nWe next turn to the presence of potential framing effects in scenario development and, \nfollowing Yaniv (2011), argue for the preservation of heterogeneity in scenario \ndevelopment teams. \n \nScenario planning and heterogeneity of participants \nIn most scenario planning exercises, the scenarios are developed by participants from \nwithin a single organization. It follows that these participants are likely to have a \nhomogeneous frame on the nature of the future. One way, in practice, used to counter \nthis potential bias is to employ outsiders - so-called \u201eremarkable people\u201f \u2013 who hold \nminority viewpoints about the future. Such deliberately-invoked diversity is likely to \nreduce frame blindness in the context of a facilitated process intervention within an \n25 \n \norganization. However, at present, the incorporation of such potential insights is \nunstructured and unevaluated.  \n \nAlso, in practice, scenario development sometimes involves a scenario team \ncomposed of representatives from multiple agencies \u2013 i.e., the scenario team is \ninitially formed from a heterogeneous constituency. Cairns, Wright, Bradfield, van \nder Heijden, & Burt (2006) have argued that the process of scenario planning can \nprovide a non-adversarial common viewpoint to unite, what may be, initially-\nfragmented groupings. By contrast, in terms of our analysis, the fragmentation should \ninstead be conserved \u2013 at least until the point when any action response to the \nconstructed set of scenarios is debated (see Goodwin & Wright, 2010).  In the more \nusual scenario development activity, conducted within a single organization, the \nconventional process results in the initial development of four skeleton scenarios that \nare then each fleshed-out by one of four sub-groups. But differences, in world-views, \nbetween these sub-groups are likely to be small.  On our analysis, once a particular \nscenario is fully developed it should then be subjected to adversarial critique by one \nor more of the other subgroups. In this way, also, the systematic introduction of \nconflict and challenge is likely to enhance the quality of the finally-developed \nscenarios. \n \nFinally, we now turn to a discussion of public engagement processes. As we will see, \nevaluation of public engagement methods has direct implications for the evaluation of \nalternative methods for group-based forecasting \u2013 in situations where outcome data, \nthat can be used for forecast verification, is not available. \n \n26 \n \nPublic engagement processes \nA contemporary domain in which many of the issues in this special issue are currently \nbeing played out is that of public engagement in agenda setting and policy making. \n(Though a caveat is needed here, as copious alternative terms have also been used to \ndescribe this general domain \u2013 for example, replacing the prefix \u201epublic\u201f with \n\u201estakeholder\u201f or \u201ecitizen\u201f and the suffix \u201eengagement\u201f with \u201ecommunication\u201f or \n\u201econsultation\u201f or participation\u201f e.g. see Rowe & Frewer, 2005, for a discussion of \ndefinitional nightmares). In essence, public (or stakeholder... etc etc) engagement is \nthe process of involving a wider range of perspectives into some decisional or agenda-\nsetting (or even forecasting) process than would traditionally be the case. The origins \nof this zeitgeist are difficult to pin down, but it has been associated with a number of \nmajor failures in the traditional decide-announce-defend approach to policy making, \nin which the public and other stakeholders would simply be the recipients of \ncommunications about the derived policy (developed by governmental agencies, \nlegislators, etc.).  \n \nAssociated with this has been a decline in public trust in government, politicians, and \nscientists, in many democratic societies over the last few decades (e.g. De Marchi & \nRavetz, 1999; Laird, 1989). Whereas in the past a compliant public might have \naccepted what governments claim as best policy, or scientists as \u201ethe truth\u201f, nowadays \nthere appears to be more dispute, disbelief and distrust, with the public not behaving \nas their traditional advisors would recommend.  The ideal of engagement is that, by \ninvolving the public (or its representatives, or other excluded stakeholders) that, \nsomehow, trust will be regained and, also, that decisions may be improved - because \nof the addition of lay knowledge and perspectives.  \n27 \n \n \nThe way in which input is gained from this wider constituency is invariably through \ngroup-based approaches (Rowe & Frewer, 2005, list over 100 \u201emethods\u201f for doing \nthis). The parallels to the topic of this special issue are thus a) the assumed benefits of \nhaving heterogenous input, and b) the importance of a process being perceived as \nacceptable to its participants for it to have utility (e.g. Webler, 1995). Indeed, the \nvalue of public engagement is seen by many as self-evident, which has hindered \nattempts at asking critical evaluative questions, such as does public engagement \nwork? Are policies derived through this approach actually \u201ebetter\u201f than those achieved \nthrough traditional approaches? Which of the various methods of engagement work \nbest in which situation? And indeed, what do we mean by \u201ework\u201f?  \n \n It is in this latter point that alternative methods of public engagement are confronted \nwith similar practical evaluative problems to those involving medium- to long-term \nforecasting \u2013 where there is the absence of the possibility of using accuracy to assess \nforecast quality. More recently, the issue of evaluation has risen up many agendas \u2013 \nparticularly as authorities have realised a need to justify their expenditures on \nexpensive engagement processes. And here, possibly, might be some lessons for the \nforecasting domain. Evaluation research has generally sought to assess two main \naspects of engagement processes for quality \u2013 the first being process acceptability to \nthe participants involved, and the second being the quality of the process used in \nenacting engagement. Various criteria have been forwarded to measure acceptability \n(see Rowe & Frewer, 2000), such as process transparency, independence of the \nfacilitation of a process from the event sponsors\u201f potentially vested interests, and the \nappropriate representativeness of participants. Process quality has been assessed \n28 \n \naccording to criteria such as the availability of appropriate resources to complete the \ntask (including time resources), the utilisation of relevant decision-structuring \nprocesses, and the adequate and full definition of the task. It is possible that these \ncriteria (or others from this domain) might be useful in the evaluation of medium- to \nlong-term forecasting processes \u2013 using acceptability and process quality measures as \nsurrogates for unobtainable validity measures. Of particular relevance is the \nconsideration of what is an adequately \u201erepresentative\u201f set of participants. The \nfindings from several studies in this issue have highlighted the importance of \nheterogenous group membership \u2013 but research in the public engagement domain \nwould prompt forecasters to think further in terms of exactly who those heterogenous \nmembers should be, and if they are to be experts in certain domains, would ask the \nquestion \u2013 what do we mean by an \u201eexpert\u201f, and how might we measure and confirm \nthis (c.f., Rowe & Wright, 2001)? All these issues now deserve a place on our future \nresearch agenda for group-based judgemental forecasting.  \n \nConclusion \nThe papers in this issue reveal that group-based forecasting is a complex, multi-\nfaceted issue. One of the distinctions made in several of the papers is between \nintellective and judgmental (forecasting) tasks \u2013 with some evidence put forward that \ndifferent factors may be more or less relevant in determining the output of groups \nconsidering each task type. In group-based forecasting practice, there are likely to be \naspects of both of these types of tasks, with experts bringing factual knowledge to \nbear to support their forecast and trying to persuade others of the correctness of their \nspecial knowledge (and thereby of their own expertise). \n \n29 \n \n But there is a third significant component to real-world forecasting that is also \ntouched upon here, particularly in the papers of Landeta & Barrutia (2011) and Klenk \n& Hickey (2011), and that is the policy making angle (after all, the \u201ePolicy Delphi\u201f is \na distinctly named variant of one of the main group-based forecasting techniques). We \nmust ever remember that forecasts are rarely, in themselves, disinterested and \ninnocent products of the group process in which they are produced. Forecasts \u2013 in \nnon-laboratory settings \u2013 often serve a purpose in policy making. If forecasts are for \nhoped-for outcomes, they may encourage policy making to foster their occurrence; if \nforecasts are for feared outcomes, then they may encourage the forecasters (and \npolicy makers \u2013 who may or may not comprise the same set of individuals) toward \nactions that might undermine the forecast. And this reality should cause us to \nreconsider how we evaluate forecasts. In laboratory studies \u2013 particularly using \nalmanac questions (intellective tasks) and short-term forecasts \u2013 forecasting accuracy \ncan be determined, and therefore the factors responsible for a particular group-based \nforecasting method producing a better forecast can be investigated. But in medium- to \nlong-term forecasts, other criteria need to be brought to bear in assessing whether a \nparticular group-based process has value.  \n \nIn summary, we believe that this special issue on group-based forecasting does \nadvance our knowledge of the domain in a number of ways. But there are others paths \nwe might follow, and we urge the research community to consider them. \n \n \n \n \n30 \n \nReferences \n \nArnold, V., Clark, N., Collier, P.A., Leech, S. A. & Sutton, S.G. (2006). The \ndifferential use and effect of knowledge-based system explanations in novice and \nexpert judgment decisions. MIS Quarterly, 30, 79-97. \n \nBenda, W. G. G. & Engels, T. C. E. (2011). The predictive validity of peer review: a \nselective review of the judgmental forecasting qualities of peers, and implications for \ninnovation in science. International Journal of Forecasting... \n \nBerg, J., Nelson, F & Rietz, T.A. (2008). Prediction market accuracy in the long run. \nInternational Journal of Forecasting, 24, 285-300. \n \nBrockriede, W. & Ehninger, D. (1960). Toulmin on argument: an interpretation and \napplication. Quarterly Journal of Speech, 46, 44-53. \n \nCairns, G., Sliwa, M. & Wright, G. Problematizing international business futures  \nthrough a \u201ecritical scenario method. Futures, in press. \n \nCairns, G., Wright, G., Bradfield, R., van der Heijden, K. & Burt, G. (2006). \nEnhancing foresight between multiple agencies:  issues in the use of scenario thinking  \nto overcome fragmentation. Futures, 38, 1011- 1025. \n \nCorey, G. A. & Merenstein, J. H. (1987). Applying the ischemic heart disease \npredictive instrument. The Journal of Family Practice, 25, 127- 133. \n31 \n \n \nDe Marchi, B. & Ravetz, J.R. (1999) Risk management and governance: a post-\nnormal science approach, Futures, 31, 743-757. \n \nGraefe, A. & Armstrong, J.S. (2011). Comparing face-to-face meetings, nominal \ngroups, Delphi and prediction markets on an estimation task. International Journal of \nForecasting... \n \nGreen, K.C. (2002). Forecasting decisions in conflict situations: a comparison of \ngame theory, role-playing, and unaided judgment. International Journal of \nForecasting, 18, 321-433. \n \nGreen, K.C. & Armstrong, J. S.  (2011). Role thinking: standing in other people\u201fs \nshoes to forecast decisions in conflicts. International Journal of Forecasting... \n \nGoodwin, P. & Wright, G. (2010) The limits of forecasting in anticipating rare  \n \nevents. Technological Forecasting and Social Change, 77, 355-368. \n \nvan der Heijden, K., Bradfield, R., Burt, G., Cairns, G. & Wright, G. (2002). The \nSixth Sense: Accelerating Organisational Learning with Scenarios, Chichester: Wiley \n \nJodlbauer, B. & Jonas, E. (2011). How does perception of strategic behaviour \ninfluence acceptance of advice? International Journal of Forecasting... \n \nKahneman, D. & Tversky, A. (1984). Choices, values, and frames. American \nPsychologist, 39, 341-350. \n32 \n \n \nKerr, N.L. & Tindale, R.S. (2011). Group-based forecasting?: a social psychological \nanalysis. International Journal of Forecasting... \n \nKlenk, N. L. & Hickey, G. M. (2011). A virtual and anonymous, deliberative and \nanalystic participation process for planning and evaluation: the concept mapping \npolicy Delphi. International Journal of Forecasting... \n \nLaird, F.N. (1989). The decline of deference: The political context of risk \ncommunication. Risk Analysis, 9, 545-550. \n \nLandeta, J. & Barrutia, J. (2011). People consultation to construct the future: a Delphi \napplication. International Journal of Forecasting... \n \nNemeth, C., Brown, K. & Rogers, J. (2001). Devil\u201fs advocate versus authentic \ndissent: stimulating quantity and quality. European Journal of Social Psychology, 31, \n707-720. \n \nNemeth,C. & Chiles, C. ( 1998). Modeling courage: the role of dissent in fostering \nindependence. European Journal of Social Psychology, 18, 275-280. \n \nOnkal, D., Lawrence, M. & Sayim, K.Z. (2011). Influence of differentiated roles on \ngroup forecasting accuracy. International Journal of Forecasting... \n \n33 \n \nRowe, G. & Frewer, L.J. (2000). Public participation methods: A framework for \nevaluation. Science, Technology, & Human Values, 25, 3-29. \n \nRowe, G. & Frewer, L.J. (2005). A typology of public engagement mechanisms. \nScience, Technology, & Human Values, 30, 251-290. \n \nRowe, G. & Wright, G. (1996). The impact of task characteristics on the performance \nof structured group forecasting techniques. International Journal of Forecasting, 12, \n73-89. \n \nRowe, G. & Wright, G. (2001). Differences in expert and lay judgments of risk: Myth \nor reality? Risk Analysis, 21 (2), 341-356. \n \nRowe, G., Wright, G. & Bolger, F. (1991). The Delphi technique: A re-evaluation of \nresearch and theory. Technological Forecasting and Social Change, 39, 235-251. \n \nSchweiger D. M, Sandberg, W.R., & Ragan, J.W. (1986). Group approaches for \nimproving strategic decision making: A comparative analysis of dialectical inquiry, \ndevil\u201fs advocacy, and consensus. Academy of Management Journal, 29, 51-71 \n \nSchweiger D. M, Sandberg, W.R., & Rechner, P.A. (1989). Experiential effects of \ndialectical inquiry, devil\u201fs advocacy, and consensus approaches to strategic decision \nmaking. Academy of Management Journal, 32, 745-772. \n \n34 \n \nSeick, W.R. & Arkes, H.R. (2005). The recalcitrance of overconfidence and its \ncontribution to decision aid neglect. Journal of Behavioral Decision Making, 18, 29-\n53. \nSniezek, J.A.  (1992). Groups under uncertainty: an examination of confidence in  \n \ngroup decision making.  Organizational Behavior and Human Decision Processes,  \n \n52, 124-155.   \n \nSoll, J.B. & Mannes, A.E. (2011). Judgmental aggregation strategies depend on \nwhether the self is involved. International Journal of Forecasting... \n \nVan de Ven, A. H. & Delbecq, A. L. (1974). The effectiveness of Nominal, Delphi, \nand Interacting group decision making processes. Academy of Management Review, \n17, 605-621. \n \nVan Swol, L. (2011). Forecasting another\u201fs enjoyment versus giving the right answer: \ntrust, shared values, task effects, and confidence in improving the acceptance of \nadvice. International Journal of Forecasting... \n \nVan Swol, L.M. & Sniezek, J.A.  (2001). Trust, confidence, and expertise in a Judge- \n \nAdvisor System.  Organizational Behavior and Human Decision Processes, 84, \n \n288-307. \n \nWebler, T. (1995). \u201eRight\u201f discourse in citizen participation: An evaluative yardstick. \nIn Fairness and competence in citizen participation: Evaluating models for \nenvironmental discourse, edited by O. Renn, T. Webler, and P. Wiedemann, 35-86. \nDordrecht, Netherlands: Kluwer Academic Publishers. \n35 \n \n \nWright, G. (2002). Game theory, game theorists, university students, role-playing and \nforecasting ability. International Journal of Forecasting, 18, 383-387. \n \nWright, G & Goodwin, P. (2009). Decision making and planning under low levels of \npredictability: enhancing the scenario method. International Journal of Forecasting, \n25, 813-825. \n \nYaniv, I. (2011). Group diversity and decision quality: amplification and attenuation \nof the framing effect. International Journal of Forecasting... \n \n \n"}