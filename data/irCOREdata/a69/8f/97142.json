{"doi":"10.1016\/j.biosystems.2008.05.009","coreId":"97142","oai":"oai:generic.eprints.org:744\/core69","identifiers":["oai:generic.eprints.org:744\/core69","10.1016\/j.biosystems.2008.05.009"],"title":"'Extremotaxis': Computing with a bacterial-inspired algorithm","authors":["Nicolau jr, D. V.","Burrage, K.","Nicolau, D. V.","Maini, P. K."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2008","abstract":"We present a general-purpose optimization algorithm inspired by \u201crun-and-tumble\u201d, the biased random walk chemotactic swimming strategy used by the bacterium Escherichia coli to locate regions of high nutrient concentration The method uses particles (corresponding to bacteria) that swim through the variable space (corresponding to the attractant concentration profile). By constantly performing temporal comparisons, the particles drift towards the minimum or maximum of the function of interest. We illustrate the use of our method with four examples. We also present a discrete version of the algorithm. The new algorithm is expected to be useful in combinatorial optimization problems involving many variables, where the functional landscape is apparently stochastic and has local minima, but preserves some derivative structure at intermediate scales","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/97142.pdf","fullTextIdentifier":"http:\/\/eprints.maths.ox.ac.uk\/744\/1\/267.pdf","pdfHashValue":"41b9a6d76a0ac7593b62c144ca38188fce9e456b","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:744<\/identifier><datestamp>\n      2015-05-29T18:27:44Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4143:4D3932<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.maths.ox.ac.uk\/744\/<\/dc:relation><dc:title>\n        'Extremotaxis': Computing with a bacterial-inspired algorithm<\/dc:title><dc:creator>\n        Nicolau jr, D. V.<\/dc:creator><dc:creator>\n        Burrage, K.<\/dc:creator><dc:creator>\n        Nicolau, D. V.<\/dc:creator><dc:creator>\n        Maini, P. K.<\/dc:creator><dc:subject>\n        Biology and other natural sciences<\/dc:subject><dc:description>\n        We present a general-purpose optimization algorithm inspired by \u201crun-and-tumble\u201d, the biased random walk chemotactic swimming strategy used by the bacterium Escherichia coli to locate regions of high nutrient concentration The method uses particles (corresponding to bacteria) that swim through the variable space (corresponding to the attractant concentration profile). By constantly performing temporal comparisons, the particles drift towards the minimum or maximum of the function of interest. We illustrate the use of our method with four examples. We also present a discrete version of the algorithm. The new algorithm is expected to be useful in combinatorial optimization problems involving many variables, where the functional landscape is apparently stochastic and has local minima, but preserves some derivative structure at intermediate scales.<\/dc:description><dc:date>\n        2008<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.maths.ox.ac.uk\/744\/1\/267.pdf<\/dc:identifier><dc:identifier>\n          Nicolau jr, D. V. and Burrage, K. and Nicolau, D. V. and Maini, P. K.  (2008) 'Extremotaxis': Computing with a bacterial-inspired algorithm.  Biosystems, 94  (1-2).   pp. 47-54.      <\/dc:identifier><dc:relation>\n        10.1016\/j.biosystems.2008.05.009<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.maths.ox.ac.uk\/744\/","10.1016\/j.biosystems.2008.05.009"],"year":2008,"topics":["Biology and other natural sciences"],"subject":["Article","PeerReviewed"],"fullText":"\u2018D\na\nb\nc\nd\na\nA\nR\nR\nA\n1\nb\n2\no\na\na\nt\np\nt\na\np\ne\na\nc\np\nv\nw\nf\ns\nb\na\n0\ndBioSystems 94 (2008) 47\u201354\nContents lists available at ScienceDirect\nBioSystems\njourna l homepage: www.e lsev ier .com\/ locate \/b iosystems\nExtremotaxis\u2019: Computing with a bacterial-inspired algorithm\nan V. Nicolau Jr. a,\u2217, Kevin Burrageb,d, Dan V. Nicolauc, Philip K. Mainia,d\nCentre for Mathematical Biology, Mathematical Institute, University of Oxford, Oxford OX1 3LB, United Kingdom\nInstitute for Molecular Biosciences, University of Queensland, St. Lucia 4072, Australia\nDepartment of Electrical and Electronic Engineering, Liverpool University, Brownlow Hill, L693GJ Liverpool, United Kingdom\nOxford Centre for Integrative Systems Biology, University of Oxford, Oxford OX13LB, United Kingdom\nr t i c l e i n f o\nrticle history:\neceived 31 May 2007\neceived in revised form 29 October 2007\na b s t r a c t\nWe present a general-purpose optimization algorithm inspired by \u201crun-and-tumble\u201d, the biased random\nwalk chemotactic swimming strategy used by the bacterium Escherichia coli to locate regions of high nutri-ccepted 23 May 2008\nent concentration The method uses particles (corresponding to bacteria) that swim through the variable\nspace (corresponding to the attractant concentration profile). By constantly performing temporal compar-\nisons, the particles drift towards the minimum or maximum of the function of interest. We illustrate the\nuse of our method with four examples. We also present a discrete version of the algorithm. The new algo-\nrithm is expected to be useful in combinatorial optimization problems involving many variables, where\nthe functional landscape is apparently stochastic and has local minima, but preserves some derivative\nscales\ni\na\n1\ni\na\ne\na\nm\nm\nc\nc\nm\nfl\nc\n(\nfi\nc\nm\nnstructure at intermediate\n. Introduction\nThe correspondence between living systems and computers has\neen stressed in recent years (e.g. Bray, 1995; Nicolau and Nicolau,\n006). Many biological processes can be thought of as processes\nf constrained optimization. Therefore, the mechanism or mech-\nnisms used by a biological system to carry out a function is\nnalogous to an algorithm or set of algorithms; the biological sys-\nem is then an unconventional computer; and an instance of such a\nrocess taking place is analogous to a computational run. Of course,\nhere are enormous differences between \u2018biological computing\u2019\nnd \u2018classical computing\u2019. Biological computations are massively\narallel, feature a large degree of stochasticity and (intrinsic and\nxtrinsic) noise \u2013 which, aside from being unavoidable, also plays\ndirect role in the computation \u2013 and, rather than being able to\nompute individual functionswithhighprecision, deal insteadwith\nroblems of a \u2018systems engineering\u2019 flavour, such as the control of\nery large systems, in the presence of non-linear constraints.\nBecause living systems are adapted to the environments in\nhich they exist and therefore to the computational tasks requiredor survival, these natural computing paradigms are expected to be\nuccessful for dealing with problems similar to those confronting\niosystems (Nicolau and Nicolau, 2006). An increasing number of\nlgorithms are based on or inspired by biological strategies. These\n\u2217 Corresponding author.\nE-mail address: nicolau@maths.ox.ac.uk (D.V. Nicolau Jr.).\nR\ns\nb\nt\na\nT\nt\n303-2647\/$ \u2013 see front matter \u00a9 2008 Published by Elsevier Ireland Ltd.\noi:10.1016\/j.biosystems.2008.05.009.\n\u00a9 2008 Published by Elsevier Ireland Ltd.\nnclude neural networks (Basheer and Hajmeer, 2000), evolution-\nry computing (Eiben and Smith, 2003), DNA computing (Adleman,\n994), particle swarm optimization (Call et al., 2007), comput-\nng with bio-agents (Nicolau et al., submitted for publication),\nnt optimization algorithms (Dorigo and Blum, 2005) and oth-\nrs. Increasingly these methods have been successfully applied to\nspectrum of problems ranging from pattern identification and\natching to aerodynamics engineering problems (Obayashi, 1997)\nChemotaxis, the process by which organisms direct their move-\nents according to certain chemicals in their environment, is\nrucial for many biological functions. Bacteria such as Escherichia\noli use chemotaxis to find food (for example, glucose) by swim-\ning towards the highest concentration of food molecules, or to\nee frompoisons (for example, phenol). Inmulticellular organisms,\nhemotaxis is critical to development as well as normal function\nWadhams and Armitage, 2004). By analogy with the process of\nnding the maximum of a function (represented by the attractant\noncentration profile in space), chemotaxis is a algorithm for opti-\nization. This computational facet of chemotaxis has already been\noted by several authors (Bremermann, 1974; Muller et al., 2002).\necently, Vergassola et al. (2007) proposed a chemotaxis-inspired\nearch method in the absence of gradients.\nIn this paper, we present a biocomputation approach that is\nasedon the \u201crun-and-tumble\u201d chemotacticmechanismof thebac-\nerium E. coli. This method is essentially a general-purpose search\nlgorithmthat canbeused tooptimize a functionor set of functions.\nhe method bears some resemblance to particle swarm optimiza-\nion (PSO) in that the potential solutions (the particles) move\n4 ioSyst\nt\na\nt\nf\nd\ns\nt\na\n2\nw\ni\ni\nT\nt\nt\nd\nr\nt\nb\na\nn\nb\np\nm\nf\nb\no\nn\nm\nr\nt\nc\nw\nb\nl\na\nm\nF\ns\nh\np\ns\nm\np\nt\na\nd\na\ni\nd\na\nf8 D.V. Nicolau Jr. et al. \/ B\nhrough the function space. However, unlike PSO or, for example,\nnt colony optimization, it does not use inter-particle communica-\nion and does not bias the trajectories based on the best solutions\nound over time, relying instead completely on the chemotactic\nrift property of bacteria to converge locally (not as a swarm) to\nolutions. We illustrate the potential of the method by applying it\no four representative optimization problems of different types.We\nlso present a discrete version of the algorithm.\n. Methods\nWe begin by briefly describing the chemotactic swimming pattern of E. coli, on\nhich our algorithm is based. E. coli is a common intestinal bacterium, cylindrical\nn shape and roughly 2\u0002m long and 1\u0002m wide. Each cell is equipped with approx-\nmately six flagella, each with a rotary motor at its base, embedded in the cell wall.\nhe flagella are randomly distributed on the cell membrane. The rotary motor can\nurn clockwise and counter-clockwise at different times and is reversible. When all\nhe motors turn in concert in a counter-clockwise direction, the flagella form a bun-\nle that propels the cell forward in a \u201crun\u201d. Runs are not perfectly rectilinear due to\notational Brownianmotion that perturbs the cell direction by roughly 0.5\n\u221a\nt, where\nis in seconds. If one or more of the motors reverse direction and turn clockwise, the\nundle becomes unstable and the cell turns in place (\u201ctumbles\u201d) in a random fashion\nnd with negligible displacement. This serves to reorient the cell; the orientation is\not perfect and there is somepersistence of direction after a tumble (themean angle\netween the direction before and after a tumble is 63\u25e6) (Locsei, 2007). We omit this\nroperty in the present work, assuming that the reorientation is perfect.\nE. coli cells use the system of motors and flagella to execute chemotactic swim-\ning towards regions of high nutrient concentration (or away from toxins) as\nollows. Due to the high stochasticity of the environment and its small size, the\nacterium cannot accurately measure an attractant gradient across its body. In lieu\nf computing a spatial gradient directly, a simple biochemical memory mecha-\nism is used to perform temporal comparisons. During swimming, the bacterium\nonitors the concentration of chemoattractant (e.g. serine or aspartate) in the envi-\nonment, comparing the average concentration measured over the last second with\nhat measured over the previous 3 s. If the comparison indicates that the attractant\noncentration has increased, the cell is more likely to continue a straight-line run,\nhile if it indicates that conditions have deteriorated, it is more likely to reorient\ny performing a tumble. In this way, the bacterium performs a biased random walk,\neading it (in a stochastic fashion) up a chemoattractant gradient. In the absence of\nny such gradient, both the run and tumble times are exponentially distributedwith\neans of 1.0 and 0.1 s, respectively (Locsei, 2007).\nig. 1. Finding the global maximum in a Gaussian gradient field. Five hundred con-\necutive iterates of the colony centre (starting at the lower right) are shown in green;\nigher values of the attractant are shown as shades of red. The circle indicates the\noint at which the 250 particles are initialized.\ni\nT\nA\na\nf\nM\np\na\na\np\nd\nb\np\nr\na\ndems 94 (2008) 47\u201354\nWepropose touseananalogous strategy to locate regions inamulti-dimensional\npace where a continuous (respectively discrete) function takes a global maxi-\num (or minimum) value. We define such a \u201cbacterial optimizer\u201d B as a set of n\narticles (b1, b2, . . . , bn), each possessing an m-dimensional position vector func-\nion pi(t) and a velocity vector function vi(t) such that pi \u2208\u0006m,vi \u2208\u0006m, i = 1, . . . , n\nnd t \u2208N. This is the continuous version of the algorithm (a discrete version is\nescribed below). Let f : \u0006m \u2192 \u0006 be the objective function and let x\u02c6i = maxt>0xi(t)\nnd g\u02c6 = maxxf (xi), i = 1, . . . , n. Let U[x, y] be a random number between x and y,\nndependently sampled from the uniform distribution and let N(\u0002,\u0003) be a ran-\nom number independently sampled from the normal distribution with mean \u0002\nnd standard deviation \u0003. The algorithm proceeds as follows (the meanings of the\nunctions T and A and the various parameters are described after the algorithm):\nConvergence can be decided either by setting an upper limit on the number of\nterations tmax or by setting an acceptable value for f (g\u02c6).\nThe tumbling probability function T is calculated as follows:\n(t, i) =\n{\npw,Ai(t) < 0\npb, Ai(t) \u2265 0\n(1)\nwhere\ni(t) =\nmin(wr+wd,t)\u2211\n\u0004=0\nf (xi(t \u2212 \u0004))M(t) (2)\nnd M : \u0006 \u2192 \u0006 is a memory comparison function, which can take any number of\norms but which we define for simplicity here as\n(\u0004) =\n\u23a7\u23a8\n\u23a9\n1\nwr\n,0 < \u0004 \u2264 wr\n\u2212 1\nwd\n,wr < \u0004 \u2264 wd\n0,otherwise\n. (3)\nThe meaning of each of the parameters is as follows. \u0005 is a speed factor for the\narticles in the functional search space, since the elements of vi are bounded by \u22121\nnd 1. \u02c7 is a strictly positive parameter that if greater than 0 ensures that the runs\nre not perfectly straight and simulates rotational Brownian motion during a run.\nw and pb are probabilities of tumbling if conditions have improved (A(t) \u2265 0) and\neteriorated (A(t) < 0), respectively. In practice, the probability of tumbling must\ne larger if conditions have deteriorated than if they have improved, so we have\nw > pb . wr and wd are the number of iterations (window lengths) over which the\necent and distant past are averaged, respectively. A balancemust be struck between\nccuracy (using longer window lengths) and fast response time to improving or\neteriorating conditions (leading to the use of shorter window lengths). Because\nioSyst\nE\nd\no\nw\nn\na\nh\nh\ni\nt\no\nn\nf\nf\nP\nw\nm\nc\np\np\nx\nb\nt\nr\nb\nn\nv\n1\nm\na\ni\n(\ns\ns\nfi\nt\na\np\nt\nt\nm\nt\nm\n(\nB\nt\nt\nt\nt\nb\nf\nw\nI\nn\nc\nO\nt\nm\np\n3\ni\nt\nt\nt\nc\ns\n3\na\nf\nb\nG\nt\nb\no\nt\nm\nt\nt\no\nf\nx\ns\n[\ns\ni\nm\n3\nc\nt\nt\ni\n(\nt\nn\np\ni\nx\nd\ns\nf\na\na\nw\nt\nt\nv\nt\nwD.V. Nicolau Jr. et al. \/ B\n. coli compares (roughly speaking) the last second of its life with the previous 3 s\nuring chemotactic swimming (Strong et al., 1998), this would suggest a simple rule\nf wd \u2248 3wr . In applying our algorithm to different functions, this is likely to be vary\nith the nature and properties of the function in question.\nAn issue particular to the use of iterative algorithms (a large class of which the\new algorithm is a member) is stalling, a phenomenon characterised by many iter-\ntions without any improvement (Li et al., 2006). Occasionally, this is caused by\naving found the global optimum but usually it is caused by the algorithm either\naving \u2018passed by\u2019 the optimum or having become stuck in one or more local min-\nma. One way to deal with stalling is to perturb the algorithm in some way in order\no escape the local minimum or to explore new regions. A potential implementation\nf this strategy for \u2018extremotaxis\u2019 is to force all the particles (bacteria) to tumble if\no global improvement has been seen for some time. This can be done by modi-\nying the probability of tumbling, calculated in the main loop of the algorithm, as\nollows:\nr(tumble) =\n{\nT(Ai(t)), \u03b5 < \u03b5critical\n1, \u03b5 \u2265 \u03b5critical\n(4)\nhere \u03b5 is the number of iterations since the last improvement in the global opti-\num found (this must be recorded over the course of the computation) and \u03b5 is the\nritical number of iterations required to trigger a forced \u2018colony\u2019 tumble.\nIt is also possible to modify our algorithm so that it can be applied to discrete\nroblems. The key change is to restrict the elements of the velocity vector vi to\nositive values smaller than 1 and to treat these as probabilities of the entries in\ni changing state. For example, for a problem in which the variables can only take\ninary values (0 or 1), an element of vi equal to 0.1 means a 10% probability that\nhe corresponding element of xi will change state at the next iteration of the algo-\nithm. Additionally, the speed \u0005 should be set to 1 in order for the probabilities to\ne guaranteed to be between 0 and 1. In problems where the variables can take a\number of discrete values (for example where they can take any positive integer\nalue), each element of xi could, of course, be incremented by 1 or decremented by\n; therefore, the \u201cdirection\u201d of the increment should be chosen at random. Finally, it\nay be desirable to use increments greater than 1 (this would correspond to using\ngreater speed, in the continuous version of the algorithm). If this is done, then\nn order to avoid equal-sized increments at each point where a variable changes\nand thus miss intermediate values), the size of the jump should be sampled from a\nuitable probability distribution with a mean of \u0007, where \u0007 is the average increment\nize.\nOne issue if using this discrete version of the algorithm is that an appropriate\ntness function may need to be more carefully chosen (or would be more difficult\no find) than in the continuous version. In the latter, the fitness is evaluated simply\ns the value of the function at the point in m-dimensional space represented by the\nosition vector. However, in the case of a discrete function, if the number of values\nhat the function can assume is small or if these values are not consecutive (or both),\nhis may not be appropriate. This is because the algorithm relies on \u201ctumbles\u201d being\nore likely when the fitness is relatively inferior and less likely when it is close to\nhe desired value. Therefore, using the value of the function as the fitness function\nay result in completely stochastic behaviour. We illustrate the issue using a simple\nNP-complete) problem: Boolean satisfiability. Here it is required to determine, for a\noolean expression in n variables, what set of values for the variables (if any) makes\nhe expression TRUE. Using a fitness function that simply takes the values 1 (for\nrue) and 0 (for false) would not be advantageous in this case, reducing effectively\no a random search through the function space (which would require exponen-\nial time proportional to 2n . A more appropriate choice of fitness function may\ne\n(x) = max\nall z\nc(z) \u2212 c(x) (5)\nhere c(x) is the number of clauses in the Boolean expression that evaluate to 0.\nn this way, the algorithm would favour position vectors x that result in a smaller\number of such clauses evaluating to 0, thus in some sense being closer to finding a\nombination of variables that will cause the Boolean function to evaluate to TRUE.\nf course, other possible fitness functions exist and in general it is to be expected\nhat the choice of fitness function would vary with the problem and the deter-\nination of a suitable such function would present difficulties for some discrete\nroblems.\n. Results\nWe implemented our algorithm using MATLAB and applied\nt to three different optimization problems. The first of these is\nrivial: finding the maximum of a two-dimensional Gaussian func-\nion. The second is finding the global minimum of a difficult\nwo-dimensional function with many local minima. The third is\noncerned with how n particles should be distributed on a sphere\no as to minimize the potential energy of the system.\n3\ns\nrems 94 (2008) 47\u201354 49\n.1. Finding the maximum of a Gaussian function\nIn order to demonstrate the operation of our algorithm, we first\npplied it to the Gaussian function:\n(x, y) = 1\n2\b\u00032\ne\u2212((x\u2212x0)\n2+(y\u2212y0)2)\/(2\u00032). (6)\nTheGaussian is anattractivefirst choice for a tworeasons. Firstly,\necause the fundamental solution of the diffusion equation is a\naussian, we might expect naturally occurring attractant gradients\no take this form and therefore, due to adaptation, we might expect\nacterial chemotaxis to be efficient at finding the global maximum\nf such a function (corresponding, in vivo, to, for example, finding\nhe point of maximum nutrient concentration in a local environ-\nent). Secondly, it possesses a continuous and smooth gradient\nhat is everywhere non-zero. It is, nonetheless, a non-trivial func-\nion.\nFig. 1 shows a typical simulation of the algorithm. A colony\nf 100 particles is initially distributed at random points chosen\nrom xinit \u2208 [\u22123,3] and yinit \u2208 [\u22123,3]. We also randomly choose\n0 \u2208 [\u22123,3] and y0 \u2208 [\u22123,3]. The initial velocity vectors are also cho-\nen at random such that directions are uniformly distributed in\n\u2212\b,\b] and the magnitude of each direction vector is 0.02. We also\net \u0003 = 1 for simplicity. The figure shows the first 500 consecutive\nterations, with the maximum value found converging to the true\naximum at (x0, y0).\n.2. Optimizing a difficult two-dimensional function\nWe next applied our algorithm to Problem 4 of the 100-digit\nhallenge (Trefethen, 2002; Strang, 2005). This problem asks for\nhe minimum of the function\nf = esin50x + sin(60ey) + sin(70 sin x)\n+ sin(sin(80y)) \u2212 sin(10(x + y)) + 1\n4\n(x2 + y2) . (7)\nIt is made difficult by the presence of many local minima\nhat are very close to the global minimum\u2014the latter is approx-\nmately fmin \u2248 \u22123.30686864747523728 and occurs at (x, y) \u2248\n\u22120.0244030796943785,2.10612427155358). Fig. 2 A illustrates\nhe difficulty, with a graph of the function showing the behaviour\near the global minimum.\nWith the bacterial algorithm, setting up the problem consists of\nlacing a number (250 in our computations) of particles at random\nn the two-dimensional function space near the minimum (\u22125 \u2264\n\u2264 5, \u22125 \u2264 y \u2264 5 are appropriate intervals) and randomising their\nirections. Again, the behaviour of the algorithm is good\u2014Fig. 2 B\nhows the percentage difference of the algorithm\u2019s best estimate\nrom fmin over the course of a computational run. The 2000 iter-\ntions require, for 250 particles, only 2\u20133 s of computer time (on\n1-GHz desktop machine running MATLAB) to find the minimum\nith 8-digit accuracy (9-digit accuracy requires tens of seconds on\nhe same machine). In the simple implementation presented here,\nhe memory function and other parameters such as the particle\nelocity, directional persistence, etc. have not been optimized; this\nime would be reduced by some (unknown) factor if these steps\nere taken..3. Finding the minimum-energy configuration of particles on a\nphere\nLastly, we applied our algorithm to a difficult n-body configu-\nation problem: how to distribute n particles on a sphere so as to\n50 D.V. Nicolau Jr. et al. \/ BioSystems 94 (2008) 47\u201354\nFig. 2. (A) A graph of the function in Eq. (7), showing the complex behaviour near\nt\nv\nt\nm\nE\nw\np\ns\ne\no\nd\nf\ns\nt\nf\nn\nc\nl\na\ns\nd\ns\nl\nTable 1\nLowest energy particle arrangements found with \u2018taxis\u2019 compared with previously\npublished values\nNumber of particles Lowest energy\nHardin et al. (1996) \u2018Taxis\u2019 algorithm\n5 6.4746915 6.3395412\n10 32.7169495 30.2804821\n15 80.6702441 77.9961307\n20 150.8815683 142.582206\n25 243.8127603 238.313036\n30 359.6039459 351.460535\nComputations were carried out on a 3.4-GHz IBM machine with 1GB RAM, running\nM TM\n3\n{\n{\n\u000b\na\nl\nt\na\ni\nt\nr\nt\nt\nr\na\nm\nc\no\na\nd\ne\ns\nf\np\nhere.he global minimum. (B) Finding the minimum of this function using taxis. The\nertical axis shows the percentage difference between the best estimate, f (g\u02c6) and\nhe (known) global minimum.\ninimise the potential energy\n=\n\u2211\ni \/= j\n1\nd(i, j)\n(8)\nhere d(i, j) is the (great-circle) distance on the sphere between\narticles i and j. This problem is computationally difficult because,\nimilarly to then-bodyproblemand toprotein folding, thepotential\nnergy space to be searched grows very rapidly with the number\nf particles\u2014at each iteration of a search algorithm, all pairwise\nistances must be re-evaluated. Additionally, because a small dif-\nerence in even a single distance can make a large difference to the\num, a brute force search will fail due to the fine required parti-\nion of the search space. Approximations to optimal configurations\nor this problem are known (Hardin and Sloane, 1995) for various\numbers of particles.\nTo apply the taxis algorithm to the problem, first we convert the\noordinates of the particles to spherical coordinates (latitude and\nongitude). For two particles i and j, let \ti and \tj be the latitudes\nnd \ni and \nj the longitudes. Then the great-circle distance on a\nphere of unit radius is(i, j) = arccos{sin\t i sin\t j + cos\ti cos\t j cos(\ni \u2212 \nj)}. (9)\nEach bacterium in the computation represents one possible\nolution, i.e. one arrangement of particles. The n-dimensional\nocation vector of each computational agent i is then pi =\nF\nm\npATLAB version 7.1. Each run consisted of 40,000 iterations, corresponding to\n0 s total for the n = 30 (slowest) case.\n(\t1, \n1); (\t2, \n2), . . . (\tn,\nn)} and the velocity vector is vi =\n\u000b1, \u000b2, . . . , \u000bn}, where \u000bj is the bearing of the jth particle, with\nj \u2208 [\u2212\b,\b]. The location and velocity vectors are initially chosen\nt random for each computational agent (we used 20 in our simu-\nations). Note that with this definition, a tumble corresponds to all\nhe particles in one potential solution reorienting.\nAt each step of the calculation, the optimum arrangement\nmong the k agents (the one with the smallest potential energy)\ns recorded and represents the best arrangement found up to\nhat point. Using larger values of k increases the probability of\napid convergence and decreases the probability of the entire sys-\nem becoming stuck in local minima, but increases the running\nime in proportion to k. Table 1 presents the results of this algo-\nithm (left column) compared with the values given by Hardin\nnd Sloane (1995). Remarkably, \u2018taxis\u2019 seems to find more opti-\nal arrangements than those previously known. Fig. 3 shows the\nonvergence of the system to these values for different numbers\nf particles (in all results shown, the computations were stopped\nfter20,000 iterations). Thealgorithmconvergesquickly and repro-\nucibly in all cases. Adding a tumbling perturbation to the system\nvery 100\u20131000 iterations to combat stalling does not have a\nignificant effect on the performance. However, since the values\nound are better than those known (Hardin and Sloane, 1995) and\nrobably very close to optimal, this is not a major considerationig. 3. Theconvergenceof a systemofagents to the lowestpotential-energyarrange-\nent of n particles on a sphere (dotted line show the results with additional\nerturbation after 1000 iterations of stalling).\nioSyst\n3\nt\nt\no\n(\nd\ng\ni\ne\ni\nt\nt\na\nS\ne\n(\nb\ns\n(\nM\nt\n(\ni\nc\nt\ni\nW\nc\np\no\ns\ne\nG\ns\no\nC\nw\nr\no\ni\no\npD.V. Nicolau Jr. et al. \/ B\n.4. Detection of microarray features\nOne the major difficulties of microarray technology relates to\nhe processing of large and, importantly, error-loaded images of\nhe dots on the chip surface (Qin et al., 2005). Whatever the source\nf these errors, those obtained in the first stage of data acquisition\nsegmentation) are passed down to the subsequent processes, with\neleterious results.\nThe interpretation of the microarray data starts with the inte-\nration of the signal compared with the background on the area of\nndividual dots \u2013 segmentation \u2013 the results being further used for\nlaborate clustering methods (Qin et al., 2005). It follows that the\nncorrect demarcation of the circular dot features will propagate\nhroughout the whole microarray data processing and will add to\nhe other sources of variability of microarray data: biological vari-\nbility, technical variability and labeling (Zakharkin et al., 2005).\neveral methods have been proposed to de-noise data (Adjeroh\nt al., 2006). These include adaptative split and merge algorithm\nBarra, 2006), polynomial-hyperbolic spot shape model in com-\nination with the Box\u2013Cox transformation (Ekstr\u00f8m et al., 2004),\npectral embedding (Higgs et al., 2006), noise-resistant algorithms\nNovikov and Barillot, 2006), background extraction (O\u2019Neill and\nagoulas, 2003), just to name few recent contributions. We testedhe performance of \u2018extremotaxis\u2019 on both computer-generated\nartificial)microarray-like spots and realmicroarray images. Think-\nng about the intensity of an image as different levels of nutrient\noncentration in space allows us to use this algorithm to iden-\nify regions of high intensity. The form of the \u2018attractant\u2019 (image\n\u0003\nt\nFig. 4. Spot regions and centres on a 2 \u00d7 2 model microarray located using the bacems 94 (2008) 47\u201354 51\nntensity profile) will affect the motion of the model bacteria.\ne initially suppose for simplicity that the intensity of a spot\nan be well described by a Gaussian function (see first exam-\nle, above) and that the spots are arranged in a rectangular array\nf boxes, each spot of different randomly chosen size (different\ntandard deviation of the Gaussian) and each placed at a differ-\nnt location within its home box (different spatial means of the\naussian).\nTo locate the centre of a spot that is located randomly inside the\nimulation area, we can compute, at periodic intervals, the \u201ccentre\nf mass\u201d of the model bacteria as follows:\nx =\nn\u2211\ni=1\nxi\nn\nCy =\nn\u2211\ni=1\nyi\nn\n(10)\nhere xi and yi are the x and y coordinates of bacterium i. As bacte-\nia aggregate around the intensity centre of the image (the centre\nf the spot), the point (cx, cy) approaches the true centre of the\nntensity spot. We can also obtain a measure of the dimensions\nf the spot by computing the standard deviation of the bacterial\nositions:\u221a\u221a\u221a n\u2211 n\u2211\nspot =\u221a\ni=1\n(xi \u2212 cx)2 +\ni=1\n(yi \u2212 cy)2\/n (11)\nGaussian intensity distributions lead to good performance of\nhe algorithm, possibly because in the natural habitat of bacteria,\nterial algorithm. Each spot is a truncated Gaussian with \u000b = 0.15 (see text).\n52 D.V. Nicolau Jr. et al. \/ BioSystems 94 (2008) 47\u201354\n0 (lef\nt\nt\n(\nH\ns\ns\nl\ns\nt\nl\na\na\nr\nt\nt\nm\ns\nt\nt\nh\nt\nw\nt\nO\na\nc\nt\nT\n4\nb\nr\ns\nm\na\na\no\nc\nt\ni\np\nm\n(\nt\nn\nn\ni\ns\na\nw\nf\np\nr\nw\ns\nc\nb\nl\ni\ne\ng\nt\nt\nm\no\no\neFig. 5. Performance of the algorithm on a real 3 \u00d7 3 microarray image after 10\nhe nutrient is dispersed through diffusive processes, which are\nypically characterised by Gaussian or Gaussian-like distributions\nsince the fundamental solution of Fick\u2019s equation takes this form).\nowever, this form may not be a realistic model for a microarray\npot since, for example, we expect the edges of the spot to be much\nharper than the long tail of a Gaussian distribution. We can simu-\nate this by truncating theGaussiansusinga thresholdof intensity\u000b,\no that for all values of intensity below \u000b we simply set the intensity\no 0. Eq. 10 can then be modified so that only the model bacteria\nocated on voxels whose intensity is non-zero will be taken into\nccount. Eq. 11 can be similarly modified. Typical results on a 2 \u00d7 2\nrray are shown in Fig. 4.\nFinally, we measured the performance of our algorithm on a\neal microarray image chosen from (Rhodes, 2005). Fig. 5 shows\nhe determined spot sizes and centres.\nImportantly, the algorithm presented here has potential even if\nhe image is not segmented, as we have assumed it to be. In this\nore general case, the model bacteria would still converge on the\npots present in the image, but it would not be possible to identify\nhenumberof spotsor setboundson their locationsapriori. To solve\nhis problem, one would need to detect when a cluster of bacteria\nas formed, this in turn requiring an algorithm dedicated to this\nask. One possibility is to mimic \u201cquorum sensing\u201d, a phenomenon\nhereby bacteria not only consume but also release a chemoat-\nractant into the environment, to which they are in turn attracted.\nther bacteria are then attracted to this region and in such a way\nstable cluster is formed. It would then be possible to identify a\nluster formed by model bacteria when the mean \u201cchemoattrac-\nant\u201d distributed over a set of voxels exceeds a known threshold.\nhis will form the subject of future work.\n. Discussion\nWe have so far presented four examples of problems that can\ne tackled with our method. Clearly, the performance of the algo-\nithm,as is thecasewithanyoptimizationalgorithm,woulddepend\ntrongly on the nature of the problem under consideration. Why\night we, in general, expect computing with taxis to perform well\nt optimizing certain difficult functions? We can speculate on an\np\ns\nt\ni\npt) and 1000 iterations (right) (determined spot areas shown by solid circles).\nnswer to this question. Because bacteria are the oldest motile\nrganisms and because the environments in which they live are\nomplex at different scales of space and time, it might be expected\nhat they be very efficient at solving optimizationproblems, includ-\nng through chemotaxis. Recent work (Nicolau et al., submitted for\nublication) suggests that run-and-tumble is evolutionarily opti-\nal and that, remarkably, this simple algorithm can for example\nas a conservative estimate) locate on average more than 92% of\nhe total available nutrient in a Gaussian field. Furthermore, other\natural algorithms and biocomputation methods such as neural\networks, evolutionary computing,DNAcomputing and (most sim-\nlar to taxis computing), particle swarm optimization have been\nuccessful. Therefore, there are general reasons to be optimistic\nbout the potential of taxis computing for global optimization.\nThe question can also be asked in the opposite direction: for\nhat types of functions would the method be expected to per-\norm well? Functions that are difficult to optimize because of the\nresence of many local extrema are good candidates because they\nesemble in some sense the natural environments of bacteria. If\ne think of the presence of many such extrema as an \u201capparent\ntochasticity\u201d in the function (from the point of view of a parti-\nle walking the functional landscape) then we can draw an analogy\netween noise in biological environment and the presence of many\nocal minima on this landscape. In other words, local fluctuations\nn the derivative of a function are analogous to noise in a natural\nnvironment, in this sense.\nOn the other hand, run-and-tumble relies on the presence of\nradients to produce a drift towards favourable environments. If\nhe functional landscape is either extremely stochastic or discon-\ninuous, no gradient will be reliably detected and, in the limit, the\nethod reduces to a diffusion-like random local search at a number\nf random points (equal to the number of bacteria in the system)\nn the landscape. This may not always be disadvantageous\u2014for\nxample, one can imagine funnel-like landscapes (similar to the\nostulated energy landscapes of folding protein) that possess\nmooth gradients on the whole but become very stochastic near\nhe global minimum. In these cases, a combination of gradient-\nnduced global drift and noise-induced random local search may\nerform well. Nonetheless, taking these ideas together, we expect\nioSyst\nt\nr\nt\np\nl\nt\np\ne\na\np\ni\nc\nt\nh\nr\nb\nl\ne\nV\nw\nt\nw\nc\nn\np\nf\nm\ns\nt\no\ne\nl\ni\nc\nt\na\ne\nw\ns\nt\na\no\nt\na\nh\nc\nc\nt\na\nl\nB\ns\no\ne\ns\nv\np\nt\nr\nb\nc\nT\nt\ne\no\nc\ns\nm\nc\nm\ni\np\nd\np\ns\nw\no\nd\ns\no\ns\ne\nd\nb\nr\nt\no\no\ns\nt\no\nn\no\np\nF\no\nc\nA\nC\na\nO\nF\ns\ni\nR\nA\nA\nA\nB\nBD.V. Nicolau Jr. et al. \/ B\nhe type of function on which taxis computing will perform well\nelative to other methods to possess local gradients on scales larger\nhan the characteristic velocity of the moving particles.\nAs mentioned, taxis computing bears some resemblance to\narticle swarm optimization. Both exhibit some attributes of evo-\nutionary computing: each particle represents a potential solution,\nhese solutions are initially randomly chosen, and the algorithm\nroceeds by evolving the solutions from iteration to iteration, with\nach iteration being based on the last. Of course, both methods\nre based on the concept of a set of particles moving through the\nroblem space.\nTwo essential differences are that (a) in PSO the particles share\nnformation about the best solutions found up to each point in the\nomputational run and (b) in PSO the velocity of each particle in\nhe swarm is changing smoothly while in the model we propose\nere, the direction of each particle is constant during a run and is\nandomized by a tumble. The first of these is particularly essential\necause it means the swarm as a whole may become trapped in\nocal minima. In PSO, at each step the velocity of particle i is re-\nvaluated according to the equation (Call et al., 2007):\nid = wVid + c1r1(bp \u2212 xid) + c2r2(bi \u2212 xid) (12)\nhereVid is the velocity of the particle, xid is the position of the par-\nicle, bp is the position of the best solution seen by the swarm as a\nhole and bi is the position of the best solution found by the parti-\nle. r1 and r2 are random numbers and c1, c2 and w are positive real\numbers representing the \u201cweights\u201d of the three terms. Because the\narticles (a) cooperate amongst themselves and (b) remember and\nactor in their best solution to date, a sufficiently good local mini-\num,once found,may trap theparticle and in somecases thewhole\nwarm. This cannot happen in taxis computing because these fea-\nures are not present; instead, each particle relies on the structure\nf the local environment combined with random reorientations to\nxplore the search space. Although it is possible (thoughnot equally\nikely as in PSO) that an individual bacterium may become trapped\nn a local minimum for a time, this cannot happen at the level of the\nolony. Furthermore, because of the stochastic nature of run-and-\numble, its escape probability from this region will be non-zero\nnd hence the residence time will be finite. Of course, the coop-\nration property of PSO is often valuable because the swarm as a\nhole can converge towards a favourable region of the problem\npace, which can then be searched more efficiently; nonetheless,\nhe reinforcement of solutions already found at both swarm level\nnd individual level means the algorithm has a higher probability\nfmissing the globalminimumof functionswith properties similar\no those described above.\nInvestigations of the performance of the discrete version of the\nlgorithm will form the subject of future work. One can speculate,\nowever, on theprospects of thismethod.On the onehand, because\nhemotaxis relies on the presence of gradients\u2014in the context of\nomputing, a direct and well-behaved relationship between posi-\nion in n-space andfitness,we do not expect themethod to perform\ns well or as consistently for discrete functions, for which there is\nittle or no such correlation. For example, the difficulty in solving\noolean satisfiability stems from the property that a change in the\ntate of one single variable (possibly among hundreds or thousands\nf such variables) will be the difference between the expression\nvaluating as TRUE or FALSE. On the other hand, a discrete ver-\nion of PSO (Yang et al., 2004) has been successfully used to solve\narious discrete problems such as the capacitated vehicle routing\nroblem (CVRP) (Ai-ling et al., 2006). Although in the worst case,\naxis computing for discrete problems may reduce to a stochastic\nandom search through the functional landscape (if tumble proba-\nility is uncorrelatedwith changes in thefitness function, or if these\nhanges are very rare), in many cases the method may work well.\nB\nB\nCems 94 (2008) 47\u201354 53\nhis is expected to be the case, for example, when the position vec-\nor (i.e. the independent variables) is simply restricted to integer\nntries, as might happen for an integer optimization problem.\nIn the numerical results presented here, we have used a mem-\nry function of the form in Eq. (3) with wd \u2248 3wr , because E. coli\nompare roughly the last second of their lives with the previous 3\n(Strong et al., 1998), and for simplicity. However, the form of the\nemory function used by a live bacterium is believed to be more\nomplicated (Segall et al., 1986). Even more importantly, the opti-\nal form of the memory function in the context of biocomputation\ns likely to be (a) different and (b) sensitive to problem or class of\nroblems under consideration. Therefore, future work will explore\nifferent memory functions and their performances for different\nroblems.Apromisingavenue is to \u201cevolve\u201d thememory function in\nilico for a particular class of problems. For example, in recent work\ne evolved the memory function of a chemotactic bacterium-like\nrganism on a computer, in the presence of a Gaussian attractant\nistribution, finding that the evolved function resembles the bipha-\nic shape believed to be at work in the chemotactic mechanism\nf E. coli. Presumably, when exposed to different functional land-\ncapes, a \u201cspecies\u201d of digital organisms equipped with the ability to\nvolve the memory function will adapt to the function in question,\neveloping an optimal or near-optimal response.\nIt was mentioned above that in PSO, the swarm converges to the\nest solution found to date and that this strategy,while running the\nisk of missing the global optimum, means that local searches near\nhe best solution found aremore efficient\u2014because they are carried\nut by more particles. In an attempt to introduce this feature into\nurmodel bymimicking the natural behaviour of bacteria, one pos-\nible variation on the algorithm presented above would also allow\nheagents todivide (produceoffspring)when in a favourable region\nf the functional landscape. This would maintain the advantage of\not swarming to a local minimum while increasing the efficiency\nf local searches (and, if the agents can also die, reducing the pro-\nortion of computational time dedicated to unpromising regions).\ninally, future work will also focus on comparing this method with\nther methods, both of a natural computing flavour and also more\nlassical methods such as steepest descent, random search, etc.\ncknowledgements\nD.V.N. would like to acknowledge the financial support from the\nlarendon Fund, the UK Overseas Research Student Award Scheme\nnd the Devorguilla Scholarship from Balliol College, University of\nxford. K.B. gratefully acknowledges support via the Federation\nellowship of the Australian Research Council. P.K.M. was partially\nupported by a Royal Society-Wolfson Merit award. The authors are\nndebted to Tjeerd olde Scheper for help with the manuscript.\neferences\ndjeroh, D.A., Zhang, Y., Parthe, R., 2006. On denoising and compression of DNAmi-\ncroarray images. Pattern Recogn. 39, 2478\u20132493.\ndleman, L.M., 1994.Molecular computationof solutions tocombinatorial problems.\nScience 266 (11), 1021\u20131024.\ni-ling, C., Gen-ke, Y., Zhi-ming, W., 2006. Hybrid discrete particle swarm optimiza-\ntion algorithm for capacitated vehicle routing problem. J. Zhejiang Univ. Sci. A 7\n(4), 607\u2013614.\narra, V., 2006. Robust segmentation and analysis of DNA microarray spots using\nan adaptative split and merge algorithm. Comput. Methods Prog. Biomed. 8,\n174\u2013180.\nasheer, I.A., Hajmeer, M., 2000. Artificial neural networks: fundamentals, comput-\ning, design, and application. J. Microbiol. Methods 43 (1), 3\u201331.\nray, D., 1995. Protein molecules as computational elements in living cells. Nature\n376, 307\u2013312.\nremermann, H.J., 1974. Chemotaxis and optimization. J. Franklin Inst. 297, 397\u2013404.\nall, S.T., Zubarev, D.Y., Boldyrev, A.I., 2007. Global minimum structure searches via\nparticle swarm optimization. J. Comput. Chem. 28, 1177\u20131186.\n5 ioSyst\nD\nE\nE\nH\nH\nL\nL\nM\nN\nN\nN\nO\nO\nQ\nR\nS\nS\nS\nT\nV4 D.V. Nicolau Jr. et al. \/ B\norigo, M., Blum, C., 2005. Ant colony optimization theory: a survey. Theor. Comp.\nSci. 344 (2-3), 243\u2013278.\niben, A.E., Smith, J.E., 2003. Introduction to Evolutionary Computing. Springer, New\nYork.\nkstr\u00f8m, C.T., Bak, S., Kristensen, C., Rudemo, M., 2004. Spot shape modelling and\ndata transformations for microarrays. Bioinformatics 20, 2270\u20132278.\nardin, R.H., Sloane, N.J.A., 1995. Codes (Spherical) and Designs (Experimental),\nProceedings of Symposia in Applied Mathematics, Vol. 50.\niggs, B.W., Jennifer Weller, J., Solka, J.L., 2006. Spectral embedding finds mean-\ningful (relevant) structure in image and microarray data. BMC Bioinformatics 7,\n74\u201387.\ni, W., Pan, P.Q., Chen, G.T., 2006. Combined projected gradient algorithm for linear\nprogramming. Optim. Method Softw. 21 (4), 541\u2013550.\nocsei, J.T., 2007. Persistence of direction increases the drift velocity of run and\ntumble chemotaxis. J. Math. Biol. 55(1), 41\u201360.\nuller, S., Marchetto, J., Airaghi, S., Koumoutsakos, P., 2002. Optimization based on\nbacterial chemotaxis. IEEE Trans. Evol. Comput. 6 (1), 16\u201329.\nicolau, D.V., Armitage, J.P., Maini, P.K., submitted for publication. In silico evolution\nof chemotactic swimming.\nicolau, D.V., Nicolau, D.V., 2006. Biocomputation. Wiley, New York.\novikov, E., Barillot, E., 2006. A noise-resistant algorithm for grid finding in microar-\nray image analysis. Mach. Vision Appl. 17, 337\u2013345.\nW\nZems 94 (2008) 47\u201354\nbayashi, S., 1997. Paretogenetic algorithmfor aerodynamicdesignusing theNavier-\nStokes equations. Wiley, New York.\n\u2019Neill, P.,Magoulas,G.D., 2003. Improvedprocessingofmicroarraydatausing image\nreconstruction techniques. IEEE Trans. Nanobiosci. 2, 176\u2013183.\nin, L., Rueda, L., Ali, A., Ngom, A., 2005. Spot detection and image segmentation in\nDNA microarray data. Appl. Bioinformatics 4 (1), 1\u201311.\nhodes, P., 2005. Enhancement of DNA and Microarray Analysis using Image Pro-\ncessing Practices (presentation).\negall, J.E., Block, S.M., Berg, H.C., 1986. Temporal comparisons in bacterial chemo-\ntaxis. Proc. Natl. Acad. Sci. U. S. A. 83 (23), 8987\u20138991.\ntrang, G., 2005. The SIAM 100-digit challenge\u2014a study in high-accuracy. Science\n307 (5709), 521\u2013522.\ntrong, S.P., Freedman, B., Bialek, W., Koberle, R., 1998. Adaptation and optimal\nchemotactic strategy for E. coli. Phys. Rev. E 57, 4604\u20134617.\nrefethen, N., 2002. A hundred-dollar, hundred-digit challenge. SIAM News 35 (1).\nergassola, M., Villermaux, E., Shraiman, B.I., 2007. Infotaxis as a strategy for search-ing without gradients. Nature 445, 406\u2013409.\nadhams, G.H., Armitage, J.P., 2004. Making sense of it all: bacterial chemotaxis.\nNat. Rev. Mol. Cell. Biol. 5, 1024\u20131037.\nakharkin, S.O., Kim, K., Mehta, T., Chen, L., Barnes, S., Scheirer, K.E., Parrish, R.S.,\nAllison, D.B., Page, G.P., 2005. Sources of variation in affymetrix microarray\nexperiments. BMC Bioinformatics 6, 214\u2013225.\n"}