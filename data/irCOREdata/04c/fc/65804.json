{"doi":"10.1198\/016214507000001166","coreId":"65804","oai":"oai:dro.dur.ac.uk.OAI2:4655","identifiers":["oai:dro.dur.ac.uk.OAI2:4655","10.1198\/016214507000001166"],"title":"Weighted repeated median smoothing and filtering.","authors":["Fried, R.","Einbeck, J.","Gather, U."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2007-12","abstract":"We propose weighted repeated median filters and smoothers for robust non-parametric regression\\ud\nin general and for robust online signal extraction from time series in particular.\\ud\nThe new methods allow to remove outlying sequences and to preserve discontinuities (shifts) in the underlying regression function (the signal) in the presence of local linear trends. Suitable weighting  of the observations according to their distances in the design\\ud\nspace reduces the bias arising from non-linearities and improves the efficiency using larger bandwidths, while still distinguishing long-term shifts from outlier sequences.\\ud\nOther localized robust regression techniques like  S-, M- and MM-estimators as well as  weighted L_1-regression are included for comparison","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/65804.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/4655\/1\/4655.pdf","pdfHashValue":"1ad354ace5d17f1b1d8933dfbad4f4ecc02184eb","publisher":"American Statistical Association","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:4655<\/identifier><datestamp>\n      2011-09-08T08:35:05Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Weighted repeated median smoothing and filtering.<\/dc:title><dc:creator>\n        Fried, R.<\/dc:creator><dc:creator>\n        Einbeck, J.<\/dc:creator><dc:creator>\n        Gather, U.<\/dc:creator><dc:description>\n        We propose weighted repeated median filters and smoothers for robust non-parametric regression\\ud\nin general and for robust online signal extraction from time series in particular.\\ud\nThe new methods allow to remove outlying sequences and to preserve discontinuities (shifts) in the underlying regression function (the signal) in the presence of local linear trends. Suitable weighting  of the observations according to their distances in the design\\ud\nspace reduces the bias arising from non-linearities and improves the efficiency using larger bandwidths, while still distinguishing long-term shifts from outlier sequences.\\ud\nOther localized robust regression techniques like  S-, M- and MM-estimators as well as  weighted L_1-regression are included for comparison.<\/dc:description><dc:subject>\n        Signal extraction<\/dc:subject><dc:subject>\n         Robust regression<\/dc:subject><dc:subject>\n         Outliers<\/dc:subject><dc:subject>\n         Breakdown point.<\/dc:subject><dc:publisher>\n        American Statistical Association<\/dc:publisher><dc:source>\n        Journal of the American Statistical Association, 2007, Vol.102(480), pp.1300-1308 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2007-12<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:4655<\/dc:identifier><dc:identifier>\n        issn:0162-1459<\/dc:identifier><dc:identifier>\n        issn: 1537-274X<\/dc:identifier><dc:identifier>\n        doi:10.1198\/016214507000001166<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/4655\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1198\/016214507000001166<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/4655\/1\/4655.pdf<\/dc:identifier><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":[" 1537-274x","issn: 1537-274X","0162-1459","issn:0162-1459"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2007,"topics":["Signal extraction","Robust regression","Outliers","Breakdown point."],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n20 June 2008\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nFried, R. and Einbeck, J. and Gather, U. (2007) \u2019Weighted repeated median smoothing and filtering.\u2019, Journal\nof the American Statistical Association., 102 (480). pp. 1300-1308.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1198\/016214507000001166\nPublisher\u2019s copyright statement:\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\n Use policy \n \nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without \nprior permission or charge, for personal research or study, educational, or not-for-profit purposes \nprovided that : \n \n\u0083 a full bibliographic reference is made to the original source \n\u0083 a link is made to the metadata record in DRO \n\u0083 the full-text is not changed in any way \n \nThe full-text must not be sold in any format or medium without the formal permission of the copyright \nholders.  \n \nPlease consult the full DRO policy for further details. \n \nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom \nTel : +44 (0)191 334 2975 | Fax : +44 (0)191 334 2971 \nhttp:\/\/dro.dur.ac.uk \nDurham Research Online \n Deposited in DRO:\n20 June 2008\nVersion of attached file:\nAccepted\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nFried, R. and Einbeck, J. and Gather, U. (2007) 'Weighted repeated median smoothing\nand filtering.', Journal of the American Statistical Association., 102 (480), pp. 1300-1308.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1198\/016214507000001166\nPublisher\u2019s copyright statement:\nWeighted Repeated Median Smoothing and Filtering\nRoland Fried\nUniversity of Dortmund\nDepartment of Statistics\n44221 Dortmund, Germany\nJochen Einbeck\nDurham University\nDep. of Mathematical Sciences\nDurham DH1 3LE, UK\nUrsula Gather\nUniversity of Dortmund\nDepartment of Statistics\n44221 Dortmund, Germany\nWe propose weighted repeated median filters and smoothers for robust\nnon-parametric regression in general and for robust online signal extrac-\ntion from time series in particular. The new methods allow to remove\noutlying sequences and to preserve discontinuities (shifts) in the un-\nderlying regression function (the signal) in the presence of local linear\ntrends. Suitable weighting of the observations according to their dis-\ntances in the design space reduces the bias arising from non-linearities\nand improves the efficiency using larger bandwidths, while still distin-\nguishing long-term shifts from outlier sequences. Other localized robust\nregression techniques like S-, M- and MM-estimators as well as weighted\nL1-regression are included for comparison.\nKEY WORDS: Signal extraction; Robust regression; Outliers; Breakdown point.\n1 INTRODUCTION 2\n1 Introduction\nOnline analysis of a variable observed in short time lags is a common task nowadays.\nA basic objective is the extraction of the time-varying level (the signal) underlying a\nnoisy time series. Relevant signal details such as monotonic trends and abrupt shifts\nneed to be preserved, while irrelevant spikes due to measurement errors should be\neliminated. Robust filtering procedures should also be fast and simple.\nStandard median filters (Tukey 1977) remove spikes and preserve shifts, but they\nhave difficulties if the signal is not almost constant within each window (e.g. Davies,\nFried and Gather 2004). For some improvement we can weight the observations\naccording to their temporal distances to the current target point, calculating a\nweighted median (hereafter: WM). While the median of observations y1, . . . , yn\nminimizes the L1-distance, the WM \u00b5\u02c6 of y1, . . . , yn with positive weights w1, . . . , wn,\nwhich dates back to Edgeworth (1887), minimizes the weighted L1-distance\n\u00b5\u02c6 = argmin\n\u00b5\nn\u2211\ni=1\nwi \u00b7 |yi \u2212 \u00b5|. (1)\nIn time series filtering with data y1, . . . , yn measured at fixed design points x1, . . . , xn,\nwe choose wi depending on the distance between xi and the target point x, wi =\nw(|x \u2212 xi|), using a monotonically decreasing weight function w. WM filters are\npopular because of their flexibility. For a given minimal length \u2113+1 of signal details\nto be preserved one can select a WM filter with window width larger than the 2\u2113+1\nnecessary for a standard median. This allows more efficient noise suppression (Yang,\nYin, Gabbouj, Astola and Neuvo 1995). Time series filtering is a special case of\nnon-parametric smoothing with a fixed design. Generally, locally weighted median\n1 INTRODUCTION 3\nsmoothing, studied firstly by Ha\u00a8rdle and Gasser (1984), allows robust nonparametric\nestimation of the conditional median \u00b5 = g(x) of a response Y given a covariate x.\nLocal linear fits are usually preferable to local constant fits (Fan, Hu and Truong\n1994). The advantages are well-known in case of L2-regression (Fan 1992, Hastie\nand Loader 1993). However, robustness is needed in the presence of outliers. Davies\net al. (2004) suggest the repeated median (RM, Siegel 1982) for the extraction\nof monotonic trends from time series. The repeated median has the same opti-\nmal asymptotic 50% breakdown point as the standard median, while relying on a\nconstant slope within each window instead of a constant level.\nWe develop weighted repeated medians (WRMs) for robust nonparametric smooth-\ning in the presence of trends. WRMs allow for application of longer windows than\n\u2018standard\u2019 RM filters, without being severely biased when the signal slope varies\nover time. For a full online analysis we approximate the signal at the current time\npoint without any time delay, giving largest weights to the most recent observations.\nWe note that there are locally weighted versions of other robust regression tech-\nniques: Equal weighting results in the highest efficiency of weighted Theil-Sen esti-\nmators and the highest asymptotic breakdown point of 29.3% among all efficiency-\noptimal weighting schemes in the case of an equally spaced design (Scholz 1978).\nSimpson and Yohai (1998) discuss the stability of one-step GM estimators (including\nweighted L1-regression) in approximately linear regression with a random design.\nThe paper is organized as follows. Section 2 reviews weighted medians and intro-\nduces weighted repeated medians and weighted L1-regression. Section 3 derives\n2 ROBUST SMOOTHING AND FILTERING 4\nanalytical properties of these methods. Section 4 reports results from simulations.\nSection 5 exemplifies the methods on some time series, followed by some conclusions.\nProofs of the analytical results are given in an appendix.\n2 Robust smoothing and filtering\nWe start with alternative derivations of weighted medians. WM filters down-weight\nremote observations, which reduces problems due to trends, but does not overcome\nthem completely. For further improvement we apply regression techniques with\nweighting according to the temporal distances. We review weighted L1-regression\nbefore introducing weighted repeated medians.\n2.1 Alternative derivations of weighted medians\nFor non-negative integer valued weights w1, . . . , wn, a simple representation of the\nweighted median of real numbers y1, . . . , yn is given by\n\u00b5\u02c6 = med{w1 \u22c4 y1, . . . , wn \u22c4 yn} (2)\nwhere w \u22c4 y denotes replication of y to obtain w identical copies of it.\nNotation (2) can be used in an extended way also for positive real weights: Let y(1) \u2264\n. . . \u2264 y(n) denote the ordered observations and w(1), . . . , w(n) the corresponding\npositive weights. Then the weighted median of y1, . . . , yn is \u00b5\u02c6 = y(k), where\nk = max\n{\nh :\nn\u2211\ni=h\nw(i) \u2265 1\n2\nn\u2211\ni=1\nwi\n}\n. (3)\n2 ROBUST SMOOTHING AND FILTERING 5\nFor example, the WM of 1, 2, 3, 7 with weights 0.1, 1.6, 1.4 and 0.5 is y(3) = 3,\nsince 0.5 + 1.4 \u2265 3.6\/2. Generally, (3) and (1) yield the same results. However,\nthe whole interval [y(k\u22121), . . . , y(k)] solves (1) if\n\u2211n\ni=k w(i) =\n1\n2\n\u2211n\ni=1 wi. The solution\ny(k\u22121) would be obtained in (3) by summing from the bottom instead of from the top.\nThis ambiguity can be solved as usually by choosing the midpoint of the interval.\nTwo WMs with respective weights w1, . . . , wn and w\n\u2032\n1, . . . , w\n\u2032\nn are equivalent iff they\nalways give the same result. This is the case iff for every index set I \u2282 {1, . . . , n}\n\u2211\ni\u2208I\nwi \u2265 0.5\nn\u2211\ni=1\nwi \u21d0\u21d2\n\u2211\ni\u2208I\nw\u2032i \u2265 0.5\nn\u2211\ni=1\nw\u2032i .\nFor n = 3, the WM with weights (w1, w2, w3) = (2, 4, 3) is equivalent to the standard\nmedian: for this the weights must be balanced, such that no subset of less than\n\u230a(n + 1)\/2\u230b weights sums up to at least half the total mass. The WM is an order\nstatistic with its rank depending on the observations and the weights.\n2.2 Weighted median smoothing and filtering\nLet y1, . . . , yN be observed at fixed design points x1 < . . . < xN under the model\nYi = g(xi) + ui + vi, i = 1, . . . , N, (4)\nwhere ui is symmetric observational noise with mean zero and finite variance \u03c3\n2, and\nvi is spiky noise from an outlier generating mechanism. The goal is to approximate\nthe signal g(x) for x \u2208 [x1, xN ], representing the level of Y as a function of x. To\ndistinguish signal and noise we assume \u00b5 = g(x) to be smooth with infrequent shifts.\nThe observational noise is assumed to be rough and the number of subsequent spikes\n2 ROBUST SMOOTHING AND FILTERING 6\nto be small as compared to the durations between the shifts.\nFan and Hall (1994) and Wang and Scott (1994) propose local constant weighted L1-\nestimates g\u02c6(x) based on (1), using weights w1(x), . . . , wN(x). In time series filtering,\nthe design is usually equidistant, xi = i, i = 1, . . . , N . In retrospective applications,\nwhen some delay is possible, we usually approximate the level in the window center\nchoosing bell-shaped weights which are symmetric to the center and monotonically\ndecreasing to both sides of it. When focusing on online analysis, where the target\npoint x at which we estimate the signal is at the end of the window, we apply\nmonotonically increasing weights (e.g. Einbeck and Kauermann 2003).\n2.3 Weighted L1-regression\nThe theoretical properties of local linear mean estimators carry over to local linear\nmedian estimators based on L1-regression (Fan et al. 1994). For the local linear\nmedian at x, \u00b5\u02c6, we fit a straight line to the data using a weight function,\n(\u00b5\u02c6, \u03b2\u02c6) = argmin\n\u00b5,\u03b2\nN\u2211\ni=1\nwi(x)|yi \u2212 \u00b5\u2212 \u03b2(xi \u2212 x)| (5)\nThe solution of weighted L1-regression (WL1) is generally not unique. In case of a\nfixed design, w1(x), . . . , wN(x) are fixed andWL1-regression minimizes the residuals\nw.r.t. a norm. Thus, the set of minimizing values is at least convex.\nSeveral algorithms have been developed for L1-regression and for quantile regression\nin general (Portnoy and Koenker 1997, Koenker 2005), which can be adapted to\n2 ROBUST SMOOTHING AND FILTERING 7\nweighted L1-regression since the ordinary L1-solution of the modified problem\nmin!\nN\u2211\ni=1\n|wi(x) \u00b7 yi \u2212 wi(x) \u00b7 \u00b5\u2212 \u03b2 \u00b7 wi(x) \u00b7 (xi \u2212 x)| (6)\nwith data (wi(x), wi(x) \u00b7 xi, wi(x) \u00b7 yi) is the same as the original WL1-solution. We\nuse an approximative L1-procedure for simplicity and increased robustness. Starting\nfrom the standard RM, the algorithm iterates a finite number of steps between\nminimization of the objective function w.r.t. \u00b5 given the current \u03b2 and vice versa.\n2.4 Weighted repeated medians\nDavies et al. (2004) investigate robust regression techniques like the standard RM\nand L1-regression for delayed signal extraction from time series. Online versions\nof such procedures are compared by Gather et al. (2006). The RM is found to be\npreferable to the alternatives in both situations. The resulting (standard) RM filters\nfit a linear trend to the data in each window, replacing the assumption of a locally\nconstant signal underlying the median by a trend with locally constant slope. This\nmotivates us to generalize the RM, permitting localization by weighting.\nConsider a window of width n with observations (x1, y1), . . . , (xn, yn), where x1 <\n. . . < xn. We define the weighted repeated median (WRM) with two possibly\ndifferent sets of weights wi, w\u02dci, i = 1, . . . , n, as\n\u03b2\u02dcWRM(x) = medj=1,...,nw\u02dcj \u22c4\n(\nmedi6=jw\u02dci \u22c4 yi \u2212 yj\nxi \u2212 xj\n)\n, (7)\n\u00b5\u02dcWRM(x) = med\n(\nw1 \u22c4\n(\ny1 \u2212 (x1 \u2212 x)\u03b2\u02dcWRM(x)\n)\n, . . . ,\nwn \u22c4\n(\nyn \u2212 (xn \u2212 x)\u03b2\u02dcWRM(x)\n))\n, (8)\n3 ANALYTICAL PROPERTIES 8\ni.e. we weight the pairwise slopes in the inner median depending on the position of\nxi, and in the outer median on the position of xj when estimating the slope \u03b2(x).\nWe choose both sets of weights wi and w\u02dci to be monotonic for online application.\nWe call two WRMs with weights w1, . . . , wn, w\u02dc1, . . . , w\u02dcn and w\n\u2032\n1, . . . , w\n\u2032\nn, w\u02dc\n\u2032\n1, . . . , w\u02dc\n\u2032\nn\nequivalent if the slope and the level estimate are always identical. A necessary\ncondition for this is the equivalence of the WMs corresponding to w1, . . . , wn and\nw\u20321, . . . , w\n\u2032\nn: if the slope estimates are identical, there are samples such that the WMs\nof the slope-corrected observations are different otherwise. The following additional\ncondition for w\u02dc1, . . . , w\u02dcn and w\u02dc\n\u2032\n1, . . . , w\u02dc\n\u2032\nn guarantees the equivalence of WRMs:\nThe weighted medians corresponding to w\u02dc1, . . . , w\u02dcn and to w\u02dc\n\u2032\n1, . . . , w\u02dc\n\u2032\nn are equiv-\nalent, and for each i \u2208 {1, . . . , n} the weighted medians corresponding to\nw\u02dc1, . . . , w\u02dci\u22121, w\u02dci+1, . . . , w\u02dcn and to w\u02dc\n\u2032\n1, . . . , w\u02dc\n\u2032\ni\u22121, w\u02dc\n\u2032\ni+1, . . . , w\u02dc\n\u2032\nn are also equivalent.\n3 Analytical properties\nAs usually we assume that for every target point x the subset of design points\nwith non-zero weights forms a window of subsequent points. We discuss analytical\nproperties of the above smoothers for a single window of width n, under the condition\n(C) y1, . . . , yn are values of a response observed at fixed x1 < . . . < xn. w1, . . . , wn\nand w\u02dc1, . . . , w\u02dcn are the corresponding sets of strictly positive weights (we sup-\npress the dependence on x since we treat a single target point x).\n3 ANALYTICAL PROPERTIES 9\n3.1 Equivariances\nEquivariances guarantee that an estimate reacts as expected to systematic changes in\nthe data. Location equivariance means that adding a constant c changes the estimate\nby c. Scale equivariance means that multiplying all of y1, . . . , yn by c changes the\nestimate by the same factor. The level estimates obtained from weighted medians\nand weighted repeated medians possess both these properties.\nWe also require that the quality of the smoothing does not depend on linear trends.\nThis can be guaranteed by applying regression equivariant estimators. When re-\ngressing a variable y on a variate z \u2208 Rd, regression equivariance means that adding\na multiple c\u2032z to y for a c \u2208 Rd changes the estimate by this vector c. (Weighted)\nRMs as defined here are equivariant w.r.t. adding a vector multiple (a, b)zi = a+bxi\nof zi = (1, xi)\n\u2032 to yi, i = 1, . . . , n. A procedure for (weighted) L1-regression fulfills\nthis equivariance if the initial estimator, e.g. the RM, fulfills it since we just act on\nthe residuals thereafter. The performance of WMs depends on trends since they do\nnot make use of the covariate values x1, . . . , xn, but regress on a constant level only.\n3.2 Removal of spiky noise\nThe removal of irrelevant spikes and the preservation of relevant signal details, in\nparticular of long-term shifts, are essential properties of robust smoothers. The per-\nformance of moving window techniques can be measured by two related quantities,\nthe breakdown point and the exact fit point of the underlying functional.\n3 ANALYTICAL PROPERTIES 10\nThe finite sample replacement breakdown point measures the minimal fraction of\ndata which can drive an estimate beyond all bounds when being set to arbitrary\nvalues (Ellis and Morgenthaler 1992). In the context of nonparametric smoothing\nby moving window techniques, this corresponds to the smallest fraction of contam-\nination within a window which can cause an arbitrarily large spike in the output.\nA single outlier can already do so in local (weighted) least squares fits. See Davies\nand Gather (2005) for a discussion of breakdown points.\nAnother popular quantity in signal extraction is the number of spikes a procedure\ncan remove completely from a prototype signal in noise-free conditions, where the\nvariance \u03c32 of the observational noise equals zero. When applying a regression\nfunctional to a moving window assuming a locally linear trend, this number of spikes\ncorresponds to the exact fit point of the functional. This is the smallest fraction of\nobservations which can cause an estimated regression hyperplane to deviate from\nanother hyperplane although all the other data points lie on the latter (Rousseeuw\nand Leroy 1987, Section 3.4). For regression and scale equivariant functionals the\nexact fit point is not smaller than the finite sample breakdown point. Let \u230aa\u230b be the\nlargest integer not larger than a. The standard median fits a constant exactly if less\nthan \u230a(n+ 1)\/2\u230b out of n observations deviate from it, which equals its breakdown\npoint. Up to \u230a(n\u22121)\/2\u230b subsequent spikes are removed completely from a constant\nsignal. In retrospective application with a symmetric window, a shift from one\nconstant to another is preserved exactly when applying an odd n = 2m + 1. In\nonline application, the shift gets delayed by m time points.\n3 ANALYTICAL PROPERTIES 11\nWithin a trend period a standard median cannot preserve exactly a shift into the\nopposite direction, and a single spike causes smearing. An advantage of regression\ntechniques is that the removal of outliers and the preservation of shifts do not depend\non linear trends since the WRM and WL1-regression are equivariant to them. The\nbreakdown and the exact fit point of the standard RM for fitting a straight line both\nequal \u230an\/2\u230b\/n. Thus, it can remove \u230an\/2\u230b\u22121 subsequent spikes from a linear trend,\nwhich is only slightly less than for the standard median when the signal is constant.\nFor the derivation of breakdown and exact fit points of robust weighted regression\nmethods, let zi \u2208 Rd be fixed regressors, \u03b3 \u2208 Rd the parameter to be estimated, and\nyi = z\n\u2032\ni \u00b7 \u03b3 + ui, i = 1, . . . , n.\nWe transfer results for standard L1-regression to the weighted case using the modi-\nfied problem (6). From He, Jureckova, Koenker and Portnoy (1990, Theorem 5.3),\nEllis and Morgenthaler (1992, Theorem 2.3) and Mizera and Mu\u00a8ller (1999, Theorem\n2) we conclude that both the breakdown and the exact fit point of WL1-regression\nequal k\/n, where k = min |I|, I \u2282 {1, . . . , n}, for which 0 6= \u03b3\u02dc \u2208 Rd exists such that\n\u2211\ni\u2208I\nwi \u00b7 |z\u2032i \u00b7 \u03b3\u02dc| \u2265\n\u2211\ni\/\u2208I\nwi \u00b7 |z\u2032i \u00b7 \u03b3\u02dc| . (9)\nSince a WM regresses on a constant, zi \u2261 1, its breakdown and exact fit point is\nthe minimal fraction of weights which sum up to at least 0.5\n\u2211n\ni=1 wi. It is straight-\nforward that a WM which is not equivalent to the standard median has breakdown\npoint smaller than the optimal value \u230a(n+1)\/2\u230b\/n of the latter. The loss in robust-\nness due to weighting is the larger, the more the weights vary.\n3 ANALYTICAL PROPERTIES 12\nCalculating the numerical value of the breakdown and exact fit point of (weighted)\nL1-regression is more difficult for d \u2265 2 since more directions need to be considered\nthen. For an algorithmic solution see Giloni and Padberg (2004).\nSimple upper bounds for simple linear regression, yi = \u00b5 + \u03b2(xi \u2212 x), result from\nchoosing the coordinate axis as directions \u03b3\u02dc in (9): The breakdown point of WL1-\nregression with weights w1, . . . , wn is not larger than min{kl, ks}\/n, where kl is the\nminimal cardinality of I \u2282 {1, . . . , n} such that\n\u2211\ni\u2208I\nwi \u2265\n\u2211\ni\/\u2208I\nwi\nand ks is the minimal cardinality of I \u2282 {1, . . . , n} such that\n\u2211\ni\u2208I\nwi|xi \u2212 x| \u2265\n\u2211\ni\/\u2208I\nwi|xi \u2212 x| .\nThis upper bound is generally not strict as it only considers two directions: For\nstandard L1-regression and an equidistant, centered design the upper bound is\n1 \u2212 1\/\u221a2 = 29.3% asymptotically, while the true value is at most 25% (Ellis and\nMorgenthaler 1992, Proposition 4.1). Nevertheless, the upper bound is attained by\nthe approximative weighted L1-algorithm outlined in Section 2.3.\nNext we address breakdown and exact fit of weighted repeated medians.\nProposition 1 Let a WRM (\u00b5\u02dc, \u03b2\u02dc) weighted by w1, . . . , wn and w\u02dc1, . . . , w\u02dcn fulfill (C).\na) A lower bound for the breakdown and the exact fit point of (\u00b5\u02dc, \u03b2\u02dc) is\nmin{ks, kl}\/n, where ks is the minimal number for which\nks\u2211\ni=1\nw\u02dc[i] \u2265\nn\u2211\ni=ks+2\nw\u02dc[i],\nwith w\u02dc[1] \u2265 w\u02dc[2] \u2265 . . . \u2265 w\u02dc[n] denoting the ordered weights, and kl is the mini-\nmal number of weights w[1] \u2265 w[2] \u2265 . . . \u2265 w[n] for which\nkl\u2211\ni=1\nw[i] \u2265\nn\u2211\ni=kl+1\nw[i].\n3 ANALYTICAL PROPERTIES 13\nb) An upper bound for the breakdown and exact fit point of (\u00b5\u02dc, \u03b2\u02dc) is min{k\u2032s \u2212\n1, kl}\/n, where kl is as in a) and k\u2032s is minimal with\nk\u2032s\u2211\ni=2\nw\u02dc[i] \u2265\nn\u2211\ni=k\u2032s+1\nw\u02dc[i].\nc) The breakdown and the exact fit point of (\u00b5\u02dc, \u03b2\u02dc) do not exceed the \u230an\/2\u230b\/n value\nof the standard repeated median.\nThe lower and the upper bound given in a) and b) are not always identical, consider\nn = 5 and (w1, . . . , w5) = (w\u02dc1, . . . , w\u02dc5) = (1, 1, 1, 3, 2), for which ks = 1, but k\n\u2032\ns = 3.\nThe lower bound is attained in the most relevant cases:\nProposition 2 Under (C), the breakdown and the exact fit point of a weighted re-\npeated median with symmetric bell-shaped or monotonic weights equal min{ks, kl}\/n.\nThere are also WRMs which attain their respective upper bound, e.g. the one for\nn = 5 mentioned above. The previous results allow to determine weighted L1- and\nWRM filters which remove outlier patches up to a given length completely while\nexactly preserving longer shifts under idealized conditions (\u03c32 = 0).\nIn the simulations we consider full online applications using the target point x = xn\nand triangular weighting schemes with wi(x) = w\u02dci(x) = i, i = 1, . . . , n. Table 1\ngives the minimal widths n necessary to remove outlier patches of different lengths\nfor weighted and standard L1- and RM filtering. An equidistant design is assumed\nfor L1, while in case of the WRM the results hold for any fixed design. n increases\nfor the WRM as compared to the standard RM, while weighting allows to decrease\nn for L1-filtering because of increased robustness. Nevertheless, the WL1 does not\nachieve the optimal robustness of the standard RM and needs somewhat larger n.\n4 MONTE CARLO STUDY 14\nTable 1: Minimal window width n necessary to remove outlier patches of length \u2113 in\nonline application, weighted L1- (left) and RM-regression (right).\n\u2113 1 2 3 4 5 6 1 2 3 4 5 6\nstandard L1 \/ RM 5 8 11 15 18 22 4 6 8 10 12 14\ntriangular, wi(x) = i 4 7 10 14 17 21 5 9 12 15 19 22\n3.3 Continuity\n(Lipschitz) continuity guarantees local stability to small changes in the data due to\nobservational noise or rounding. Every WM is Lipschitz continuous with constant\n1 as changing every observation by less than \u03b4 changes any order statistic at most\nby \u03b4, and a WM always corresponds to one of these. For fixed design, the slope\nestimate of a WRM changes at most by 2\u03b4\/min(xi \u2212 xi\u22121), so that the WRM level\nis Lipschitz continuous with constant 2max{|x1\u2212x|, |xn\u2212x|}\/min(xi\u2212xi\u22121) since\nnone of the slope corrected observations changes more.\n4 Monte Carlo study\nRobust filters should preserve long-term shifts as discussed in Section 3.2, while\nremoving irrelevant outlier sequences. We compare the online filters via simulations,\nconcentrating on equidistant designs as in time series filtering. Data are generated\n4 MONTE CARLO STUDY 15\nfrom model (4) with standard Gaussian white noise ui. The signal is a sine function,\ng(xi) = \u03bd \u00b7 0.5 \u00b7 sin(i \u00b7\u03c0\/100), i = 1, . . . , 100, where \u03bd \u2208 {0, 1, . . . , 20} determines the\ndegree of non-linearity. We treat a single window with target point x = 50.\nIn intensive care, five subsequent strongly deviant observations in hemodynamic\nseries point at a relevant shift, while shorter sequences are typically irrelevant (Imhoff\net al. 2002). Accordingly, we fix widths in preliminary experiments with the aim\nof closely tracking large shifts from the fifth deviant observation on. Choosing the\nwidth maximal under this restriction optimizes both efficiency and robustness.\nFor the standard RM and L1-regression we select n = 11. Using triangular weights\nwe choose for the WRM the maximal width leading to elimination of at most \u2113 = 4\nobservations (n = 18 according to Table 1), while we choose n = 16 for WL1. We\nalso include the fast S-estimator from R with n = 10 (command fast.s), from the\nR-package MASS (command lqs) the least median of squares (LMS) with n = 9,\nthe least trimmed squares (LTS) with n = 10, and the S-estimator with n = 10;\nfrom the R-package RRCOV the reweighted LTS (RLTS, command ltsReg) with\nn = 11; also from MASS (command rlm) the M-estimators with the Huber, the\nHampel and Tukey\u2019s bisquare-function all with n = 14 and the MM-estimator with\nn = 10, and finally the MM-estimator from package ROBLM with n = 11. We\nfind the latter to outperform the other MM-estimator in our context. Similarly, the\nLTS and the S-estimator showed little advantage over the LMS and RLTS, and the\nM-estimators with the Huber or the Hampel function over Tukey\u2019s biweight.\nComparing the ability of the procedures to distinguish relevant from irrelevant pat-\n4 MONTE CARLO STUDY 16\nterns, we generate data resembling the intrusion of a shift into the window, adding\nthe same constant c to an increasing number of observations at the window end. In\naccordance to the above demands, up to four deviant observations are regarded as\noutliers and should not affect the estimation, while from five deviant observations on\nthe shift should be reproduced. Figure 1 compares the bias for the signal value before\nthe shift caused by \u2113 = 1, 2, . . . , 10 shifted observations at times t = 50, 49, . . . , 41,\ncalculated from 2000 windows each. An ideal curve stays at zero up to \u2113 = 4 and\nthen increases abruptly to the added c representing the new level.\nThe RMs are less biased than the L1-estimates in case of four outliers, with the\ndifferences being small between the weighted and the unweighted versions. Addi-\ntionally, the RMs reproduce the shift well from the fifth observation on, while the\nL1-estimates overshoot the new signal value for some observations. The LMS and\nthe RLTS are even less biased than the RMs in case of two to four outliers, but\nthe RLTS overshoots shifts. The MM resists a few outliers very well, but becomes\nbiased in case of three or four outliers and also overshoots shifts. The M-estimator,\nfinally, would strongly smooth shifts. The LMS has been proposed repeatedly for\nimage analysis because of its excellent edge preservation (Meer et al. 1991, Mu\u00a8ller\n1999, Rousseeuw and Van Aelst 1999). It is followed by the RLTS or the WRMs\nin this exercise, depending on whether one considers blurring of edges to be worse\nthan overshooting or vice versa. We obtain similar results for the estimation of the\nslopes (not shown here). The LMS is generally the least biased, followed by the\nRLTS and the WRM. All these results have been confirmed for other shift sizes and\n5 APPLICATION TO TIME SERIES 17\nfor window widths chosen for preserving shifts from the fourth observation on.\nFigure 2 compares the efficiencies for Gaussian noise as a function of the non-linearity\n\u03bd, in the absence of outliers and shifts. Because of a bias for \u03bd 6= 0 we measure the\nefficiency by the percentage mean square error MSE as compared to the standard\nRM, obtained from 10000 runs for each \u03bd = 0, . . . , 20. Weighting allows to gain\na considerable amount of efficiency both for the RM and L1-regression due to the\nlonger window widths possible then. This is even more true for the slope. Only\nthe M-estimator is more efficient than these, but as we have seen before it does not\nreproduce shifts. The RLTS is somewhat less efficient than the unweighted RM and\nL1, while the LMS is much less efficient.\n5 Application to time series\nFor further comparison we apply the filters to some time series. The simulated data\ndepicted in Figure 3 are generated by overlaying a senoidal signal of length N = 250\nwith a shift by standard Gaussian white noise. A temporary shift of duration seven\nis inserted at xi = 70 to investigate the preservation of relevant patterns.\nThe procedures are challenged by inserting irrelevant sequences of up to three out-\nliers of size ten. We choose the widths cited in Section 4 for tracking shifts after \u2113 = 4\nobservations. Therefore, the filters resist the irrelevant outliers, while delaying the\nshifts by four observations. We only present the results for the WRM, the LMS and\nthe RLTS since they outperformed the other methods in the simulation study. As\n6 CONCLUSIONS 18\nwas to be expected, the RLTS (like the MM- and the L1) overshoots the shifts. The\nLMS (and also the LTS and the S-estimator) provides wiggly outcomes according to\nits large variability. The lack of stability of the LMS has been noted before in time\nseries filtering (Davies et al. 2004). Like the MM it reacts to irrelevant patterns in\nthe data, e.g. at t = 200. The WRMs provide stable outcomes and track the shifts\nwell, although they are somewhat affected by long outlier sequences.\nWe also consider real data representing the arterial pressure of a patient in intensive\ncare, see also Figure 3. The filters are applied using the same widths as before. The\nMM-, RLTS- and L1-filters again overshoot the shifts. The WRM provides the best\nresults since it tracks the shifts well like the LMS and LTS, while being less variable.\n6 Conclusions\nWe have investigated weighted repeated median and weighted L1-filters for robust\ndetail-preserving online smoothing of noisy data with underlying trends. In case of\nthe repeated median, weighting the observations according to their distance in the\ndesign space improves the local adaption to nonlinear regression functions, allows to\nuse longer windows and increases the efficiency as compared to the standard version,\nretaining the suppression of outlying spikes and the preservation of relevant shifts.\nIn case of L1-regression, on the contrary, weighting can increase the robustness and\nthe discrimination between sequences of relevant and irrelevant length.\nWe have compared the methods to several competitors also based on robust regres-\n6 CONCLUSIONS 19\nsion, namely LMS-, LTS-, S-, M- and MM-estimators. The LMS- and LTS can\nprovide even higher robustness and better edge preservation than WRMs, but this\nadvantage is outweighted by a much higher variability and a lack of stability of the\noutcome, rendering these methods little reliable in automatic application. M- or\nMM-estimators do not preserve shifts well. WRMs combine the advantages of sta-\nbility, good edge preservation, high robustness and considerable Gaussian efficiency.\nAdditionally, WRMs with triangular weights allow application of linear time algo-\nrithms based on updating the output from the previous time point (Bernholt and\nFried 2003). Experiments with different error distributions resulted in an average\ncomputation time for an update of about 2n \u00b7 10\u22126 seconds on a 2 GHz Intel Core\nDuo with 512 MB DDR2\/667 when using a window of width n, while exact compu-\ntation of the LMS and related methods needs O(n2) time. WRMs with triangular\nweight functions are thus much faster than these competitors and can be used to\nanalyze high-frequency data in real time. An implementation of the WRM filter is\navailable in the R-package ROBFILTER to be found at http:\/\/cran.r-project.org.\nA question not addressed here is the automatic identification of level shifts, for\nwhich many different rules have been suggested. A comparison of the different\npossibilities arising in combination with WRMs is beyond the scope of this paper.\nSince we can tune a WRM to track level shifts with a prescribed delay of, say, \u2113\nobservations, the following approach seems natural. Future signal values can be\npredicted by extrapolation of the regression line fitted to the most recent window.\nAs an intruding shift starts to influence the filter output when \u2113 + 1 observations\n6 CONCLUSIONS 20\nare shifted and the output reaches the new level, it suggests itself to compare the\ncurrent filter output at time t to its prediction calculated at time t \u2212 \u2113 \u2212 1. A\nshift is detected if this difference is large relative to its standard deviation. The\nvariance can be estimated by exponential smoothing of the squared differences even\nin case of a time-varying variability. We tested this approach on some examples\nusing percentiles of the standard Gaussian distribution as thresholds and found it\nto work well, albeit sometimes more sensitive to small changes than desired. This\ncan be overcome by using thresholds corresponding to relevant changes.\nIn the simulations we have concentrated on the typical equidistant designs arising in\ntime series filtering. Non-equidistant designs are found e.g. in option pricing. The\nanalytical results for the WRM presented here remain valid then. Based on our so far\nlimited experience we can say that the above comparisons with respect to variability,\nrobustness and shift preservation carry over to more general situations, assuming\nthat there are no outliers in the design space. Therefore we tentatively recommend\nWRMs also for online application with a non-equidistant design, although more\ninvestigations are needed w.r.t. the suitable choice of the window width.\nAll these results rely on outlier patches being well separated. When such patches\noccur close to each other, using a standard RM with a reasonable width may still\nbe the best decision since it can deal with the largest fraction of outliers.\nAcknowledgements\nWe thank Steve Portnoy, an associate editor and two referees for many helpful\n6 CONCLUSIONS 21\nsuggestions, which lead to substantial improvements of an earlier draft. Part of\nthis work was carried out while the second author was working under the Science\nFoundation Ireland basic research grant 04\/BR\/M0051. The financial support\nof the Deutsche Forschungsgemeinschaft (SFB 475, \u201dReduction of complexity in\nmultivariate data structures\u201d) is gratefully acknowledged.\nAppendix: Proofs\nProof of Proposition 1. Since for regression and scale equivariant functionals like\nWRMs the exact fit point (EFP) is at least as large as the finite-sample breakdown\npoint (BP), it suffices to prove a) for the BP and b) for the EFP.\na) Less than k = min{ks, kl} modifications have bounded effects on the level and\nthe slope: When excluding an unmodified, \u2018clean\u2019 observation yj, the sum of the\nweights is still larger for the clean than for the modified observations. Hence,\nfor every clean yj the inner median in the slope corresponds to a clean pair and\nis bounded. The WRM slope is bounded by the same quantity. The weighted\nmajority of the slope corrected yj and thus the WRM level is then also bounded.\nb) Because of regression equivariance we may assume that all observations are zero,\nand need to find k = min{k\u2032s \u2212 1, kl} substitutions causing the fit to deviate from\nthe horizontal axis. If k = k\u2032s \u2212 1, let the positions I = {i1, . . . , ik+1} correspond to\nthe largest weights w\u02dc[1] \u2265 . . . \u2265 w\u02dc[k+1]. Set the rightmost k of these observations,\ni.e. with largest x, on an increasing line with slope b > 0 through the leftmost of\n6 CONCLUSIONS 22\nthem. For each observation in I the total weight of the other observations in I is at\nleast the total weight of the unmodified zero observations. The corresponding inner\nmedians and the WRM slope is hence at least b\/2. If k = kl, set the k observations\nwith largest wi to an arbitrary value M , obtaining a WRM level of at least M\/2.\nc) The standard RM has maximal BP among regression equivariant methods\nincluding WRMs. Its EFP is maximal as it equals its upper bound, ks = k\n\u2032\ns \u2212 1. 2\nProof of Proposition 2. It is sufficient to prove that the EFP equals its lower bound.\nWe assume w.l.o.g. that all n observations equal zero and show that k = min{kl, ks}\nmodifications can make the WRM line deviate from the horizontal axis.\nSymmetric bell-shaped weights and monotonic weights can be treated in the same\nway. The k largest weights w\u02dcj are at subsequent positions xi\u2212k+1 < . . . < xi.\nIf ks \u2264 kl, proceed as follows: If w\u02dc1 + . . . + w\u02dci\u2212k \u2265 w\u02dci+1 + . . . + w\u02dcn, set the k\nobservations at xi\u2212k+1, . . . , xi to an increasing line with slope 1 through (xi\u2212k, 0).\nw\u02dci\u2212k is the (k+1)th largest w\u02dcj then. The pairwise slope is 1 if both design points are\nselected from xi\u2212k, . . . , xi, it is strictly positive if one is from x1, . . . , xi\u2212k\u22121 and the\nother from xi\u2212k+1, . . . , xi, and it is zero if both are from x1, . . . , xi\u2212k, xi+1, . . . , xn.\nThe inner median corresponding to xi\u2212k is strictly positive since the total weight\nof the modified is at least that of the unmodified observations. This also holds for\nthose at xi\u2212k+1, . . . , xi since the pairwise slopes through x1, . . . , xi are larger than\nzero. Since the total weight at xi\u2212k, . . . , xi is larger than the rest, the WRM slope\nis larger than zero and the WRM line deviates from the horizontal axis.\nIf w\u02dc1 + . . . + w\u02dci\u2212k < w\u02dci+1 + . . . + w\u02dcn, set the observations at xi\u2212k+1, . . . , xi to an\n6 CONCLUSIONS 23\nincreasing line through (xi+1, 0) and use the same arguments as before interchanging\nthe role of x1, . . . , xi\u2212k and xi+1, . . . , xn.\nIf kl < ks, set the k observations with largest wi to 1. From the proof of Proposition\n1a) follows that the slope estimate is zero, but the level estimate is at least 0.5. 2\nReferences\nBernholt, T., and Fried, R. (2003). \u201cComputing the Update of the Repeated Me-\ndian Regression Line in Linear Time,\u201d Information Processing Letters, 88, 111\u2013117.\nDavies, P. L., Fried, R., and Gather, U. (2004), \u201cRobust Signal Extraction for On-\nline Monitoring Data,\u201d Journal of Statistical Planning and Inference, 122, 65\u201378.\nDavies, P. L., and Gather, U. (2005), \u201cBreakdown and Groups,\u201d Annals of Statis-\ntics, 33, 977\u20131035.\nEdgeworth, F. Y. (1887), \u201cA New Method of Reducing Observations Relating to\nSeveral Questions,\u201d Phil. Mag., 24, 184\u2013191.\nEinbeck, J., and Kauermann, G. (2003), \u201cOnline Monitoring with Local Smoothing\nMethods and Adaptive Ridging,\u201d Journal of Statistical Computation and Simula-\ntion, 73, 913\u2013929.\nEllis, S. P., and Morgenthaler, S. (1992), \u201cLeverage and Breakdown in L1 Regres-\nsion,\u201d Journal of the American Statistical Association, 87, 143\u2013148.\nFan, J. (1992), \u201cDesign-adaptive Nonparametric Regression,\u201d Journal of the Amer-\nican Statistical Association, 87, 998\u20131004.\nFan, J., and Hall, P. (1994), \u201cOn Curve Estimation by Minimizing Mean Absolute\n6 CONCLUSIONS 24\nDeviation and its Implications,\u201d Annals of Statistics, 22, 867\u2013885.\nFan, J., Hu, T.-C., and Truong, Y. K. (1994), \u201cRobust Nonparametric Function\nEstimation,\u201d Scandinavian Journal of Statistics, 21, 433\u2013446.\nGather, U., Schettlinger, K., and Fried, R. (2006), \u201cOnline Signal Extraction by\nRobust Linear Regression,\u201d Computational Statistics, 21, 33\u201351.\nGiloni, A., and Padberg, M. (2004), \u201cThe Finite Sample Breakdown Point of L1-\nregression,\u201d SIAM Journal of Optimization, 14, 1028\u20131042.\nHa\u00a8rdle, W., and Gasser, T. (1984), \u201cRobust Non-parametric Function Fitting,\u201d\nJournal of the Royal Statistical Society, Ser. B, 46, 42\u201351.\nHastie, T., and Loader, C. (1993), \u201cLocal Regression: Automatic Kernel Carpen-\ntry,\u201d Statistical Science, 8, 120\u2013129.\nHe, X., Jureckova, J., Koenker, R., and Portnoy, S. (1990), \u201cTail Behavior of Re-\ngression Estimators and Their Breakdown Points,\u201d Econometrica, 58, 1195\u20131214.\nImhoff, M., Bauer, M., Gather, U., and Fried, R. (2002), \u201cPattern Detection in In-\ntensive Care Monitoring Time Series with Autoregressive Models: Influence of the\nModel Order,\u201d Biometrical Journal, 44, 746\u2013761.\nKoenker, R. (2005), Quantile Regression, Cambridge: Cambridge University Press.\nMeer, P., Mintz, D., Rosenfeld, A., and Kim, D. Y. (1991), \u201cRobust Regression\nMethods in Computer Vision: A Review,\u201d International Journal of Computer Vi-\nsion, 6, 59\u201370.\nMizera, I., and Mu\u00a8ller, Ch. H. (1999), \u201cBreakdown Points and Variation Exponents\nof Robust M-Estimators in Linear Models,\u201d Annals of Statistics, 27, 1164\u20131177.\n6 CONCLUSIONS 25\nMu\u00a8ller, Ch. H. (1999), \u201cOn the Use of High Breakdown Point Estimators in the\nImage Analysis,\u201d Tatra Mountains Math. Publ., 17, 283\u2013293.\nPortnoy, S., and Koenker, R. (1997), \u201cThe Gaussian Hare and the Laplacian Tor-\ntoise: Computability of Squared-Error versus Absolute-Error Estimators,\u201d Statisti-\ncal Science, 12, 279\u2013300.\nRousseeuw, P. J., and Leroy, A. (1987), Robust Regression and Outlier Detection,\nNew York: Wiley.\nRousseeuw, P. J., and Van Aelst, S. (1999), \u201cPositive-breakdown Robust Methods\nin Computer Vision,\u201d In: Computing Science and Statistics, Vol. 31, K. Berk and\nM. Pourahmadi (eds.), Interface Foundation of North America, 451\u2013460.\nScholz, F.-W. (1978), \u201cWeighted Median Regression Estimates,\u201d Annals of Statis-\ntics, 6, 603\u2013609.\nSiegel, A. F. (1982), \u201cRobust Regression using Repeated Medians,\u201d Biometrika, 68,\n242\u2013244.\nSimpson, D. G., and Yohai, V. J. (1998), \u201cFunctional Stability of One-Step GM-\nEstimators in Approximately Linear Regression,\u201d Annals of Statistics, 26, 1147\u2013\n1169.\nTukey, J. W. (1977), Exploratory Data Analysis, Reading, Mass.: Addison-Wesley\n(preliminary edition 1971).\nWang, F. T., Scott, D. W. (1994), \u201cThe L1 Method for Robust Nonparametric Re-\ngression,\u201d Journal of the American Statistical Association, 89, 65\u201376.\nYang, R., Yin, L., Gabbouj, M., Astola, J., and Neuvo, Y. (1995), \u201cOptimal\n6 CONCLUSIONS 26\nWeighted Median Filtering under Structural Constraints,\u201d IEEE Transactions on\nSignal Processing, 43, 591\u2013604.\n6 CONCLUSIONS 27\nFigure 1: Bias for the level due to an increasing number of observations shifted by\nc = 10 (top), c = 20 (center) and c = 100 (bottom) at the end of the window. Left, solid:\nRM (\u0003), WRM (\u25b3); left, dashed: L1 (\u0003), WL1 (\u25b3); right, solid: M with the bisquare\n(\u25e6), MM (\u22c4); right, dashed: LMS (\u22c4), RLTS (\u25e6).\n2 4 6 8 10\n0\n2\n4\n6\n8\n10\n12\nnumber of outliers\nbi\nas\n2 4 6 8 10\n0\n5\n10\n15\n20\n25\nnumber of outliers\nbi\nas\n2 4 6 8 10\n0\n20\n40\n60\n80\n10\n0\n12\n0\nnumber of outliers\nbi\nas\n2 4 6 8 10\n0\n2\n4\n6\n8\n10\n12\nnumber of outliers\nbi\nas\n2 4 6 8 10\n0\n5\n10\n15\n20\n25\nnumber of outliers\nbi\nas\n2 4 6 8 10\n0\n20\n40\n60\n80\n10\n0\n12\n0\nnumber of outliers\nbi\nas\n6 CONCLUSIONS 28\nFigure 2: Efficiency for the level (left) and the slope (right) due to an increasing nonlin-\nearity. Solid: RM (\u0003), WRM (\u25b3), M-bisquare (\u25e6), MM (\u22c4); dashed: L1-regression (\u0003),\nWL1 (\u25b3), LMS (\u22c4), RLTS (\u25e6).\n0 5 10 15 20\n0\n50\n10\n0\n15\n0\nnonlinearity\ne\nffi\ncie\nnc\ny\n0 5 10 15 20\n0\n50\n10\n0\n15\n0\n20\n0\n25\n0\n30\n0\nnonlinearity\ne\nffi\ncie\nnc\ny\n6 CONCLUSIONS 29\nFigure 3: Simulated (left) and real time series (right), underlying signal (bold dotted)\nand filter outputs (solid): WRM (top), LMS (center), and RLTS (bottom).\n0 50 100 150 200 250\n\u2212\n5\n0\n5\n10\ntime\nva\nlu\ne\n0 50 100 150 200 250\n\u2212\n5\n0\n5\n10\ntime\nva\nlu\ne\n0 50 100 150 200 250\n\u2212\n5\n0\n5\n10\ntime\nva\nlu\ne\n0 50 100 150 200 250\n10\n5\n11\n0\n11\n5\n12\n0\n12\n5\n13\n0\n13\n5\ntime\nva\nlu\ne\n0 50 100 150 200 250\n10\n5\n11\n0\n11\n5\n12\n0\n12\n5\n13\n0\n13\n5\ntime\nva\nlu\ne\n0 50 100 150 200 250\n10\n5\n11\n0\n11\n5\n12\n0\n12\n5\n13\n0\n13\n5\ntime\nva\nlu\ne\n"}