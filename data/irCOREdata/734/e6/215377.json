{"doi":"10.1093\/biomet","coreId":"215377","oai":"oai:eprints.lse.ac.uk:29141","identifiers":["oai:eprints.lse.ac.uk:29141","10.1093\/biomet"],"title":"Estimating linear dependence between nonstationary time series using the locally stationary wavelet model","authors":["Sanderson, Jean","Fryzlewicz, Piotr","Jones, M. W."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-06","abstract":"Large volumes of neuroscience data comprise multiple, nonstationary electrophysiological or neuroimaging time series recorded from different brain regions. Accurately estimating the dependence between such neural time series is critical, since changes in the dependence structure are presumed to reflect functional interactions between neuronal populations. We propose a new dependence measure, derived from a bivariate locally stationary wavelet time series model. Since wavelets are localized in both time and scale, this approach leads to a natural, local and multi-scale estimate of nonstationary dependence. Our methodology is illustrated by application to a simulated example, and to electrophysiological data relating to interactions between the rat hippocampus and prefrontal cortex during working memory and decision making","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/215377.pdf","fullTextIdentifier":"http:\/\/eprints.lse.ac.uk\/29141\/1\/Estimating_linear_dependence_between_nonstationary_time_series_using_the_locally_stationary_wavelet_model%28lsero%29.pdf","pdfHashValue":"80842a3380da00881c84788252e53290ba718a23","publisher":"Oxford University Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lse.ac.uk:29141<\/identifier><datestamp>\n      2017-05-04T09:35:36Z<\/datestamp><setSpec>\n      74797065733D4445505453:4C53452D5354<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lse.ac.uk\/29141\/<\/dc:relation><dc:title>\n        Estimating linear dependence between nonstationary time series using the locally stationary wavelet model<\/dc:title><dc:creator>\n        Sanderson, Jean<\/dc:creator><dc:creator>\n        Fryzlewicz, Piotr<\/dc:creator><dc:creator>\n        Jones, M. W.<\/dc:creator><dc:subject>\n        QH301 Biology<\/dc:subject><dc:subject>\n        HA Statistics<\/dc:subject><dc:description>\n        Large volumes of neuroscience data comprise multiple, nonstationary electrophysiological or neuroimaging time series recorded from different brain regions. Accurately estimating the dependence between such neural time series is critical, since changes in the dependence structure are presumed to reflect functional interactions between neuronal populations. We propose a new dependence measure, derived from a bivariate locally stationary wavelet time series model. Since wavelets are localized in both time and scale, this approach leads to a natural, local and multi-scale estimate of nonstationary dependence. Our methodology is illustrated by application to a simulated example, and to electrophysiological data relating to interactions between the rat hippocampus and prefrontal cortex during working memory and decision making.<\/dc:description><dc:publisher>\n        Oxford University Press<\/dc:publisher><dc:date>\n        2010-06<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lse.ac.uk\/29141\/1\/Estimating_linear_dependence_between_nonstationary_time_series_using_the_locally_stationary_wavelet_model%28lsero%29.pdf<\/dc:identifier><dc:identifier>\n          Sanderson, Jean and Fryzlewicz, Piotr and Jones, M. W.  (2010) Estimating linear dependence between nonstationary time series using the locally stationary wavelet model.  Biometrika, 97 (2).  pp. 435-446.  ISSN 0006-3444     <\/dc:identifier><dc:relation>\n        http:\/\/biomet.oxfordjournals.org\/<\/dc:relation><dc:relation>\n        10.1093\/biomet\/asq007<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lse.ac.uk\/29141\/","http:\/\/biomet.oxfordjournals.org\/","10.1093\/biomet\/asq007"],"year":2010,"topics":["QH301 Biology","HA Statistics"],"subject":["Article","PeerReviewed"],"fullText":" \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nJean Sanderson, Piotr Fryzlewicz and M. W. Jones \nEstimating linear dependence between \nnonstationary time series using the locally \nstationary wavelet model \n \n \n \nArticle (Accepted version) \n(Unrefereed) \n \n \nOriginal citation: \nSanderson, Jean and Fryzlewicz, Piotr and Jones, M. W. (2010) Estimating linear dependence \nbetween nonstationary time series using the locally stationary wavelet model.  Biometrika, 97 (2). \npp. 435-446. ISSN 0006-3444 \nDOI:  10.1093\/biomet\/asq007 \n \n\u00a9 2010 Biometrika Trust \n \nThis version available at: http:\/\/eprints.lse.ac.uk\/29141\/ \nAvailable in LSE Research Online: November 2011 \n \nLSE has developed LSE Research Online so that users may access research output of the \nSchool. Copyright \u00a9 and Moral Rights for the papers on this site are retained by the individual \nauthors and\/or other copyright owners. Users may download and\/or print one copy of any \narticle(s) in LSE Research Online to facilitate their private study or for non-commercial research. \nYou may not engage in further distribution of the material or use it for any profit-making activities \nor any commercial gain. You may freely distribute the URL (http:\/\/eprints.lse.ac.uk) of the LSE \nResearch Online website. \n \nThis document is the author\u2019s final accepted version of the journal article. There may be \ndifferences between this version and the published version.  You are advised to consult the \npublisher\u2019s version if you wish to cite from it. \nMeasuring dependence between non-stationary time series\nusing the locally stationary wavelet model\nJean Sanderson, Piotr Fryzlewicz and Matthew Jones\nNovember 28, 2008\nAbstract\nLarge volumes of neuroscience data comprise multiple, non-stationary electrophys-\niological or neuroimaging time series recorded from different brain regions. Estimating\nthe dependence between such neural time series accurately is critical, since changes\nin the dependence structure are presumed to reflect functional interactions between\nneuronal populations. We propose a new method of wavelet coherence, derived from\nthe new bivariate locally stationary wavelet (LSW) time series model. Since wavelets\nare localised in both time and scale, this approach leads to a natural, local and mul-\ntiscale estimate of nonstationary dependence. Our methodology is illustrated by ap-\nplication to a simulated example, and to electrophysiological data relating to inter-\nactions between the rat hippocampus and prefrontal cortex during working memory\nand decision-making. We thereby demonstrate that this novel LSW model can be of\nuse in systems neuroscience applications.\n1 Introduction\nIt is often of interest to study the interdependence between two simultaneously recorded\ntime series. However, in the majority of applications we cannot assume that the underlying\ndata are stationary. For example, we later consider a problem from neuroscience in which\npopulation local field potential (LFP) recordings were made from two areas of a rat\u2019s\nbrain as it performed a maze-based task designed to invoke spatial working memory and\ndecision-making. Estimating the time-varying dependence structure between these signals\ncan provide direct insight into the timing and fundamental nature of interactions between\nbrain regions (Varela et al., 2001); in our example, the dynamic modulation of hippocampal\nand cortical activities is likely to reflect behaviour-dependent interactions between the\ntwo structures (Jones and Wilson, 2005). However, since the rat\u2019s behaviour, sensory\nenvironment and the excitability and rhythmicity of its neuronal networks can vary over\nsub-second timescales, it is likely that the resulting time series and the dependence between\nthem will also be non-stationary. Dynamic methods are therefore necessary to capture\nthe subtleties of neuronal network coordination.\nWe begin by introducing some relevant concepts from stationary time series analysis.\nGiven two stationary time series, {X\n(1)\nt } and {X\n(2)\nt }, their cross-covariance function has\n1\na Fourier representation in terms of the cross-spectral density function, f (1,2)(\u03c9):\nc\n(1,2)\nX (\u03c4) =\n\u222b pi\n\u2212pi\nf (1,2)(\u03c9)exp(i\u03c9\u03c4)d(\u03c9)\nThe cross-covariance function provides a natural estimate of the relationship between\nthe two series in the time domain, while the cross-spectral density function can be used\nsimilarly in the spectral domain (Brillinger, 1975). The coherence function is derived by\nnormalising the cross-spectrum by the individual spectra and, roughly speaking, measures\nthe correlation between the signals as a function of frequency.\nAlthough the coherence function provides a good option for stationary series, other\nmethods are necessary when the data is non-stationary. An extension to Fourier analysis,\nto allow for non-stationarity, is the windowed, or short time Fourier transform (Gabor,\n1946) which estimates the spectrum over time as well as frequency by applying the Fourier\ntransform to a localised time window that slides along the time axis; the data within each\ntime window is presumed stationary. One drawback to this approach is that the window\nwidth is constant over all frequencies. It is desirable for the time window to change so that\nwe use a larger window to observe more information for lower frequencies, and a smaller\nwindow for more precise time resolution at high frequencies. Complex demodulation was\nintroduced by Tukey (1961) and allows the frequency components of a time series to be\ndescribed as a function of time. This method can also be used for bivariate situations, but\nunlike conventional spectral analysis which estimates components at multiple frequencies\nsimultaneously, complex demodulation estimates the contribution of one specific reference\nfrequency. Bivariate ARMA models with time dependent parameters have been used to\nestimate coherence (Schack and Krause, 1995). Complications with this procedure arise\nas it is necessary to select the order of the ARMA model which may change over time.\nDahlhaus (2000) introduces a method for modelling multivariate processes based on the\nlocally stationary process model (Dahlhaus, 1997). Other methods for the analysis of\nnon-stationary bivariate time series include the Auto-SLEX method (Ombao et al., 2001).\nUsing this method, the data is automatically segmented into approximately stationary\ndyadic blocks. SLEX basis waveforms are used instead of the Fourier exponentials of the\nclassical representation, allowing estimation of the time-varying spectra and coherence.\nDue to the natural localisation in both time and scale, wavelets are a popular tool\nfor modelling the dependence between two non-stationary series; a thorough introduction\nto wavelets can be found in Vidakovic (1999). Unlike time resolved Fourier coherence,\nthe wavelet transform uses shorter windows for higher frequencies, which leads to more\n\u2018natural\u2019 localisation. The concept of the wavelet cross spectrum, in terms of the con-\ntinuous wavelet transform (CWT), was introduced by Hudgins et al. (1993). Wavelet\ncross spectral analysis using the CWT has been applied to applications such as clima-\ntology (Maraun and Kurths (2004), Grinsted et al. (2004)) and neuroscience (Lachaux\net al., 2002). Wavelet cross-covariance and correlation has also been defined based on the\nmaximal overlap discrete wavelet transform (MODWT) (see Whitcher et al. (2000) and\nSerroukh and Walden (2000)). Their formulation assumes that the d\u2019th order backwards\n2\ndifferences of the series can be modelled as a stationary process. A similar concept to that\nof cross-wavelet analysis is that of Hilbert wavelet pairs (Whitcher et al., 2005). Using\nthis approach, the coherence and phase between the signals can be estimated with less\nredundancy than with the CWT.\nIn this paper we propose a novel measure of wavelet coherence termed \u2018locally sta-\ntionary wavelet coherence\u2019. This is derived from the locally stationary wavelet time series\nmodel of Nason et al. (2000). Following the work of Dahlhaus (1997), the model adopts\nthe rescaled time principle, replacing the exponentials in the Fourier representation by a\nsystem of non-decimated wavelets. An important difference between the LSW model and\nprevious wavelet coherence measures lies in the particular bias correction implied by the\nLSW model. The LSW coherence provides a measure of the dependence between the inno-\nvations of each process, and is therefore uncontaminated by within-process dependence of\neach series. Furthermore, our formulation provides a model that is theoretically tractable\nand which can be estimated efficiently by means of the non decimated wavelet transform.\nIn section 2 we introduce the joint LSW process model and introduce several related\nquantities, such as the evolutionary wavelet cross spectrum and the local cross-covariance,\nand also recall briefly the univariate equivalents to these quantities. Section 3 details\nthe estimation procedure based on the LSW periodograms and cross-periodogram. In\nSection 4 we then illustrate the methodology with a simulated example and application\nto experimental neurophysiological data.\n2 Wavelet coherence using the LSW model\n2.1 Definition of the LSW model for a bivariate time series\nWe begin by defining the joint LSW process model. Assuming that both series can be\nmodelled as LSW processes, the model is defined as follows.\nDefinition 2.1. The joint LSW process (X\n(1)\nt,T , X\n(2)\nt,T )t=0,...,T\u22121, for T = 2\nj \u2265 1 is a trian-\ngular stochastic array with representation\nX\n(1)\nt,T =\n\u22121\u2211\nj=\u2212\u221e\n\u221e\u2211\nk=\u2212\u221e\nW\n(1)\nj (k\/T )\u03c8j,t\u2212k\u03be\n(1)\nj,k\nX\n(2)\nt,T =\n\u22121\u2211\nj=\u2212\u221e\n\u221e\u2211\nk=\u2212\u221e\nW\n(2)\nj (k\/T )\u03c8j,t\u2212k\u03be\n(2)\nj,k\nwhere {\u03c8k,t} are discrete, real valued, compactly supported, non-decimated wavelet vec-\ntors with scale and location parameters j \u2208 {\u22121,\u22122, ...} and k \u2208 Z, respectively. For\neach j < \u22121, the functions W\n(i)\nj (k\/T ) are assumed to be Lipschitz continuous with Lip-\nschitz constants, Lj , fulfilling\n\u2211\n\u22121\nj=\u2212\u221e 2\n\u2212jLj < \u221e. Similarly, \u03c1j(k\/T ) are assumed to\n3\nbe Lipschitz continuous with Lipschitz constants, Rj , fulfilling\n\u2211\n\u22121\nj=\u2212\u221e 2\n\u2212jRj < \u221e. The\nfunctions are defined on rescaled time z = k\/T \u2208 [0, 1] which enables asymptotic estima-\ntion. Also, \u03be\n(i)\nj,k are zero mean orthonormal identically distributed random variables with\nthe following properties\n\u2022 cov(\u03be\n(i)\nj,k, \u03be\n(i)\nj\u2032,k\u2032) = \u03b4j,j\u2032\u03b4k,k\u2032\n\u2022 cov(\u03be\n(1)\nj,k , \u03be\n(2)\nj\u2032,k\u2032) = \u03b4j,j\u2032\u03b4k,k\u2032\u03c1j(k\/T )\nwhere \u03b4i,j is the Kronecker delta function, giving \u03b4i,j = 1 for i = j and 0 otherwise.\nIn Definition 2.1, the parametersW\n(i)\nj (k\/T ) can be thought of as time and scale depen-\ndent transfer functions and the non-decimated wavelet vectors, \u03c8j , can be thought of as\nbuilding blocks analogous to Fourier exponentials in the spectral domain representation.\nEach process is composed of the sum of components over wavelet scale, j, and thus pro-\nvides a multiscale representation of the process. The quantity \u03c1j(k\/T ) is a direct measure\nof the dependence between the innovation sequences of each process at scale j. There-\nfore, unlike other measures of dependence such as the cross-correlation function, \u03c1j(k\/T )\ndoes not depend on the parameters of the model. This dependence measure is our main\nquantity of interest within the model.\nHaar wavelets are perhaps the simplest example of a wavelet system that can be used\nin this formulation and they are defined by:\n\u03c8j,k = 2\nj\/2I0,...,2\u2212j\u22121\u22121(k)\u2212 2\nj\/2I2\u2212j\u22121,...,2\u2212j\u22121(k)\nHere the notation j = \u22121 denotes the finest scale wavelet, j = \u22122 the next finest scale\nand so forth. Other real-valued Daubechies\u2019 compactly supported wavelets (Daubechies,\n1992) can also be used. Throughout the paper we assume that the errors are normally\ndistributed. In principle other distributions could be used, however this assumption is\nmade to ensure theoretical tractability.\nWe do not generally deal with the parameters W\n(i)\nj (k\/T ) directly, instead they are\ndescribed by a quantity termed the evolutionary wavelet spectrum. The evolutionary\nwavelet spectrum of the process X\n(i)\nt,T is defined by S\n(i)\nj (z) = |W\n(i)\nj (z)|\n2 . Analogous to\nthe spectrum, f(\u03c9), in the classical stationary representation, the evolutionary wavelet\nspectrum quantifies the contribution to variance within a LSW process over scale, j, and\nrescaled time, z = k\/T . The EWS is the main quantity of interest within the univariate\nLSW framework, however in the bivariate setting we are interested not only in these quan-\ntities, but also in the dependence between the series as quantified by the LSW coherence,\n\u03c1j(k\/T ). The LSW coherence is analogous to the classical coherence, K(\u03c9), in spectral\ndomain setting and quantifies the dependence between the series over scale and time. To\ndemonstrate how these quantities provide a description of the joint process, we provide a\nbrief example.\n4\ntime\nsc\na\nle\n\u2212\n1\n\u2212\n2\n\u2212\n3\n\u2212\n4\na)\ntime\nsc\na\nle\n\u2212\n1\n\u2212\n2\n\u2212\n3\n\u2212\n4\nb)\ntime\nsc\na\nle\n\u2212\n1\n\u2212\n2\n\u2212\n3\n\u2212\n4\nc)\nX1\nX2\ntime\nd)\nFigure 1: Example of the construction of an LSW process. a) EWS of X(1), b) EWS of\nX(2), c) LSW coherence between series, d) simulation of X(1) and X(2).\nWe start by specifying the EWS of two processes, and the dependence between them\nas measured by \u03c1j(k\/T ). Suppose that for both series the spectrum is zero at all scales\napart from j = \u22122. At this scale, both spectra are constant at S\n(1)\n\u22122(z) = S\n(2)\n\u22122(z) = 5. The\nprocesses are assumed to have zero coherence at all scales apart from j = \u22122, and at this\nscale the LSW coherence is 0 for the first half of the series, then increases to 1 for the\nlatter half of the series. The EWS and coherence are plotted in Figure 2, along with one\nsimulation of time series with these properties. The resulting series are independent until\ntime T\/2, and then exactly the same for the latter half of the series since the coherence is\nspecified to be 1 at the only non-zero scale.\n2.2 The evolutionary wavelet cross-spectrum and coherence\nThe evolutionary wavelet cross-spectrum provides a measure of the dependence between\ntwo time series. It provides a natural stage in estimating the LSW coherence and is defined\nas follows.\n5\nDefinition 2.2. The evolutionary wavelet cross-spectrum of the LSW processes X\n(1)\nt,T and\nX\n(2)\nt,T , is given by\nCj(z) =W\n(1)\nj (z)W\n(2)\nj (z)\u03c1j(z)\nA measure of local cross-covariance may be associated with the evolutionary wavelet\ncross-spectrum. As shown by Proposition 2.4, the LSW process cross-covariance given by\nc\n(1,2)\nT (z, \u03c4) = cov(X\n(1)\nt,T , X\n(2)\nt+\u03c4,T ), asymptotically tends to the local cross-covariance.\nDefinition 2.3. The local cross-covariance of the LSW processes X\n(1)\nt,T and X\n(2)\nt,T , is given\nby\nc(1,2)(z, \u03c4) =\n\u22121\u2211\nj=\u2212\u221e\nCj(z)\u03a8j(\u03c4) (1)\nwhere \u03a8j(\u03c4) =\n\u2211\nk \u03c8jk(0)\u03c8jk(\u03c4) is the autocorrelation wavelet.\nProposition 2.4. Assuming there exists a constant, C, such that for all j, |W\n(i)\nj (z)| \u2264\nC2j\/2. As T \u2192\u221e, then for \u03c4 \u2208 Z and z \u2208 (0, 1), |c\n(1,2)\nT (z, \u03c4)\u2212 c\n(1,2)(z, \u03c4)| = O(T\u22121).\nThe assumption of Proposition 2.4 is satisfied if, for example, Xt,T is a white noise\nprocess with spectrum given by Sj(z) = |Wj(z)|\n2 = 2j . The evolutionary wavelet cross-\nspectrum describes the contribution to the cross-covariance at a particular time, z, and\nscale, j. This interpretation becomes clear when considering the cross-covariance at zero\nlag. Since \u03a8j(0) = 1, substituting into equation (1) we have c\n(1,2)(z, 0) =\n\u2211\n\u22121\nj=\u2212\u221eCj(z).\nThe relationship between the cross-spectrum and the local cross-covariance is invertible,\nwith the inverse relationship given by\nCj(z) =\n\u2211\nl\nA\u22121j,l\n\u2211\n\u03c4\nc(1,2)(z, \u03c4)\u03a8l(\u03c4) (2)\nwhere Ai,j is the autocorrelation wavelet inner product matrix Ai,j =\n\u2211\n\u03c4 \u03a8i(\u03c4)\u03a8j(\u03c4).\nThe evolutionary wavelet cross-spectrum, Cj(z), provides a measure of the dependence\nbetween the two series. However it is clear that deviations in Cj(z) can be caused by\nfluctuations in the EWS of each of the processes as well as by changes in the dependence,\nand hence the cross-spectrum cannot be used alone as a measure of dependence between\nthe series.\nThe locally stationary wavelet coherence, \u03c1j(z), can be represented in terms of the\nevolutionary wavelet cross-spectrum and the individual EWS of each process as shown in\nequation (3), providing a normalised measure of the relationship between two series.\n\u03c1j(z) =\nCj(z)\u221a\nS\n(1)\nj (z)S\n(2)\nj (z)\n(3)\nThe locally stationary wavelet coherence ranges from \u22121, indicating complete negative\ncorrelation, to +1 indicating complete correlation. A value of close to zero indicates a lack\nof correlation between the two series at the given scale and location.\n6\n2.3 Relationship between LSW and Fourier coherence\nIn order to understand the relationship between the Fourier and LSW coherence, we\nexpress both quantities in terms of the covariance function. Using the definition of the\nLSW cross spectra from equation (3), and the corresponding representation of the auto\nspectra, we obtain the following expression for the LSW coherence:\n\u03c1j(z) =\n\u2211\n\u03c4 c\n(1,2)(z, \u03c4)\n\u2211\nlA\n\u22121\njl \u03a8l(\u03c4)\u221a\u2211\n\u03c4 c\n(1)(z, \u03c4)\n\u2211\nlA\n\u22121\njl \u03a8l(\u03c4)\n\u2211\n\u03c4 c\n(2)(z, \u03c4)\n\u2211\nlA\n\u22121\njl \u03a8l(\u03c4)\n(4)\nThe equivalent representation for the Fourier coherence is:\nK(\u03c9) =\n|\n\u2211\n\u03c4 c\n(1,2)\nX (\u03c4)e\n\u2212i\u03c9\u03c4 |\u221a\u2211\n\u03c4 c\n(1)\nX (\u03c4)cos(\u03c9\u03c4)\n\u221a\u2211\n\u03c4 c\n(2)\nX (\u03c4)cos(\u03c9\u03c4)\n(5)\nSince in the LSW setting we have a measure of the localised covariance, the LSW co-\nherence is defined over rescaled time, z = k\/T , giving a localised measure of the coherence.\nIn the case where the series is stationary, there is no dependence on z in equation (4) and\nthe two representations become more comparable.\nComparing equations (4) and (5) in the stationary setting, we see that the functions\u2211\nlA\n\u22121\njl \u03a8l(\u03c4) in the LSW representation perform the same task as the Fourier exponentials\nin (4). These functions are a modified version of the autocorrelation wavelets, \u03a8j(\u03c4) and\nunlike the Fourier exponentials which have a constant amplitude over the the entire length\nof T , the amplitude of these functions decay as |\u03c4 | \u2192 \u221e. The rate of decay depends on\nthe scale, j, with the functions decaying at a much faster rate for fine scales. Putting more\nweight on small lags of \u03c4 makes sense in the locally-stationary setting as our estimators\nof the spectra of each process are defined locally, and so in this setting the associated\ndependencies are are only significant locally.\n3 Estimation\nThe basic pre-estimator of the evolutionary wavelet spectrum is a quantity known as the\nwavelet periodogram. Similarly, the wavelet cross-spectrum is estimated from the wavelet\ncross-periodogram. Following (Nason et al., 2000), the empirical non decimated wavelet\ncoefficients for the LSW process X\n(i)\nt,T , constructed using the wavelet system \u03c8, are given\nby d\n(i)\nj,t,T =\n\u2211\nsX\n(i)\ns,T\u03c8j,s\u2212t. The non decimated wavelet coefficients are used to construct\nthe wavelet periodogram and cross periodogram:\nDefinition 3.1. The wavelet periodograms for the LSW processes X\n(i)\nt,T , for i = 1, 2, are\ngiven by I\n(i)\nj,t,T = |d\n(i)\nj,t,T |\n2. The wavelet cross-periodogram is given by I\n(1,2)\nj,t,T = d\n(1)\nj,t,Td\n(2)\nj,t,T .\n7\nProposition 3.2. Assume there exists a constant, C, such that for all j, |W\n(i)\nj (z)| \u2264\nC2j\/2. The expectation of the cross-periodogram, I\n(1,2)\nj,t,T , is given by\nE(I\n(1,2)\nj,t,T ) =\n\u22121\u2211\ni=\u2212\u221e\nW\n(1)\ni (t\/T )W\n(2)\ni (t\/T )\u03c1i(t\/T )Aij +O(T\n\u221212\u2212j)\nAlso, the variance is given by\nvar(I\n(1,2)\nj,t,T ) =\n\u22121\u2211\ni=\u2212\u221e\nS\n(1)\ni (t\/T )Ai,j\n\u22121\u2211\ni=\u2212\u221e\nS\n(2)\ni (t\/T )Ai,j\n+\n( \u22121\u2211\ni=\u2212\u221e\nW\n(1)\ni (t\/T )W\n(2)\ni (t\/T )\u03c1i(t\/T )Ai,j\n)2\n+O(2\u2212jT\u22121)\nWe can see from Proposition 3.2 that the expectation of the wavelet cross-periodogram\nis composed of the sum of wavelet cross-spectra, Cj(z). The cross-periodogram is therefore\na natural estimator of the wavelet cross-spectrum, but we first need to correct for the bias\nincurred by the matrix Ai,j . Also, since the cross-periodogram has non-vanishing variance,\nit needs to be smoothed to obtain consistency. For this we use simple moving average\nsmoothing. Other more advanced smoothing techniques such as DWT shrinkage are also\npotentially viable.\nThe estimator of the cross-spectrum is therefore constructed by firstly correcting the\nperiodogram to give I\u02dcl(t\/T ) =\n\u2211\n\u22121\nj=\u2212J\u2217 I\n(1,2)\nj,t,T A\n\u22121\nl,j for some J\n\u2217 < log2(T ), chosen to ensure\nthe consistency of I\u02dcl(z), then smoothing over time to give C\u02c6l(t\/T ) =\n1\n2M+1\n\u2211M\nm=\u2212M I\u02dc\n(1,2)\nl,t+m,T\nProposition 3.3. Suppose that the assumptions from Proposition 1 and 2 hold, and\nlet J\u2217 = \u03b1 log2(T ) where \u03b1 \u2208 (0, 1). The estimator C\u0302l(t\/T ) converges in probability to\nW\n(1)\nl (t\/T )W\n(2)\nl (t\/T )\u03c1l(t\/T ) for each fixed scale l, provided that MT\n\u03b1\u22121 \u2192 0 as T \u2192 \u221e\nand M \u2192\u221e.\nThe wavelet periodograms, I\n(i)\nj,t,T for i = 1, 2 are smoothed and corrected similarly to\ngive S\u03021l (t\/T ) and S\u0302\n2\nl (t\/T ).\nProposition 3.4. Suppose that the assumptions from Proposition 1 and 2 hold, and let\nJ\u2217 = \u03b1 log2(T ) where \u03b1 \u2208 (0, 1). Then S\u0302\n(i)\nl (t\/T ) converges in probability to S\n(i)\nl (t\/T )\nprovided that MT\u03b1\u22121 \u2192 0 as T \u2192\u221e and M \u2192\u221e for each fixed scale l.\nFrom Propositions 3.3 and 3.4, we see that as the length of the series increases, our\nestimators of the cross and auto spectra converge in probability to the expected quanti-\nties. Given a sufficiently large length of series, this is valid for any choice of smoothing\nparameter, M .\nGiven estimates of the cross-spectrum, C\u0302l(t\/T ), and individual spectra, S\u0302\n(i)\nl (t\/T ), of\neach process and provided that S\n(1)\nl (t\/T ) > 0 and S\n(2)\nl (t\/T ) > 0, the estimator of the\n8\nlocally stationary wavelet coherence given by\n\u03c1\u0302l(t\/T ) =\nC\u0302l(t\/T )\u221a\nS\u0302\n(1)\nl (t\/T )S\u0302\n(2)\nl (t\/T )\nconverges in probability to \u03c1l(t\/T ) by Slutsky\u2019s theorem (Slutsky, 1925).\n3.1 Practical considerations\nThe corrected and time-smoothed periodograms provide a consistent estimator of the\nLSW cross-spectrum. In practice, however, the estimator can become unstable. This is\nlargely because after the correction step in the estimation procedure it is possible that the\nestimator may take values close to or below zero. There are several options to overcoming\nthis problem. For example we could bound the denominator to be above zero, or correct the\nspectra using a non-negative constraint such as with non-negative least squares (Lawson\nand Hanson, 1974).\nWe recommend overcoming the problems with stability of the estimator by smoothing\nthe cross-periodogram over scale as well as time. Though this introduces extra bias into\nthe estimator, we can minimise this bias by suitable choice of the smoothing parameters.\nSmoothing over scale reduces the variance of the estimator further and enables us to form\nan estimator of the cross-spectrum that is stable upon normalisation.\nWe smooth over scale allowing a different vector of weights for each scale. This pro-\nduces a J \u00d7 J matrix of weights, denoted D, where the element Dl,j is the contribution of\nscale j, to the smoothed estimate of scale l, and the columns thus sum to 1. Correcting\nfor bias using the bias correction matrix A\u22121 has the effect of applying a scale factor of\nabout 2j to scale j, so that fine scales are scaled up in comparison to coarse scales. Since\nwe choose to smooth over scale after correcting, we smooth using the following scheme:\na) Firstly the estimates are re-scaled by 2\u2212j , b) the estimates are smoothed over scale,\nc) the scaling factor of 2\u2212j is re-introduced. The corrected, scale smoothed parameter is\ntherefore given by:\n\u02dc\u02dcI\n(1,2)\nl,t,T =\n( \u22121\u2211\nj=\u2212\u221e\n2\u2212jDl,j I\u02dc\n(1,2)\nj,t,T\n)\n2l\nIn practice, we generally smooth over the neighbouring 3 or 5 scales. Different choices of\nsmoothing weights are allowed for the estimates of each scale. For fine scales it is often\nnot necessary to smooth over scale at all so that at fine scales of l we may have Dl,l = 1.\nDetails of our choice of smoothing parameters are provided in the examples.\nThere are also other issues that can affect the quality of the estimate, namely the\nchoice of wavelet and the choice of time-smoothing parameters, Mj . The wavelet can be\nchosen according to the properties of the time series that it is representing. For example\nif the series contains sharp jumps then the Haar wavelet might be preferable, whereas\nfor series with smoother features other wavelets, such as Daubechies\u2019 least asymmetric\n(Daubechies, 1992), might be preferable. For smoothing over time we choose to increase\n9\nthe width of the smoothing window as the scale becomes coarser. This is natural since\nthe wavelet coefficients at coarse scales display a stronger degree of auto-correlation.\n4 Applications\n4.1 Simulated example\nAs an example, we apply our method to data simulated from a bivariate LSW process with\na known, non-stationary coherence structure that varies between scales. For even scales we\ntake \u03c1j(k\/T ) = 0.2, and for odd scales we assume a non-stationary structure that forms an\n\u2018inverted v\u2019 between 0.2 and 0.8. The autospectra of each process are taken to be S\n(1)\nj (z) =\nS\n(2)\nj (z) = 2\nj (i.e. that of white noise). In this case we take T = 213, giving a decomposition\nof 13 scales. The results from one such simulation, using Daubechies least asymmetric\nwavelet with support N = 5, are shown in Figure 2 below. The estimates are smoothed\nover scale using weights on the leading scales of Dl,l = (0.95, 0.95, 0.95, 0.9, 0.9, 0.9) for l =\n1 to 6. For each estimate we smooth over the neighboring 5 scales, using larger weightings\nfor scales immediately next to the scale of interest. So for scale \u22124, for example, we take\nthe off-diagonal weights to be D,4 = (0,\n1\u2212D4,4\n6 ,\n1\u2212D4,4\n3 , D4,4,\n1\u2212D4,4\n3 ,\n1\u2212D4,4\n6 , 0, ..., 0). For\nsmoothing over time, we use a bandwidth of Mj\/T = (0.025, 0.05, 0.075, 0.1, 0.125, 0.15).\nThe estimated coherence structure from one realisation of the process is shown in Figure\n??. The estimated coherence follows the true coherence closely.\n4.2 Application to neuroscience data\nWe now illustrate our method with application to experimental neuroscience data, previ-\nously described by Jones and Wilson (2005). We consider the coherence between local field\npotentials (LFP) in two functionally and anatomically connected areas of a rat\u2019s brain: the\nhippocampus and the prefrontal cortex. The LFP provide a measure of averaged activity\nover local neuronal populations, and the estimated LSW coherence presents an indication\nof the extent to which activities in the two areas are coordinated, decomposed over scale\nand over time. The data consist of 13 trials, with each trial comprising a \u2018forced turn\u2019\nepoch and a \u2018choice\u2019 epoch. Each of these epochs are 6s in duration, corresponding to the\nsection of the task when the rat is moving along the central arm of the maze, immediately\nprior to reaching a T-junction decision point. During forced turn runs the direction at\nthe T-junction is pre-determined by a movable barrier, whereas in choice epochs the rat\nis free to choose either direction and so presumably employs cognitive processes including\nspatial working memory and decision-making. It is expected that hippocampal-prefrontal\ninteractions are thereby selectively recruited during forced-turn runs. Trials in which the\nrat made the incorrect decision at the turning point were not included in the analysis.\nThe LFP data are sampled at 625Hz, however in Jones and Wilson (2005), significant\ncoherence was found in the theta frequency range (4\u221212Hz) so we choose to downsample\n10\nthe data by taking every third measurement. Using the approximation that a wavelet\nscale j, corresponds to Fourier frequencies in the interval (2\nj\u22121\n\u2206t ,\n2j\n\u2206t) (Percival and Walden,\n2000, Chapter 4.), this downsampling ensures that the theta frequency band is contained\nwithin scales \u22124 and \u22125. We smooth over all 3 subsamples to ensure that information is\nnot lost. Other amounts of downsampling were also investigated to ensure that features\nwere not missed, but this extra analysis offered no additional insight.\nSince choice runs invoke spatial working memory, we would expect to see a change in\ncorrelation structure when the rat reaches the decision point. For forced runs we would\nnot expect to observe this change. The LSW coherence averaged over all 13 trials is\nshown in Figure 3. These estimates have been produced using Daubechies extremal phase\nwavelet (N=8), scale-smoothing with a weighting of Dl,l = (0.95, 0.9, 0.9, 0.9, 0.85, 0.75) on\nthe leading terms and off diagonal elements as with the previous example. For smoothing\nover scale we used a bandwidth of Mj\/T = (0.05, 0.1, 0.15, 0.2, 0.25, 1) so that we are\nsmoothing over an increased number of observations as the scale becomes coarser. Figure\n3 shows an area of increased coherence towards the end of the series at scale \u22124 for the\nchoice run data. This feature is not observed in the forced run data.\nSince we are interested in whether the dependence structure changes over the course\nof the experiment (when the rat is forced to make a decision), we judge the significance\nof our results by looking at the mean coherence before and after the decision point. Con-\nfidence intervals are constructed by simulation: given the estimated coherence structure\n{\u03c1\u02c6j(z)}\n\u22121\nj=\u2212\u221e and autospectra {S\u02c6\n(1)\nj (z)}\n\u22121\nj=\u2212\u221e, {S\u02c6\n(2)\nj (z)}\n\u22121\nj=\u2212\u221e, we simulate n new series.\nFor each series we take the same estimator, the mean before and after the decision point,\nand then take the n\u03b1 and n(1\u2212 \u03b1) ordered values to give the lower and upper confidence\nlimits. The estimate and resulting 90% confidence intervals for scale \u22124 of the choice\ndata is shown in Figure 4. The mean coherence is significantly different between the two\nsections of the data, suggesting that there is a significant change in interaction between\nthe hippocampus and prefrontal cortex before and after the decision point. This effect\nwas only observed at scale \u22124 on the choice run data sets.\n11\nTime\nLS\nW\n c\noh\ner\nen\nce\n0 2000 4000 6000 8000\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\na)\nTime\nLS\nW\n c\noh\ner\nen\nce\n0 2000 4000 6000 8000\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nb)\nTime\nLS\nW\n c\noh\ner\nen\nce\n0 2000 4000 6000 8000\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nc)\nTime\nLS\nW\n c\noh\ner\nen\nce\n0 2000 4000 6000 8000\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nd)\nTime\nLS\nW\n c\noh\ner\nen\nce\n0 2000 4000 6000 8000\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\ne)\nTime\nLS\nW\n c\noh\ner\nen\nce\n0 2000 4000 6000 8000\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nf)\nFigure 2: Results showing the true coherence (dashed) and estimated coherence (solid)\nfrom the simulation example. Subplots a) to f) correspond to scales -1 to -6 respectively.\n12\ntime\nsc\na\nle\n\u22120.3\n\u22120.2\n\u22120.1\n0.0\n0.1\n0.2\n0.3\n0 205 410 615 820 1024\n\u2212\n1\n\u2212\n2\n\u2212\n3\n\u2212\n4\n\u2212\n5\n\u2212\n6\na)\ntime\nsc\na\nle\n\u22120.3\n\u22120.2\n\u22120.1\n0.0\n0.1\n0.2\n0.3\n0 205 410 615 820 1024\n\u2212\n1\n\u2212\n2\n\u2212\n3\n\u2212\n4\n\u2212\n5\n\u2212\n6\nb)\nFigure 3: Trial averages for the choice runs (a) and forced runs (b), scales \u22121 to \u22126 using\nDaubechies extremal phase wavelet, N=8.\n13\nTime\nco\nhe\nre\nnc\ne\n0 200 400 600 800 1000\n\u2212\n0.\n4\n\u2212\n0.\n3\n\u2212\n0.\n2\n\u2212\n0.\n1\n0.\n0\n0.\n1\n0.\n2\nFigure 4: LSW coherence for scale \u22124. The decision point is marked with a vertical\nline, means before and after this point are plotted in bold and the associated confidence\nintervals are plotted with dashed lines.\n5 Discussion\nWe have shown that the LSW theory can be usefully extended to the bivariate setting,\nproviding a measure of dependence between non-stationary time series. In the described\nneuroscience application the method is applied to trials of 6s in duration and the findings\nare in agreement with the previous results from Jones and Wilson (2005) using the short\ntime Fourier transform. With the short time Fourier transform it is necessary to take\na narrow window width in order to assume stationarity. This means that the frequency\nresolution will be poor in comparison to results from wavelet analysis where the width\nof the analysing wavelet is naturally adjusted to scale. Our method of LSW coherence\ncan also be easily applied to much longer series without the problem of choosing a fixed\nwindow length that is hard to tune over a broad range of frequencies. Also of note,\nour LSW model demonstrates significant variation in hippocampal-prefrontal coherence\naveraged over a limited dataset of only 13 trials; this is a valuable facet when analyses are\napplied to experimental data.\nLSW coherence provides a representation of the dependence structure with less redun-\ndancy than similar methods using the CWT. Methods using complex continuous wavelets\ndo however have the advantage that this allows for an estimation of the phase between\nthe signals. Estimates of phase offsets can be useful in neuroscience, since they allow\n14\nus to make inferences regarding the connectivity of neuronal networks (for example, the\nsynaptic time delay between connected structures). The sign of our estimate of \u03c1j(z) can\nprovide some information; in particular changes in the sign of the estimated coherence\nwould imply a shift of phase relationships. The extension of our method to allow for\nphase estimation would be very interesting and provide a more complete estimate of the\nrelationship between the signals. This is possible in the model, perhaps through the use\nof complex wavelets.\nAnother important consideration is the application to multivariate data. Existing\nmethods of assessing multivariate dependence include those of Dahlhaus (2000), who ex-\ntended univariate locally stationary processes to a multivariate setting, Ombao et al.\n(2005) which is an extension to the SLEX framework and Guo and Dai (2006) who use a\nCholesky decomposition approach. The application of our method to multivariate cases is\npossible using the current methodology, by taking pairwise combinations of multivariate\nseries. Generalisation to a fully multivariate model is also possible and will be a consider-\nation in future work that allows application to multi-site recordings of neuronal activity\nfrom disparate brain structures.\nA Appendix\nFor use in the following proofs, we first justify the Lipschitz property of the product\nW\n(1)\ni (k\/T )W\n(2)\ni (k\/T )\u03c1i(k\/T ).\nLemma A.1. Due to the Lipschitz property ofW\n(i)\nj (k\/T ), we have |W\n(i)\nj (k\/T )\u2212W\n(i)\nj (t\/T )| \u2264\nT\u22121(Lj |k \u2212 t|) for some Lipschitz constant Li, and similarly |\u03c1j(k\/T ) \u2212 \u03c1j(k\/T )| \u2264\nT\u22121(Rj |k \u2212 t|). Furthermore, we assume that there exists a positive constant, c, such\nthat for all j, |Wj(z)| \u2264 C2\nj\/2. By definition we have |\u03c1j(z)| \u2264 1. Then by the property\nof products of Lipschitz continuous variables, denoting Bi = max(Li, Ri), we have\n|W\n(1)\ni (k\/T )W\n(2)\ni (k\/T )\u03c1i(k\/T )\u2212W\n(1)\ni (t\/T )W\n(2)\ni (t\/T )\u03c1i(t\/T )| \u2264 T\n\u22121CBi|k \u2212 t|\nProof of Proposition 2.4\nc\n(1,2)\nT (z, \u03c4) = cov(X\n(1)\nt,T , X\n(2)\nt+\u03c4,T )\n= E\n( \u22121\u2211\nj=\u2212\u221e\n\u221e\u2211\nk=\u2212\u221e\nW\n(1)\nj (k\/T )\u03c8j,t\u2212k\u03be\n(1)\nj,k\n\u22121\u2211\nj=\u2212\u221e\n\u221e\u2211\nk=\u2212\u221e\nW\n(2)\nj (k\/T )\u03c8j,t+\u03c4\u2212k\u03be\n(2)\nj,k\n)\n=\n\u22121\u2211\nj=\u2212\u221e\n\u221e\u2211\nk=\u2212\u221e\nCj(k\/T )\u03c8j,t\u2212k\u03c8j,t+\u03c4\u2212k\n=\n\u22121\u2211\nj=\u2212\u221e\nCj(t\/T )\u03a8j(\u03c4) +\n\u22121\u2211\nj=\u2212\u221e\n\u221e\u2211\nk=\u2212\u221e\nT\u22121CBj |k \u2212 t|\u03a8j(\u03c4)\n15\nusing Lemma A.1. The support of \u03a8j(\u03c4) is bounded by K2\n\u2212j , and so the distance |k\u2212 t|\nis bounded by this amount also. Since\n\u2211\nj Bj2\n\u2212j <\u221e and \u03a8j(\u03c4) = O(1), this gives\n|c\n(1,2)\nT (z, \u03c4)\u2212 c\n(1,2)(z, \u03c4)| = |T\u22121CK\n\u22121\u2211\nj=\u2212\u221e\nBj2\n\u2212j\u03a8j(\u03c4)| = O(T\n\u22121)\nProof of Proposition 3.2 The process cross-covariance is given by\nE(I\n(1,2)\nj,t,T ) = E\n[\u2211\ns\n\u22121\u2211\ni=\u2212\u221e\n\u221e\u2211\nk=\u221e\nW\n(1)\ni (k\/T )\u03c8i,s\u2212k\u03be\n(1)\ni,k \u03c8j,s\u2212t\n\u2211\ns\n\u22121\u2211\ni=\u2212\u221e\n\u221e\u2211\nk=\u2212\u221e\nW\n(2)\ni (k\/T )\u03c8i,s\u2212k\u03be\n(2)\ni,k \u03c8j,s\u2212t\n]\n= E\n[ \u22121\u2211\ni=\u2212\u221e\n\u221e\u2211\nk=\u221e\nW\n(1)\ni (k\/T )\u03a8i,j(k \u2212 t)\u03be\n(1)\ni,k\n\u22121\u2211\ni=\u2212\u221e\n\u221e\u2211\nk=\u2212\u221e\nW\n(2)\ni (k\/T )\u03a8i,j(k \u2212 t)\u03be\n(2)\ni,k\n]\n=\n\u22121\u2211\ni=\u2212\u221e\n\u221e\u2211\nk=\u221e\nW\n(1)\ni (k\/T )W\n(2)\ni (k\/T )\u03c1i(k\/T )\u03a8\n2\ni,j(k \u2212 t)\n=\n\u22121\u2211\ni=\u2212\u221e\n(\nW\n(1)\ni (t\/T )W\n(2)\ni (t\/T )\u03c1i(t\/T ) +O(T\n\u22121Bi(2\n\u2212i + 2\u2212j))\n) \u221e\u2211\nk=\u2212\u221e\n\u03a82i,j(k \u2212 t)\n=\n\u22121\u2211\ni=\u2212\u221e\nW\n(1)\ni (t\/T )W\n(2)\ni (t\/T )\u03c1i(t\/T )Aij +RestT\nsince the length of support of \u03a82i,j(k\u2212 t) is bounded by K(2\n\u2212i+2\u2212j) and cov(\u03be\n(1)\nj,k , \u03be\n(2)\nj\u2032,k\u2032) =\n\u03b4j,j\u2032\u03b4k,k\u2032\u03c1j(k\/T ). The remainder is given by\nRestT \u2264\n\u22121\u2211\ni=\u2212\u221e\nT\u22121CKbi(2\n\u2212i + 2\u2212j)Aij\n\u2264 T\u221212\u2212jCK\n\u22121\u2211\ni=\u2212\u221e\nBi2\n\u2212i\n\u22121\u2211\nk=\u2212\u221e\n2kAik + T\n\u221212\u2212jCK\n\u22121\u2211\ni=\u2212\u221e\nBi2\n\u2212i\n\u22121\u2211\nk=\u2212\u221e\n2kAkj\n= O(T\u221212\u2212j)\nas\n\u2211\n\u22121\nk=\u2212\u221e 2\nkAik = 1 and\n\u2211\n\u22121\ni=\u2212\u221eBi2\n\u2212i \u2264 \u221e. For the variance, we decompose the product\nusing Isserlis\u2019 Theorem for zero mean Gaussian random variables (Isserlis, 1918) to give\nvar(I\n(1,2)\nj,k,T ) = E(I\n(1)\nj,t,T )E(I\n(2)\nj,t,T ) + E(I\n(1,2)\nj,t,T )\n2\n=\n\u22121\u2211\ni=\u2212\u221e\nS\n(1)\ni (t\/T )Ai,j\n\u22121\u2211\ni=\u2212\u221e\nS\n(2)\ni (t\/T )Ai,j +\n( \u22121\u2211\ni=\u2212\u221e\nCi(t\/T )Aij\n)2\n+O(2\u2212jT\u22121)\nIn order to find the expectation and MSE of the estimator C\u0302l(z), we first provide the\nintermediate result for the smoothed cross-periodogram.\n16\nLemma A.2. The expectation of the smoothed cross-periodogram is given by\nE(I\u02dc\n(1,2)\nj,t,T ) =\n\u22121\u2211\ni=\u2212\u221e\nW\n(1)\ni (t\/T )W\n(2)\ni (t\/T )\u03c1i(t\/T )Aij +O(T\n\u221212\u2212jM)\nAlso, the variance is given by\nvar(I\u02dc\n(1,2)\nj,t,T ) =\n1\n(2M + 1)2\nM\u2211\nm=\u2212M\n\u2211\n\u03c4\n[ \u22121\u2211\ni=\u2212\u221e\nS\n(1)\ni (t\/T )\n)\nA\u03c4i,j\n\u22121\u2211\ni=\u2212\u221e\nS\n(2)\ni (t\/T )\n)\nA\u03c4i,j\n+\n( \u22121\u2211\ni=\u2212\u221e\nW\n(1)\ni (t\/T )W\n(2)\ni (t\/T )\u03c1i(t\/T )A\n\u03c4\ni,j\n)2\n+O(T\u221212\u2212j |m|)\n]\nand the MSE by\nMSE(I\u02dc\n(1,2)\nj,t,T ) = O(T\n\u221212\u22122jM2)\nProof of Lemma A.2 The expectation can be proved using very similar techniques to\nthat of the expectation in Proposition 1. The variance is given by\nvar(I\u02dc\n(j)\nt,T ) =\n1\n(2M + 1)2\nM\u2211\nm=\u2212M\n\u2211\n\u03c4\nCov\n(\nI\n(1,2)\nj,t+m,T , I\n(1,2)\nj,t+m+\u03c4,T\n)\n(6)\nwhere \u03c4 = m\u2032 \u2212m. The covariance can be decomposed using Isserlis\u2019 Theorem, to give\ncov\n(\nI\n(1,2)\nj,t+m,T , I\n(1,2)\nj,t+m+\u03c4,T\n)\n= E\n(\nd\n(1)\nj,t+m,T , d\n(1)\nj,t+m+\u03c4,T\n)\nE\n(\nd\n(2)\nj,t+m,T , d\n(2)\nj,t+m+\u03c4,T\n)\n+ E\n(\nd\n(1)\nj,t+m,T , d\n(2)\nj,t+m+\u03c4,T\n)\nE\n(\nd\n(2)\nj,t,T , d\n(1)\nj,t+m+\u03c4,T\n)\nHere the first product is of the autocovariances of the wavelet periodogram ordinates and\nthe last product consists of the cross-covariances (for a fixed scale, j). These expectations\ncan be shown using similar techniques as with the cross periodogram to take the values:\ncov(dj,t+m,T , dj,t+M+\u03c4,T ) =\n\u22121\u2211\ni=\u2212\u221e\nSi(t\/T )A\n\u03c4\ni,j +O(T\n\u221212\u2212j |m|)\ncov(d\n(1)\nj,t+m,T , d\n(2)\nj,t+\u03c4+m,T ) =\n\u22121\u2211\ni=\u2212\u221e\nW\n(1)\ni (t\/T )W\n(2)\ni (t\/T )\u03c1i(t\/T )A\n\u03c4\ni,j +O(T\n\u221212\u2212j |m|)\nwhere A\u03c4i,j is defined as\n\u2211\nn\u03a8i,j(n)\u03a8i,j(n+ \u03c4), and has the property |\n\u2211\nj A\n\u03c4\ni,j2\nj | \u2264 1.\nSubstituting the relevant terms into equation (6) and summing over m, the variance is\ngiven by\nvar(I\u02dc\n(1,2)\nj,t,T ) =\n1\n(2M + 1)2\nM\u2211\nm=\u2212M\n\u2211\n\u03c4\n[ \u22121\u2211\ni=\u2212\u221e\nS\n(1)\ni\n( t\nT\n)\nA\u03c4i,j\n\u22121\u2211\ni=\u2212\u221e\nS\n(2)\ni\n( t\nT\n)\nA\u03c4i,j\n+\n( \u22121\u2211\ni=\u2212\u221e\nCi\n( t\nT\n)( t\nT\n)\nA\u03c4i,j\n)2\n+O(T\u221212\u2212j |m|)\n]\n=\n1\n(2M + 1)2\nM\u2211\nm=\u2212M\n[\nI + II +O(T\u221212\u2212j |m|)\n]\n17\nTo find a bound for the variance we look at these terms separately. The first term can\nbe bounded by\nI \u2264\n\u2211\n\u03c4\n|\n\u22121\u2211\ni=\u2212\u221e\nS1i\n( t\nT\n)\nA\u03c4i,j |.\n\u2211\n\u03c4\n|\n\u22121\u2211\ni=\u2212\u221e\nS2i\n( t\nT\n)\nA\u03c4i,j |\n=\n\u2211\n\u03c4\n|\n\u22121\u2211\ni=\u2212\u221e\nS\n(1)\ni (t\/T )\n\u2211\nn\n\u03a8i(n)\u03a8j(n+ \u03c4)|.\n\u2211\n\u03c4\n|\n\u22121\u2211\ni=\u2212\u221e\nS\n(2)\ni (t\/T )\n\u2211\nn\n\u03a8i(n)\u03a8j(n+ \u03c4)|\n\u2264\n\u2211\nn\n|c(1)(t, n)|\n\u2211\n\u03c4\n|\u03a8j(n+ \u03c4)|.\n\u2211\nn\n|c(2)(t, n)|\n\u2211\n\u03c4\n|\u03a8j(n+ \u03c4)|\n\u2264 K12\n\u2212j\n\u2211\nn\nc(1)(t, n).K22\n\u2212j\n\u2211\nn\nc(2)(t, n) = O(2\u2212j)O(2\u2212j) = O(2\u22122j)\nassuming supz\u2208[0,1]\n\u2211\n\u03c4 |c(t, \u03c4)| < \u221e, and using that\n\u2211\n\u03c4 |\u03a8j(\u03c4)| = O(2\n\u2212j). The second\nterm can be bounded similarly to II \u2264 O(2\u22122j). Hence substituting the relevant terms,\nthe variance is of order\nvar(I\u02dc\n(j)\nt,T ) =\n1\n(2M + 1)2\nM\u2211\nm=\u2212M\n[\nO(2\u22122j) +O(2\u22122j) +\n\u2211\n\u03c4\nO(T\u221212\u2212j |m|)\n]\n= O(T\u221212\u22122jM)\nand the mean squared error is given by\nMSE(I\u02dc\n(j)\nt,T ) = O(T\n\u221212\u22122jM) + (O(T\u221212\u2212jM))2 = O(T\u221212\u22122jM2)\nUsing these results, we are now in a position to prove the consistency of our estimator\nC\u02c6l(z)\nProof of Proposition 3.3 The expectation of the smoothed and corrected cross-periodogram\nis given by\nE(C\u02c6l(t\/T )) =\n\u22121\u2211\nj=\u2212J\u2217\nE(I\u02dc\n(1,2)\nj,t,T )A\n\u22121\nl,j\n=\n\u22121\u2211\nj=\u2212J\u2217\n(\n\u22121\u2211\ni=\u2212\u221e\nW\n(1)\ni (t\/T )W\n(2)\ni (t\/T )\u03c1i(t\/T )Aij +O(T\n\u221212\u2212jM))A\u22121l,j\n= W\n(1)\nl (t\/T )W\n(2)\nl (t\/T )\u03c1l(t\/T ) +O(T\n\u22121M)\nThe MSE is given by\nMSE(C\u02c6l(t\/T )) = E(C\u02c6l(t\/T )\u2212W\n(1)\nl (t\/T )W\n(2)\nl (t\/T )\u03c1l(t\/T ))\n2\n= E(\n\u22121\u2211\nj=\u2212J\u2217\nI\u02dc\n(1,2)\nj,t,T A\n\u22121\nl,j \u2212\n\u22121\u2211\nj=\u2212\u221e\n\u03b2j(t\/T )A\n\u22121\nl,j )\n2\n18\nwhere \u03b2j(z) =\n\u2211\n\u22121\ni=\u2212\u221eW\n(1)\ni (z)W\n(2)\ni (z)\u03c1i(z)Ai,j . This can be split into two terms:\nMSE(C\u02c6l(t\/T )) \u2264 2E\n( \u22121\u2211\nj=\u2212J\u2217\n(I\u02dc\n(1,2)\nj,t,T \u2212 \u03b2j(t\/T ))A\n\u22121\nl,j\n)2\n+ 2\n(\u2212J\u2217\u22121\u2211\nj=\u2212\u221e\n\u03b2j(t\/T )A\n\u22121\nl,j\n)2\n= I + II\nThe first term is given by\nI \u2264 2\n\u22121\u2211\nj=J\u2217\nE(I\u02dc\n(1,2)\nj,t,T \u2212 \u03b2j(t\/T ))\n2(A\u22121l,j )\n2 \u2264 2\n\u22121\u2211\nj=\u2212J\u2217\nC2l2jO(2\u22122jT\u22121M2)\n= O(2lT\u22121M2)\n\u2211\nj=\u2212J\u2217\n2\u2212j = O(2lT\u22121M2).O(T\u03b1) = O(2lT\u03b1\u22121M2)\nas E(I\u02dc\n(1,2)\nj,t,T \u2212 \u03b2j(t\/T ))\n2 is the MSE of I\u02dc\n(1,2)\nj,t,T , found previously, and we know that A\n\u22121\nl,j \u2264\nC2l\/22j\/2. For the second term we further assume \u03b2j(t\/T ) < C1 (as in Fryzlewicz and\nNason (2006)), to give\nII \u2264 2\n(\u2212J\u2217\u22121\u2211\nj=\u2212\u221e\nC12\nl\/22j\/2\n)2\n\u2264 2.2l\n(\u2212J\u2217\u22121\u2211\nj=\u2212\u221e\nC12\nj\/2\n)2\n= O(2l)\nCombining these results givesMSE(C\u02c6l(t\/T )) = O(2\nlT\u03b1\u22121M2). The estimator C\u0302l(t\/T ),\nthen converges in probability toW\n(1)\nl (t\/T )W\n(2)\nl (t\/T )\u03c1l(t\/T ) for each fixed scale l, provided\nthat MT\u03b1\u22121 \u2192 0 as T \u2192\u221e and M \u2192\u221e.\n19\nReferences\nBrillinger, D. R. (1975). Time Series: Data Analysis and Theory. Holf, Rinehart and\nWinston.\nDahlhaus, R. (1997). Fitting time series models to nonstationary processes. Ann. Stat.,\n25:1\u201337.\nDahlhaus, R. (2000). A likelihood approximation for locally stationary processes. Ann.\nStat., 28:1762\u20131794.\nDaubechies, I. (1992). Ten Lectures on Wavelets. Society for Industrial and Applied\nMathematics, Philadelphia, PA, USA.\nFryzlewicz, P. and Nason, G. P. (2006). Haar-Fisz estimation of evolutionary wavelet\nspectra. J. Roy. Statist. Soc. Ser. B, 68:611\u2013634.\nGabor, D. (1946). Theory of communication. J. Inst. Elec. Eng., 93:429\u2013457.\nGrinsted, A., Moore, J. C., and Jevrejeva, S. (2004). Application of the cross wavelet\ntransform and wavelet coherence to geophysical time series. Nonlin. Process. Geophys.,\n11:561\u2013566.\nGuo, W. and Dai, M. (2006). A likelihood approximation for locally stationary processes.\nStatist. Sinica, 16:825\u2013845.\nHudgins, L., Friehe, A., and Mayer, M. (1993). Wavelet transforms and atmospheric\nturbulence. Phys. Rev. Lett., 71:3279\u20133282.\nIsserlis, L. (1918). On a formula for the product-moment coefficient of any order of a\nnormal frequency distribution in any number of variables. Biometrika, 12:134\u2013139.\nJones, M. and Wilson, M. A. (2005). Theta rhythms coordinate hippocampal-prefrontal\ninteractions in a spatial memory task. PLoS Biol., 3:2187\u20132199.\nLachaux, J., Lutz, A., Rudrauf, D., Cosmelli, D., Le Van Quyen, M., Martinerie, J., and\nVarela, F. (2002). Estimating the time-course of coherence between single-trial brain\nsignals: an introduction to wavelet coherence. Neurophysiol. Clin., 32:157\u2013174.\nLawson, C. L. and Hanson, B. J. (1974). Solving Least Squares Problems. Prentice-Hall.\nMaraun, D. and Kurths, J. (2004). Cross wavelet analysis: significance testing and pitfalls.\nNonlin. Process. Geophys., 11:505\u2013514.\nNason, G., von Sachs, R., and Kroisandt, G. (2000). Wavelet processes and adaptive\nestimation of the evolutionary wavelet spectrum. J. R. Statist. Soc. B, 62:271\u2013292.\nOmbao, H. C., Raz, J. A., von Sachs R., and Malow, B. (2001). Automatic statistical\nanalysis of bivariate nonstationary time series. J. Am. Stat. Assoc., 96:543\u2013560.\n20\nOmbao, H. C., von Sachs R., and Guo, W. (2005). SLEX analysis of multivariate nonsta-\ntionary time series. J. Am. Stat. Assoc., 100:519\u2013531.\nPercival, D. B. and Walden, A. T. (2000). Spectral Analysis of Time Series. Wavelet\nMethods for Time Series Analysis (Cambridge Series in Statistical and Probabilistic\nMathematics).\nSchack, B. and Krause, W. (1995). Dynamic power and coherence analysis of ultra short-\nterm cognitive processes - a methodological study. Brain Topogr., 8:127\u2013136.\nSerroukh, A. and Walden, A. T. (2000). Wavelet scale analysis of bivariate time series i:\nMotivation and estimation. J. Nonparametr. Stat., 13:1\u201336.\nSlutsky, E. (1925). U\u00a8ber stochastische Asymptoter und Grenzwerte. Metron, 5:3\u201389.\nTukey, J. (1961). Discussion emphasising the connection between analysis of variance and\nspectrum analysis. Technometrics, 3:1\u201329.\nVarela, F., Lachaux, L., Rodriguez, E., and Martinerie, J. (2001). The brainweb: Phase\nsynchronization and large-scale integration. Nat. Rev. Neurosci., 2:229\u201339.\nVidakovic, B. (1999). Statistical Modeling by Wavelets. Wiley.\nWhitcher, B., Craigmile, P., and Brow, P. (2005). Time-varying spectral analysis in\nneurophysiological time series using hilbert wavelet pairs. Signal Process., 85:2065\u2013\n2081.\nWhitcher, B., Guttorp, P., and Percival, D. (2000). Wavelet analysis of covariance with\napplication to atmospheric time series. J. Geophys. Res., 105:14941 \u2013 14962.\n21\n"}