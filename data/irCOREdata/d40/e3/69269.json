{"doi":"10.1109\/MMCS.1999.778573","coreId":"69269","oai":"oai:eprints.lancs.ac.uk:27735","identifiers":["oai:eprints.lancs.ac.uk:27735","10.1109\/MMCS.1999.778573"],"title":"A Bandwidth Friendly Search Engine.","authors":["Clare, Bradford","Marshall, Ian W."],"enrichments":{"references":[{"id":976713,"title":"and A.Broder \u201c A technique for measuring the relative size and overlap of public Web search engines\u201d.","authors":[],"date":"1998","doi":"10.1016\/S0169-7552(98)00127-5","raw":"K. Bharat and A.Broder \u201c A technique for measuring the relative size and overlap of public Web search engines\u201d. Computer Networks and ISDN Systems, 30, pp379. 1998","cites":null},{"id":977101,"title":"et al \u201c","authors":[],"date":null,"doi":null,"raw":"C Roadknight et al \u201c Analysis of Artificial Neural Network Data Models\u201d Proceedings of Intelligent Data Analysis \u201997.","cites":null},{"id":977213,"title":"Revett \u201cNetworked information management\u201d","authors":[],"date":"1997","doi":"10.1023\/A:1018609413277","raw":"N J Davies and M C Revett \u201cNetworked information management\u201d BT Technology Journal 15 No2, April 1997, pp194","cites":null},{"id":976970,"title":"The Harvest information discovery and access system, in:","authors":[],"date":null,"doi":"10.1016\/0169-7552(95)00098-5","raw":"C.M. Bowman, P.B. Danzig, D.R. Hardy, U. Manber, and M.F. Schwartz,  The Harvest information discovery and access system, in: Proc. of the 2nd World Wide Web Conference","cites":null},{"id":977360,"title":"The influence of geographical and cultural issues on the cache proxy server workload\u201d Almeida et al,","authors":[],"date":null,"doi":null,"raw":"\u201cThe influence of geographical and cultural issues on the cache proxy server workload\u201d Almeida et al, Computer Networks and ISDN Systems, Vol. 30, issues 1-7. Article SP4.","cites":null}],"documentType":{"type":1}},"contributors":["IEEE Computer Society, Missing"],"datePublished":"1999","abstract":"The Internet plays host to many millions of documents and images and is increasing in size all the time. As a result locating web content is becoming increasingly difficult for users, and search traffic from users and spiders is increasing rapidly. A directory of the contents of the emerging cache hierarchy would be more complete than existing tools and avoid the need for spider traffic. However it is also essential to minimise the user traffic. A promising approach is to encourage users to refine their queries through categories. Automatic categorisation of a cache directory is demonstrated, and an adaptive categorisation scheme is proposed","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/69269.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/27735\/1\/27735.pdf","pdfHashValue":"ecd1db931c18ec522fdcc96a9356e9f7b4331741","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:27735<\/identifier><datestamp>\n      2018-01-24T01:58:08Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D626F6F6B5F73656374696F6E<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        A Bandwidth Friendly Search Engine.<\/dc:title><dc:creator>\n        Clare, Bradford<\/dc:creator><dc:creator>\n        Marshall, Ian W.<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        The Internet plays host to many millions of documents and images and is increasing in size all the time. As a result locating web content is becoming increasingly difficult for users, and search traffic from users and spiders is increasing rapidly. A directory of the contents of the emerging cache hierarchy would be more complete than existing tools and avoid the need for spider traffic. However it is also essential to minimise the user traffic. A promising approach is to encourage users to refine their queries through categories. Automatic categorisation of a cache directory is demonstrated, and an adaptive categorisation scheme is proposed.<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:contributor>\n        IEEE Computer Society, Missing<\/dc:contributor><dc:date>\n        1999<\/dc:date><dc:type>\n        Contribution in Book\/Report\/Proceedings<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/27735\/1\/27735.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/MMCS.1999.778573<\/dc:relation><dc:identifier>\n        Clare, Bradford and Marshall, Ian W. (1999) A Bandwidth Friendly Search Engine. In: IEEE international conference on multimedia computing and systems. IEEE, pp. 720-724. ISBN 0769502539<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/27735\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1109\/MMCS.1999.778573","http:\/\/eprints.lancs.ac.uk\/27735\/"],"year":1999,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution in Book\/Report\/Proceedings","NonPeerReviewed"],"fullText":"A Bandwidth Friendly Search Engine\nClare Bradford and Ian.W.Marshall\nBT Labs, Martlesham Heath, Ipswich, UK. IP5 3RE.\ne-mail :clare.bradford@bt.com and ian.w.marshall@bt.com\nAbstract\nThe Internet plays host to many millions of documents and\nimages and is increasing in size all the time. As a result\nlocating web content is becoming increasingly difficult for\nusers, and search traffic from users and spiders is\nincreasing rapidly.  A directory of the contents of the\nemerging cache hierarchy would be more complete than\nexisting tools and avoid the need for spider traffic.\nHowever it is also essential to minimise the user traffic.  A\npromising approach is to encourage users to refine their\nqueries through categories.  Automatic categorisation of a\ncache directory is demonstrated, and an adaptive\ncategorisation scheme is proposed.\nIntroduction\nThe Internet has become a significant publication\nmedium, it plays host to many millions of documents and\nimages and is increasing in size all the time [1]. The\nmassive size of the internet has created a market need for\nimpressive navigational tools. The largest search engines\ncurrently cover less than half the cacheable contents of the\nWeb [2], and are unable to accurately interpret the simple\nsearch terms preferred by users. Should search engine\ndevelopers create bigger indexes to give better internet\ncoverage, thereby increasing the numbers of spiders\naccessing and querying new sites ? We believe this would\nsimply add to network congestion without benefit.  It would\nmerely increase the number of irrelevant search results the\nuser is required to sort through, thereby increasing not only\nthe spider traffic but the user traffic too.  On the other hand,\nin order to provide better chances of finding material the\nuser would find useful, the source database must clearly\nbecome more comprehensive.\n The Harvest project partly addressed this by making\ndirectories from a cache [3]. Providing a query interface to\na cache hierarchy will deliver two immediate benefits.  It\nprovides access to a directory which is more complete than\nany current search engine without requiring spiders to\npopulate the database, and it enables the query responses to\nbe ordered on the basis of file popularity, increasing the\nprobability that useful pages are early in the response list.\nThe Harvest catalogue servers maintained a flat\ndirectory, similar to early search engines, that was not able\nto prioritise responses to match user requirements. This\napproach is not scalable since the user has to filter too many\nresponses. We have attempted to address this issue by\nstructuring the database and enabling the structure to adapt\nto changing user requirements.\nOrdering responses on the basis of file popularity is\nnot sufficient \u2013 many popular pages are only relevant to a\nminority of queries but are written so as to give them a high\nprobability of appearing in the response to any simple\nquery.  It is necessary to enable users to ask the search tool\nto filter responses which are not relevant, without\nsignificantly increasing query complexity.  A common\napproach is to divide the directory into a number of\ncategories which the user can select before entering the\nquery.\nIn the next section we present a design for an adaptable\ncategorised query engine based on a hierarchy of cache\ndirectories, and report some results derived from a partial\nimplementation.  Our initial implementation demonstrates\nfor the first time that automated categorisation of a cache\ndirectory, with sufficient accuracy to be useful, is possible.\nFurther work is required to implement adaptive\ncategorisation.\nDesign\nThe design, shown in Figure 1, consists of a server\nbased query engine that searches a local cache. The cache\nhas been categorised and indexed and the query engine\nserves results from this sorted cache. The query engine,\nproxy server and indexer are adequately documented\nelsewhere [4], and we do not attempt to duplicate this here.\nThe next part of this section concentrates on the categoriser.\nFigure 1 System Design\nA schematic of our design for an adaptive categoriser\nof a cache index is shown in Figure 2 and described in some\ndetail below\nFigure 2 Adaptive categorisation at a single node\nThe category Proposer (A) is a learning agent whose\nprimary role is to collect and analyse information contained\nin the search index, and propose category lists to the rule\ngenerator (B).  The analysis is initially based on\nwordcounts, metadata expressions, origin server, and\nupdate\/time-to-live, but could be based on any information\nstored in the index.  The analysis could be performed by a\nneural network, with adjustable weights for each input\nmetric.  The network would be trained on a standard dataset\nthen allowed to adapt to the needs of its user community.\nThe index itself uses a static schema but the proposer is\nintended to be adaptive, so that the agent will learn which\nmetrics are most useful and be able to react to underlying\ntrends which may render a particular metric more or less\nuseful with time.  It should also be able to respond to\noccasional new fields being added to the index, using an\noperator initiated retrain function.  The proposer will have\nan internal feedback loop for learning purposes, enabling it\nto autonomously learn how best to propose a fixed number\nof categories with an evenly distributed population, and\nwith comprehensible descriptors derived from metadata\nkeywords or from popular query terms.  External feedback\nto enable adjustment of metrics is provided by the Measure\nfunction (D).  This feedback is primarily a list of popular\nquery terms generated by users.  A will only initiate a\nproposal when D supplies A with a significantly modified\nlist of popular queries, or when D reports a significant\nfailure such (e.g. falls in usage, repeated queries, and\nidentical queries of multiple categories), or when an\noperator requests it. A sends the proposed categories, and a\nlisting of its node weights to the rule generator (B).\nThe rule generator, B, is responsible for converting the\nneural net\u2019s weights into a set of rules that can be used by\nthe categoriser, C.  The rule generator is intended to use the\ntechniques described in [5], governed by a set of metarules\nprovided by the operator.  The generator is able to modify\nits metarules in response to feedback from the measure\nfunction, from the operator, and from internal metrics.  The\ninternal metrics would include consistency checks and\nrulebase minimisation, but others may also be required.\nThe Categoriser, C, is the classification agent, which\nuses input from its own \u2018knowledge\u2019 and the rules given via\nB to associate objects in the index with categories.  The\nsame object can appear within many categories, but the\ncategoriser will calculate a different degree of fit for each\nassociation, and store it in the index.  This enables the query\nengine to order responses, to a search within a category, on\nthe basis of fit to the category. It is envisaged that the user\nwill have the option to specify different orderings of the\nresponse when they make a query. The object associations\nand metrics are located within a structured directory,\nresulting in fast retrieval via the query engine. The feedback\nloop throughout the system is continually active. Should D\ndetermine that categories are not representative of the\nsearches requested, then A and B will suggest a re-analysis\nof the knowledge base in C. This is to ensure that the system\nis truly adaptive in a changing user environment. Without\nthis flexibility, the tool would lay idle in preference to more\ncustomised alternatives.\nThe measure function, D, is the overall system\ncontroller.  It provides the management interface to the\noperator, and the external feedback to the other modules.\nThe feedback is based on analysis of the query engine usage\nlogs.  The logged parameters it monitors and acts on\ndetermine the system\u2019s ability to respond to changes in user\nbehaviour. For instance, consider a user returning to the\nsearch page within a short time frame. This could be for\nmany reasons, each of which would result in the\ncommunication of different messages. Firstly, the initial\nsearch could have given inaccurate search results. In this\ncase user will probably refine their query, and D would\nindicate this to A, sending parameters to convey the\nspecifics. A second reason for the subsequent visit is that\nthe search proved very useful, provided a directly relevant\nresult and the user simply wanted another query served.\nTwo different results from seemingly the same user action,\nthe second requiring no system update. D monitors\nusability, in terms of how often the service is used by users.\nAgain there could be different reasons for the drop in usage,\nA\nCategory\nProposer\nB\nRule\nGenerator\nC\nCategoriser\nD\nMeasure\nQuery\nengine\nProxy server\nand cache\nIndex\nCategoriser\nuser\nquery\nresults\nsomeone could change projects (requiring less searching),\nthey could be on leave, or in fact could be using other\nsearch tools. If it was only used to search certain subjects\nand generic search engines still used for others, then the\ncategory scope is not broad enough for the users interest\nand should be revised. D would indicate this requirement to\nA. In the future, D might iteroperate with system\napplications such as performance monitor tools to facilitate\neffective user modelling. Our early design, however, will\nencompass \u2018best guess\u2019 criteria based on past usage\npatterns.\nThe categoriser enables the cache based search system\nto be adaptive, in that metrics will report the success of user\nqueries. Metrics will provide information as to the\nusefulness, usability and accuracy of the query engine by\nanalysing usage patterns of the service. The system is fully\nautomated and self modifying.  User-input is not requested,\nthe metrics are fully automated and the system adaptive to\nthe results. This provides for consistency of analysis (human\ndetermined feedback is very subjective) and, with the right\nmetrics, accurate user modelling. The combination of\nmetadata and XML aware resources, with intelligent agents\nwill give a tool capable of analysing and categorising both\nstructured and unstructured data. Integrating system\napplications with intelligent agents can provide for a\npowerful metrics tool and the system will be capable of\nlearning and modifying its actions.\nThe system described thus far is only what would be\nimplemented on a single cache.\nThe most obvious additional requirement in a\nmultinode implementation is to synchronise the actions of\nthe autonomous adaptive nodes.  In the context of our\ndesign this is expressed as the need to share knowledge of\ncategories across all nodes participating in a particular\nquery.  There are several options for achieving this.  The\nparent could simply send hourly cache digests [6] to its\nchildren, however the hashed information in current digests\nis insufficient for categorisation, and it is not yet clear how\nmuch more is required.  Alternatively, it could periodically\nsend its entire index, but this would be very bandwidth\nintensive (our current indexes are around 8% of the cache\nsize). The best answer is probably to send the rules used by\nthe categoriser, so that the children\u2019s categorisers can\ngenerate mappings between their own categories and those\nof their parent. A global categorisation scheme does not\nseem appropriate since this would not take advantage of\nuser communities and localisation. It would also require\nuniversal agreement between cache administrators, which\ncould impede the uptake of the proposal. Hence our design\nallows child nodes to define local categories, and map\nqueries  to its parents categories if the user indicates a need\nfor broader search. A consequence of this decision is that\nthe categorisation becomes weaker as the hierarchy is\ntraversed.  This is because the mappings will necessarily be\nless reliable than local categorisation based on full\nknowledge of the parent\u2019s index. The child would make\nrequests of one or more parent categories in response to a\nuser query of a local category.  It can map the responses\nfrom its parent into its own categories before passing them\nback to the user but there may be responses missing which\nthe local category would have caught but are not in the\nparent category (or the converse). Since many searches\nwould be served by the local cache, and even a wider search\nwould still serve more relevant results that generic search\nengines, this approach is considered to be well worth\ninvestigating.\nThe design of the implemented hierarchy might also\ninclude sibling searches prior to querying a parent,\nhowever, this would be just an ordering process and does\nnot impact the need to provide category information to any\ncache which is forwarding queries.\nIt only remains to specify how the user indicates the\ndesired breadth of search \u2013 at present we envisage a simple\nslider on the query engine interface page.\nOur implementation\nFor an initial study we created a categorised index of a\nsmall 2-layer cache hierarchy used by our own research\nteam.  The cache hierarchy was implemented using MS\nProxy Server and Microsoft Index Server was used to index\nthe cache directories [7].  The index contained filename,\nmetadata and unformatted text for each page, and was\napprox 8% of the total size of the cache.  The indexer\ncreated an index for its local cache only.  Queries could be\nmade on the local cache, or on both the local cache and its\nparent.  Queries of the parent cache were supported using a\ncopy of the parent cache\u2019s index stored on the local cache,\nand updated on a daily basis.  A standard Microsoft Index\nServer active server page query form was customised to\ndisplay the subject areas that represented our categories. A\nsmart spider [8] was used to categorise the cache indexes.\nAs the categories were not adaptive it was not necessary to\nrecategorise the index copied from the parent cache to\nmatch the local categories.  A team of volunteers were\nrequested to test the service. Users were requested to return\nfeedback to test the implementation in terms of relevancy of\nresults and their impression as to the idea of pre-searching\ncategorisation.  This generated useful qualitative feedback\nsummarised in the next section.  We also used the index\nserver query logs to get an idea of how frequently users\nwere entering the same query into different categories.  Due\nto the small community involved the results are not\nquantitatively useful, but they were sufficient to show that\nthe query logs are a useful input for the category proposer in\nour design\nFor a more controlled test, it was necessary to analyse\nthe effectiveness of the spider on a known set of categories.\nThe chosen source was an Encarta \u201998 CD [9]. The subject\ncategories of the chosen pages were masked before the\nspider analysed the pages, and the results compared\nbetween the spider\u2019s categorisation and the encyclopaedia\u2019s\nclassification. The source pages were chosen completely at\nrandom within a chosen classification on the CD, and in\naddition to this, extra pages were extracted from\nclassifications not included in the spider\u2019s categories. This\nwas to represent Web \u2018junk\u2019 pages, in that they weren\u2019t\nexpected to fall into any of the chosen categories and hence\nsearching via categories should not have resulted in one of\nthe pages being served (thus testing whether irrelevant\ndocuments are presented to a user\u2019s query).\nFor each categorisation attempt we noted the number\nof words required in the spider vocabulary files, the time it\ntook to analyse pages and the extent of human intervention\nrequired to help the spider, in time and activity. The\ncategorisation was then characterised using several metrics.\nThe metrics were the number of pages accepted into the\ncorrect category, the number of pages failed (not\ncategorised that should have been), and those ambiguously\ncategorised (accepted into more than one category, both\nwhere this double classification was valid and where one of\nthe classifications was incorrect).  Double classification is\ncharacteristic of subject classification [10], however, this is\nnot necessarily a problem since many \u2018distinct\u2019 subject\nareas overlap.  For instance, if the page subject is an ice-\nbreaking trawler then from our categories, oceans and\ntransport could both be deemed correct classifications.\nSometimes, however, the second classification can be\nincorrect, made because a couple of words lead to a\nmisleading diagnosis.\nResults\nThe search engine was deemed easy to use and\ninformative by the users who tested it. Perhaps the most\nimportant observation in the reports received was that all\nsearch results were deemed 100% relevant. Although the\ntests were carried out on a source which was not totally\nrepresentative of Web contents (our cache matches our\ninterests well), they do suggest the categorisation is of real\nbenefit. The benefit would remain for searching a larger,\nless focused source.  It would probably be greater.  This is\nlargely because the search topic is selected before a search\nactually takes place, hence users only search on a relevant\ninformation source, and only receive relevant results.\nFigure 3 Categorisation Results\nThe key results from the Encarta based tests are shown\nin Figure 3. A 95% success rate in subject categorisation\nwas achieved using the Smart Spider to analyse Encarta.\nThere were 11 pages accepted into more than one category,\nand upon further investigation all categories were deemed\nvalid. The spider used flat vocabulary files in order to\nassociate words with categories and used rules files in order\nto specify acceptance criteria. Both rules files and\nvocabulary files could be extended, and by using them\nefficiently a 95% success rate was achieved with the\nspider\u2019s classifications. That is 95% of the source pages on\nthe Encarta CD, under the headings we used, were found by\nthe spider. The first time the spider categorised it\u2019s source\nled to a 70% success rate. Extending the vocabulary files\nand rule base, using a simple heuristic of one additional rule\nper new keyword, increased this number to first 80% and\nthen 95%.  Further extension using the simple rule\ngeneration strategy led to a reduction in performance\nbecause inappropriate pages were captured.  The\nmodification of the vocabulary files and the knowledge base\nwas very time consuming. Testing required 14 hours to\nanalyse the pages that were incorrectly classified and to\nmake appropriate changes to the associated rule and\nvocabulary files. An agent with an automated learning\ncapability would significantly reduce the operator effort\nrequired.  However, by continually updating the words and\nassociations within a particular subject, using an\nautomateable rule generation heuristic, we can eventually\nlose accuracy. Spider administrators would need to\nintervene to perform more sophisticated rule generation if\nmore accuracy was required, seeking the best trade-off\nbetween capturing more pages in a category and capturing\nirrelevant pages.\nThis is best discussed by example. Should \u2018ice-breaker\ntrawler\u2019 be deemed to be correctly categorised as a subject\nwithin oceans, the categoriser could ensure that terms such\nas ice and ice breaker were within the oceans topic.\nHowever, this could result in information concerning the\nAntarctic and domestic ice-picks as being categorised\nwithin this topic.  Correct inclusion could increase only\nslightly whilst incorrect inclusion would increase\ndramatically.  Our aim was to find the trade-off between\nCategorisation Results\n212\n4\n9\n225\nCorrect Category\nWrong Category\nNot Categorised\nTotal\nsource-page analysis accuracy and the strength of the\nsubject categorisation.\nWe believe that the feedback that all search results\nwere relevant to the search topic indicates the 95% accuracy\nreached was satisfactory.\nThe feedback and results received from our testing\nadds credibility to the concept that an efficient search\nengine fronting a scalable, categorised cache hierarchy\nwould indeed be a tool worth developing.\nDiscussion\nThe work presented here has successfully shown that a\nmanaged cache directory provides an effective search base.\nIn the future the directory will be exposed to a larger user\ncommunity in order to obtain more meaningful web based\nmeasurements. In parallel with this we will implement the\nadaptation mechanism and investigate the sharing of\ncategories between caches. We must also further automate\nthe categoriser, increasing intelligence and reducing the\nhuman-hours required to \u2018teach\u2019 the agent. The end result\nshould be an automated and truly distributed, scalable\nsearch engine that covers far more of the Web\u2019s cachable\ncontent, generates less traffic and is more attractive to end\nusers, than current search tools.\nIn order for communications between caches to be\neffective, and to enable the largest traffic savings,\ncooperation between cache owners and implementers will\nbe necessary. A standard is required in order for one cache\nto be able to understand and make use of the categories\nused by another.  We have not yet progressed far enough to\nbe able to identify the minimum information that must be\nshared, but it seems likely that caches will need to export\ntheir categorisation rules in an agreed format.\nThe process overheads of the proposed adaptive\nmechanism are not understood.  Our experiments have run\non old PCs (180 MHz Pentium), for up to 10  concurrent\nusers, so we are confident the overheads are reasonable.\nIt is possible the categorisations will be unstable\n(oscillatory) for some user communities, so we may need to\nadd an additional feedback loop to the design to damp\noscillations. The identification of this type of instability\ncould even be used to optimise the locality of user interests\nand further enhance response times.\nCache studies [11] have shown that there are different\naccess patterns according to the culture of users, and the\nchoice of server locations should be directly related to the\nfindings of these studies. At present enough is known about\nthe locality of user diversity to justify our choice of local\ncategories rather than global categories, but further work is\nrequired to identify an optimum granularity, and this work\nis likely to be community specific. Highly diversified\ncompanies (conglomerates) will have many communities\nwith only a small overlap of interests, whereas more\nfocused companies are likely to have only one or a few\ndistinct communities.  The choice of location is thus\ncompany dependent.\nConclusions\nWe have proposed a design for a search tool based on\nadaptive categorisation of the contents of a cache hierarchy.\nWe have demonstrated that automatic indexing and\nintelligent categorisation of Web cache content can provide\nan effective search tool.  We have discussed the extra work\nrequired to fully justify our design, and the design issues\nthat remain to be resolved.  Initial indications are that our\ndesign is extremely promising and could potentially solve\nmany current problems in web content location.\nReferences\n[1]http:\/\/www.computer.org\/internet\/v2n5\/w5news-data.htm\n[2]K. Bharat and A.Broder \u201c A technique for measuring the\nrelative size and overlap of public Web search engines\u201d.\nComputer Networks and ISDN Systems, 30, pp379. 1998\n[3]C.M. Bowman, P.B. Danzig, D.R. Hardy, U. Manber, and M.F.\nSchwartz, The Harvest information discovery and access\nsystem, in: Proc. of the 2nd World Wide Web Conference\n[4]http:\/\/www.missouri.edu\/~libnh\/ASI\/tools.htm\n[5]C Roadknight et al \u201c Analysis of Artificial Neural Network\nData Models\u201d Proceedings of Intelligent Data Analysis \u201997.\n[6]http:\/\/www.in.com.pl\/linux\/documentation\/squid-\n1.2.beta22\/FAQ\/FAQ-16.html\n[7]http:\/\/www.microsoft.com\/products\/prodref\/590_ov.htm\n[8]http:\/\/www.smart-spider.com\/\n[9]Microsoft Encarta \u201998  - British Edition\n[10]N J Davies and M C Revett \u201cNetworked information\nmanagement\u201d BT Technology Journal 15 No2, April 1997,\npp194\n[11]\u201cThe influence of geographical and cultural issues on the\ncache proxy server workload\u201d Almeida et al,\nComputer Networks and ISDN Systems, Vol. 30, issues 1-7.\nArticle SP4.\n"}