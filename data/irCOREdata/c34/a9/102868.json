{"doi":"10.1109\/ICIF.2005.1591879","coreId":"102868","oai":"oai:epubs.surrey.ac.uk:2425","identifiers":["oai:epubs.surrey.ac.uk:2425","10.1109\/ICIF.2005.1591879"],"title":"Multimodal Image Registration with Applications to Image Fusion","authors":["Heather, Jamie P","Smith, Moira I"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2005","abstract":"This paper presents an algorithm for accurately aligning two images of the same scene captured simultaneously by sensors operating in different wavebands (e.g. TV and IR). Such a setup is common in image fusion systems where the sensors are physically aligned as closely as possible and yet significant image mis-alignment remains due to differences in field of view, lens distortion and other camera characteristics. Our proposed registration method involves numerically minimising a global objective function defined in terms of local normalised correlation measures. The algorithm is demonstrated on real multimodal imagery and\\ud\napplications to imagefusion are considered. In particular\\ud\nwe illustrate thatfused image quality is closely related to\\ud\nthe degree ofregistration accuracy achieved. To maintain\\ud\nthis accuracy in real systems it is often necessary to\\ud\ncontinuously update the transform over time. Thus we\\ud\nextend our registration approach to execute in real time\\ud\non live imagery, providing optimal fused imagery in the\\ud\npresence ofrelative sensor motion andparallax effects","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"Institute of Electrical and Electronics Engineers","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:2425<\/identifier><datestamp>\n      2017-10-31T14:05:32Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:656C656374726F6E6963656E67696E656572696E67:63637372<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/2425\/<\/dc:relation><dc:title>\n        Multimodal Image Registration with Applications to Image Fusion<\/dc:title><dc:creator>\n        Heather, Jamie P<\/dc:creator><dc:creator>\n        Smith, Moira I<\/dc:creator><dc:description>\n        This paper presents an algorithm for accurately aligning two images of the same scene captured simultaneously by sensors operating in different wavebands (e.g. TV and IR). Such a setup is common in image fusion systems where the sensors are physically aligned as closely as possible and yet significant image mis-alignment remains due to differences in field of view, lens distortion and other camera characteristics. Our proposed registration method involves numerically minimising a global objective function defined in terms of local normalised correlation measures. The algorithm is demonstrated on real multimodal imagery and\\ud\napplications to imagefusion are considered. In particular\\ud\nwe illustrate thatfused image quality is closely related to\\ud\nthe degree ofregistration accuracy achieved. To maintain\\ud\nthis accuracy in real systems it is often necessary to\\ud\ncontinuously update the transform over time. Thus we\\ud\nextend our registration approach to execute in real time\\ud\non live imagery, providing optimal fused imagery in the\\ud\npresence ofrelative sensor motion andparallax effects.<\/dc:description><dc:publisher>\n        Institute of Electrical and Electronics Engineers<\/dc:publisher><dc:date>\n        2005<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/2425\/1\/SRF002663.pdf<\/dc:identifier><dc:identifier>\n          Heather, Jamie P and Smith, Moira I  (2005) Multimodal Image Registration with Applications to Image Fusion   7th IEEE International Conference on Information Fusion (FUSION) Vols 1 and 2.  pp. 372-379.      <\/dc:identifier><dc:relation>\n        http:\/\/ieeexplore.ieee.org\/xpls\/abs_all.jsp?arnumber=1591879<\/dc:relation><dc:relation>\n        10.1109\/ICIF.2005.1591879<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/2425\/","http:\/\/ieeexplore.ieee.org\/xpls\/abs_all.jsp?arnumber=1591879","10.1109\/ICIF.2005.1591879"],"year":2005,"topics":[],"subject":["Article","PeerReviewed"],"fullText":"2005 7th International Conference on Information Fusion (FUSION)\nMultimodal Image Registration with Applications to\nImage Fusion\nJamie P. Heather\nWaterfall Solutions Ltd\n32 London Road, Guildford, UK\njamie.heather(i)waterfallsolutions.co.uk\nDr Moira I. Smith\nWaterfall Solutions Ltd\n32 London Road, Guildford, UK\nmoira.smith(i)waterfallsolutions.co.uk\nAbstract - This paper presents an algorithm for\naccurately aligning two images of the same scene\ncaptured simultaneously by sensors operating in different\nwavebands (e.g. TV and IR). Such a setup is common in\nimage fusion systems where the sensors are physically\naligned as closely as possible and yet significant image\nmis-alignment remains due to differences in field of view,\nlens distortion and other camera characteristics. Our\nproposed registration method involves numerically\nminimising a global objective function defined in terms of\nlocal normalised correlation measures. The algorithm is\ndemonstrated on real multimodal imagery and\napplications to imagefusion are considered. In particular\nwe illustrate thatfused image quality is closely related to\nthe degree of registration accuracy achieved. To maintain\nthis accuracy in real systems it is often necessary to\ncontinuously update the transform over time. Thus we\nextend our registration approach to execute in real time\non live imagery, providing optimal fused imagery in the\npresence ofrelative sensor motion andparallax effects.\nKeywords: Image registration, image fusion.\n1 Introduction\nImage registration is the process of spatially aligning\ntwo or more images of the same scene, possibly recorded\nat different moments in time. This broad definition\nencompasses a multitude of image alignment problems in\nthe fields of medicine, defence, remote sensing, computer\nvision and pattern recognition. In each case the\nfundamental problem is the same; to find a mapping\n(x,y) -) (u,v), u = u(x,y), v = v(x,y)(1\nbetween the pixels (x, y) in one image and the pixels (u, v)\nin another. The complexity of the solution will depend on\nthe application under consideration. In the simplest case, a\nstraightforward geometric translation or rotation may be\nsufficient to accurately align the two images. More\nadvanced global approaches include affine, polynomial\nand projective transformations. Where a global\ntransformation is not appropriate, a piecewise, elastic\nmembrane or optical flow technique may be applied\ninstead. Brown [1] presents an overview of these various\ntransformations and the established methods for obtaining\na registration solution. She also emphasises that before\ntrying to solve an image registration problem it is vital to\nfirst appreciate the cause of the mis-alignment and then\nselect a transformation which appropriately models it.\nSources of mis-alignment can be grouped into two\ncategories:\n* Spatial mis-alignments commonly arise due to\ndifferences in sensor position\/viewing angle and also\ndue to differences between the sensors themselves\n(e.g. field ofview, resolution, lens distortion).\n* Temporal mis-alignments occur when there is some\nrelative motion between the sensor and the objects in\nthe scene and the images are captured at different\npoints in time.\nFigure 1 below illustrates these sources of spatial and\ntemporal alignment for a two-camera setup.\nyaw\n< pitch\noffset ,,^ , roll\noffset4 ^ ..\nFigure 1. Example sources of image mis-alignment\nThe possibilities above give rise to a large number of\nvery different image registration problems. In this paper\nwe consider the specific problem of aligning two images\ncaptured simultaneously from (approximately) the same\nviewpoint by rigidly mounted sensors operating in\ndifferent regions of the electromagnetic spectrum (Figure\n2). This setup is common in image fusion systems which\naim to combine the complementary features from two or\nmore wavebands into a single image with extended\ninformation content. The fused image offers significant\nbenefit over working with the raw sensor outputs\nincluding increased situational awareness and improved\ntarget detection\/identification accuracy. A variety of\nsimple and advanced image fusion algorithms have been\n0-7803-9286-8\/05\/$20.00 @2005 IEEE 372\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 19,2010 at 15:48:36 UTC from IEEE Xplore.  Restrictions apply. \ndeveloped over the last two decades [2] but they all\noperate at the pixel level and make the fundamental\nassumption that the source imagery is properly spatially\naligned. The accuracy of the registration process is\ntherefore critical to overall system performance.\nFigure 2. The raw outputs from a TV sensor (top) and an IR\nsensor (bottom). A significant difference in scale is immediately\nobvious and closer inspection of the IR image reveals a barrel\ndistortion effect not present in the TV image.\nImage fusion systems usually employ one of two sensor\nconfigurations (Figure 3). In the first illustration the\nsensors are closely mounted side-by-side and boresighted\nfor a particular distance (often infinity). In terms of the\nregistration problem this simple setup has an immediate\ndisadvantage: parallax effects. The separation between the\noptical paths of the sensors means that it is not possible to\nfind a fixed transformation which will always map one\nsource image onto the other. Instead the transformation\nhas a complex dependency on distance to objects in the\nscene. In practice, for closely mounted sensors\nboresighted at infinity, the parallax effect becomes\nnegligible (i.e. sub-pixel) when the distance exceeds a\ncertain value (usually a few hundred metres).\nFigure 3. Simple examples of boresighted (left) and common\naperture (right) sensor configurations.\nParallax effects can be avoided (to a large extent)\nthrough the use of a common aperture. In this\nconfiguration (also illustrated) one or more beam-splitters\nare employed, thereby allowing the two sensors to share a\ncommon optical path. In practice some design\ncompromises are usually necessary due to the physical\nconstraints of sensor\/beam splitter size (particularly in\nsystems with more than two cameras) and consequently\nsome small degree of parallax may remain. Common\naperture systems can also be expensive and, as such, may\nnot be the preferred 'off-the-shelf' low-cost solution.\nInitially we will assume that parallax effects are\nnegligible in our image fusion system (either because the\nobjects in the scene are sufficiently distant or through the\nuse of a common aperture). The additional assumption\nthat the two sensors are synchronised allows us to seek a\nfixed transformation mapping the pixels in one image\n(referred to as the 'reference' image) onto those in the\nother image (referred to as the 'input' image). Any image\nmis-alignments are now due to differences in sensor\ncharacteristics (e.g. field of view, pixel resolution, optical\ndistortion) and can generally be represented using an \\!h\ndegree polynomial transformation\nu(x, y) = E aijx y' , v(x, y) = bibjjx y1 (2)\ni-O j=O i-o j=O\nwhere the coefficients aij and b,,j are constants to be\ndetermined. When N = 1 equation (2) reduces to the\npopular affine transformation which is capable of\nrepresenting translations, rotations and shears. Adding\nhigher order terms to the equation allows compensation of\ncomplex lens distortions such as pin cushion or barrel\ndistortion effects. A second or third degree polynomial is\nsufficient for most practical applications.\nAt this point we note that the projective transform, as\ndefined by the equations\n) C,X+C2y+ C3 V(X y)= d1x+d2y+d3 (3)\ncux+cxy+, dx+dy,+yI\n373\nI\/I 11I\nt i\niI\nII I\n,I i\nI,:1,\n1\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 19,2010 at 15:48:36 UTC from IEEE Xplore.  Restrictions apply. \n(where c1,..., c5,dl,..., d5 are constants), is often argued to\nbe a superior representation of image mis-alignment in\nimage fusion systems. This is because the underlying\nmodel assumes the scene is being viewed from different\nviewpoints and hence is able to compensate for parallax\neffects. However, the model also assumes the scene is\nplanar (i.e. completely flat) which is generally only a\nvalid approximation in airborne applications, where\nparallax effects are less noticeable anyway. Moreover,\neven when the scene is effectively planar the transform\ncoefficients must be re-calculated whenever the cameras\nare moved to a new viewing angle (Figure 4). We\ntherefore advocate that for most practical image fusion\napplications the projective transform offers no benefit\nover the more flexible polynomial transform (although in\nsection 5 we consider the real-time update of transform\ncoefficients).\nProjectively warping image A and overlaying on image B\nE\nX@E ~~~~~Good\nE\n_ alignment\nApplying the same transformation from a different viewpoint\nE\nE p :: Tt 0 tf_ 0 ;0:: ;0 ;0_alignmentL AL\nFigure 4. For 'flat scenes the projective transfofm is able to\ncompensate for parallax effects but the transform coefficients must\nbe re-calculated if the viewpoint changes.\nDetermining optimal global transform coefficients for a\nmultimodal registration problem is no trivial task due to\nthe complex relationship between the wavebands. The\nintensity of a particular image pixel is determined not only\nby the camera response but also by a number of physical\nproperties (e.g. materials in the scene, atmospheric\nconditions and background radiation) and hence, even\nafter accurate spatial alignment, multimodal imagery often\nremains uncorrelated. Consequently, many established\nregistration techniques (e.g. from the mature field of\ncomputer vision [3]) cannot be applied directly to\nmultimodal imagery.\nIn this paper we propose an algorithm for automatically\ndetermining optimal transform coefficients for aligning\nmultimodal imagery (under the assumptions described\nabove). Our approach is based on previous work by Irani\nand Anandan [4] which split the registration problem into\ntwo stages:\ni. Identification of a suitable image representation\nbased on a multi-scale analysis of spatial structure.\nThis representation is (relatively) invariant to raw\nimage intensity and hence is ideal for assessing\nalignment ofmultimodal imagery.\nii. Formulation of a new automatic alignment technique\nutilising normalised correlation as a local similarity\nmeasure.\nThis method has been demonstrated on real multimodal\nimagery and appears to work well. Our approach also\nutilises an invariant image representation and local\ncorrelation measures but they are formulated into a global\nobjective function to be minimised. We then have a\nrigorously-defined optimisation problem which is solved\nto provide highly accurate registered images.\n2 Approach\nWe present an overview of our automatic registration\nalgorithm as an optimisation problem where an objective\nfunction is to be minimised. The objective function\nprovides an assessment of 'how good' a particular choice\nof transform coefficients is. It is constructed by locally\nassessing the similarity between the reference image and\nthe transformed image and then summing over all local\nregions. Standard correlation techniques can be used as\nsimilarity measures provided the multimodal imagery is\nfirst decomposed into an intensity invariant\nrepresentation. However, the weighting of alignment\nachievement across the field of view (particularly for\nlarge fields of view) should also be considered. This\nregion could, for example, be driven as a foveal patch in\npiloting applications.\n2.1 Image correlation techniques\nImage correlation techniques have been successfully\napplied in a wide range of applications for many years,\nand particularly in the field of computer vision. Perhaps\nthe most common example is in stereo-vision, where two\nsnapshots taken from different viewpoints are used to\nrecover depth information. Here the sensors are usually\nidentical and hence a straightforward mean-square-\ndifference calculation\nkA = _(A(x,y)-B(x,y)y\nx.y\n(4)\nis suitable as a similarity measure which is minimised\nwhen the two images A and B are perfectly aligned. A\nrelated measure is the nornalised correlation\n374\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 19,2010 at 15:48:36 UTC from IEEE Xplore.  Restrictions apply. \nEA(x,y)B(x,y)\nA= xY)A 2B gBx,y )\nwhich lies in the range [-I 1] and is invariant to scalar\nmultiplication. This function attains its maximum\/\nminimum value when B = caA for some positive\/negative\nconstant a respectively, and is zero when the two images\nare uncorrelated. The final measure we introduce here is\nthe statistical correlation\nCB\nY_(A(x, y)-|1A XB(X, y) PB )\nX,y\nx,\n(AxY- xA2,y(( y_p)\nwhich also lies in the range [-I I] and is invariant to both\nscalar multiplication and addition. It is therefore the most\nrobust of the three measures.\n2.2 Invariant image representation\nThe correlation techniques above can be made more\nrobust by applying them to local image regions rather than\nto whole images. We can construct a global similarity\nmeasure by summing the correlation over all local regions\nand the registration task is then defined as an optimisation\nproblem. However, this global similarity measure is still\nnot sufficiently robust to be applied directly to multimodal\nimagery, where the wavebands are often uncorrelated\n(both globally and locally) and may exhibit disjoint\nfeatures. Correlation techniques can be used though if we\nchoose an image representation that emphasises common\nspatial structures (lines, cormers, contours, etc) and\nsuppresses low spatial frequency features which tend to\nbe more modality-dependent.\nLaplacian filtering is an obvious approach for\nextracting high frequency spatial detail. In practice\nthough, this filter tends to extract 'too much' information\nfor registration due to its rotational invariance. Irani and\nAnandan suggest the more selective approach of applying\ndirectional derivative filters to the raw imagery and then\noptimising the transformation coefficients for vertical,\nhorizontal and diagonal alignment. To improve\nperformance the filtered images are also squared to make\nthe representation more invariant to contrast reversals.\nThese energy images tend to be well-correlated (at least\nlocally) when the original images are well-aligned and\nhence are a good invariant image representation. In terms\nof implementation we propose a variant of the method\ndescribed above; instead of transforming the energy\nimages and then testing alignment it is better to transform\nthe raw images and then calculate their energy and\nalignment. This ensures we are comparing like-with-like.\nFigure 5 demonstrates the benefit of this approach when\ntesting for large image distortions.\nCalculating (horizontal) energy ofA and warping o\n(D ~~~~~~~~~~~a.\nCalculating energy of reference image B .\n0\n~~~~~~~~~~~0\nE | -..\nWarping image A and then calculating energy\nFigure 5. Trivial example illustrating why image warping should be\napplied before the energy calculation when testing alignment. In\nthis case a rotation of 90' accurately aligns the source images but\nonly one approach reveals this.\nIrani and Anandan also propose using image pyramids\nto aid the registration process. Pyramids are a familiar tool\nin image fusion for performing multi-scale image analysis\n[2] and are easily created by successively filtering and\ndown-sampling the source images. A common example is\nthe Gaussian pyramid which takes the name of its filter\n(Error! Reference source not found.).\nIncorporating a multi-resolution image representation\ninto the registration algorithm greatly simplifies the search\nfor the optimal transform coefficients; crude estimates are\nobtained by aligning the small images at the base of the\npyramid and these are then steadily refined by repeating\nthe alignment process on the higher levels. This approach\nquickly locates the global minimum of the objective\nfunction while avoiding local minima and is thus able to\novercome fairly large image distortions.\nIn practice, the need for coarse-to-fine registration can\nbe greatly reduced by providing a good initial guess for\nthe transform coefficients, e.g. by manually specifying a\nfew tie points and then applying least squares [1]. This\nlevel of human guidance is usually acceptable for image\nfusion applications where the registration process only\nneeds to be performed once for a particular sensor\nconfiguration.\nFigure 6. A Gaussian image pyramid with 4 levels of resolution.\n375\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 19,2010 at 15:48:36 UTC from IEEE Xplore.  Restrictions apply. \nHowever, image pyramids are still useful because often\nthe information they contain in their lower levels is just as\nrelevant for assessing alignment as the full size imagery.\nThis is particularly true when aligning noisy imagery or\nwarping a low resolution sensor onto a higher resolution\nsensor (Figure 7). Thus we propose that instead of\nperforming coarse-to-fine registration a better strategy is\nto align all the levels in the image pyramid\nsimultaneously.\nTV sensor image IR sensor image\noptimisation technique this makes real-time alignment of\nmultimodal imagery a feasible proposition.\nTransformation\nAffine\nNumber of coefficients\n1 ~~~6\nProjective 12\nQuadratic 12\nCubic 20\nTable 1. Some common parametric transforms and the number of\ncoefficients that the registration algorithm must determine.\nThe following notation is now introduced: R(x, y) and\nU(u,v) denote the reference image and the unregistered\nimage respectively. We will map the pixels (x, y) in R\nonto the pixels (u, v) in U using a global transformation\nparametrised by the vector p. Our transformed image T is\nthen a function of both position (x, y) and the chosen\nparameters p,\nT(x, y; p) = U[u(x, y; p), v(x, y; p)]. (7)\nVertical energy images (level 1)\nVertical energy images (level 2)\nFigure 7. Coarse-to-fine processing can sometimes lead the\nregistration process away from an optimal solution. In the example\nabove the IR image contains a fixed noise pattem which manifests\nitself in the first energy image (corresponding to high spatial\nfrequencies) but is suppressed in the lower levels of the pyramid.\nConsequently a better end result is achieved by considering all\nresolution levels simultaneously when aligning the images.\n3 Algorithm\nWe formulate our objective function as a sum of\nsquares and then, given an initial guess for the transform\ncoefficients, rapidly search for the local minimum.\nProvided the initial guess is sufficiently accurate the\nalgorithm will terminate with the optimal set of transform\ncoefficients. An advantage of working with global\nparametric transforms (as opposed to more advanced\nmodels of image distortion, such as optical flow [5][6]) is\nthe relatively low dimensionality of the search space.\nTypically we must solve for a dozen or so unknown\ncoefficients (Table 1) and when combined with our fast\nWhen expressed in this way it is clear that in practical\nterms the backward mapping from the reference image\nonto the unregistered image is more useful for\nconstructing the transformed image than the forward\nmapping, even ifthis is a little counter-intuitive.\nOur aim is to find the parameter vector p that gives the\nbest possible spatial alignment between the images T and\nR. In section 2.2 it was proposed that alignment should be\ntested not just on the full size images but also on their\nmulti-scale decompositions. This can be achieved by\ncomputing the Gaussian pyramids of T and R and then\nsumming the similarity measures across the different\nscales. Unfortunately, though this leads to an objective\nfunction with a complex dependency on p which makes\noptimisation difficult. A much simpler strategy is to\ncalculate the Gaussian pyramids of the original images U\nand R and then appropriately transform the unregistered\nimages at each level. Thus our method begins with the\nconstruction of the pyramid {U,,U2,...,U)} defined\niteratively by\nU=RU for1=1U1{REDUCE(U,-,) forl=2,3,...,L (8)\nwhere REDUCE denotes the filter\/decimate operation [2].\nEach image in this pyramid is then warped by applying a\nrescaled version of the original transformation\nT, (x,y;p) = U, (u[x,y; g (p)],v[x, y; g (p)]) (9)\nwhere the function g, describes the effect of the image\ndown-sampling that takes place between the l' and the th\nlevels of the pyramid on the original transformation\nparameters. In most cases g, is easily determined and\ntakes a fairly simple form.\n376\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 19,2010 at 15:48:36 UTC from IEEE Xplore.  Restrictions apply. \nFollowing the construction of the transformed pyramid\n{T,,T ,...,T } the next step is to convert to an 'invariant'\nimage representation. For this we introduce the directional\nderivative operator Vk defined by\na a\nVk =ak +Pk - (10)\nwhere Ctk and Pk are constants satisfying ak, + Pk2 = 1.\nWe use the four directional derivatives defined in Table 2\nto assess the horizontal, vertical and diagonal alignment\nof each transformed image against the corresponding\nreference image. In practice a numerical approximation to\nthe Vk operator must be employed due to the discrete\nnature of the images. This is implemented as a\nconvolution operation with a small discrete kernel (e.g. a\nSobel filter). To make the representation invariant to\ncontrast reversals we will also square the filtered images\n(as discussed in section 2.2).\n1\n2\n3\n4\nDescription\nHorizontal derivative\nVertical derivative\nFirst diagonal derivative\nSecond diagonal derivative\natk\n1\n0\n1\/42\n1\/42\n0\n1\n1\/42\n-1AN2 I\nTable 2. Four directional derivatives and their corresponding\nvalues of ak and Pk in definition (10).\nAt each level of resolution we assess local similarity\nbetween the corresponding squared derivative images to\ndetermine the degree of alignment. Clearly there are many\npossible ways of dividing the images into local regions;\npartitioning into disjoint blocks or sliding neighbourhood\nprocessing are two common approaches. Whatever\nmethod is chosen let us denote the family of local regions\nfor the 1 resolution level as\nQZ, {Co,\" (d, (oM,}(II)\nwhere okl is the set of all pixels (x, y) that make up the\nmts local region and M, is the number of local regions at\nthis resolution. Defining an image similarity measure\nthen allows the formulation of our global objective\nfunction which is minimized over all possible choices of\np. Solving using our fast optimization technique tends to\nbring rapid convergence and usually only a few iterations\nare necessary to achieve very good image alignment.\n4 Results\nThe algorithm outlined above has been successfully\napplied with a variety of multimodal imagery with\nexcellent results. In particular, the algorithm has been\nused to align the TV and IR sensors in a real-time system\ndeveloped by Waterfall Solutions and QinetiQ which\nutilises a proprietary high-performance multi-resolution\nfusion technique. A cubic transform was used to\ntransform the IR image onto the TV image (both\nmeasuring 768x576 pixels), compensating for the\nsignificant difference in scale and the barrel distortion\neffect between the two. Iterations were initialised by\nspecifying a few tie points by hand and calculating a\ncrude affme transformation. The results are shown in\nFigure 8. Accurate image alignment can be observed by\nforming the composite of the transformed IR and the\nreference TV image (Figure 9).\nTV image\nIR image (with overlayed grid) & registered IR image\nFigure 8. Registration of TV and IR images captured by an image\nfusion system. A grid has been overlayed on the IR image to help\nvisualise the transform.\nIn terms of image fusion the excellent performance of\nthe registration algorithm can be seen in the high quality\nimage generated by our proprietary fusion algorithm\n(Figure 10). The relationship between registration\naccuracy and fused image quality is discussed further in\nthe following section.\n377\n---\nk |L\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 19,2010 at 15:48:36 UTC from IEEE Xplore.  Restrictions apply. \nFigure 9. Image composite formed by selecting altemate square\nblocks from the TV image and the registered IR image. Accurate\nalignment has been achieved across all image regions.\nFigure 10. Accurate image registration results in a crisp, sharp\nfused image.\n5 Real-Time Fusion Systems\nIn many image fusion systems the sensors are\nsynchronised and rigidly mounted in place and hence a\nfixed transform is usually considered sufficient to align\nthe source imagery. After boresighting the sensors as\nclosely as possible in the laboratory an appropriate set of\nimagery must be captured to enable accurate registration\nto be performed. This should consist of two (or more)\ngood quality images containing a large number of\ncommon features, distributed throughout the image, that\ncan aid the registration process. The algorithm described\nin the preceding sections is ideal for determining an\naccurate set of transform coefficients given such imagery.\nThese coefficients can then be hard-coded into the fusion\nsystem.\nIn practice there are a number of factors that can cause\nthe source images in a fusion system to become mis-\naligned. If the system is mounted on a moving platform\n(e.g. an aircraft) then operational phenomena such as\naccelerations, vibrations and even temperature changes\ncan cause the sensors to gradually 'drift' from their\noriginal alignment over time. On a much shorter time\nscale sensor vibration can cause individual image frames\nto become mis-aligned. In both cases the extent of the\nmis-alignment is often fairly small but nevertheless can\nlead to a noticeable reduction in fused image quality\n(Figure 11). A more significant issue though in many\nfusion systems is parallax. It is important to appreciate\nthat the initial registration coefficients will only remain\nvalid ifthe distance between the sensors and the objects in\nthe scene does not change from the initial setup. This\nrequirement is clearly unrealistic and often violated when\na system is put into service, leading to large mis-\nalignments and poor quality fused imagery.\nFigure 11. The images above show the fused image before (left)\nand after (right) introducing a mis-alignment of 3 pixels in the x and\ny directions.\nIn view of the limitations described above it is highly\ndesirable in many real-time fusion systems to\ncontinuously update the transform coefficients over time\nrather than use a fixed transformation. This helps ensure\nthat fused image quality is maintained at all times\n(although the extent to which the system can compensate\nfor parallax will depend on the underlying transform: the\npolynomial transform is a relatively poor model of this\neffect; the projective transform is a better choice when the\nscene is planar - see section 1). As it stands the algorithm\ndescribed in section 3 is too complex to execute as part of\na real-time system (at least with currently affordable\ncomputing technology) but such an implementation does\nbecome feasible after a straight-forward modification of\nthe algorithm; we propose to perform one iteration of the\noptimization method for each time-step in the real-time\nsystem.\nThe modified method begins as before: after acquiring\nthe latest source images they are decomposed into\nGaussian pyramids and a new set of candidate transform\ncoefficients is calculated and accepted\/rejected according\nto the observed convergence as before. At this point\nthough instead of looping around for another attempt at\nrefining the coefficients we use the current best guess to\nregister the images and then the fusion algorithm is\napplied. The process is repeated at the following time-step\nto further refine the transform coefficients for the next set\nof acquired images. The rapid convergence of the\noptimisation method ensures that given an accurate initial\nguess at the transform coefficients our latest estimate is\nnever far from the optimal solution. The extended\nregistration algorithm has been applied to short image\nsequences with promising results. In terms of\nimplementation, two points are observed:\nIt is possible for one or more of the source sensors to\ndegrade over time (e.g. due to noise or saturation) or\nfor the different wavebands to exhibit a lack of\ncommon features. Thus for a robust implementation\nsome restraint must be placed on the registration\nprocess to prevent it from going too far 'off-course'.\nThe magnitude of the objective function can be used\nas the basis for this controlling logic.\n* The registration algorithm described above naturally\ncombines with a pyramidal fusion scheme; the image\npyramids generated at each timestep may be re-used\nat the fusion stage, reducing the number of\ncalculations that must be performed there.\n378\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 19,2010 at 15:48:36 UTC from IEEE Xplore.  Restrictions apply. \n6 Discussion\nThis paper has reported on some findings from recent\nresearch conducted by Waterfall Solutions into improved\nmethods for registering multi-modal imagery. A number\nof issues associated with registering images of different\nmodality have been highlighted, and particular emphasis\nhas been placed on exploitation within image fusion\nsystems for visible and infrared imagery. A new approach\nfor achieving off-line, highly accurate image registration\nhas been discussed and pictorial results provided.\nConsideration has also been given to the suitability of the\nalgorithm to real-time applications and the very\nencouraging progress made in this area has been reported.\nFurther research into a number of important aspects of\nthis work continues, including: registration requirements\ndefinition; faster methods for improved real-time\nperformance; metrics for establishing registration\naccuracy achieved; and human factors impact of image\nregistration on fused imagery.\nAcknowledgements\nThe authors would like to thank QinetiQ for the use of\ntheir source imagery in this paper.\nReferences\n[1] L.G. Brown, A Survey of Image Registration\nTechniques, ACM Computing Surveys, Vol. 24, No. 4,\npp. 325-376, 1992.\n[2] M.I. Smith and J.P. Heather, A Review of Image\nFusion Technology in 2005, SPIE Defense & Security\nSymposium, Orlando, Florida, 28 Mar - 1 Apr 2005, Vol.\n5782.\n[3] U. Dhond and J. Aggrawal, Structurefrom Stereo: A\nReview, IEEE Trans. on Systems, Man, and Cybernetics\n19, pp. 1489-1510, 1989.\n[4] M. Irani and P. Anandan, Robust Multi-Sensor\nImage Alignment, Sixth Int. Conf. on Computer Vision,\nBombay, India, 4- 7 Jan 1998, pp. 959-966.\n[5] J.L. Barron, D.J. Fleet and S.S. Beauchemin,\nPerformance of Optical Flow Techniques, Int. Journal of\nComp. Vision 12, pp. 43-77, 1994.\n[6] R. Szeliski and J. Coughlan, Spline-Based Image\nRegistration, International Journal of Computer Vision,\nVol. 22, pp. 199-218, 1997.\n379\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 19,2010 at 15:48:36 UTC from IEEE Xplore.  Restrictions apply. \n"}