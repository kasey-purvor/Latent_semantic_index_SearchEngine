{"doi":"10.1109\/ICIF.2006.301574","coreId":"71266","oai":"oai:eprints.lancs.ac.uk:4375","identifiers":["oai:eprints.lancs.ac.uk:4375","10.1109\/ICIF.2006.301574"],"title":"Structural Similarity-Based Object Tracking in Video Sequence","authors":["Loza, A.","Mihaylova, L.","Canagarajah, N.","Bull, D."],"enrichments":{"references":[{"id":16357920,"title":"A new quality metric for image fusion.","authors":[],"date":"2003","doi":"10.1109\/icip.2003.1247209","raw":"G. Piella and H. Heijmans. A new quality metric for image fusion. In Proceedings of the Intl. Conf. on Image Processing, Barcelona, Spain, 2003.","cites":null},{"id":16357899,"title":"Estimation with Applications to Tracking and Navigation.","authors":[],"date":"2001","doi":"10.1002\/0471221279","raw":"Y. Bar-Shalom, X. R. Li, and T. Kirubarajan. Estimation with Applications to Tracking and Navigation. John Wiley and Sons, 2001.","cites":null},{"id":16357928,"title":"Image quality assessment: from error visibility to structural similarity.","authors":[],"date":"2004","doi":"10.1109\/tip.2003.819861","raw":"Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600\u2013612, Apr. 2004.","cites":null},{"id":16357905,"title":"Kernelbased object tracking.","authors":[],"date":"2003","doi":"10.1109\/tpami.2003.1195991","raw":"D. Comaniciu, V. Ramesh, and P. Meer. Kernelbased object tracking. IEEE Trans. on Pattern Analysis and Machine Intelligence, 25(5):564\u2013577, 2003.","cites":null},{"id":16357912,"title":"Methods of fused image analysis and assessment.","authors":[],"date":null,"doi":"10.1163\/156856807781503659","raw":"A.   Loza, T. D. Dixon, E. F. Canga, S. G. Nikolov, D. R. Bull, C. N. Canagarajah, J. M. Noyes, and T. Troscianko. Methods of fused image analysis and assessment. In Proceedings of the Advanced Study Institute Conference, Albena, Bulgaria, 16\u2013 27 May (to appear), 2005.","cites":null},{"id":16357902,"title":"Particle \ufb01ltering with multiple cues for object tracking in video sequences.","authors":[],"date":"2005","doi":"10.1117\/12.585882","raw":"P. Brasnett, L. Mihaylova, N. Canagarajah, and D. Bull. Particle \ufb01ltering with multiple cues for object tracking in video sequences. In Proc. of SPIE\u2019s 17th Annual Symposium on Electronic Imaging, Science and Technology, V. 5685, pages 430\u2013441, 2005.","cites":null},{"id":16357908,"title":"Sequential Monte Carlo methods for dynamic systems.","authors":[],"date":"1998","doi":"10.2307\/2669847","raw":"J. Liu and R. Chen. Sequential Monte Carlo methods for dynamic systems. Journal of the American Statistical Association, 93(443):1032\u20131044, 1998.","cites":null},{"id":16357931,"title":"Statistical Pattern Recognition.","authors":[],"date":"2003","doi":"10.1002\/0470854774","raw":"A. Webb. Statistical Pattern Recognition. John Wiley & Sons, 2003.","cites":null},{"id":16357916,"title":"Target Tracking Movie Demos.","authors":[],"date":"2004","doi":null,"raw":"PerceptiVU, Inc. Target Tracking Movie Demos. http:\/\/www.perceptivu.com\/MovieDemos.html.[8] P. P\u00b4 erez, J. Vermaak, and A. Blake. Data fusion for tracking with particles. Proceedings of the IEEE, 92(3):495\u2013513, March 2004.","cites":null},{"id":16357894,"title":"The Bhattacharyya metric as an absolute similarity measure for frequentcy coded data.","authors":[],"date":"1997","doi":null,"raw":"F. Aherne, N. Thacker, and P. Rockett. The Bhattacharyya metric as an absolute similarity measure for frequentcy coded data. Kybernetica, 32(4):1\u20137, 1997.","cites":null},{"id":16357924,"title":"The Unscented Kalman \ufb01lter.","authors":[],"date":"2001","doi":"10.1002\/0471221546.ch7","raw":"E. Wan and R. van der Merwe. The Unscented Kalman \ufb01lter. In S. Haykin, editor, Kalman Filtering and Neural Networks, chapter 7, pages 221\u2013 280. Wiley Publishing, Sep. 2001.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2006-07-10","abstract":"This paper addresses the problem of object tracking in video sequences. The use of a structural similarity measure for tracking is proposed. The measure reflects the distance between two images by comparing their structural and spatial characteristics and has shown to be robust to illumination and contrast changes. As a result it guarantees robustness of the tracking process under changes in the environment. The previously used Bhattacharyya distance is not robust to such changes. Additionally, when a tracker is run with the Bhattacharyya distance, histograms should be calculated in order to find the likelihood function of the measurements. With the new function there is no need to calculate histograms. A particle filter (PF) is implemented where this measure is used for computing the distance between the reference and current frame. The algorithm performance has been tested and evaluated over real-world video sequences, and has been shown to outperform methods based on colour and edge histograms","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/71266.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/4375\/1\/SSIM_Fusion2006.pdf","pdfHashValue":"b4582144ef7315d62070560ab5bba72c29d67110","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:4375<\/identifier><datestamp>\n      2018-01-24T02:11:19Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D626F6F6B5F73656374696F6E<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Structural Similarity-Based Object Tracking in Video Sequence<\/dc:title><dc:creator>\n        Loza, A.<\/dc:creator><dc:creator>\n        Mihaylova, L.<\/dc:creator><dc:creator>\n        Canagarajah, N.<\/dc:creator><dc:creator>\n        Bull, D.<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        This paper addresses the problem of object tracking in video sequences. The use of a structural similarity measure for tracking is proposed. The measure reflects the distance between two images by comparing their structural and spatial characteristics and has shown to be robust to illumination and contrast changes. As a result it guarantees robustness of the tracking process under changes in the environment. The previously used Bhattacharyya distance is not robust to such changes. Additionally, when a tracker is run with the Bhattacharyya distance, histograms should be calculated in order to find the likelihood function of the measurements. With the new function there is no need to calculate histograms. A particle filter (PF) is implemented where this measure is used for computing the distance between the reference and current frame. The algorithm performance has been tested and evaluated over real-world video sequences, and has been shown to outperform methods based on colour and edge histograms.<\/dc:description><dc:date>\n        2006-07-10<\/dc:date><dc:type>\n        Contribution in Book\/Report\/Proceedings<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/4375\/1\/SSIM_Fusion2006.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/ICIF.2006.301574<\/dc:relation><dc:identifier>\n        Loza, A. and Mihaylova, L. and Canagarajah, N. and Bull, D. (2006) Structural Similarity-Based Object Tracking in Video Sequence. In: Information Fusion, 2006 9th International Conference on. . ISBN 1-4244-0953-5<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/4375\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1109\/ICIF.2006.301574","http:\/\/eprints.lancs.ac.uk\/4375\/"],"year":2006,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution in Book\/Report\/Proceedings","NonPeerReviewed"],"fullText":"Structural Similarity-Based Object Tracking\nin Video Sequences\nArtur  Loza\u2217, Lyudmila Mihaylova\u2217\u2217, Nishan Canagarajah\u2217 and David Bull\u2217\n\u2217 Department of Electrical and Electronic Engineering, University of Bristol, UK\n\u2217\u2217 Department of Communication Systems, Lancaster University, UK\n[artur.loza, nishan.canagarajah, dave.bull]@bristol.ac.uk, mila.mihaylova@ieee.org\nAbstract - This paper addresses the problem of ob-\nject tracking in video sequences. The use of a struc-\ntural similarity measure for tracking is proposed.\nThe measure reflects the distance between two im-\nages by comparing their structural and spatial char-\nacteristics and has shown to be robust to illumina-\ntion and contrast changes. As a result it guarantees\nrobustness of the tracking process under changes in\nthe environment. The previously used Bhattacharyya\ndistance is not robust to such changes. Addition-\nally, when a tracker is run with the Bhattacharyya\ndistance, histograms should be calculated in order\nto find the likelihood function of the measurements.\nWith the new function there is no need to calculate\nhistograms. A particle filter (PF) is implemented\nwhere this measure is used for computing the dis-\ntance between the reference and current frame. The\nalgorithm performance has been tested and evaluated\nover real-world video sequences, and has been shown\nto outperform methods based on colour and edge his-\ntograms.\nKeywords: Similarity measure, object tracking, video\nsequences, particle filtering.\n1 Introduction\nRecently there has been an increasing interest in tar-\nget tracking in video sequences. This problem faces\nmany challenges, some of them are related to the mod-\nels of the moving object, the measurement model and\nthe function characterising the similarity between two\nimages\/video frames. One of the particularities of ob-\nject tracking in video sequences compared to track-\ning with radar data is that there is no measurement\nmodel in explicit form. Some image features, such as\ncolour, motion, and edges, can be used to track the\nmoving object [8]. The performance of the tracking\nalgorithm depends also on the measure characterising\nthe similarity or dissimilarity between the two subse-\nquent images\/video frames. Often used functions are\nthe Bhattacharyya distance [1, 4] and the non-metric\nKullback-Leibler measure.\nIn this paper we propose the use of structural sim-\nilarity measure for object tracking in video sequences\nby means of a particle filter. The motivation of apply-\ning particle filtering is that it has been proven to be a\nscalable and powerful approach, able to cope with non-\nlinearities, and work under uncertainties, which makes\nit a suitable approach for object tracking in video se-\nquences (see for example [8] and [3]). The similarity\nmeasure proposed in [11] captures spatial characteris-\ntics of an image and has shown to be robust to illumi-\nnation and contrast changes. It has been used for the\npurposes of quality assessment of distorted and fused\nimages [6, 9], but not for tracking. In the present pa-\nper, we show how this measure can be applied for track-\ning purposes. It allows one to substitute histograms\nand to calculate in a straightforward way the measure-\nment likelihood function within particle filtering. We\nshow that it is a good and fast alternative to histogram\nbased tracking.\nThe remaining part of the paper is organised as fol-\nlows. Section 2 presents the image similarity measure\nfor tracking. Section 3 describes the motion model of\nthe region surrounding the object of interest and the\nlikelihood model. Section 4 presents a particle filter\nwith the proposed similarity measure. Section 5 con-\ntains results over real-world video sequences. Finally,\nSection 6 discusses the results and open issues for fu-\nture research.\n2 Distance measure\n2.1 Structural similarity measure\nThe proposed method uses a similarity measure com-\nputed directly in the image spatial domain. This ap-\nproach differs significantly from most of the particle\nfilter algorithms, that compare image distributions rep-\nresented by their sample histograms [8].\nAlthough many simple image similarity measures\nexist, for example, Minimum Mean Square Error,\nMean Absolute Error or Peak-Signal to Noise Ratio,\nmost of these mathematical measures have failed to\ncapture the perceptual similarity of images when sub-\njected to varying luminance, contrast, compression or\nnoise [11]. Recently, based on the premise that the hu-\nman visual system is highly tuned to extracting struc-\ntural information, a new image metric has been de-\nveloped, called the Structural SIMilarity (SSIM) In-\ndex [11]. The SSIM index, S, between two images, a\nand b is defined as follows:\nS(a,b) =\n(\n2\u00b5a\u00b5b\n\u00b52a + \u00b52b\n)\u03b2 ( 2\u03c3a\u03c3b\n\u03c32a + \u03c32b\n)\u03b1(\n\u03c3ab\n\u03c3a\u03c3b\n)\u03b3\n, (1)\nwhere \u00b5 and \u03c3 stand for mean and sample standard\ndeviation, respectively, and \u03c3ab corresponds to sample\ncovariance. The three components of S, reading from\nthe left, measure how close the luminance, contrast\nand structural similarity of the two images are. Such\na combination of the three image properties can be\nseen as a case of a image cue fusion. The exponents\n\u03b1, \u03b2, \u03b3 \u2265 0, \u03b1+\u03b2+\u03b3 > 0 are used to adjust the impact\nof each measurement on the final value of S.\nIt can easily be shown that the measure defined\nin (1) is symmetric and has a unique upper bound:\nS(a,b) \u2264 c0, S(a,b) = c0 = 1 iff a = b. For detailed\nanalysis of the SSIM measure, the reader is referred\nto [11].\n2.2 Image dissimilarity\nBelow, we present a method of evaluating the likeli-\nhood function L (see Section 3), based on the similarity\nbetween two grayscale images, represented here as vec-\ntors formed from the image regions. One of the ways to\nconvert similarity S(a,b) into normalised dissimilarity\nD(a,b) is as follows [12]:\nD(a,b) =\nc0 \u2212 S(a,b)\nc1\n,\nwhere c0 and c1 are chosen to map a distance into the\ninterval [0, 1]. An alternative way [12],\nD(a,b) =\nc0\nS(a,b)\n\u2212 1 (2)\nis preferred, however, as it only requires knowledge of\nmaximal value of S and is more sensitive to very dis-\nsimilar vectors. The dissimilarity between images used\nin the method proposed in this paper is obtained by\nsubstituting (1) into (2) (as noted in the previous para-\ngraph, c0 = 1):\nD(a,b) =\n(\n2\u00b5a\u00b5b\n\u00b52a + \u00b52b\n)\u2212\u03b1(\n\u03c3ab\n\u03c3a\u03c3b\n)\u2212\u03b2 ( 2\u03c3a\u03c3b\n\u03c32a + \u03c32b\n)\u2212\u03b3\n\u22121.\n(3)\nIt can be shown that this measure satisfies nonneg-\nativity (if the absolute value of sample covariance is\nused), reflexivity and symmetry conditions. For a dis-\nsimilarity measure to be a metric (distance) a triangle\ninequality has to be satisfied. However, for our pur-\nposes, the descriptiveness and discriminating ability of\nthe measure are sufficient and this condition is not ver-\nified.\n3 Motion and likelihood models\nThe initial (reference) region surrounding the object\nof interest is chosen manually and is denoted as tref .\nIn our case this is a rectangular region, and we are\ntracking its centre. The model used for this region is\ngiven below.\n3.1 Motion model\nThe motion of the moving object is modelled by the\nrandom walk model,\nxk+1 = Fxk + vk, (4)\nwith a state vector x = (xk, yk, sk)T comprising the\npixel coordinates of the centre of the region surround-\ning the object, and the region scale sk. F is the tran-\nsition matrix (F = I in the random walk model) and\nvk is the process noise assumed to be white, Gaussian,\nwith a covariance matrix Q = diag(\u03c32x, \u03c3\n2\ny, \u03c3\n2\ns). The es-\ntimation of the scale permits to adjust the region size\nof the moving objects, e.g., when it goes away from the\ncamera, when it gets closer to it, or when the camera\nzoom varies.\n3.2 Likelihood model\nThe normalised distance between the two regions tref\n(reference region) and tx (current region), for particle\n`, is calculated according to (3), and then substituted\ninto the likelihood function:\nL(zk+1|x(`)k+1) \u221d exp\n(\u2212D2(tref , tx)\/D2min) , (5)\nwhere ` = 1, 2, . . . , N and Dmin = min\nx\n{D(tref , tx)}.\nThis likelihood function is then used to evaluate the\nimportance weights of the particle filter, to update the\nparticles and finally the overall estimate of the cen-\ntre of the current region tx. Here z is a notation of\nthe measurement vector, although with the SSIM we\nhave no measurement in explicit form. We extract di-\nrectly the structural properties of the region through\nthe SSIM that are related to the estimates of the cen-\ntre of the region of interest and we use directly the\ndistance between the reference and current region.\n4 A particle filter for object\ntracking\nParticle filtering is a method relying on sample-based\nreconstruction of probability density functions. Multi-\nple particles (samples) of the state are generated, each\none associated with a weight which characterises the\nquality of a specific particle. An estimate of the vari-\nable of interest is obtained by the weighted sum of par-\nticles. Two major stages can be distinguished in the\nParticle Filter (PF) method: prediction and update.\nDuring prediction, each particle is modified according\nto the state model of the region of interest in the video\nframe, including the addition of random noise in or-\nder to simulate the effect of the noise on the state. In\nthe update stage, each particle\u2019s weight is re-evaluated\nbased on the new data. An inherent problem with par-\nticle filters is degeneracy (the case when only one par-\nticle has a significant weight). A resampling procedure\nhelps to avoid degeneracy by eliminating particles with\nsmall weights and replicating the particles with larger\nweights. Various approaches for resampling have been\nTable 1: The particle filter with structural similarity\nmeasure\nInitialisation\n1. for ` = 1, 2, . . . , N , generate samples {x(`)0 } from the\ninitial distribution p(x0). Initialise weights W\n(`)\n0 =\n1\/N\nFor k = 0, 1, . . . ,\nPrediction Step\n2. For ` = 1, . . . , N , sample\nx\n(`)\nk+1 \u223c p(xk+1|x(`)k ) from the motion model for the\nobject region.\nMeasurement Update: evaluate the importance weights\n3. The cue is used as \u201cmeasurement\u201d. Compute the\nweights\nW\n(`)\nk+1 \u221dW (`)k L(zk+1|x(`)k+1). (6)\nbased on the likelihood L(zk+1|x(`)k+1) (5) of the cue.\n4. Normalise the weights, cW (`)k+1 = W (`)k+1\/PN`=1W (`)k+1.\nOutput\n5. The posterior mean state estimate xk+1 is computed\nusing the collection of samples (particles)\nx\u02c6k+1 =\nNX\n`=1\ncW (`)k+1x\u02c6(`)k+1. (7)\nSelection step (resampling)\n6. Multiply\/ suppress samples x\n(`)\nk+1 with high\/ low im-\nportance weights cW (`)k+1, in order to introduce vari-\nety and obtain N new random samples. The residual\nresampling algorithm described in [5, 10] is applied.\nThis is a two step process making use of sampling-\nimportance-resampling scheme.\n* For ` = 1, 2, . . . , N , set W\n(`)\nk = W\u02c6\n(`)\nk = 1\/N .\nproposed; for the work here the residual resampling\nmethod [5] was used.\nThe PF developed in this paper based on the simi-\nlarity measure is given in Table 1.\n5 Performance evaluation\nThe performance of our method is demonstrated over\nthree video sequences, in which we aim at tracking a\npre-selected moving person. The reference frames are\nshown in Figure 1. The first sequence, cross, originates\nfrom our database and contains three people walking\nquickly in front of a stationary camera. The main diffi-\nculties posed by this sequence are the colour similarity\nbetween the tracked object, the background and other\npassing people, and a temporal near-complete occlu-\nsion of the tracked person by a passer-by.\nThe second sequence used, man, has been obtained\nfrom [7]. It is a long recording showing a person walk-\ning along a car park. Apart from some similarities to\nthe nearby cars, and the shadowed areas, the video\ncontains numerous instabilities. These result from a\nshaking camera (changes in the camera pan and tilt),\nfast zoom-ins and zoom-outs, and a slightly altered\nview angle towards the end of the sequence.\ncross man doorway_ir\nFigure 1: Reference frames from the test videos\n10 20 30 40 50 60 70 800\n20\n40\n60\n80\n100\nframe index\nE x\ny\ncross\n \n \ncolour cue\ncolour & edges cue\nSSIM cue\nFigure 2: Plot of the RMSE of the object\u2019s central\npoint for sequence cross. The frames marked by the\nvertical lines are given in Figure 5\n200 300 400 500 600 700 8000\n10\n20\n30\n40\n50\n60\n70\nframe index\nE x\ny\nman\n \n \nedges cue\ncolour & edges cue\nSSIM cue\nFigure 3: Plot of the RMSE of the object\u2019s central\npoint for sequence man. The frames marked by the\nvertical lines are given in Figure 6\nThe third sequence, doorway ir, being a part of our\nmultimodal database, contains an infra-red recording\nof two people walking towards a stationary camera.\nThe two persons look quite similar and the tracked\nobject is often partially occluded by nearby objects.\nIn order to assess the performance of our tracking\nalgorithm based on the similarity measure, we compare\nit with particle filtering tracking based on colour and\nedge cues proposed in [3]. The results presented below\nshow that the PF with similarity measure outperforms\nthe PF based on a single (colour or edge) and on fused\n(colour-and-edge) cue. In the PF based on fused cues,\nthe likelihood is calculated as a product of the likeli-\nhoods of the separate cues as shown in [3].\n50 100 150 2000\n10\n20\n30\n40\n50\n60\n70\nframe index\nE x\ny\ndoorway_ir\n \n \nedges cue\ncolour & edges cue\nSSIM cue\nFigure 4: Plot of the RMSE of the object\u2019s central\npoint for sequence doorway ir. The frames are marked\nby the vertical lines are given in Figure 7\nThe model parameters are as follows: \u03c3x = 2.5, \u03c3y =\n10, \u03c3s = 0.01 (for the cross sequence), \u03c3x = \u03c3y = 2.5,\n\u03c3s = 0.05, (for the man and doorway ir sequence).\nThe standard deviations of the noises are tuning pa-\nrameters, although adaptations procedures are possi-\nble. This is an open issue that can be investigated in\nfuture, together with the necessity of finding an adap-\ntive procedure for tuning the parameters of the SSIM,\n\u03b1, \u03b2 and \u03b3. Relatively low number N = 100 of particles\nhas been used for all videos. The similarity measure\nhas been calculated in the way proposed in [11], with\n\u03b1 = \u03b2 = \u03b3 = 1.\nThe combined Root Mean Squared Error [2]\nExy(i) =\n\u221a\u221a\u221a\u221a 1\nM\nM\u2211\nm=1\n(x(i)\u2212 x\u02c6m(i))2 + (y(i)\u2212 y\u02c6m(i))2\n(8)\nhas been used to evaluate the performance of the devel-\noped technique. The pixel coordinates (x(i), y(i)) indi-\ncate the true position of the object and (x\u02c6m(i), y\u02c6m(i))\nstand for estimated position in current frame i in\nm = 1, 2, . . . ,M independent Monte Carlo realisations\n(M = 50 in our experiments). The manually created\nground truth (the tracking box surrounding the object)\nhas been used as the true coordinates.\ncolour cue\ncolour & edges cue\nframe 39 frame 66\nSSIM cue\nframe 76\nFigure 5: Frames with the tracker output superim-\nposed, sequence cross\nThe error estimates are shown in Figure 2\u20134. Al-\nthough all four described methods (based on colour,\nedges, colour-and-edges, and similarity measure) have\nbeen used, only the performance of the best three\nmethods is shown in the plots for clarity. It can clearly\nbe seen that the proposed method based on structural\nsimilarity, while never loosing the object, outperforms\nthe other methods at nearly all instances.\nA closer look at the selected output frames will illus-\ntrate the performance of different methods. Figures 5\u2013\n7 show the object tracking boxes constructed from the\nmean locations and scales estimated during the tests.\nedges cue\ncolour & edges cue\nframe 324 frame 674\nSSIM cue\nframe 866\nFigure 6: Frames with the tracker output superim-\nposed, sequence man\nedges cue\ncolour & edges cue\nframe 86 frame 159\nSSIM cue\nframe 236\nFigure 7: Frames with the tracker output superim-\nposed, sequence doorway ir\nIn the sequence cross, Figure 5, the first passer-by\ncauses the colour and colour-and-edges cue tracker to\nloose the object (frames 66\u201376). Both trackers also\nseem to be attracted by the road sign (frame 39). The\nSSIM cue tracker is not distracted even by the tempo-\nrary occlusion (frame 76).\nThe shaking camera in the sequence man (Figure 6,\nframe 324), introduces a small bias in the SSIM posi-\ntion estimate (while retaining correct scale), and the\nremaining trackers choose the wrong scale (whilst re-\ntaining the correct position). The two compared meth-\nods do not perform well in case of similar objects ap-\npearing close-by (shadow, tyre, frame 674) and rapid\nzoom of the camera (frame 866). Our method, how-\never, seems to cope with both situations.\nAlthough all the methods tested were able to track\nthe person in the sequence doorway ir, Figure 7, the\nproposed method is the most precise with respect both\nto position and correct scaling of the tracking box, for\nmost frames in the video.\n6 Conclusions\nThe new tracking scheme was tested with real-world\nvideo sequences and has been shown to perform reli-\nably under different conditions. Colour cue itself can-\nnot provide stable tracking under changing illumina-\ntion and when there are regions with similar colour,\nsuch as those of the object. The fused colour-and-edge\ncue cannot provide a reliable tracking performance un-\nder ambiguous situations neither, especially with mov-\ning camera (with changes in the pan, tilt and zoom).\nThe proposed particle filter based on the structural\nsimilarity measure shows the most stable and reliable\nperformance. This is due to the fact that this mea-\nsure captures the spatial similarity between the re-\ngions of interest, independently of the colour. It mea-\nsures only relative changes in contrast and luminance\nwhich makes it more robust to the changes in the en-\nvironment. The implemented tracking algorithm uses\na changeable size of the tracking window, which makes\nit suitable for many real-world applications (where the\ncamera\u2013object distance varies significantly).\nThis paper presents early results obtained with this\nnew method. Future work will be focussed on the ex-\ntension of the presented method to achieve a degree\nof rotation invariance, and on theoretical justification\nof the results. The good performance of our methods\nwhen applied to both infrared and colour footage in-\ndicates that the structural similarity could be used in\nmultimodal and fused video tracking. These predic-\ntions will also be verified by future investigation.\nAcknowledgements\nThe authors are grateful to the financial support by\nthe UK MOD Data and Information Fusion Defence\nTechnology Centre, by projects 2.1 \u2018Image and video\nsensor fusion\u2019 and 2.2 \u2018Communication optimisation for\ndistributed sensor systems\u2019.\nReferences\n[1] F. Aherne, N. Thacker, and P. Rockett. The\nBhattacharyya metric as an absolute similarity\nmeasure for frequentcy coded data. Kybernetica,\n32(4):1\u20137, 1997.\n[2] Y. Bar-Shalom, X. R. Li, and T. Kirubarajan. Es-\ntimation with Applications to Tracking and Navi-\ngation. John Wiley and Sons, 2001.\n[3] P. Brasnett, L. Mihaylova, N. Canagarajah, and\nD. Bull. Particle filtering with multiple cues\nfor object tracking in video sequences. In Proc.\nof SPIE\u2019s 17th Annual Symposium on Electronic\nImaging, Science and Technology, V. 5685, pages\n430\u2013441, 2005.\n[4] D. Comaniciu, V. Ramesh, and P. Meer. Kernel-\nbased object tracking. IEEE Trans. on Pattern\nAnalysis and Machine Intelligence, 25(5):564\u2013577,\n2003.\n[5] J. Liu and R. Chen. Sequential Monte Carlo meth-\nods for dynamic systems. Journal of the American\nStatistical Association, 93(443):1032\u20131044, 1998.\n[6] A.  Loza, T. D. Dixon, E. F. Canga, S. G. Nikolov,\nD. R. Bull, C. N. Canagarajah, J. M. Noyes, and\nT. Troscianko. Methods of fused image analysis\nand assessment. In Proceedings of the Advanced\nStudy Institute Conference, Albena, Bulgaria, 16\u2013\n27 May (to appear), 2005.\n[7] PerceptiVU, Inc. Target Tracking Movie Demos.\nhttp:\/\/www.perceptivu.com\/MovieDemos.html.\n[8] P. Pe\u00b4rez, J. Vermaak, and A. Blake. Data fu-\nsion for tracking with particles. Proceedings of\nthe IEEE, 92(3):495\u2013513, March 2004.\n[9] G. Piella and H. Heijmans. A new quality metric\nfor image fusion. In Proceedings of the Intl. Conf.\non Image Processing, Barcelona, Spain, 2003.\n[10] E. Wan and R. van der Merwe. The Unscented\nKalman filter. In S. Haykin, editor, Kalman Fil-\ntering and Neural Networks, chapter 7, pages 221\u2013\n280. Wiley Publishing, Sep. 2001.\n[11] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli.\nImage quality assessment: from error visibility to\nstructural similarity. IEEE Transactions on Im-\nage Processing, 13(4):600\u2013612, Apr. 2004.\n[12] A. Webb. Statistical Pattern Recognition. John\nWiley & Sons, 2003.\n"}