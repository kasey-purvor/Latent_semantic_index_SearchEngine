{"doi":"10.1016\/j.spl.2010.12.011","coreId":"196837","oai":"oai:lra.le.ac.uk:2381\/9041","identifiers":["oai:lra.le.ac.uk:2381\/9041","10.1016\/j.spl.2010.12.011"],"title":"On the Local Asymptotic Behavior of the Likelihood Function for Meixner L\u00e9vy Processes under High-Frequency Sampling","authors":["Kawai, Reiichiro","Masuda, Hiroki"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-12-24","abstract":"We discuss the local asymptotic behavior of the likelihood function associated with all the four characterizing parameters (\u03b1,\u03b2,\u03b4,\u03bc) of the Meixner L\u00e9vy process under high-frequency sampling scheme. We derive the optimal rate of convergence for each parameter and the Fisher information matrix in a closed form. The skewness parameter \u03b2 exhibits a slower rate alone, relative to the other three parameters free of sampling rate. An unusual aspect is that the Fisher information matrix is constantly singular for full joint estimation of the four parameters. This is a particular phenomenon in the regular high-frequency sampling setting and is of essentially different nature from low-frequency sampling. As soon as either \u03b1 or \u03b4 is fixed, the Fisher information matrix becomes diagonal, implying that the corresponding maximum likelihood estimators are asymptotically orthogonal","downloadUrl":"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S0167715210003512,","fullTextIdentifier":"https:\/\/lra.le.ac.uk\/bitstream\/2381\/9041\/1\/mxlan.pdf","pdfHashValue":"f360bd6a204b5305d4301797a109581715ec9c3f","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:lra.le.ac.uk:2381\/9041<\/identifier><datestamp>\n                2015-12-01T09:36:23Z<\/datestamp><setSpec>\n                com_2381_445<\/setSpec><setSpec>\n                com_2381_9549<\/setSpec><setSpec>\n                col_2381_3823<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nOn the Local Asymptotic Behavior of the Likelihood Function for Meixner L\u00e9vy Processes under High-Frequency Sampling<\/dc:title><dc:creator>\nKawai, Reiichiro<\/dc:creator><dc:creator>\nMasuda, Hiroki<\/dc:creator><dc:subject>\nFisher information matrix<\/dc:subject><dc:subject>\nHigh-frequency sampling<\/dc:subject><dc:subject>\nL\u00e9vy process<\/dc:subject><dc:subject>\nLocal asymptotic normality<\/dc:subject><dc:subject>\nMeixner process<\/dc:subject><dc:description>\nWe discuss the local asymptotic behavior of the likelihood function associated with all the four characterizing parameters (\u03b1,\u03b2,\u03b4,\u03bc) of the Meixner L\u00e9vy process under high-frequency sampling scheme. We derive the optimal rate of convergence for each parameter and the Fisher information matrix in a closed form. The skewness parameter \u03b2 exhibits a slower rate alone, relative to the other three parameters free of sampling rate. An unusual aspect is that the Fisher information matrix is constantly singular for full joint estimation of the four parameters. This is a particular phenomenon in the regular high-frequency sampling setting and is of essentially different nature from low-frequency sampling. As soon as either \u03b1 or \u03b4 is fixed, the Fisher information matrix becomes diagonal, implying that the corresponding maximum likelihood estimators are asymptotically orthogonal.<\/dc:description><dc:date>\n2011-02-07T15:15:18Z<\/dc:date><dc:date>\n2011-02-07T15:15:18Z<\/dc:date><dc:date>\n2010-12-24<\/dc:date><dc:type>\nArticle<\/dc:type><dc:identifier>\nStatistics and Probability Letters, 2011, 81 (4), pp. 460-469<\/dc:identifier><dc:identifier>\n0167-7152<\/dc:identifier><dc:identifier>\nhttp:\/\/www.sciencedirect.com\/science\/article\/pii\/S0167715210003512<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/2381\/9041<\/dc:identifier><dc:identifier>\n10.1016\/j.spl.2010.12.011<\/dc:identifier><dc:language>\nen<\/dc:language><dc:rights>\nThis is the author\u2019s final draft of the paper published as Statistics and Probability Letters, 2011, 81 (4), pp. 460-469.  The final published version is available at http:\/\/www.sciencedirect.com\/science\/article\/pii\/S0167715210003512, Doi: 10.1016\/j.spl.2010.12.011.<\/dc:rights><dc:publisher>\nElsevier<\/dc:publisher>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":[{"title":null,"identifiers":["issn:0167-7152","0167-7152"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":["Fisher information matrix","High-frequency sampling","L\u00e9vy process","Local asymptotic normality","Meixner process"],"subject":["Article"],"fullText":"On the Local Asymptotic Behavior of the Likelihood Function for\nMeixner Le\u00b4vy Processes under High-Frequency Sampling\nREIICHIRO KAWAI\u0003AND HIROKI MASUDA\u2020\nAbstract\nWe discuss the local asymptotic behavior of the likelihood function associated with all the four characterizing param-\neters (a;b ;d ;m) of the Meixner Le\u00b4vy process under high-frequency sampling scheme. We derive the optimal rate\nof convergence for each parameter and the Fisher information matrix in a closed form. The skewness parameter b\nexhibits a slower rate alone, relative to the other three parameters free of sampling rate. An unusual aspect is that the\nFisher information matrix is constantly singular for full joint estimation of the four parameters. This is a particular\nphenomenon in the regular high-frequency sampling setting and is of essentially different nature from low-frequency\nsampling. As soon as either a or d is fixed, the Fisher information matrix becomes diagonal, implying that the\ncorresponding maximum likelihood estimators are asymptotically orthogonal.\nKeywords: Fisher information matrix, high-frequency sampling, Le\u00b4vy process, local asymptotic normality, Meixner\nprocess.\n2010 Mathematics Subject Classification: 60G51, 62E20.\n1 Introduction and Preliminaries\nThe local asymptotic normality (LAN, for short) property is a vital concept in asymptotically optimal statistical anal-\nyses. In short, the LAN property for a differentiable statistical model is defined through the following locally asymp-\ntotically quadratic structure of a log-likelihood ratio\nLn (q +Rn(q)u)\u0000Ln(q) = hRn(q)u;\u00d1Ln(q)i\u0000 12 hu;I (q)ui+oPq (1) (1.1)\nfor each u, where Pq is a probability measure associated with the parameter q , where Rn(q) are nonrandom positive\ndefinite matrices such that Rn(q)! 0 in norm, where Rn(q)>\u00d1Ln(q) converges in law toN (0;I (q)) under Pq , and\nwhere I (q) is a nonnegative definite deterministic matrix, called the Fisher information matrix. Once the identity\n(1.1) is confirmed with nonsingularI (q), one can formulate asymptotic optimality of estimation and testing hypoth-\nesis in terms of the sequence Rn(q)>\u00d1Ln(q). (See Le Cam [7], Le Cam and Yang [8], and van der Vaart [14] for a\nsystematic account of the LAN theory.)\nIn this article, we discuss the local asymptotic behavior of the likelihood function associated with the four-\nparameter Meixner Le\u00b4vy process observed under high-frequency sampling scheme (in the sense of (2.1) below). The\nMeixner process has been recognized as a successful class of Le\u00b4vy processes for the purpose of practical modeling,\nsuch as mathematical finance and possibly turbulence, as well as of sufficient theoretical interest. Previously, Kawai\nand Masuda [5] studied the full-parameter (4-parameter) LAN for the normal inverse Gaussian Le\u00b4vy process discretely\nobserved at high frequency. The normal inverse Gaussian and Meixner Le\u00b4vy processes share the feature of explicit\nlikelihood function for arbitrary sampling frequency. As opposed to the case of the normal inverse Gaussian Le\u00b4vy\nprocess, the Fisher information matrix in the Meixner case is inevitably singular.\nWe begin with some fundamental facts of the Meixner process with the most popular parametrization. (We refer\nthe reader to Grigelionis [2] and Schoutens and Teugels [13] for general details, and also Grigoletto and Provasi [3] and\nPublished in Statistics and Probability Letters (2011) 81(4) 460-469.\n\u0003Email Address: reiichiro.kawai@gmail.com. Postal Address: Department of Mathematics, University of Leicester, Leicester LE1 7RH, UK.\n\u2020Email Address: hiroki@math.kyushu-u.ac.jp. Postal Address: Graduate School of Mathematics, Kyushu University, Motooka 744, Nishi-ku,\nFukuoka 819-0395, Japan.\n1\nKawai [6] for numerical aspects of the Meixner process.) The Meixner distribution, denoted by Meixner(a ;b ;d ;m),\nis infinitely divisible and selfdecomposable, and admits a probability density\nx 7! (2cos(b=2))\n2d\n2paG(2d )\nexp\n\u0014\nb\na\n(x\u0000m)\n\u0015\f\f\f\fG\u0012d + i x\u0000ma\n\u0013\f\f\f\f2 ; x 2 R; (1.2)\nwhere a > 0, jb j < p , d > 0, m 2 R. Loosely speaking, a stands for the tail heaviness, b the skewness, d the scale,\nand m the location. When b = 0, the distribution is symmetric around m . We write\nq := (a; b ; d ; m) 2Q;\nthe parameter space Q being a bounded convex domain satisfying\nQ\u0000 \u001a \b(a; b ; d ; m) 2 R4\f\fa > 0; jb j< p; d > 0; m 2 R\t :\nThe Le\u00b4vy measure of Meixner(a;b ;d ;m) admits the Lebesgue density\ng(z;q) := d\nexp(b z=a)\nzsinh(pz=a)\n; z 2 R0 := Rnf0g:\nLet fXt : t \u0015 0g be a Le\u00b4vy process satisfying\nL (X1) =Meixner(a;b ;d ;m);\nwhich we call aMeixner (Le\u00b4vy) process, which is of infinite variation. We denote by Pq the distribution of X associated\nwith the parameter q 2 Q and by Eq the expectation taken under the probability measure Pq . In what follows, every\nstochastic asymptotics is taken under Pq . The characteristic function ofL (X1) is given in closed form by\nEq\n\u0002\neiyX1\n\u0003\n= eiym\n\u0012\ncos(b=2)\ncosh((ay\u0000 ib )=2)\n\u00132d\n; y 2 R;\nwhich implies that for each c> 0 and t > 0,\nL (c(Xt \u0000 tm)) =Meixner(ca; b ; td ; 0): (1.3)\nOne of the remarkable properties of the Meixner process is its asymptotic behavior with respect to observation time,\njust like normal inverse Gaussian processes, tempered stable processes of Rosin\u00b4ski [11] and layered stable processes\nof Houdre\u00b4 and Kawai [4]. On the one hand, over short time intervals, it approximates a stable process; as h # 0, a\nscaled Meixner process \u001a\n1\nhad\n(Xht \u0000htm) : t \u0015 0\n\u001b\ntends to a standard Cauchy (Le\u00b4vy) process, where the convergence is in the weak sense of random processes in the\nspace of ca`dla`g functions from [0;+\u00a5) into R equipped with the Skorohod topology. (See also Lemma 3.1 below.) In\na long time frame, on the other hand, it is close to a Brownian motion; as h \"+\u00a5, another scaled Meixner process(\ncos(b=2)\na\nr\n2\nhd\n\u0012\nXht \u0000ht\n\u0012\nm+ad tan\nb\n2\n\u0013\u0013\n: t \u0015 0\n)\napproaches to the standard Brownian motion. (These can be proved in a similar manner to Houdre\u00b4 and Kawai [4] and\nRosin\u00b4ski [11].) The stable-type and Gaussian-type behaviors above have long been considered to be very appealing\nin various applications, for example, to model turbulence in statistical physics, asset price dynamics in mathematical\nfinance, network dynamics in transportation management and computer science, and animal movement patterns in\nmathematical biology.\n2\n2 Local Asymptotic Behavior of Likelihood Function\nConsider the sample (Xtn;1 ;Xtn;2 ; : : : ;Xtn;n) observed at equidistant observation points in time,\ntn;k := kDn; k = 1; : : : ;n;\nwith a sequence fDngn2N of positive stepsizes satisfying\nDn # 0 and nDn \" \u00a5; as n \"+\u00a5: (2.1)\nLet us state the main claim of this article. To maintain the flow, we defer the proof to Section 3.1.\nTheorem 2.1. The LAN property (1.1) holds for each q 2Q, with\nRn(q) = Rn := diag\n\u0012\n1p\nn\n;\n1p\nnDn\n;\n1p\nn\n;\n1p\nn\n\u0013\n;\nI (q) :=\n26664\n1\n2a2 0\n1\n2ad 0\n0 d2cos2(b=2) 0 0\n1\n2ad 0\n1\n2d 2 0\n0 0 0 12a2d 2\n37775 : (2.2)\nThe corresponding specific form of \u00d1Ln(q) in (1.1) is given through (3.2) and (3.5) below.\nObserve that the Fisher information matrix (2.2) is singular, that is, the determinant jI (q)j \u0011 0 for each q 2 Q.\nDue to this fact, on the one hand, the conventional asymptotic optimality theory, in particular, the usual asymptotic\nefficiency of the maximum likelihood estimator is no longer assured for the full joint estimation of the four parameters\nin high-frequency data framework. Also, this fact acts as a practical warning in the maximum likelihood estimation\nfor the Meixner L\u2019evy process with a very small d under low-frequency sampling scheme, as the parameter d and\nthe time t play the same role. On the other hand, it is clear that the singularity is caused solely by the off-diagonal\nelements between a and d . Nevertheless, as soon as a or d is fixed, the Fisher information matrix then reduces to\nR3\u00023 and purely diagonal. This ensures that the maximum likelihood estimators are asymptotically independent.\nLet us discuss the singularity issue for Meixner Le\u00b4vy processes in terms of sampling scheme. First, it will shortly\nturn out to be obvious from Lemma 3.3 that, under low-frequency sampling with Dn \u0011 D> 0, the Fisher information\nmatrix remains involved with infinite sums (or, the psi function, as in Grigoletto and Provasi [3, Appendix A]), while\nI (q) of (2.2) is pretty simple. Therefore, it seems difficult to analyze the determinant of the corresponding Fisher\ninformation matrix for the low-frequency case. Next, it is worthwhile to compare with the continuous sampling setting,\nbased upon the following result. We postpone its proof to Section 3\nProposition 2.2. Let T > 0 and let qk := (ak;bk;dk;mk) 2 Q, k = 1;2. The probability measures Pq1 jFT and Pq2 jFT\nare equivalent iff a1d1 = a2d2 and m1 = m2.\nThis proposition implies that singularity in studying likelihood becomes more noteworthy in the continuously observed\ncase than in the high-frequency sampling case: in the latter case, the likelihood itself does exist for every admissible\nparameter values, while the Fisher information may be singular; in the former case, the likelihood itself may not exist.\nEspecially what is interesting is that the location parameter m is required to be fixed.\nLet us next discuss the singularity issue for different classes of Le\u00b4vy processes under high-frequency sampling.\nIt is well known that a similar phenomenon is observed in the case of non-Gaussian stable Le\u00b4vy process. Precisely,\nthe joint maximum-likelihood estimation of the stability index and the scale parameter leads to a constantly singular\nFisher information matrix. (See A\u00a8\u00a8t-Sahalia and Jacod [1] and Masuda [10], for example.) Inferring from this, we\nsuspect that the singularity arises from every Le\u00b4vy process whose short-range behavior can be approximated in law by\na stable process with unknown stability index and scale parameter. Typical examples are tempered stable processes\nRosin\u00b4ski [11] and layered stable processes Houdre\u00b4 and Kawai [4]. In this direction, the present setting of Meixner\nprocesses is not directly relevant since its short-time stability index is necessarily 1 (see Lemma 3.1 later). As observed\nin the Fisher information matrix (2.2), the singularity issue in our framework comes instead from (a ;d ).\nIn principle, unlike the low-frequency sampling case, the high-frequency sampling scheme yields different op-\ntimal rates of convergence for different characterizing parameters. There exist several case studies in the literature\n3\nthat address the joint LAN property for univariate Le\u00b4vy processes. The most well known case is the scaled Wiener\nprocess with drift, Xt = tm+sWt , where the LAN property holds true for q := (m;s) at rate R\u00001n = diag(\np\nnDn;\np\nn)\nwith a diagonal Fisher information matrix. According to the fact that f(Xtn;k \u0000Xtn;k\u00001 \u0000 mDn)=(s\np\nDn)gk\u0014n forms a\nsequence of iid-N (0;1) random variables together with the fully explicit and simple structure of the Gaussian den-\nsity, we can directly see that the stochastic expansion of the form (1.1) is valid with the Fisher information matrix\ndiag(1=s2;2=s2). (See also A\u00a8\u00a8t-Sahalia and Jacod [1, Section 4.1] for a closely related result.) Also, in the case of\ninverse Gaussian subordinators or gamma processes, both of which are characterized by the two parameters (d ;g), the\nLAN property holds true at rate (\np\nn;\np\nnDn) with a diagonal Fisher information matrix (See Masuda [9] for details).\nMore recently, the authors derive in [5] the LAN property for the normal inverse Gaussian (NIG) process, which is\ncharacterized by the four parameters (a ;b ;d ;m). Again, the LAN property holds true at rate (\np\nnDn;\np\nnDn;\np\nn;\np\nn)\nwith a block-diagonal Fisher information matrix. Interestingly, the NIG process suffers no singularity issue, while,\nwhen suitably normalized, sharing the same short-range behavior of Cauchy type with the Meixner process.\nFinally, let us note that the LAN property may be investigated, either singular or non-singular, only when the\nlikelihood function is available in a sufficiently tractable form, such as (3.1) and (3.2). But, this is very rare. For\nexample, the likelihood function for tempered stable processes is unknown in a closed form. Note also that the\navailability of an explicit likelihood function may not be enough. For example, without the reproducing property such\nas (1.3), even the explicit likelihood function is intractable in the high-frequency sampling framework. (A typical\nexample is the generalized hyperbolic Le\u00b4vy process.) This also ensures the importance of case studies.\n3 Proofs\nThroughout the proofs, we denote by G(z) the Gamma function G(z) :=\nR +\u00a5\n0 x\nz\u00001e\u0000xdx for z 2C satisfying Re(z)> 0,\ndenote by z the Riemann zeta function, that is, z (s) := \u00e5+\u00a5k=1 k\n\u0000s for s 2 C satisfying Re(s) > 1, and write g :=\nlimn\"+\u00a5(\u00e5nk=1 k\u00001\u0000 lnn)\u0019 0:5772 for the Euler-Mascheroni constant.\n3.1 Proof of Theorem 2.1\nIt holds by (1.3) that for each n 2 N,\nxn;k := Xtn;k \u0000Xtn;k\u00001 \u0018Meixner(a ;b ;Dnd ;Dnm); k = 1; : : : ;n:\nIn view of this fact and the probability density function (1.2), define for each n 2 N and k = 1; : : : ;n,\n`n;k(q) := 2Dnd ln\n\u0012\n2cos\nb\n2\n\u0013\n\u0000 ln(2pa)\u0000 lnG(2Dnd )+ ba\n\u0000\nxn;k\u0000Dnm\n\u0001\n+ ln\n\f\f\f\fG\u0012Dnd + i xn;k\u0000Dnma\n\u0013\f\f\f\f2 : (3.1)\nThanks to the stationarity and independence of increments of Le\u00b4vy processes, the log-likelihood function to be maxi-\nmized with discrete observations fXtn;kgk=1;:::;n is as simple as\nLn(q) :=\nn\n\u00e5\nk=1\n`n;k(q): (3.2)\nWe begin with the local Cauchy approximation. Let us define a sequence fen;kgk=1;:::;n of iid random variables by\nen;k := en;k(a;d ;m;Dn) :=\nxn;k\u0000Dnm\nDnad\n\u0018Meixner\n\u0012\n1\nDnd\n;b ;Dnd ;0\n\u0013\n: (3.3)\nIn the pathwise sense, the random variables fen;kgk=1;:::;n depend on (a ;d ;m;Dn) and are independent of b . In contrast,\nthe lawL (en;1) depends on (b ;d ;Dn) and is independent of (a ;m). We can show that the lawL (en;1) has the mean,\nthe variance, the skewness and the kurtosis, respectively,\ntan\nb\n2\n;\n1\n2Dnd cos2(b=2)\n; sin\nb\n2\nr\n2\nDnd\n; 3+\n2\u0000 cos(b )\nDnd\n:\n(See, for example, Grigelionis [2].) It then follows that as n \"+\u00a5, the first four moments are of order O(1), O(D\u00001n ),\nO(D\u00003=2n ) and O(D\u00002n ) if b 6= 0, while 0, O(D\u00001n ), 0 and O(D\u00002n ) if b = 0.\n4\nWe denote by the standard Cauchy distribution the infinitely divisible distribution with characteristic function\ny 7! e\u0000jyj and the probability density function\nf(x) :=\n1\np(1+ x2)\n; x 2 R: (3.4)\nThe following lemma indicates that the random variables en;k act as suitably normalized increments.\nLemma 3.1. The lawL (en;1) converges to the standard Cauchy distribution, as n \"+\u00a5.\nProof. The claim can be deduced readily by observing that, for each y 2 R,\nEq\n\u0002\neiyen;1\n\u0003\n=\n\u0012\ncos(b=2)\ncosh((y=(Dnd )\u0000 ib )=2)\n\u00132Dnd\n=\n\u0012\n2\ney=(2Dnd )(1\u0000 i tan(b=2))+ e\u0000y=(2Dnd )(1+ i tan(b=2))\n\u00132Dnd\n\u0018\n8><>:\n\u0010\n2\n1\u0000i tan(b=2)\n\u00112Dnd\ne\u0000y; if y> 0;\u0010\n2\n1+i tan(b=2)\n\u00112Dnd\ney; if y< 0;\n! e\u0000jyj;\nas n \"+\u00a5, with the help of the Le\u00b4vy continuity theorem.\nLet us rewrite (3.1) as\n`n;k(q) = 2Dnd ln [2cos(b=2)]\u0000 ln(2pa)\u0000 lnG(2Dnd )+Dnbden;k+ ln\n\f\fG\u0000Dnd \u00001+ ien;k\u0001\u0001\f\f2 :\nNote that the above likelihood function is smooth in q . In our proof of Theorem 2.1, we will need to specify the partial\nderivatives of `n;k(q), with respect to q , up to the second order. To this end, we define an array fgn;k(q)gn2N;k=1;:::;n\nof random vectors in R4 by\ngn;k(q) := \u00d1`n;k(q) =\n2666664\n\u0000 1a \u0000 Dnbda en;k+ 2Dnda en;kIm\n\u0010\nG0(Dnd (1+ien;k))\nG(Dnd (1+ien;k))\n\u0011\nDnd\n\u0000\nen;k\u0000 tan(b=2)\n\u0001\n2Dn ln [2cos(b=2)]\u00002Dn G\n0(2Dnd )\nG(2Dnd ) +2DnRe\n\u0010\nG0(Dnd (1+ien;k))\nG(Dnd (1+ien;k))\n\u0011\n\u0000Dn ba +Dn 2a Im\n\u0010\nG0(Dnd (1+ien;k))\nG(Dnd (1+ien;k))\n\u0011\n3777775=:\n266664\ng(1)n;k(q)\ng(2)n;k(q)\ng(3)n;k(q)\ng(4)n;k(q)\n377775 : (3.5)\n(See Grigoletto and Provasi [3, Appendix A] for derivation of the gradient.) In order to complete the proof of Theorem\n2.1, it suffices to prove the following lemma. See Section 4.1 in Kawai and Masuda [5] and the references therein for\ndetails of the sufficiency of the items (i) to (iii) for proving the LAN.\nLemma 3.2. (i) It holds that\nlim\nn\"+\u00a5\nn\n\u00e5\nk=1\nRnEq\nh\ngn;k(q)gn;k(q)>\ni\nRn =I (q); (3.6)\nwhere I (q) is defined by (2.2).\n(ii) It holds that\nlim\nn\"+\u00a5\nRn\n \nn\n\u00e5\nk=1\nEq\n\u0002\ngn;k(q)\n\u0003\nEq\n\u0002\ngn;k(q)\n\u0003>!Rn = 0;\nwhere the right hand side indicates the zero matrix in R4\u00024.\n(iii) It holds that\nlim\nn\"+\u00a5\nsup\nq2Q\nn\n\u00e5\nk=1\n\u0010\nEq\nh\f\fRngn;k(q)\f\f4i+Eq h\f\fRnHessq (ln;k(q))Rn\f\f2i\u0011= 0:\n5\nNote that the claim (iii) verifies the Lindeberg condition: for every a> 0,\nlim\nn\"+\u00a5\nn\n\u00e5\nk=1\nEq\nh\f\fRngn;k(q)\f\f2 1\u0000\f\fRngn;k(q)\f\f\u0015 a\u0001i= 0;\nwhere 1(A) denotes the indicator function of a set A. To prove Lemma 3.2, let us prepare some useful asymptotics.\nLemma 3.3. It holds almost surely that\n\u00b6\n\u00b6a\nIm\n\u0012\nG0(Dnd (1+ ien;1))\nG(Dnd (1+ ien;1))\n\u0013\n=\u0000Dnden;1\na\n+\u00a5\n\u00e5\nk=0\n(k+Dnd )2\u0000 (Dnden;1)2\n((k+Dnd )2+(Dnden;1)2)2\n\u0018\u0000Dnden;1\na\n \n1\n(Dnd )2\n1\u0000 e2n;1\n(1+ e2n;1)2\n+z (2)\n!\n;\n\u00b6\n\u00b6d\nIm\n\u0012\nG0(Dnd (1+ ien;1))\nG(Dnd (1+ ien;1))\n\u0013\n=\u00002D2nden;1\n+\u00a5\n\u00e5\nk=0\nk+Dnd\n((k+Dnd )2+(Dnden;1)2)2\n\u0018\u00002D2nden;1\n \n1\n(Dnd )3(1+ e2n;1)2\n+z (3)\n!\n;\n\u00b6\n\u00b6m\nIm\n\u0012\nG0(Dnd (1+ ien;1))\nG(Dnd (1+ ien;1))\n\u0013\n=\u0000Dn\na\n+\u00a5\n\u00e5\nk=0\n(k+Dnd )2\u0000 (Dnden;1)2\n((k+Dnd )2+(Dnden;1)2)2\n\u0018\u0000Dn\na\n \n1\n(Dnd )2\n1\u0000 e2n;1\n(1+ e2n;1)2\n+z (2)\n!\n;\n\u00b6\n\u00b6a\nRe\n\u0012\nG0(Dnd (1+ ien;1))\nG(Dnd (1+ ien;1))\n\u0013\n=\u00002(Dnden;1)\n2\na\n+\u00a5\n\u00e5\nk=0\nk+Dnd\n((k+Dnd )2+(Dnden;1)2)2\n\u0018\u00002(Dnden;1)\n2\na\n \n1\n(Dnd )3\n1\n(1+ e2n;1)2\n+z (3)\n!\n;\n\u00b6\n\u00b6d\nRe\n\u0012\nG0(Dnd (1+ ien;1))\nG(Dnd (1+ ien;1))\n\u0013\n= Dn\n+\u00a5\n\u00e5\nk=0\n(k+Dnd )2\u0000 (Dnden;1)2\n((k+Dnd )2+(Dnden;1)2)2\n\u0018 Dn\n \n1\n(Dnd )2\n1\u0000 e2n;1\n(1+ e2n;1)2\n+z (2)\n!\n;\n\u00b6\n\u00b6m\nRe\n\u0012\nG0(Dnd (1+ ien;1))\nG(Dnd (1+ ien;1))\n\u0013\n=\u00002Dn\na\n+\u00a5\n\u00e5\nk=0\nk+Dnd\n((k+Dnd )2+(Dnden;1)2)2\n\u0018\u00002Dn\na\n \n1\n(Dnd )3\n1\n(1+ e2n;1)2\n+z (3)\n!\n;\nwhere all the asymptotics hold when n \"+\u00a5.\nProof of Lemma 3.3. Recall the definition of the digamma function\nG0(z)\nG(z)\n=\u00001\nz\n\u0000 g\u0000\n+\u00a5\n\u00e5\nl=1\n\u0012\n1\nl+ z\n\u0000 1\nl\n\u0013\n; z 2 C;\nand also that for each x> 0 and y 2 R,\nRe\n\u0012\nG0(x+ iy)\nG(x+ iy)\n\u0013\n=\u0000 x\nx2+ y2\n\u0000 g\u0000\n+\u00a5\n\u00e5\nl=1\n\u0012\nl+ x\n(l+ x)2+ y2\n\u0000 1\nl\n\u0013\n;\nIm\n\u0012\nG0(x+ iy)\nG(x+ iy)\n\u0013\n=\n+\u00a5\n\u00e5\nl=0\ny\n(l+ x)2+ y2\n;\nwhere the both infinite sums are well defined. Note that with respect to the variable y, the former is even, while the\nlatter is odd.\nIt remains to justify the interchange of the differentiation and the infinite sum as\n\u00b6\n\u00b6a\n+\u00a5\n\u00e5\nk=1\n1\n(k+Dnd )2+((xn;1\u0000Dnm)=a)2 =\n+\u00a5\n\u00e5\nk=1\n\u00b6\n\u00b6a\n1\n(k+Dnd )2+((xn;1\u0000Dnm)=a)2 ;\nfor the first claim, for instance. The proof entails rather lengthy algebra of somewhat routine nature. To avoid over-\nloading this proof, we only consider the first claim and omit all the rest. For convenience, we use the notation\nH(k;a) :=\n1\n(k+Dnd )2+((x\u0000Dnm)=a)2 ;\n6\nwith x 2 R, d , m , Dn fixed. It holds by the Taylor theorem that for l > 0 and for each k 2 N,\f\f\f\fH(k;a+l )\u0000H(k;a)l\n\f\f\f\f= \f\f\f\fZ 10 \u00b6\u00b6aH(k;a+l s)ds\n\f\f\f\f\n\u0014\nZ 1\n0\n2\na+l s\n\u0012\n(x\u0000Dnm)=(a+l s)\n(k+Dnd )2+((x\u0000Dnm)=(a+l s))2\n\u00132\nds\n\u0014 2\na3\n(x\u0000Dnm)2\n(k+Dnd )4\n:\nHence, we get\n+\u00a5\n\u00e5\nk=1\n\f\f\f\fH(k;a+l )\u0000H(k;a)l\n\f\f\f\f\u0014 2(x\u0000Dnm)2a3 +\u00a5\u00e5k=1 1(k+Dnd )4 \u0014 2(x\u0000Dnm)\n2\na3\nz (4);\nwhich justifies the interchange with the help of the dominated convergence theorem. The asymptotics are straight-\nforward by splitting the sum into two parts k = 0 and k \u0015 1 with the help of the definition of the Riemann zeta\nfunction.\nWe are now in a position to prove Lemma 3.2.\nProof of Lemma 3.2. We use the expressions (3.5). It is straightforward that as n \"+\u00a5,\n\u00002DnG\n0(2Dnd )\nG(2Dnd )\n! 1\nd\n;\nand that almost surely as n \"+\u00a5,\nRe\n\u0012\nG0(Dnd (1+ ien;1))\nG(Dnd (1+ ien;1))\n\u0013\n=\u0000 1\nDnd\n1\n1+ e2n;1\n\u0000 g\u0000\n+\u00a5\n\u00e5\nl=1\n\u0012\nl+Dnd\n(l+Dnd )2+(Dnden;1)2\n\u0000 1\nl\n\u0013\n\u0018\u0000 1\nDnd\n1\n1+ e2n;1\n;\nIm\n\u0012\nG0(Dnd (1+ ien;1))\nG(Dnd (1+ ien;1))\n\u0013\n=\n1\nDnd\nen;1\n1+ e2n;1\n+\n+\u00a5\n\u00e5\nl=1\nDnden;1\n(l+Dnd )2+(Dnden;1)2\n\u0018 1\nDnd\nen;1\n1+ e2n;1\n:\nBy using the above results, the first derivatives (3.5), and Lemma 3.1, we get\ng(1)n;1(q)\u0018\u0000\n1\na\n1\u0000 e2n;1\n1+ e2n;1\n\u0000 Dnbd\na\nen;1 \u0018\u0000 1a\n1\u0000 e2n;1\n1+ e2n;1\n;\ng(2)n;1(q) = Dnd\n\u0012\nen;1\u0000 tan b2\n\u0013\n;\ng(3)n;1(q)\u0018 2Dn ln(2cos(b=2))\u0000\n1\nd\n1\u0000 e2n;1\n1+ e2n;1\n\u0018\u0000 1\nd\n1\u0000 e2n;1\n1+ e2n;1\n;\ng(4)n;1(q)\u0018\u0000Dn\nb\na\n+\n2\nad\nen;1\n1+ e2n;1\n\u0018 2\nad\nen;1\n1+ e2n;1\n;\nas n \"+\u00a5. Let us denote by Il1;l2(q) the (l1; l2)-entry of I (q). In view of (3.3), (3.6), and Lemma 3.1 together with\n7\nthe bounded convergence theorem, we readily deduce that\nI1;1(q) =\n1\na2\nZ\nR\n\u0012\n1\u0000 x2\n1+ x2\n\u00132\nf(x)dx=\n1\n2a2\n;\nI2;2(q) =\nd\n2(cos(b=2))2\n;\nI3;3(q) =\n1\nd 2\nZ\nR\n\u0012\n1\u0000 x2\n1+ x2\n\u00132\nf(x)dx=\n1\n2d 2\n;\nI4;4(q) =\n4\na2d 2\nZ\nR\n\u0012\nx\n1+ x2\n\u00132\nf(x)dx=\n1\n2a2d 2\n;\nI1;2(q) =I2;1(q) = 0;\nI2;3(q) =I3;2(q) = 0;\nI3;4(q) =I4;3(q) =\u0000 2ad 2\nZ\nR\n1\u0000 x2\n1+ x2\nx\n1+ x2\nf(x)dx= 0;\nI1;3(q) =I3;1(q) =\n1\nad\nZ\nR\n1\u0000 x2\n1+ x2\n1\u0000 x2\n1+ x2\nf(x)dx=\n1\n2ad\n;\nI2;4(q) =I4;2(q) = 0;\nI1;4(q) =I4;1(q) =\u0000 2a2d\nZ\nR\n1\u0000 x2\n1+ x2\nx\n1+ x2\nf(x)dx= 0:\nThis completes the proof of the item (i).\n(ii) With the help of Lemma 3.3 and the asymptotic behaviors of gn;1(q) given in the proof of Lemma 3.2 together\nwith the bounded convergence theorem, it holds that as n \"+\u00a5,\np\nnRnEq [gn;1(q)]\u0018\n26664\n\u0000 1a\nR\nR\n1\u0000x2\n1+x2 f(x)dx\n0\n\u0000 1d\nR\nR\n1\u0000x2\n1+x2 f(x)dx\n2\nad\nR\nR\nx\n1+x2 f(x)dx\n37775= 0;\nwhich is enough to prove the claim.\n(iii) With the help of the asymptotic behaviors of gn;1(q) given in the proof of Lemma 3.2 and notation r\n(2)\nn := 1=\np\nnDn\nand r(k)n := 1=\np\nn for k = 1;3;4, one can check that\nn\n\u00e5\nk=1\nEq\n\u0014\f\f\fr(1)n g(1)n;k(q)\f\f\f4\u0015\u0018 1na4\nZ\nR\n\u0012\n1\u0000 x2\n1+ x2\n\u00134\nf(x)dx=\n3\n8na4\n;\nn\n\u00e5\nk=1\nEq\n\u0014\f\f\fr(2)n g(2)n;1(q)\f\f\f4\u0015= 1nDn 3dDn+2\u0000 cosb4d 3(cos(b=2))4 ;\nn\n\u00e5\nk=1\nEq\n\u0014\f\f\fr(3)n g(3)n;1(q)\f\f\f4\u0015\u0018 1n2d 4\nZ\nR\n\u0012\n1\u0000 x2\n1+ x2\n\u00134\nf(x)dx=\n3\n8nd 4\n;\nn\n\u00e5\nk=1\nEq\n\u0014\f\f\fr(4)n g(4)n;1(q)\f\f\f4\u0015\u0018 1n2\n\u0012\n2\nad\n\u00134 Z\nR\n\u0012\nx\n1+ x2\n\u00134\nf(x)dx=\n3\n8na4d 4\n;\neach of which tends to zero as n \"+\u00a5. Thanks to the compactness of the set Q, it follows that\nlim\nn\"+\u00a5\nsup\nq2Q\nn\n\u00e5\nk=1\nEq\nh\f\fRngn;k(q)\f\f4i= 0:\n8\nIn view of (3.5) and Lemma 3.3, we can derive each entry of the Hessian matrix Hessq (`n;k(q)) of the likelihood as\n\u00b6 2a`n;k(q) =\n1\na2\n+\n2bDnden;k\na2\n\u0000 2(Dnden;k)\n2\na2\n+\u00a5\n\u00e5\nl=0\n3(l+Dnd )2+(Dnden;k)2\n((l+Dnd )2+(Dnden;k)2)2\n\u0018 1\na2\n1\u00004e2n;k\u0000 e4n;k\n(1+ e2n;k)2\n;\n\u00b6 2b `n;k(q) =\u0000Dn\nd\n2(cos(b=2))2\n;\n\u00b6 2d `n;k(q) =\u00004D2n\n+\u00a5\n\u00e5\nl=0\n1\n(l+2Dnd )2\n+2D2n\n+\u00a5\n\u00e5\nl=0\n(l+Dnd )2\u0000 (Dnden;k)2\n((l+Dnd )2+(Dnden;k)2)2\n\u0000 1\nd 2\n1\u00004e2n;k\u0000 e4n;k\n(1+ e2n;k)2\n;\n\u00b6 2m`n;k(q) =\n2D2n\na2\n+\u00a5\n\u00e5\nl=0\n\u0000(l+Dnd )2+(Dnden;k)2\n((l+Dnd )2+(Dnden;k)2)2\n\u0018 2\na2d 2\n\u00001+ e2n;k\n(1+ e2n;k)2\n;\nand\n\u00b6a\u00b6b `n;k(q) =\u0000\nDnd\na\nen;k;\n\u00b6a\u00b6d `n;k(q) =\u0000\n4Dn\na\n+\u00a5\n\u00e5\nl=0\n(l+Dd )(Dnden;k)2\n((l+Dd )2+(Dden;k)2)2\n\u0018\u0000 4\nad\n \nen;k\n1+ e2n;k\n!2\n;\n\u00b6a\u00b6m`n;k(q) =\nbDn\na2\n\u0000 4Dn\na2\n+\u00a5\n\u00e5\nl=0\nDnden;k(l+Dnd )2\n((l+Dnd )2+(Dnden;k)2)2\n\u0018\u0000 4\na2d\nen;k\n(1+ e2n;k)2\n;\n\u00b6b\u00b6d `n;k(q) =\u0000Dn\ntan(b=2)\n2\n;\n\u00b6b\u00b6m`n;k(q) =\u0000Dn\n1\na\n;\n\u00b6d\u00b6m`n;k(q) =\u0000\n4D2n\na\n+\u00a5\n\u00e5\nl=0\n(l+Dnd )Dnden;k\n((l+Dnd )2+(Dnden;k)2)2\n\u0018\u0000 4\nad 2\nen;k\n(1+ e2n;k)2\n;\nwhere all the asymptotics hold almost surely as n \"+\u00a5. It is straightforward to deduce that as n \"+\u00a5,\nsup\nq2Q\nn\n\u00e5\nk=1\nEq\nh\f\fRnHessq (`n;k(q))Rn\f\f2i=\n2664\nO(1=n) O(Dn=n) O(1=n) O(1=n)\nO(Dn=n) O(1=n) O(Dn=n) O(Dn=n)\nO(1=n) O(Dn=n) O(1=n) O(1=n)\nO(1=n) O(Dn=n) O(1=n) O(1=n)\n3775 ;\nwhere the squared norm inside the expectation are understood to be componentwise. The proof of Lemma 3.2 is\ncomplete.\n3.2 Proof of Proposition 2.2\nThe mean of X1 is given by m0(q) := m+ad tan(b=2), so that we may write\nEq\n\u0002\neiyX1\n\u0003\n= exp\n\u0014\niym0(q)+\nZ\nR0\n\u0000\neiyz\u00001\u0000 iyz\u0001g(z;q)dz\u0015 ; y 2 R:\nAccording to Sato [12, Theorem 33.1], for each T > 0, the measures Pq1 jFT and Pq2 jFT are equivalent iff the following\nconditions are fulfilled:\n(a) g(z;q2) = g(z;q1;q2)g(z;q1) for some Borel function g(\u0001;q1;q2) : R! (0;\u00a5);\n(b) m0(q2) = m0(q1)+\nR\nR z(g(z;q1;q2)\u00001)g(z;q1)dz;\n(c)\nR\nR(1\u0000\np\ng(z;q1;q2))2g(z;q1)dz<+\u00a5.\nHence, it suffices to show that these three conditions hold true iff a1d1 = a2d2 and m1 = m2.\n9\nConcerning the behaviors of the Le\u00b4vy density g(z;q) near the origin and at infinity, in view of the series expansion\nz=sinh(z) = 1\u0000 z2=6+O(z4) as jzj ! 0, it is easy to see that the Le\u00b4vy density g(z;q) admits the following expansion\ng(z;q) =\nad\npz2\n\u0012\n1+\nb\na\nz+O(z2)\n\u0013\n; (3.7)\nas jzj ! 0. Note that sinh(x) behaves like ex=2 as x \"+\u00a5, while it behaves like \u0000e\u0000x=2 as x # \u0000\u00a5. We thus get\ng(z;q)\u0018\n\u001a\n2d z\u00001 expf\u0000(p\u0000b )z=ag; z \"+\u00a5;\n2d jzj\u00001 expf\u0000(p+b )jzj=ag; z # \u0000\u00a5: (3.8)\nIn particular, (3.7) as well as the fact that g(z;q)> 0 for every z 6= 0 ensures (a); more specifically, the singularities of\ng(z;q1) and g(z;q2) at the origin are canceled out since g(z;q2)=g(z;q1)\u0018 a2d2=(a2d2) as jzj ! 0.\nWe turn to (c) with g(z;q1;q2) = g(z;q2)=g(z;q1). Due to (3.7) and (3.8), it holds that\u0010\n1\u0000\np\ng(z;q1;q2)\n\u00112\ng(z;q1) =\n\u0010p\ng(z;q1)\u0000\np\ng(z;q2)\n\u00112\n\u0018\n8><>:\n1\npjzj2\n\u0010\u0000p\na1d1\u0000\np\na2d2\n\u00012\n+\n\u0000p\na1d1\u0000\np\na2d2\n\u0001\u0010\nb1\nq\nd1\na1\n\u0000b2\nq\nd2\na2\n\u0011\nz+O(z2)\n\u0011\n; jzj ! 0;\nC+z\u00001 exp(\u0000q+z); z \"+\u00a5;\nC\u0000jzj\u00001 exp(\u0000q\u0000jzj); z # \u0000\u00a5;\nfor some positive constants C\u0006 and q\u0006, depending on (q1;q2). Hence, (c) holds true iff a1d1 = a2d2, which we will\nimpose in the rest of this proof.\nThe remaining (b) is equivalent to\nm1+a1d1 tan\nb1\n2\n\u0000m2\u0000a2d2 tan b22 =\nZ\nR0\n\u0012\nd1\nexp(b1z=a1)\nsinh(pz=a1)\n\u0000d2 exp(b2z=a2)sinh(pz=a2)\n\u0013\ndz:\nIn the case a1d1 = a2d2 =:C > 0, the last display can be rewritten as\nm1\u0000m2+C\n\u0012\ntan\nb1\n2\n\u0000 tan b2\n2\n\u0000\nZ\nR0\n\u0012\nexp(b1z=a1)\na1 sinh(pz=a1)\n\u0000 exp(b2z=a2)\na2 sinh(pz=a2)\n\u0013\ndz\n\u0013\n= 0:\nWe now show that the function\nf (a1;b1;a1;b2) := tan\nb1\n2\n\u0000 tan b2\n2\n\u0000\nZ\nR0\n\u0012\nexp(b1z=a1)\na1 sinh(pz=a1)\n\u0000 exp(b2z=a2)\na2 sinh(pz=a2)\n\u0013\ndz\u0011 0;\nrendering that (b) holds true iff m1 = m2, which completes the proof of the proposition. First, we observe that\nf (a1;0;a2;0) =\nZ\nR0\n\u0012\n1\na2 sinh(pz=a2)\n\u0000 1\na1 sinh(pz=a1)\n\u0013\ndz\u0011 0;\nsince the integrand is odd, continuous in R, and exponentially decreasing as jzj \" +\u00a5. Next, using the fact that the\nvariance a2k dk=(2cos\n2(bk=2)) of Meixner(ak;bk;dk;mk) equals\nR\nR z\n2g(z;qk)dz, we derive\n1\na2k\nZ\nR0\nz\nexp(bkz=ak)\nsinh(pz=ak)\ndz=\n1\n2cos2(bk=2)\n:\nHence, we get\n\u00b6\n\u00b6b1\nf (a1;b1;a1;b2) =\n1\n2(cos(b1=2))2\n\u0000 1\na21\nZ\nR0\nz\nexp(b1z=a1)\nsinh(pz=a1)\ndz\u0011 0;\nand (\u00b6=\u00b6b2) f (a1;b1;a1;b2)\u0011 0 in a similar manner. These imply that f (a1;b1;a1;b2)\u0011 0. The proof is complete.\nAcknowledgements\nThe authors are grateful to the anonymous referee for a very careful reading and various helpful comments, which\nhave let to improvements in presentation. Research of HM is supported in part by Grant-in-Aid for Young Scientists\n(B) Japan.\n10\nReferences\n[1] A\u00a8\u00a8t-Sahalia, Y. and Jacod, J. (2008) Fisher\u2019s information for discretely sampled Le\u00b4vy processes, Econometrica, 76 727\u2013761.\n[2] Grigelionis, B. (1999) Processes of Meixner type, Lithuanian Mathematics Journal, 39(1) 33\u201341.\n[3] Grigoletto, M., Provasi, C. (2009) Simulation and estimation of the Meixner distribution, Communications in Statistics - Simulation and\nComputation, 38(1) 58\u201377.\n[4] Houdre\u00b4, C., Kawai, R. (2007) On layered stable processes, Bernoulli, 13(1) 252\u2013278.\n[5] Kawai, R., Masuda, H., Local asymptotic normality for normal inverse Gaussian Le\u00b4vy processes with high-frequency sampling, available at\nhttp:\/\/hdl.handle.net\/2324\/17018.\n[6] Kawai, R., Likelihood ratio gradient estimation for Meixner distribution and Levy processes. available at\nhttp:\/\/sites.google.com\/site\/reiichirokawai\/\n[7] Le Cam, L. (1960) Locally asymptotically normal families of distributions. Certain approximations to families of distributions and their use\nin the theory of estimation and testing hypotheses. Univ. California Publ. Statist. 3, 37\u201398.\n[8] Le Cam, L., Yang, G. L. (1990) Asymptotics in Statistics. Some Basic Concepts. Second Edition, Springer Series in Statistics. Springer-\nVerlag, New York.\n[9] Masuda, H. (2009) Notes on estimating inverse-Gaussian and gamma subordinators under high-frequency sampling, Annals of the Institute\nof Statistical Mathematics 61, 181\u2013195.\n[10] Masuda, H. (2009) Joint estimation of discretely observed stable Le\u00b4vy processes with symmetric Le\u00b4vy density, J. Japan Statist. Soc. 39,\n49\u201375.\n[11] Rosin\u00b4ski, J. (2007) Tempering stable processes, Stochastic Processes and their Applications, 117(6) 677\u2013707.\n[12] Sato, K. (1999) Le\u00b4vy Processes and Infinitely Divisible Distributions, Cambridge University Press, Cambridge.\n[13] Schoutens, W., Teugels, J. L. (1998) Le\u00b4vy processes, polynomials and martingales, Communications in Statistics: Stochastic Models, 14,\n335\u2013349.\n[14] van der Vaart, A. W. (1998) Asymptotic Statistics, Cambridge University Press, Cambridge.\n11\n"}