{"doi":"10.1016\/S0098-3004(00)00043-1","coreId":"63414","oai":"oai:nora.nerc.ac.uk:2403","identifiers":["oai:nora.nerc.ac.uk:2403","10.1016\/S0098-3004(00)00043-1"],"title":"Geoscience after IT: Part H. Familiarization with managing the information base","authors":["Loudon, T.V."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":["Loudon, T.V."],"datePublished":"2000-04","abstract":"The geoscience record stores information for later reuse. The management of bibliographic, cartographic and quantitative information have different backgrounds. All involve: deciding what to keep; structuring the record so that information can be found when needed; maintaining search tools, indexes and abstracts; defining the content by reference to metadata. The current approaches to managing the literature, spatial information and quantitative data may be subsumed in a more comprehensive object-oriented model of the information base","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/63414.pdf","fullTextIdentifier":"http:\/\/nora.nerc.ac.uk\/2403\/1\/Part_H.pdf","pdfHashValue":"1bd57ec764d0b050da138a20faa458f9a9189906","publisher":"Pergamon Science-Elsevier Ltd, Oxford","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:nora.nerc.ac.uk:2403<\/identifier><datestamp>\n      2012-11-22T11:48:20Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D5338<\/setSpec><setSpec>\n      7375626A656374733D5339<\/setSpec><setSpec>\n      7375626A656374733D533130<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/nora.nerc.ac.uk\/id\/eprint\/2403\/<\/dc:relation><dc:title>\n        Geoscience after IT: Part H. Familiarization with managing the information base<\/dc:title><dc:creator>\n        Loudon, T.V.<\/dc:creator><dc:subject>\n        Computer Science<\/dc:subject><dc:subject>\n        Data and Information<\/dc:subject><dc:subject>\n        Earth Sciences<\/dc:subject><dc:description>\n        The geoscience record stores information for later reuse. The management of bibliographic, cartographic and quantitative information have different backgrounds. All involve: deciding what to keep; structuring the record so that information can be found when needed; maintaining search tools, indexes and abstracts; defining the content by reference to metadata. The current approaches to managing the literature, spatial information and quantitative data may be subsumed in a more comprehensive object-oriented model of the information base.<\/dc:description><dc:publisher>\n        Pergamon Science-Elsevier Ltd, Oxford<\/dc:publisher><dc:contributor>\n        Loudon, T.V.<\/dc:contributor><dc:date>\n        2000-04<\/dc:date><dc:type>\n        Publication - Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/nora.nerc.ac.uk\/id\/eprint\/2403\/1\/Part_H.pdf<\/dc:identifier><dc:identifier>\n         \n\n  Loudon, T.V..  2000  Geoscience after IT: Part H. Familiarization with managing the information base.   Computers & Geosciences, 26 (3, Sup). A63-A73.  https:\/\/doi.org\/10.1016\/S0098-3004(00)00043-1 <https:\/\/doi.org\/10.1016\/S0098-3004(00)00043-1>     \n <\/dc:identifier><dc:relation>\n        http:\/\/www.elsevier.com\/wps\/find\/journaldescription.cws_home\/398\/description#description<\/dc:relation><dc:relation>\n        doi:10.1016\/S0098-3004(00)00043-1<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/nora.nerc.ac.uk\/id\/eprint\/2403\/","http:\/\/www.elsevier.com\/wps\/find\/journaldescription.cws_home\/398\/description#description","doi:10.1016\/S0098-3004(00)00043-1"],"year":2000,"topics":["Computer Science","Data and Information","Earth Sciences"],"subject":["Publication - Article","PeerReviewed"],"fullText":"<<<Back to Table of Contents       \nOn to Part I: A view of the conventional geoscience information system>>> \n \n \nGeoscience after IT: Part H \n \nFamiliarization with managing the information base \n \nT. V. Loudon \nBritish Geological Survey, West Mains Road, Edinburgh EH9 3LA, U.K. \ne-mail: v.loudon@bgs.ac.uk \n \n \nPostprint of article in Computers & Geosciences, 26 (3A) April 2000, pp. A63-A73 \n \nAbstract - The geoscience record stores information for later reuse. The management \nof bibliographic, cartographic and quantitative information have different \nbackgrounds. All involve: deciding what to keep; structuring the record so that \ninformation can be found when needed; maintaining search tools, indexes and \nabstracts; defining the content by reference to metadata. The current approaches to \nmanaging the literature, spatial information and quantitative data may be subsumed in \na more comprehensive object-oriented model of the information base. \n \nKey Words - Information management, disposal, search, metadata, object-oriented \nmethods. \n \n \n1. The framework \n \n1.1 The requirement \n \nThe availability of quantities of data for analysis and display created a need to \norganize and store this information. Users could then revisit results and explore other \nways of analyzing the data. The discovery of interesting relationships within one \ndataset might lead to investigation of similar relationships elsewhere, through access \nto a wide variety of related datasets. A database could combine data from many \nsources, and the user could select subsets for retrieval. A clearly defined interface \nensured that retrieved data could be accepted by the programs for analysis. The \nprograms could be reused with a variety of data, and datasets could be reused for \nanalysis by a variety of programs (part I, section 2.3). Management of the database \ncame to be regarded as a task in its own right, and tools such as relational database \nmanagement systems were developed. They have been successfully (and often \nunsuccessfully) applied to geoscience data. \n \nDatabases are appropriate not only for quantitative data, but also for indexes to other \ntypes of information. For example, cores, samples and specimens can be cataloged, \nand the records stored and retrieved from a database. The names of wells, boreholes \nor outcrops from which they were obtained can be recorded, with information about \nlocations and depths, dates, investigators and the like. The result is structured, tabular \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \ninformation that can be linked to other datasets by means of key fields, such as well \nname. \n \nWhen spatial information, traditionally shown on maps, became available to the \ncomputer, similar requirements arose for spatial data management. Geographic \ninformation systems were extended to include data management, with new options \nsuch as finding specimens from an area shown as Upper Cretaceous (points in \npolygon), or finding outcrops within the Upper Cretaceous where the lithology is \noolitic limestone (polygon overlap).  \n \nLibrarians and archivists have a long-standing interest in managing information. \nIndeed the separation of their work from that of the geoscientist might be seen as an \nearly distinction between information management and analysis. Their use of IT to \norganize and manage their collections has contributed many decades of experience in \nclassifying, storing and retrieving the documents that record geoscience knowledge. \n \nThe different types of data management have tended to remain separate, with \nscientists looking after their own databases, cartographers and image analysts \nmanaging spatial data, and librarians managing published documents. Although they \ndeal with different information types, however, the activities are similar for all. The \ninformation is likely to be: \n\u2022 recorded and edited \n\u2022 acquired in a collection and in due course disposed of \n\u2022 assigned an identifier  \n\u2022 annotated with source and date \n\u2022 structured, marked up, linked to other information \n\u2022 classified, described and indexed \n\u2022 assessed and evaluated \n\u2022 stored \n\u2022 retrieved \n\u2022 copyrighted to establish and maintain intellectual property rights \n\u2022 supplied on request to users who are entitled to access it \n\u2022 updated \n \nWith published documents such as papers, books and maps, the three main players are \nthe author, publisher and reader. Editors act as intermediaries between the author and \npublisher, and librarians and booksellers as intermediaries between the publisher and \nreader. The counterparts of author, publisher and reader in the more general situation \nare the contributor, manager and user of information.  The manager is responsible for \nmost of the tasks just listed, with contributors responsible for the content they supply, \nand users for their own selection and retrieval (M 1). \n \n1.2 Acquisition, context and disposal \n \nThere are fundamental questions of what information is worth holding, for whom, and \nfor how long. The answers should determine whether it is stored, how it is stored and \nhow it is made available. With or without IT, storing information is not necessarily a \nuseful activity. Each scientific study is project-oriented, taking place within a specific \nframework of business needs and scientific theory, hypotheses and models. \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \nInevitably, much information has little value outside the project, and can be disposed \nof when the project is complete.  \n \nThe value of data is determined by its significance to a model. Data items which \ncontrol or elucidate aspects of the model have greater value than those which merely \nconfirm what is already known. Mapping a thousand square miles of exposed \nsandstone might add little to the model of the area. Discovering one microscopic \nfossil in the same formation might be of intense interest, throwing new light on the \nmodel, both locally and globally. The model is paramount. \n \nTo be useful to others, information must be placed in a context in which it can be \nunderstood. It must have links to integrate it with the main body of scientific \nknowledge. Communication between author and reader depends on mutual \nunderstanding, and this in turn depends on standardization and quality control \nimposed by managers and editors. Its success, or otherwise, can be seen in the ease \nwith which the reader accepts the conventions and language of a published paper or \nmap.  \n \nThe final products from a project are generally documents. They are the main level at \nwhich information is communicated. They provide the context within which the raw \nand processed data and other evidence can be understood. There is no scientific link \nbetween say, a single measurement of gravity at a point and the proportion of tin in a \nstream sample at the same point. The connection is through the products - the gravity \nmap and the geochemical map - and an interpretation of the patterns of distribution of \nthe variables against the background of the underlying geological model, such as the \npossibility of a buried granite body. The availability of the final products is central to \nthe ability to integrate different sources of geoscience information. Standards, for \nexample for a geological map, may apply to the published end product rather than to \nthe internal details of a project. \n \nThe long-term structure of recorded geoscience knowledge is based on publications. \nThe formal literature must be organized in such a way that the user can find relevant \ninformation. It is the responsibility of editors, with the help of referees, to ensure that \na coherent set of documents is produced, and the responsibility of librarians to ensure \nthat the records are secure and accessible. Each publication is a major work that can \nbe carefully assessed, cataloged and stored. Systems, including a legal framework, are \nin place to safeguard copyright and to hold and disseminate publications in perpetuity. \nThus, although most libraries dispose of material they no longer require, even \ndocuments with obsolete ideas and disproved concepts are not totally banished from \nthe record (I 6).  \n \nEach project is exploring unknown scientific territory. It is unrealistic to suppose that \ntotally predetermined standards can be followed, as this would assume that all \neventualities could be foreseen. Data collected for each project are partly specific to \nthat project. The project objectives influence the design features of the investigation, \nincluding the underlying model, sampling scheme, sampling density, types of data, \ndata collection methods, operational definitions, instrumentation and measurement \nprocedures. The data can be fully interpreted and evaluated only in the context of the \nproject design. In these circumstances, supporting data must be linked through the \nproject documents, rather than directly within a comprehensive database (D 4). This \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \nsuggests that informal records should be retrievable by tying them to the formal \nliterature. Archived specimens or data could be referred to in the published paper, \nthus enabling readers to locate that additional information. But this is not always \npracticable. \n \nExcept for some long-term archives, project data are seldom seen as part of the formal \nliterature because they are ephemeral and subject to change. They are likely to be \nmaintained and possibly made available by the originating individual or organization, \nbut may be altered as ideas change, and disposed of when the owner loses interest. \nThey are thus not part of the permanent record, even if some at least ought to be. If \nthe user knows the reasons for the study and something of the manner in which it was \ncarried out, data can be abstracted from it that are of value in other contexts. For \nexample, a local geological model might be based on fortuitously exposed rock \noutcrops, but could also make opportunistic selective reuse of data collected for other \npurposes. Those data might be derived from other projects employing different \nmodels, such as oil exploration, mining and quarrying, underground water production, \nand site investigation. Direct reference to the data and project descriptions through \none coherent database rather than through the literature could make the links clearer \nand simplify the interfaces for analysis and display. \n \nThe conventional literature is also inappropriate for maintaining some large datasets. \nProperties such as gravity can be measured consistently by instruments calibrated \nbetween projects and following a predetermined sampling pattern. Such \nmeasurements over a large area have obvious value in regional studies. Similar \ncomments could be made about, say, aeromagnetic, seismic, downhole logging, \ngeotechnical and geochemical surveys. Even borehole descriptions that follow \ndetailed guidelines can be consistent and comparable over wide areas (NLfB, 1999). \nIn effect, these become regional projects focused on narrow aspects of geoscience. \nThey may co-exist with projects with different scope and aims. For example, a site \ninvestigation might follow external standards in describing the borehole records \nbecause, although they added a small overhead cost, this could be justified by the \nvalue added to the data. \n \nIT raises the possibility of smaller but more frequent updates to the knowledge base, \nnew criteria for acquisition and disposal, and a shared context that integrates formal \nand informal documents. Possible consequences are discussed in M 2.1. \n \n1.3 Search strategies \n \nA crucial aspect of managing information of any kind is the ability to find appropriate \nmaterial when it is needed. If the user knows the name of an object, it should be \npossible to discover where to find it by looking it up in a catalog or index. If not, \nvarious other search techniques are possible, based on a description of the topics of \ninterest. The search can be modified by \n\u2022 extending the area of interest  \no use broader terms in a bibliographic search \no zoom out in a geographic search \no extend the range in a database search  \n\u2022 restricting the area of interest  \no use narrower terms in a bibliographic search \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \no zoom in, in a geographic search \no narrow the limits in a database search \n\u2022 moving to related concepts  \no use related terms in a bibliographic search \no pan to adjacent areas in a geographic search \no select related criteria from the data model in a database search \n \nIdeally, a computer system would allow the user to combine bibliographic, \ngeographic and database criteria, using the technique most appropriate to each stage \nof the search. Various commercial systems, including GIS, are moving in this \ndirection. \n \nThe human brain is adept at searching, particularly through well-structured material. \nThe scientists\u2019 background knowledge enables them to detect clues to what is \nrelevant, even where this is not expressed directly either in the material or in their \nown explicit search criteria. Users can assess the relevance of documents or images by \nbrowsing through them, focusing on and following up items of interest. Abstracts and \nindex maps may save the trouble of looking through the full document. The structure \nof objects may be shown by a paper\u2019s table of contents, a map explanation or a \ndatabase entity-relationship diagram, and this may narrow the area of search. \n \nControl numbers, like the familiar UDC of library shelves (H 2), indicate subject \nareas and organize ideas in hierarchical classifications. Because similar numbers refer \nto similar topics, browsing among adjacent objects, for example on library shelves, \ncan be profitable. Classification of documents implies that catalogers have examined \ntheir content, and assigned categories accordingly. As this is a worldwide activity, the \ncategories should follow a global standard, and classification should follow standard \ncataloging rules (H 2). A computer index of titles or abstracts can be searched for a \nkeyword or combinations of keywords (H 2), as can the original document if the full \ntext is held on a computer. \n \nDocuments can also be found from a general written account of the subject by \nfollowing references to more specialized papers, or by looking at a small-scale map to \nfind areas to examine at a larger scale. References and hypermedia links may lead to \nother relevant material. It helps if the strength of the links can be estimated, in terms \nof current access rates, numbers of previous users making the connections, or their \nevaluation of the links\u2019 significance. Examples can be seen in Web documents, such \nas electronic bookshops (Amazon.com, 1996). \n \nSpatial data represented on a map or in a GIS, can be searched by geographic location \nusing the grid of geographic coordinates, by looking on the map face for the color \ncodes and symbols shown in the explanation, or by relating the geoscience data to \nfeatures on the underlying topographic base map or to other map overlays. Because \nthe location of data on the map reflects their position on the ground, the map (or GIS) \ncan be browsed for items within an area, close to a feature, coinciding with a point, \nand so on. \n \nClassification of items in a database (H 3) can follow similar procedures to those used \nby the librarian. A data model can show the relationships of concepts. Terms can be \ndefined and standardized in data dictionaries. Data searches can therefore be narrowed \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \nto appropriate variables, including spatial coordinates, then to a selected range of their \nvalues. If data are to be widely shared, the data models and dictionaries must follow \nwidely accepted standards. This has been achieved in limited areas, such as global \nstudies of oceanography, seismology or geomagnetism. At present, most geoscience \ndatabases are restricted to the organization that created them. However, the work of \nsuch organizations as POSC (1999) suggests that a more general framework for \ngeoscience is emerging. \n \nComputer systems lack the scientists\u2019 background knowledge, but (being machines) \ncan efficiently perform mechanical searches for specific keywords or numeric values, \nand can follow recorded links. The past experience of other workers can be recorded, \nsuch as: many past users looking for references to \u201ccarbonates\u201d found that some \nreferences to \u201climestone\u201d were also relevant. As always, the computer and human \nbrain should pull together, making best use of the abilities of each. A well-structured \nsearch might alternate between the computer extending the search on the basis of \nlinks and structure, and the scientist narrowing or redirecting the search on the basis \nof background knowledge of the subject and the requirements. \n \nThings can be found more easily if they are carefully organized. We now look in turn \nat how librarians, database managers and cartographers set about this task. There are \nobvious benefits in combining the best features of all their approaches, and it is not \nsurprising that their methods are tending to converge. In H 5, we consider how \ndistributed objects can assist convergence. \n \n2. Documents \n \nThe number of relevant published documents in most fields is large enough for \nlibrarians to benefit from computer support in acquiring, storing and cataloging them. \nAs can be seen in electronic journals such as D-Lib (D-Lib, 1995), many librarians are \nenthusiastic pioneers of IT methods. \n \nA library requires a description of all the bibliographic items in the collection, such as \nbooks, serials, maps, videotapes or computer files. The objectives are to know what is \nthere and where it is, to arrange the material sensibly on shelves, to help users to find \nthe information they require, and to monitor its use. The items must be uniquely \nidentified, and should be retrievable by author, title, or subject. A catalog is therefore \nrequired containing at least this information. There are obvious advantages in \nadopting standard cataloging procedures. An initial aim was to help libraries to \nexchange information on their holdings, and obtain comprehensive lists of new \npublications as a guide for acquisitions. Obtaining entries from a central source, such \nas the Library of Congress, can reduce the considerable cost of cataloging. Sets of \nnational and multinational standards have evolved (see Mulvany, 1994), many based \non the International Standard Bibliographical Description (ISBD). \n \nFor referencing or cataloging purposes, each item must be identified uniquely. The \nInternational Standard Book Number (ISBN) and Serial Number (ISSN) address this \nneed. The ISBN and ISSN indicate by numeric codes the language of publication, the \npublisher\u2019s imprint, and a number assigned by the publisher for the edition of the \nbook, or issue of the serial. This control number is likely to appear on the cover as a \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \nmachine-readable bar code. Other control numbers may be assigned by the library, \nparticularly for works that have no standard numbers. \n \nThe Anglo-American Cataloguing Rules (AACR2) are widely followed in most \nEnglish-speaking countries and have been translated into many other languages. They \nhelp to ensure that each catalog entry has a similar style. The Dublin Core (DCMI, \n1998; L 3) has comparable objectives for simpler systems. Subject matter can be \nclassified and assigned numeric codes, such as the long-established Dewey Decimal \nClassification. Developed from it is the Universal Decimal Classification (UDC), a \nmore general classification covering the whole field of knowledge. The classification \nis hierarchical, going from the general to the particular. Thus 54 indicates chemistry, \ncrystallography and mineralogy, 549 mineralogy, 549.32 sulfides of metals, \n549.324\/.326 disulfides of iron and related sulfides, and 549.324.31 pyrite, \nmelnikovite. Where books are arranged on library shelves by such a numbering \nsystem, those on similar topics should be together, making it easier to browse through \nthe subjects of interest.  \n \nAn obvious snag with this arrangement is that a document may deal with more than \none topic. For example, it might deal with the geophysical as well as the \nmineralogical consequences of pyrite deposits. The UDC code can accommodate \nseveral subjects, separating the codes by devices such as colons, that indicate the \nrelationships between the subjects. Multi-dimensional shelving to reflect this would \nbe inconvenient. With a computer catalog, of course, there is no difficulty in handling \nclassification by multiple criteria. Users can access the catalog remotely from the \ndesktop, through a simple user interface. Many libraries provide on-line public-access \ncatalogs (OPACs) of this kind (L 3), which are freely accessible and easy to use. Lists \nof OPACs can be found on the World Wide Web (NISS, 1999). \n \nA list of references at the end of a paper points to earlier, related work. With computer \nsupport, the process can be reversed to point to papers written later which refer to the \ntarget paper. A citation index produced in this way, such as the Science Citation \nIndex, enables you to start from a key reference in your subject area, and locate later \nworks which deal with the same topic (Garfield, 1983; Institute for Scientific \nInformation, 1999). These forward references can also be used to analyze the structure \nof cross-reference in the literature, and to throw light on the range and number of \nreferences to a paper and the caliber of the journals in which they appear. This is one \ntool used to evaluate the contributions of individuals or organizations to the scientific \nliterature. Access to a citation index can be expensive, and the coverage of the \nliterature is inevitably incomplete. \n \nRather than classifying a paper with a long string of UDC codes it may be easier to \nrecord a list of keywords reflecting the main subjects. A thesaurus, such as GeoRef \n(Shimomura, 1989), is normally available to indicate which terms have been used for \ncataloging. For searching, it may suggest synonyms (\u201csee also\u201d), broader, narrower \nand related terms, if the first choice is not appropriate. Lexicons may also be available \nwith definitions of the terms. Geoscience catalogs of this kind are commercially \navailable, on-line or on disk. They may be available as a library service on a local \ncomputer network. \n \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \nRetrieval by combinations of keywords can be effective, but is not classification in the \nstrict sense, which depends on analysis of idea content. The UDC is \u201ca universal \nclassification in that an attempt is made to include in it every field of knowledge, not \nas a patchwork of isolated, self-sufficient groupings, but as an integrated pattern of \ncorrelated subjects\u201d, (British Standards Institution, 1963, p6). The notion of providing \na map of human knowledge reappears in the concept of ontology as used by workers \nin machine intelligence (L 5), the pattern of linkages in hypermedia (E 4), and in the \nobject-class hierarchies and entity-relationship diagrams used in database work (H 5, \nH 3). UDC provides perhaps the simplest notation, with a view to mapping fields of \nknowledge onto the linear sequence of the library shelf. Remarkably, this simple \napproach has proved to be of considerable long-term value. \n \nThe storage and exchange of bibliographical records on computer systems require \ndecisions about the format in which they are held. The MARC format (MAchine \nReadable Cataloguing) meets this need. Various incompatible versions arose, leading \nto the development of UNIMARC, which facilitates the exchange of records created \nin any MARC format (Library of Congress, 1999). It specifies a wide range of fields \nthat can be identified by a standard tag or by their position in the record. Librarians \nhave for some time been using the ANSI Z39.50 protocol for communicating between \ntheir computer systems in order to access bibliographic catalogs (Biblio Tech Review, \n1999). With its help, they, or end-users, can access several on-line public-access \ncatalogs (OPACs) in numerous libraries worldwide (NISS, 1999) from a single \nsearch. Extended services include ordering or borrowing a document, collecting fees \nand even updating the database. New and more general exchange formats, based on \nSGML (E 6), wait in the wings. \n \nFrom the point of view of the librarian, as opposed to the user, computer systems can \nalso help with their housekeeping tasks. These include stock control, keeping track of \nloans, acquisitions, disposals, and exchanges. Bar codes attached to each document \nidentify it when it is borrowed or returned, and similar codes on borrowers\u2019 cards \nmean that the transaction can be largely automated. The computer records should help \nlibrarians to manage their collection, and to combine cataloging activities in a virtual \nunion catalog. Library software integrates the housekeeping tasks with the user \nservices mentioned earlier. The integration of library tasks with broader information \nsystems is discussed in M. \n \nLibrarians are traditionally concerned with published documents. As these are \ncomplete and unchanging, editing and digitizing would normally be completed before \npublication (D 6) and are not seen as part of the management process. However, there \nis a growing need to manage electronic documents. These may be continually \nmodified to reflect the most recent views and to maintain links to datasets and spatial \ndata. They must be continually modified to conform to current formats and updated \nsystems. One short-term possibility is to store the electronic documents as part of a \ndatabase. In the long term, information management may merge the library and \ndatabase functions. Meanwhile, libraries must evolve to meet new requirements, \nincluding the growing number of electronic publications, the requirement to digitize \nand markup existing publications, and the need to keep track of numerous versions of \na document. The possible consequences are discussed in L 3.  \n \n \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \n3. Database \n \nData are collected within a project to meet its objectives and methods. The dataset \nmay nevertheless be retained as a persistent object, which continues to exist beyond \nthe project. Its continuing value was considered in H 1.2, including reuse by the \ninvestigators, or others, to follow the reasoning behind the project\u2019s conclusions, to \nconfirm the results, to add to the data, and possibly to verify the data by repeating at \nleast some of the observations. The persistent object may be evaluated, authorized, \nand published or deposited in an archive for long-term availability. The stored object \nis likely to be a computer file, which is readily exchanged and designed to interface \nwith programs for analysis. The programs may be included as part of the same object \nor may be persistent objects in their own right. An account of the data or programs is \nlikely to be published in a conventional journal, and the same editorial team may \nevaluate them and provide archiving facilities. \n \nSuccessful sharing of detailed information depends of a common purpose. Data are \nmore consistent when they are collected in a similar way by similar instruments. The \nextent of integration may depend on the supporting information technology. Long \nago, paper records about the geology of an oil field, for example, were seldom \nexchanged far beyond the original operators. When service companies started to offer \nbetter instrumentation (for instance in seismic exploration, downhole logging and core \nanalysis) standardization became easier, and data were more widely exchanged. As \nelectronic methods of data storage and analysis developed, global standards were \nintroduced, for example, POSC (1999), and data management began to be seen as a \ntask that need not be part of the core activity of an oil company but could be \noutsourced to a shared facility. Wider standardization between broad fields of activity, \nsay, between oil exploration and geological survey, is more difficult, with fewer \nobvious gains on either side. Overlapping standards are nevertheless developing, often \ndriven by IT solutions that are adopted in a number of different fields. \n \nClose dependence of data on the project limits their value in other contexts. Ideally, \ndatasets would not be limited to a specific project, but would play a wider part in \ngeoscience knowledge as a whole. Data might then be continually revised as more is \nlearned. The effect of changes in one area might be propagated through to all related \nareas. The concept of a database was devised with such integration in mind. It began \nwith the hope of bringing together all the data from an organization, making them \nmore generally useful outside their original context, and avoiding repeated collection \nof the same information for different purposes. \n \nA database provides a structure in which a wide variety of data from many projects \ncan be recorded and kept up to date by a database management system, while \nmaintaining internal consistency and providing a uniform interface to the outside \nworld. The trade-off is that greater generality can bring additional, often unacceptable, \noverheads to an individual project. They include the need to analyze and preplan the \nactivity, and to follow standards that may create additional work and reduce flexibility \nwith no obvious benefit to the project. Individual contributions, and hence specific \nresponsibility and credit, can sometimes be lost within an integrated database. \nNevertheless, the gains even from limited integration can be substantial, and large \norganizations and scientific consortiums have made good use of database techniques. \nThe implications are relevant to most geoscientists. \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \n \nRelational databases provide a widely used structure for geoscience data. Relational \ndatabase management systems provide the means of managing them. Data are stored \nin tables of a particular kind known as relations. The columns may be referred to as \nvariables or domains and the rows as tuples. The relational design aims to reduce \nredundancy, that is, repetition of information. The reason is that redundancy causes \nproblems when information is changed. The same changes must be made wherever \nthe information is repeated, otherwise inconsistencies arise. If information is held \nonce only, only one item need be altered. All references to that information will \nautomatically access the revised item.  \n \nIn a relational database, the relations are designed (by a process known as \nnormalization) to avoid repetition. For example, if a number of beds from the same \nborehole are described, the data about the borehole are held once, and each bed \ndescription refers to them rather than repeating them. The reference is by means of a \nkey field - a column in the table that contains the identifier for the borehole data (see \nFig. 1). As well as reducing redundancy, this structure has the advantage that each \nrelation contains uniform sets of data, with similar variables for each record. \n \n \nFig. 1. Organizing lithological descriptions in a relational database. On the left is a list of data for each \nbed, with the data for the borehole repeated each time. On the right, borehole information is placed in \none table, and bed descriptions in another, thus reducing the need for repetition. The starred items, QS \nand NUMB, are together a unique identifier for the borehole. They form a \u201ccomposite key\u201d. It alone is \nrepeated in each bed (or interval) to link its description to the appropriate borehole. LithCode is a \n\u201cforeign key\u201d which links the compact, standard lithology code to a dictionary that provides the full \ndescriptive terms. This structure is reflected in the form shown in part D, Fig. 3. \n \nThe relational database is appropriate only for well-structured data, that is, items such \nas categories or numerical data that fall naturally into a tabular arrangement. Some \nflexibility can be obtained, however, by regarding other data, such as a string of text, \nan image, or a string of points representing a line on a map, as a binary large object \n(BLOB). It can be held separately, in its own format, and referenced when required \nfrom a key field in a relation. \n \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \nThe benefits of a relational scheme come from a structure that can accommodate \nsimple datasets, and can also be scaled up to encompass large and complex datasets. It \nis a robust structure that can be up-dated, corrected, re-organized and extended as \nrequired. It can be linked to simple procedures for editing data through forms on \nscreen. Data can be retrieved through a standard language (E 6), SQL (Structured \nQuery Language). This provides a simple means of specifying which items have to \nbe retrieved. The user specifies whether a variable or each of a combination of \nvariables is equal to, greater than or less than specified values (numerically or \nalphabetically). SQL also allows the user to indicate which variables are to be \nreturned and in what format. It thus provides a convenient and flexible interface \nbetween most relational database management systems and programs for analysis of \nthe retrieved data. \n \nRetrieval requires some knowledge of the structure and contents of the database. The \ndatabase may be used informally as a working tool, where all concerned are familiar \nwith the contents. An outsider, however, needs access to metadata describing the \ncontent and layout. For a large relational database, the design of the relations and their \nlinks must be carefully considered. The completed design is referred to as a data \nmodel or schema. Systems analysis and data analysis may precede the implementation \nof the database. The results of the data analysis can be expressed in entity-\nrelationship diagrams (see L Fig. 5). The data are regarded as a set of entities, such \nas wells, cores, lithologic descriptions, stratigraphic classifications, and so on. The \ndiagrams show the relationship between the entities. For example, samples might \nbe_part_of cores, and the cores might contain samples. The diagrams may be \nsupplemented by data dictionaries that list the recorded variables, and contain \ndefinitions of the variable and entity names. By examining the diagrams, users should \nbe able to see what information is available and how it is related to the items of \nprimary interest. From the data dictionaries they should be able to establish the exact \nsense in which terms are used. \n \nPreliminary planning (D 4) is required to achieve a shared understanding of all the \ndata, and avoid redundancy and ambiguity. There are various levels at which this can \nbe attempted: within a project, an organization, an activity (such as oil exploration), or \nfor the science as a whole.  At each level there is a trade-off between local and general \nobjectives. The complexity of the analysis obviously differs for each. A small project \nmay call for no more than an informal record of metadata. At the other extreme, for a \nlarge organization or a general synthesis within an area of science, skilled specialists \nmay be needed to conduct the analysis. CASE tools (computer-aided support \nenvironment) can provide computer support, from constructing entity-relationship \ndiagrams to structuring the database. It has been said that data analysis aims to ensure \nthat there is a common understanding of all the data held in a database, with no \nduplication, ambiguity or redundancy. These attributes could not apply to geoscience \nliterature, where ambiguity and redundancy are essential. In principle, analysis is not \nrestricted to a database, but could include separate text and spatial information. The \nneed for a more comprehensive view leads to object-oriented methods (H 5). \n \n4. Spatial data \n \nSome vendors of Geographic Information Systems like to demonstrate the ability of \ntheir product to zoom in from a map of the whole country to an enlarged area, \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \npanning across the map to center on the point of interest. As the detail increases, a \ntown takes shape, then individual streets and their names appear. Moving to a larger \nscale, buildings are individually identified. A new theme is selected, perhaps utilities - \ngas, water, sewers, electricity, telephone and television cables - showing their depth \nbelow street level and their exact position superimposed on a street plan. Or perhaps \nthe interior plan of a building is displayed, zooming in to individual offices to show \nthe position of the furniture, the wattage of a light bulb and the date it was last \nrenewed, or perhaps a photograph and CV of the occupant with a number on which to \nclick to establish a videophone link.   \n \nThe demonstration is misleading, of course, and usually followed by a confession that \nwith any other combination of areas and topics, the screen would be an embarrassing \nblack. Unfortunately, most spatial data are collected in diverse projects to \nincompatible standards and can neither be shared nor integrated. Until global \nsolutions (L 4) are widely adopted, each organization, or worse, each application, may \nhave to adopt its own standards, and plan, where appropriate, for future migration to \nglobal standards.  \n \n \n \nFig. 2. Spatial search. Within a single topic, such as Drift geology, GIS enables the user to zoom in to \nan area of interest, seeing how it relates to the broader picture. These extracts are from the BGS \nGeoscience Data Index. British Geological Survey \u00a9NERC. All rights reserved. Base maps reproduced \nby kind permission of the Ordnance Survey \u00a9 Crown Copyright NC\/99\/225. \n \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \nWhat it suggests, however, is the power of spatial search allied to visualization. \nSystems of this kind have been demonstrated in geoscience and are partially \nimplemented within a few organizations (M 2.3). They offer the ability to see what \ndata are available for which topics and where they are located. The user can visualize \nspatial data in their correct relative positions (see Fig. 2), and, maintaining spatial \nrelationships, examine the pattern of their distribution and the spatial correlation with \npatterns of other types of data. The user might select a sequence of operations, such as \nthe following. Look at the surface geology, superimpose contours on a subsurface \nhorizon, zoom out to view the regional setting, zoom in to see which fossil species \nwere found at an outcrop, and examine some thin sections on screen. Look (Fig. 3) at \nthe gravity map and magnetic anomalies, examine well records, core descriptions and \ndownhole logs, see the 3d seismic reconstruction (J Fig. 1) slice by slice, look at \nenhanced photographs of the landscape and processed satellite imagery (as in J Fig. \n2). Pan southwestward to look at geochemical stream sediment analyses downstream, \nusing quantitative tools, such as SQL, to select only analyses with appropriate \nconcentrations of defined elements.  \n \n \n \nFig. 3. Aspects of geoscience as topics on a map. From a GIS-based system, the user can select and \ncompare many properties for the same area. These extracts are from the BGS Geoscience Index. British \nGeological Survey \u00a9NERC. All rights reserved. Base maps reproduced by kind permission of the \nOrdnance Survey \u00a9 Crown Copyright NC\/99\/225. \n \nThe search techniques just described are specific to spatial information. Because we \nare well able to visualize space, it can be a powerful metaphor for other data. For \nexample, an organization chart can show a hierarchy of employees, with related \ndepartments placed side by side to represent their organizational closeness. It could be \nmore effective than an office plan for locating and communicating with appropriate \nstaff. A stratigraphic table presents a sequence of formations laid out in space to \ncorrespond to their sequence in time. Entity-relationship diagrams display a spatial \nimage of related concepts. Cross-plots of quantitative values (F Fig. 3) correspond to \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \na map in space. Techniques for spatial analysis are therefore not confined to data in \ngeographical space, but also apply to a variety of other data represented in \nmetaphorical space. \n \nThe tools for managing spatial data are available in many geographic information \nsystems (GIS). They access an underlying database in which most of the data are \nspatially indexed. They must be able to provide rapid access to spatial objects. For \nthis, the structures in which the data are held are crucial. The spatial coherence of the \nobjects and phenomena, that is, the fact that they lie within a limited, continuous area, \ndetermines the form of a suitable structure. Arranging the data first by the x-\ncoordinate, then by the y-coordinate would not be appropriate, as it would split the \ncoherent object into successive strips mingled with pieces of unrelated objects that \nhappened to be as far east. Instead a quadtree structure divides space into squares, \nwhich in turn are divided into smaller squares and so on. The larger squares can be \nreferenced to show the approximate location of relevant data, and successive layers in \nthe tree of smaller squares give progressively more detailed views of the object. \nProvided the quadtree structures match, they give an efficient route to studying the \nspatial correlation among different objects (Mark et al., 1989). Even pixels in raster \nformat (G 1) can have a quadtree index. Octrees are the three-dimensional equivalent, \nand a means of representing, modeling and correlating objects in three-dimensional \nspace (see Jones 1989). The three-dimensional equivalent of a pixel is known as a \nvoxel. \n \nSpatial objects, which might include sets of points, areas and volumes dealing with a \nparticular topic, must be identifiable. Users must be able to refer to the object or \nobject class, and to do so must be aware of its existence. The system must therefore \ndisplay metadata in a well-organized form which is easy to search. They may be \nprovided as an index, list or menu. To take full advantage of the spatial aspects of the \nsystem, it should also be capable of displaying a summary of the object\u2019s spatial \ndistribution as a small-scale map. Users should be able to select objects by pointing to \nthem on a display, or defining the search area. The area may be a circle, rectangle, \nswathe of specified width along a line, or an irregular polygon. All of these should be \nselectable by moving the screen cursor. The area of search might alternatively be a \npolygon or polygons representing another object or object class. Objects might be \nretrieved depending on whether they are wholly within the search area or only partly \nwithin it, and might be retrieved in their entirety or only that part which lies within the \nsearch area. By this means, point data lying within a formation could be retrieved, or \nthe parts of a formation coinciding with a particular lithology.  \n \nAt all times, the display should provide a means of visualizing the disposition and \nconfiguration of the objects of interest. It must be possible to zoom in to see detail and \nto zoom out to see the context. This implies, however, that the spatial information \nmust be available at various levels of detail, and generalization is unlikely to be a \ncompletely automatic process. The problems with providing comprehensive access to \nspatial data thus seem to arise, not from deficiencies in IT, but from the absence of \nstandard methods for providing spatial data and the absence of incentives to make \nthem available in an appropriate form. Solutions have been proposed (see L 4). \n \n \n \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \n5. Object-oriented methods \n \nBy involving computer systems in the management and manipulation of information, \nwe introduce machines into an intuitive process. In order to clarify the role of the \nmachine, we need to take an introspective view of the process, thinking explicitly \nabout the structure of our information, thought processes and objectives. Formal \nanalysis may not be required, but it can help to have some knowledge of current \nmethodologies for analysis, such as the entity-relationship modeling mentioned in H \n3. Object-oriented methods (see also J 2.4) offer a more comprehensive and flexible \napproach to including the full range of information types and to providing a natural \nmeans of handling distributed objects and relationships. They are well suited to \ngeoscience with its complex objects in various versions and its long and complex \ntransactions (Worboys et al., 1990). \n \nObject-oriented (O-O) methods address the way we represent our ideas about the real \nworld and how, by abstracting and formalizing our knowledge, we can implement \nthem on a computer system. Starting from our perception of the real world, the \nmethods proceed through analysis and design to programming and database (J 2.4). \nThey offer an integrated view of large and complex problems and can place it in a \nsystematic engineering discipline. This is the realm of the specialized consultant, and \nat first sight has little relevance for the circumscribed problems of the geoscientist. \nNevertheless, the insights justify some acquaintance with the techniques. Some major \nstudies, notably POSC (1999), take an object-oriented view of geoscience \ninformation, and large organizations are moving towards a similar framework. \nGeoscientists should be aware of these developments and may even be able to align \ntheir ideas with them.  \n \nObject-oriented analysis and design do not necessarily lead to O-O programming or \ndatabase, although this may prove desirable in the long run. O-O programs are \nappropriate for developing the graphical user interface, but less so for numerical \ncalculation. At the time of writing, RDBMS are more robust and better supported than \nOODBMS. \n \nAnalysis is seen as the practice of studying a problem domain. It leads to a consistent \nset of diagrams and protocols constituting an abstract system, which can convincingly \nbe defended as an adequate understanding of the problem. This leads in turn to a \ncomplete, consistent and feasible statement of what is needed from the computer \nsystem. Design then takes this specification of externally observable behavior and \nadds details needed for actual computer system implementation. These include details \nof human management, task management and data management. Careful design \nensures that objects can be reused for other purposes, and that the system as a whole \ncan readily be altered and extended. \n \nSome authors, such as Henderson (1993), make a distinction (not followed here) \nbetween entities, which they define as existing in the real world, and objects, which \nare their counterparts in the computer implementation. The object is a record with \nattributes, each of which has a value, together defining the state of the object. A \nmethod alters the state of an object or causes the object to send messages - the means \nof communicating with another object. The interface is limited to message passing. \nThis encapsulation hides the structure and implementation details of the object from \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \nother objects. It ensures a simple interface that shows only the external aspects of the \nobject, which are accessible to other objects. It reflects abstraction, the principle of \nignoring those aspects of a subject which are not relevant to the current purpose, in \norder to concentrate more fully on those that are.  \n \nThe objects correspond to entities in the real world whose states and relationships we \nwish to track. As the entities change, there are parallel changes in the objects. The \nobjects are grouped into classes, descriptions of one or more objects (an individual \nobject is sometimes referred to as an instance) with common features. Instances \ninherit the features of the class they belong to, and possibly those of a higher level \nsuperclass (see, for example, Cattell, 1991, Graham, 1994 or Blaha, Premerlani, \n1998).  \n \nThe influential Object Management Group (OMG) is committed to consistent \ndevelopment of these methods. Their documents can be found on the Web (OMG, \n1997; Netscape Communications Corporation, 1997). Particularly relevant is their \nwork on global exchange of objects through a standard interface - the common object \nrequest broker architecture CORBA. As objects are not restricted to particular \ninformation types, and distributed objects can be held on any server for access by \nany client, O-O methods seem to offer a flexible basis for integrating a great deal of \ngeoscience work. They offer the prospect of harnessing the power of hypermedia to \nlink diverse information types and objects distributed among many repositories, \nthrough a uniform user interface.  \n \nParts A - H dwell on the benefits of IT and the nature of IT tools. The extent to which \nthe benefits can be achieved depends on future developments. For a clearer view of \nhow geoscience and IT will interact, we now need to reconsider our own methods of \ninvestigation: how we observe, remember and record, how we build knowledge from \ninformation and cope with changing ideas. These methods relate to the strengths and \nweaknesses of older systems as well as the potential of IT - the flexibility of \nhypermedia, the developing standards for the global network of cross-referenced \nknowledge, and the particular value of well-organized structures of geoscience \nknowledge. They are outlined in I - K  and should help us to understand the emerging \ngeoscience information system, and to build on initiatives and opportunities such as \nthose reviewed in L - M. \n \n6. References \n \nBlaha, M., Premerlani, W., 1998. Object-oriented Modeling and Design for Database \nApplications. Prentice-Hall, Upper Saddle River, New Jersey, 484pp. \n \nBritish Standards Institution, 1963. Guide to the Universal Decimal Classification \n(UDC). British Standards Institution, London, 128pp.  \n \nCattell, R.G.G., 1991. Object Data Management: Object-oriented and Extended \nRelational Database Systems. Addison-Wesley, Reading, Mass. 318pp. \n \nGarfield, Eugene, 1983. Citation Indexing: its Theory and Application in Science, \nTechnology, and Humanities. Wiley, New York, 274pp. \n \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \nGraham, I., 1994. Object Oriented Methods, 2nd edn. Addison-Wesley, Wokingham, \n473pp. \n \nHenderson, P., 1993. Object-oriented Specification and Design with C++. McGraw-\nHill, Maidenhead, Berks., 263pp. \n \nJones, C.B., 1989. Data structures for three-dimensional spatial information systems \nin geology. International Journal of Geographical Information Systems, 3(1), 15-31. \n \nMark, D.M., Lauzon, J.P., Cebrian, J.A., 1989. A review of quadtree-based strategies \nfor interfacing coverage data with Digital Elevation Models in grid form.  \nInternational Journal of Geographical Information Systems, 3(1), 3-14. \n \nMulvany, N. C., 1994. Indexing Books. University of Chicago Press, Chicago, 320pp. \n \nShimomura, Ruth H. (Ed), 1989. GeoRef Thesaurus and Guide to Indexing, 6th edn. \nAmerican Geological Institute, Falls Church, Va. \n \nWorboys, M.F., Hearnshaw, H.M., Maguire, D.J., 1990. Object-oriented data \nmodelling for spatial databases. International Journal of Geographical Information \nSystems, 4(4), 369-383. \n \n6.1 Internet references \n \nAmazon.com, 1996.  Welcome to Amazon.com. http:\/\/www.amazon.com\/ \n \nBiblio Tech Review, 1999.  Information technology for libraries. Z39.50 - Part 1 - an \noverview. http:\/\/www.gadgetserver.com\/bibliotech\/html\/z39_50.html \n \nDCMI, 1998. Dublin Core metadata initiative, home page. http:\/\/purl.oclc.org\/dc\/ \n \nD-Lib, 1995. D-Lib Magazine. The magazine of digital library research. Corporation \nfor National Research Initiatives, Reston, Virginia. http:\/\/www.dlib.org \n \nInstitute for Scientific Information, 1999. Home page with information on ISI citation \ndatabases. http:\/\/www.isinet.com\/ \n \nLibrary of Congress, 1999. The Library of Congress standards. \nhttp:\/\/lcweb.loc.gov\/loc\/standards\/ \n \nNISS, 1999. Library OPACs in HE [Higher Education in UK].   \nhttp:\/\/www.niss.ac.uk\/lis\/opacs.html \n \nNLfB, 1999. Die Bohrdatenbank von Niedersachsen (in German).  \nhttp:\/\/www.bgr.de\/z6\/index.html \n \nNetscape Communications Corporation, 1997. White paper - CORBA: catching the \nnext wave. http:\/\/developer.netscape.com\/docs\/wpapers\/corba\/index.html \n \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \nLoudon, T.V., 2000. Geoscience after IT: Part H (postprint, Computers & Geosciences, 26(3A)) \n \nOMG, 1997. The OMG (Object Management Group, Inc.) home page. \nhttp:\/\/www.omg.org\/ \n \nPOSC, 1997.  POSC Specifications - Epicentre 2.2. Petrotechnical Open Software \nCorporation, Houston, Texas. http:\/\/www.posc.org\/Epicentre.2_2\/SpecViewer.html \n \nPOSC, 1999. POSC Specifications - Epicentre 2.2, upgrade to version 2.2.2. \nPetrotechnical Open Software Corporation, Houston, Texas. http:\/\/www.posc.org\/ \n \n \n \n \n \n \nDisclaimer: The views expressed by the author are not necessarily those of the British \nGeological Survey or any other organization. I thank those providing examples, but should \npoint out that the mention of proprietary products does not imply a recommendation or \nendorsement of the product. \n \n<<<Back to Table of Contents       \nOn to Part I: A view of the conventional geoscience information system>>> \n"}