{"doi":"10.1017\/S0261444800014464","coreId":"71410","oai":"oai:eprints.lancs.ac.uk:1036","identifiers":["oai:eprints.lancs.ac.uk:1036","10.1017\/S0261444800014464"],"title":"Language testing and assessment (Part 1):state-of-the-art review","authors":["Alderson, J. Charles","Banerjee, Jayanti"],"enrichments":{"references":[{"id":16366031,"title":"(2001).Towards authenticity of task in test development.","authors":[],"date":null,"doi":"10.1177\/026553220101800205","raw":"Wu,W. M. & STANSFIELD, C.W. (2001).Towards authenticity of task in test development. Language Testing, 18(2), 187-206.","cites":null},{"id":16365485,"title":"(2001a).Testing is too important to be left to the tester. Paper presented at the 3rd Annual Language Testing Symposium, Dubai, United Arab Emirates.","authors":[],"date":null,"doi":null,"raw":"ALDERSONj. C. (2001a).Testing is too important to be left to the tester. Paper presented at the 3rd Annual Language Testing Symposium, Dubai, United Arab Emirates.","cites":null},{"id":16365643,"title":"A Common European Framework of reference for learning, teaching and assessment. Cambridge:","authors":[],"date":"2001","doi":null,"raw":"COUNCIL OF EUROPE (2001). A Common European Framework of reference for learning, teaching and assessment. Cambridge: Cambridge University Press.","cites":null},{"id":16365638,"title":"A computerised English language proofing cloze program.","authors":[],"date":"1997","doi":"10.1080\/0958822970100106","raw":"CONIAM, D. (1997). A computerised English language proofing cloze program. Computer-Assisted Language Learning, 10(1), 83-97.","cites":null},{"id":16365951,"title":"A fully automatic homework checking system.","authors":[],"date":"1995","doi":"10.1515\/iral.1995.33.1.35","raw":"SCIARONE.A. G. (1995). A fully automatic homework checking system. IRAL, 33(1), 35^6.","cites":null},{"id":16365985,"title":"A job-relevant listening summary translation exam in Minnan. In","authors":[],"date":"2000","doi":null,"raw":"STANSFIELD, C. W.,WU,W. M. & VAN DER HEIDE, M. (2000). A job-relevant listening summary translation exam in Minnan. In A. J. Kunnan (Ed.), Fairness and validation in language assessment (Studies in Language Testing Series, Vol. 9, 177-200). Cambridge: University of Cambridge Local Examinations Syndicate and Cambridge University Press.","cites":null},{"id":16365876,"title":"A second look at grading and classroom performance: report of a research study.","authors":[],"date":"1993","doi":"10.1111\/j.1540-4781.1993.tb01960.x","raw":"MOELLER,A.J. & RESCHKE, C. (1993). A second look at grading and classroom performance: report of a research study. Modern Language Journal, 77(2), 163-9.","cites":null},{"id":16365781,"title":"Allgemeine Giitekriterien fiir Lernzielkontrollen (Common standards for the control of learning).","authors":[],"date":"1999","doi":null,"raw":"KIEWEG, W (1999). Allgemeine Giitekriterien fiir Lernzielkontrollen (Common standards for the control of learning). Der Fremdsprachliche Unterricht Englisch, 3 7(1), 4\u201411.","cites":null},{"id":16365584,"title":"alternatives in language assessment.","authors":[],"date":"1998","doi":"10.2307\/3587999","raw":"BROWN, J. D. & HUDSON,T. (1998).The alternatives in language assessment. TESOL Quarterly, 32(4), 653-75.","cites":null},{"id":16365699,"title":"An investigation of background knowledge in the assessment of language proficiency. In","authors":[],"date":"1997","doi":null,"raw":"Fox, J., PYCHYL, T. & ZUMBO, B. (1997). An investigation of background knowledge in the assessment of language proficiency. In A. Huhta,V. Kohonen, L. Kurki-Suonio & S. Luoma (Eds.), Current developments and alternatives in language assessment (367-83).Jyvaskyla: University ofjyva'skyla.","cites":null},{"id":16365595,"title":"An oral interview procedure for assessing second language abilities in children.","authors":[],"date":"1995","doi":"10.1177\/026553229501200202","raw":"CARPENTER, K., FUJII, N. & KATAOKA, H. (1995). An oral interview procedure for assessing second language abilities in children. Language Testing, 12(2), 157-81.","cites":null},{"id":16365680,"title":"Analysing oral proficiency test performance in general and specific-purpose contexts.","authors":[],"date":"1992","doi":"10.1016\/0346-251x(92)90043-3","raw":"DOUGLAS, D. & SELINKER, L. (1992). Analysing oral proficiency test performance in general and specific-purpose contexts. System, 20(3), 317-28.","cites":null},{"id":16365464,"title":"and teaching: the dream and the reality.","authors":[],"date":"1998","doi":null,"raw":"ALDERSON, J. C. (1998).Testing and teaching: the dream and the reality. novELTy, 5(4), 23-37.","cites":null},{"id":16365742,"title":"Applying ethical standards to portfolio assessment of writing in English as a second language. In M. Milanovic & N. Saville (Eds.), Performance testing, cognition and assessment:","authors":[],"date":"1996","doi":null,"raw":"HAMP-LYONS, L. (1996). Applying ethical standards to portfolio assessment of writing in English as a second language. In M. Milanovic & N. Saville (Eds.), Performance testing, cognition and assessment: Selected papers from the 15th Language Testing Research Colloquium (Studies in LanguageTesting Series, Vol. 3,151-64). Cambridge: Cambridge University Press.","cites":null},{"id":16365738,"title":"Approaches to alternative assessment.","authors":[],"date":"1995","doi":"10.1017\/s0267190500002695","raw":"HAMAYAN, E. (1995). Approaches to alternative assessment. Annual Review of Applied Linguistics, 15,212-26.","cites":null},{"id":16365912,"title":"as levers for change. In","authors":[],"date":"1988","doi":null,"raw":"PEARSON, I. (1988).Tests as levers for change. In D. Chamberlain & R. Baumgardner (Eds.), ESP in the classroom: Practice and evaluation (Vol. 128, 98-107). London: Modern English Publications.","cites":null},{"id":16365749,"title":"Assessing college writing portfolios: principles for practice, theory, research.","authors":[],"date":"1999","doi":null,"raw":"HAMP-LYONS, L. & CONDON, W (1999). Assessing college writing portfolios: principles for practice, theory, research. Cresskill, NJ: Hampton Press.","cites":null},{"id":16365973,"title":"Assessing integrated language and content instruction.","authors":[],"date":"1993","doi":"10.2307\/3587399","raw":"SHORT, D. (1993). Assessing integrated language and content instruction. TESOL Quarterly, 27(4), 627-56.","cites":null},{"id":16365988,"title":"Assessing language skills for specific purposes: describing and analysing the 'behaviour domain'. In","authors":[],"date":"2001","doi":null,"raw":"TARONE, E. (2001). Assessing language skills for specific purposes: describing and analysing the 'behaviour domain'. In C. Elder, A. Brown, E. Grove, K. Hill, N. Iwashita.T. Lumley,T. F. McNamara & K. O'Loughlin (Eds.), Experimenting with uncertainty: essays in honour of Alan Davies (Studies in Language Testing Series, Vol. 11, 53-60). Cambridge: University of Cambridge Local Examinations Syndicate and Cambridge University Press.","cites":null},{"id":16365667,"title":"Assessing languages for specific purposes. Cambridge:","authors":[],"date":"2000","doi":"10.1017\/cbo9780511732911","raw":"DOUGLAS, D. (2000). Assessing languages for specific purposes. Cambridge: Cambridge University Press.","cites":null},{"id":16365482,"title":"Assessing reading. Cambridge:","authors":[],"date":"2000","doi":"10.1017\/cbo9780511732935","raw":"ALDERSON, J. C. (2000d). Assessing reading. Cambridge: Cambridge University Press.","cites":null},{"id":16365830,"title":"Assessing students' oral language: one school district's response.","authors":[],"date":"1995","doi":"10.1111\/j.1944-9720.1995.tb00771.x","raw":"MANLEY, J. H. (1995). Assessing students' oral language: one school district's response. Foreign Language Annals, 28(1), 93-102.","cites":null},{"id":16365694,"title":"Assessing the language proficiency of teachers: are there any border controls?","authors":[],"date":"2001","doi":"10.1177\/026553220101800203","raw":"ELDER, C. (2001). Assessing the language proficiency of teachers: are there any border controls? LanguageTesting, 18(2), 149-70.","cites":null},{"id":16366026,"title":"Assessing young learners: what makes a good test? Paper presented at the Association of Language Testers","authors":[],"date":"2001","doi":null,"raw":"WILSON, J. (2001). Assessing young learners: what makes a good test? Paper presented at the Association of Language Testers in Europe (ALTE) Conference, Barcelona, 5-7 July 2001.","cites":null},{"id":16365784,"title":"Assessment and ESL.","authors":[],"date":"1995","doi":null,"raw":"LAW, B. & ECKES, M. (1995). Assessment and ESL. Winnipeg, Canada: Peguis.","cites":null},{"id":16365565,"title":"Assessment and reporting in language learning programs: Purposes, problems and pitfalls.","authors":[],"date":"1995","doi":null,"raw":"BRINDLEY, G. (1995). Assessment and reporting in language learning programs: Purposes, problems and pitfalls. Plenary presentation at the International Conference on Testing and Evaluation in Second Language Education, Hong Kong University of Science and Technology, 21-24 June 1995.","cites":null},{"id":16365632,"title":"Assessment and testing.","authors":[],"date":"2000","doi":"10.1017\/s0267190500200093","raw":"CLAPHAM, C. (2000b). Assessment and testing. Annual Review of Applied Linguistics, 20,147-61.","cites":null},{"id":16365629,"title":"Assessment for academic purposes: where next?","authors":[],"date":"2000","doi":"10.1016\/s0346-251x(00)00034-8","raw":"CLAPHAM, C. (2000a). Assessment for academic purposes: where next? System, 28,511-21.","cites":null},{"id":16365701,"title":"Assessment in English for Academic Purposes: putting content validity in its place.","authors":[],"date":"1999","doi":"10.1093\/applin\/20.2.221","raw":"FULCHER, G. (1999a). Assessment in English for Academic Purposes: putting content validity in its place. Applied Linguistics, 20(2), 221-36.","cites":null},{"id":16365546,"title":"Assessment in German at KS3: how can it be consistent, fair and appropriate? Deutsch: Lehren und Lernen,","authors":[],"date":"1998","doi":null,"raw":"BARNES, A. & POMFRETT, G. (1998). Assessment in German at KS3: how can it be consistent, fair and appropriate? Deutsch: Lehren und Lernen, 17,2-6.","cites":null},{"id":16365687,"title":"assessment of a foreign language at the end of primary (elementary) education.","authors":[],"date":"2000","doi":"10.1177\/026553220001700203","raw":"EDELENBOS, P. &VlNJE, M. P. (2000).The assessment of a foreign language at the end of primary (elementary) education. LanguageTesting, 17(2), 144-62.","cites":null},{"id":16365977,"title":"assessment of language proficiency in bilingual children: An analysis of theories and instrumentation.","authors":[],"date":"1981","doi":null,"raw":"STANSFIELD, C.W. (1981).The assessment of language proficiency in bilingual children: An analysis of theories and instrumentation. In R.V Padilla (Ed.), Bilingual education and technology.","cites":null},{"id":16365663,"title":"Assessment of language proficiency in the perspective of the 21st century.","authors":[],"date":"1992","doi":"10.1075\/z.62","raw":"DE JONG,J. H. A. L. (1992). Assessment of language proficiency in the perspective of the 21st century. AILA Review, 9,39-45.","cites":null},{"id":16365794,"title":"Authenticity in language testing: some outstanding questions.","authors":[],"date":"2000","doi":"10.1177\/026553220001700102","raw":"LEWKOWICZ, J. A. (2000). Authenticity in language testing: some outstanding questions. Language Testing, 17(1), 43-64.","cites":null},{"id":16365815,"title":"Authenticity of discourse in a specific purpose test.","authors":[],"date":"1998","doi":null,"raw":"LUMLEY, T. & BROWN, A. (1998). Authenticity of discourse in a specific purpose test. In E. Li & G.James (Eds.), Testing and evaluation in second language education (22-33). Hong Kong: The Language Centre.The University of Science and Technology.","cites":null},{"id":16365921,"title":"Autonomy and individualisation in language learning: institutional implications.","authors":[],"date":"1988","doi":null,"raw":"PUGSLEY,J. (1988). Autonomy and individualisation in language learning: institutional implications. ELT Documents, 131, 54-61.","cites":null},{"id":16365765,"title":"Backwash and TOEFL","authors":[],"date":"1993","doi":null,"raw":"HUGHES, A. (1993). Backwash and TOEFL 2000. Unpublished manuscript, University of Reading.","cites":null},{"id":16365653,"title":"Bilingualism and special education: Issues in assessment and pedagogy.","authors":[],"date":"1984","doi":"10.1017\/s0272263100007373","raw":"CUMMINS, J. (1984a). Bilingualism and special education: Issues in assessment and pedagogy. Clevedon, England: Multilingual Matters.","cites":null},{"id":16365651,"title":"Changing definitions of language proficiency: functions of language assessment in educational programmes for recent immigrant learners of English in Canada.","authors":[],"date":"1995","doi":null,"raw":"CUMMING, A. (1995). Changing definitions of language proficiency: functions of language assessment in educational programmes for recent immigrant learners of English in Canada. Journal of the CAAL, J 7(1), 35-48.","cites":null},{"id":16365763,"title":"changing face of child language assessment: 1985-1995. Child Language Teaching and Therapy,","authors":[],"date":"1995","doi":"10.1177\/026565909501100103","raw":"HOWARD, S., HARTLEYJ. & MUELLER, D. (1995).The changing face of child language assessment: 1985-1995. Child Language Teaching and Therapy, 11(1), 7-22.","cites":null},{"id":16365786,"title":"Classroom-based assessment - why and how?","authors":[],"date":"1989","doi":null,"raw":"LEE, B. (1989). Classroom-based assessment - why and how? British Journal of Language Teaching, 27(2), 73\u20146.","cites":null},{"id":16365713,"title":"Classroom-based assessment. In F. Genesee (Ed.), Educating second language children. Cambridge:","authors":[],"date":"1994","doi":null,"raw":"GENESEE, F. & HAMAYAN, E.V. (1994). Classroom-based assessment. In F. Genesee (Ed.), Educating second language children. Cambridge: Cambridge University Press.","cites":null},{"id":16365769,"title":"Code of practice for foreign\/ second language testing.","authors":[],"date":"1997","doi":null,"raw":"ILTA - INTERNATIONAL LANGUAGE TESTING ASSOCIATION (1997). Code of practice for foreign\/ second language testing.","cites":null},{"id":16365766,"title":"Combined assessment model for EAP writing workshop: portfolio decision-making, criterionreferenced grading and contract negotiation.","authors":[],"date":"1996","doi":null,"raw":"HUGHES WILHELM, K. (1996). Combined assessment model for EAP writing workshop: portfolio decision-making, criterionreferenced grading and contract negotiation. TESL Canada Journal, 14(1), 21-33.","cites":null},{"id":16365872,"title":"Comparing language qualifications in different languages: a framework and code of practice.","authors":[],"date":"1995","doi":"10.1016\/0346-251x(96)81094-3","raw":"MILANOVIC, M. (1995). Comparing language qualifications in different languages: a framework and code of practice. System, 23(4), 467-79.","cites":null},{"id":16365607,"title":"Computeradaptive testing in second language contexts.","authors":[],"date":"1999","doi":"10.1017\/s0267190599190147","raw":"CHALHOUB-DEVILLE, M. & DEVILLE, C. (1999). Computeradaptive testing in second language contexts. Annual Review of Applied Linguistics, 19,273-99.","cites":null},{"id":16365705,"title":"Computerising an English language placement test.","authors":[],"date":"1999","doi":"10.1093\/elt\/53.4.289","raw":"FULCHER, G. (1999b). Computerising an English language placement test. ELTJournal, 53(4), 289-99.","cites":null},{"id":16365715,"title":"Computers and language testing: a harmonious relationship? Francophonie,","authors":[],"date":"1997","doi":null,"raw":"GERVAIS, C. (1997). Computers and language testing: a harmonious relationship? Francophonie, 16,3-7.","cites":null},{"id":16365580,"title":"Computers in language testing: present research and some future directions.","authors":[],"date":"1997","doi":null,"raw":"BROWN, J. D. (1997). Computers in language testing: present research and some future directions. Language Learning and Technology, 3(1), 44-59.","cites":null},{"id":16365441,"title":"Computers in language testing.","authors":[],"date":"1986","doi":"10.1177\/026553229701400101","raw":"ALDERSON, J. C. (1986a). Computers in language testing. In G. N. Leech & C. N. Candlin (Eds.), Computers in English language education and research (pp. 99-111). London: Longman.","cites":null},{"id":16365683,"title":"Considerations in developing or using second\/foreign language proficiency computer-adaptive tests.","authors":[],"date":"1999","doi":null,"raw":"DUNKEL, P. (1999). Considerations in developing or using second\/foreign language proficiency computer-adaptive tests. Language Learning and Technology, 2(2), 77\u201493.","cites":null},{"id":16365521,"title":"construct validation of self-ratings of communicative language ability.","authors":[],"date":"1989","doi":"10.1177\/026553228900600104","raw":"BACHMAN, L. F. & PALMER, A. S. (1989).The construct validation of self-ratings of communicative language ability. Language Testing, 6(1), 14-29.","cites":null},{"id":16365776,"title":"Context-sensitive assessment of modern languages in primary (elementary) and early secondary education: Scotland and the European experience.","authors":[],"date":"2000","doi":"10.1177\/026553220001700202","raw":"JOHNSTONE, R. (2000). Context-sensitive assessment of modern languages in primary (elementary) and early secondary education: Scotland and the European experience. Language Testing, 17(2), 123-43.","cites":null},{"id":16365891,"title":"Covert language assessment in academic writing.","authors":[],"date":"1997","doi":"10.1177\/026553229701400305","raw":"NORTON, B. & STARFIELD, S. (1997). Covert language assessment in academic writing. Language Testing, 14(3), 278\u201494.","cites":null},{"id":16365610,"title":"Criteria for oral assessment.","authors":[],"date":"1992","doi":"10.1080\/09571739285200331","raw":"CHAMBERS, F. & RICHARDS, B. (1992). Criteria for oral assessment. Latiguage Learning Journal, 6, 5-9.","cites":null},{"id":16365826,"title":"Criterion-referenced test development: linking curricula, teachers and tests.","authors":[],"date":"1994","doi":"10.2307\/3587557","raw":"LYNCH, B. & DAVIDSON, F. (1994). Criterion-referenced test development: linking curricula, teachers and tests. TESOL Quarterly, 28(4), 727-43.","cites":null},{"id":16365967,"title":"Critical language testing and beyond, plenary paper presented at the American Association for Applied Linguistics,","authors":[],"date":"1997","doi":null,"raw":"SHOHAMY, E. (1997b). Critical language testing and beyond, plenary paper presented at the American Association for Applied Linguistics, Orlando, Florida. 8-11 March.","cites":null},{"id":16365768,"title":"Deficiency and development.","authors":[],"date":"1990","doi":null,"raw":"HURMANJ. (1990). Deficiency and development. Francophonie, 1, 8-12.","cites":null},{"id":16365661,"title":"Demands of being professional in language testing.","authors":[],"date":"1997","doi":"10.1177\/026553229701400309","raw":"DAVIES, A. (1997). Demands of being professional in language testing. Language Testing, 14(3), 328-39.","cites":null},{"id":16365971,"title":"Democratic assessment as an alternative.","authors":[],"date":"2001","doi":"10.1177\/026553220101800404","raw":"SHOHAMY, E. (2001b). Democratic assessment as an alternative. Language Testing, 18(4), 373-92.","cites":null},{"id":16365771,"title":"Design and evaluation of a computer-based TOEFL tutorial.","authors":[],"date":"1998","doi":"10.1016\/s0346-251x(98)00034-7","raw":"Code of Ethics. [http:\/\/www.surrey.ac.uk\/ELI\/ltrfile\/ltrframe.html] JAMIESON, J., TAYLOR, C, KIRSCH, I. & EIGNOR, D. (1998). Design and evaluation of a computer-based TOEFL tutorial. System, 26(4), 485-513.","cites":null},{"id":16365633,"title":"Designing an ability scale for English across the range of secondary school forms. Hong Kong Papers in Linguistics and Language Teaching,","authors":[],"date":"1994","doi":"10.1177\/026553229501200203","raw":"CONIAM, D. (1994). Designing an ability scale for English across the range of secondary school forms. Hong Kong Papers in Linguistics and Language Teaching, 17,55-61.","cites":null},{"id":16365904,"title":"Development and implementation of student portfolios in foreign language programs.","authors":[],"date":"1996","doi":"10.1111\/j.1944-9720.1996.tb01254.x","raw":"PADILLA.A. M., ANINAO.J. C. & SUNG, H. (1996). Development and implementation of student portfolios in foreign language programs. Foreign Language Annals, 29(3), 429-38.","cites":null},{"id":16365665,"title":"Developments in language testing.","authors":[],"date":"1995","doi":"10.1017\/s0267190500002671","raw":"DOUGLAS, D. (1995). Developments in language testing. Animal Review of Applied Linguistics, 15,167-87.","cites":null},{"id":16365543,"title":"Dictionary use in the teaching and examining of MFLs at GCSE.","authors":[],"date":"1999","doi":"10.1080\/09571739985200051","raw":"BARNES, A., HUNT, M. & POWELL, B. (1999). Dictionary use in the teaching and examining of MFLs at GCSE. Language Learning Journal, 19,19-27.","cites":null},{"id":16365882,"title":"Die Problematik objektiver Leistungsmessung in einem kommunikativen Fremdsprachenunterricht: am Beispiel des Franzosischen.","authors":[],"date":"1993","doi":null,"raw":"MUNDZECK, F. (1993). Die Problematik objektiver Leistungsmessung in einem kommunikativen Fremdsprachenunterricht: am Beispiel des Franzosischen. Fremdsprachenunterricht, 46($), 449-54.","cites":null},{"id":16365760,"title":"Diffusion of innovations in English language teaching:","authors":[],"date":"1989","doi":"10.2307\/329862","raw":"HENRICHSEN, L. E. (1989). Diffusion of innovations in English language teaching: The ELEC effort in fapan, 1956-1968. New York: Greenwood Press.","cites":null},{"id":16365456,"title":"Do corpora have a role in language assessment? In","authors":[],"date":"1996","doi":null,"raw":"ALDERSON, J. C. (1996). Do corpora have a role in language assessment? In J. Thomas & M. Short (Eds.), Using corpora for language research (pp. 248\u201459). Harlow:Longman.","cites":null},{"id":16365924,"title":"doctors' written communicative competence: an experimental technique in English for specialist purposes. Quantitative Linguistics,","authors":[],"date":"1987","doi":null,"raw":"REA-DICKINS, P. (1987).Testing doctors' written communicative competence: an experimental technique in English for specialist purposes. Quantitative Linguistics, 34,185-218.","cites":null},{"id":16366015,"title":"Does Grammar-Translation come from the Entrance Examination? Preliminary findings from classroombased research.","authors":[],"date":"1996","doi":"10.1177\/026553229601300306","raw":"WATANABE,Y. (1996). Does Grammar-Translation come from the Entrance Examination? Preliminary findings from classroombased research. LanguageTesting, 13(3), 319-33.","cites":null},{"id":16365648,"title":"Does language assessment facilitate recent immigrants' participation in Canadian society?","authors":[],"date":"1994","doi":null,"raw":"CUMMING, A. (1994). Does language assessment facilitate recent immigrants' participation in Canadian society? TESL Canada Journal, 11 (2), 117-33.","cites":null},{"id":16366018,"title":"Does the university entrance examination motivate learners? A case study of learner interviews.","authors":[],"date":"2001","doi":null,"raw":"WATANABE,Y. (2001). Does the university entrance examination motivate learners? A case study of learner interviews. Akita Association of English Studies (ed.). Trans-equator exchanges: A collection of acadmic papers in honour of Professor David Ingram, 100-10.","cites":null},{"id":16365514,"title":"Does washback exist?","authors":[],"date":"1993","doi":"10.1093\/applin\/14.2.115","raw":"ALDERSON, J. C. & WALL, D. (1993). Does washback exist? Applied Linguistics, 14(2), 115-29. ALTE (1998)MLTJ5 handbook ojEuropean examinations and examination systems. Cambridge: UCLES.","cites":null},{"id":16365860,"title":"effect of interlocutor and assessment mode variables in overseas assessments of speaking skills in occupational settings.","authors":[],"date":"1997","doi":"10.1177\/026553229701400202","raw":"McNAMARA.T. F. & LUMLEY,T. (1997).The effect of interlocutor and assessment mode variables in overseas assessments of speaking skills in occupational settings. Language Testing, 14(2), 140-56.","cites":null},{"id":16366028,"title":"Effect of semantic inconsistency on sentence grammaticality judgements for children with and without language-learning disabilities.","authors":[],"date":"1999","doi":"10.1177\/026553229901600304","raw":"WINDSORJ. (1999). Effect of semantic inconsistency on sentence grammaticality judgements for children with and without language-learning disabilities. LanguageTesting, 16(3), 293-313.","cites":null},{"id":16365788,"title":"English as an additional language within the National Curriculum: A study of assessment practices.","authors":[],"date":"1996","doi":null,"raw":"LEUNG, C. &TEASDALE, A. (1996). English as an additional language within the National Curriculum: A study of assessment practices. Prospect, 12(2), 58-68.","cites":null},{"id":16365497,"title":"English language education in Hungary, Part II: Examining Hungarian learners' achievements in English. Budapest: The British Council.","authors":[],"date":"2000","doi":null,"raw":"ALDERSONJ. C, NAGY, E. & OVEGES^E. (Eds.) (2000a). English language education in Hungary, Part II: Examining Hungarian learners' achievements in English. Budapest: The British Council.","cites":null},{"id":16365696,"title":"English language education in Hungary: A baseline study. Budapest: The British Council.","authors":[],"date":"1999","doi":null,"raw":"FEKETE, H., MAJOR, E. & NIKOLOV, M. (Eds.) (1999). English language education in Hungary: A baseline study. Budapest: The British Council.","cites":null},{"id":16365844,"title":"ESL bandscales, NLLIA ESL development: language and literacy in schools project. Canberra: National Languages and Literacy Institute of Australia.","authors":[],"date":"1994","doi":null,"raw":"MCKAY, P., HUDSON, C. & SAPUPPO, M. (1994). ESL bandscales, NLLIA ESL development: language and literacy in schools project. Canberra: National Languages and Literacy Institute of Australia.","cites":null},{"id":16365517,"title":"ESL Scales.","authors":[],"date":"1994","doi":null,"raw":"AUSTRALIAN EDUCATION COUNCIL (1994). ESL Scales. Melbourne: Curriculum Corporation.","cites":null},{"id":16365598,"title":"ESU Framework: Performance scales for English language examinations.","authors":[],"date":"1989","doi":null,"raw":"CARROLL, B. J. & WEST, R. (1989). ESU Framework: Performance scales for English language examinations. Harlow: Longman.","cites":null},{"id":16365460,"title":"Ethics and language testing. Paper presented at the annualTESOL Convention,","authors":[],"date":"1997","doi":null,"raw":"ALDERSON,J. C. (1997). Ethics and language testing. Paper presented at the annualTESOL Convention, Orlando, Florida.","cites":null},{"id":16365746,"title":"Ethics in language testing.","authors":[],"date":"1998","doi":"10.1007\/978-1-4020-4489-2_29","raw":"HAMP-LYONS, L. (1998). Ethics in language testing. In C. M. Clapham & D. Corson (Eds.), Language testing and assessment (Vol. 7). Dordrecht, The Netherlands: Kluwer Academic Publishing.","cites":null},{"id":16365976,"title":"ethics of gatekeeping tests: what have we learned in a hundred years?","authors":[],"date":"1997","doi":"10.1177\/026553229701400302","raw":"Language Teaching, 1-13. SPOLSKY,B. (1997).The ethics of gatekeeping tests: what have we learned in a hundred years? LanguageTesting, 14(3), 242-7.","cites":null},{"id":16365777,"title":"EUROCERT: an international standard for certification of language proficiency.","authors":[],"date":"1990","doi":null,"raw":"KALTER, A. O. & VOSSEN, P. W. J. E. (1990). EUROCERT: an international standard for certification of language proficiency. AILA Review, 7,91-106.","cites":null},{"id":16365805,"title":"Evaluating foreign languages in Scottish primary schools: report to Scottish Office. Stirling:","authors":[],"date":"1993","doi":null,"raw":"Low, L., DUFFIELD, J., BROWN, S. & JOHNSTONE, R. (1993). Evaluating foreign languages in Scottish primary schools: report to Scottish Office. Stirling: University of Stirling: Scottish CILT.","cites":null},{"id":16366020,"title":"Evaluation in ELT.","authors":[],"date":"1994","doi":null,"raw":"WEIR, C. J. & ROBERTS, J. (1994). Evaluation in ELT. Oxford: Blackwell Publishers.","cites":null},{"id":16365796,"title":"Evaluation, a way of involving the learner. In","authors":[],"date":"1985","doi":null,"raw":"LEWKOWICZ, J. A., & MOON, J. (1985). Evaluation, a way of involving the learner. In J. C. Alderson (Ed.), Lancaster Practical 234http:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9 Language testing and assessment (Part 1) Papers in English Language Education (Vol. 6: Evaluation), 45-80. Oxford: Pergamon Press.","cites":null},{"id":16365778,"title":"Examinations as instruments for educational change: Investigating the washback effect of the Nepalese English exams.","authors":[],"date":"1990","doi":null,"raw":"KHANIYAH.T. R. (1990a). Examinations as instruments for educational change: Investigating the washback effect of the Nepalese English exams. Unpublished PhD dissertation, University of Edinburgh, Edinburgh.","cites":null},{"id":16365990,"title":"Examining the relationship between computer familiarity and performance on computer-based language tasks.","authors":[],"date":"1999","doi":"10.1111\/0023-8333.00088","raw":"TAYLOR, C, KIRSCH, I., EIGNOR, D. & JAMIESON, J. (1999). Examining the relationship between computer familiarity and performance on computer-based language tasks. Language Learning, 49(2), 219-74.","cites":null},{"id":16365954,"title":"Examining validity in a performance test: the listening summary translation exam (LSTE). Language Testing,","authors":[],"date":"1996","doi":"10.1177\/026553229601300106","raw":"SCOTT, M. L., STANSFIELD, C. W. & KENYON, D. M. (1996). Examining validity in a performance test: the listening summary translation exam (LSTE). Language Testing, 13,83-109.","cites":null},{"id":16366013,"title":"Examining washback: The Sri Lankan impact study.","authors":[],"date":"1993","doi":"10.1177\/026553229301000103","raw":"WALL, D. & ALDERSON, J. C. (1993). Examining washback: The Sri Lankan impact study. LanguageTesting, 10(1), 41-69.","cites":null},{"id":16365822,"title":"Exemplar assessment activities.","authors":[],"date":"1993","doi":null,"raw":"LUMLEY, T., RASO, E. & MINCHAM, L. (1993). Exemplar assessment activities. In NLLIA (Ed.), NLLIA ESL Development: Language and Literacy in Schools. Canberra: National Languages and Literacy Institute of Australia.","cites":null},{"id":16365474,"title":"Exploding myths: Does the number of hours per week matter? novELTy,","authors":[],"date":"2000","doi":null,"raw":"ALDERSON, J. C. (2000b). Exploding myths: Does the number of hours per week matter? novELTy, 7(1), 17-32.","cites":null},{"id":16365560,"title":"Foreign languages in primary and preschool education: context and outcomes. A review of recent research within the European Union.","authors":[],"date":"1998","doi":null,"raw":"BLONDIN, C, CANDELIER, M., EDELENBOS, P., JOHNSTONE, R., KUBANEK-GERMAN, A. & TAESCHNER, T. (1998). Foreign languages in primary and preschool education: context and outcomes. A review of recent research within the European Union. London: CILT.","cites":null},{"id":16365885,"title":"Foreign languages in the National Curriculum \u2014 what to teach and how to test? A proposal for the Languages Task Group.","authors":[],"date":"1989","doi":null,"raw":"NEIL, D. (1989). Foreign languages in the National Curriculum \u2014 what to teach and how to test? A proposal for the Languages Task Group. Modern Languages, 70(1), 5\u20149.","cites":null},{"id":16365711,"title":"Formative assessment in ELT primary 233http:\/\/journals.cambridge.org Downloaded: 26","authors":[],"date":"2000","doi":null,"raw":"GATTULLO, F. (2000). Formative assessment in ELT primary 233http:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9 Language testing and assessment (Part 1) (elementary) classrooms: an Italian case study. Language Testing, 77(2), 278-88.","cites":null},{"id":16366033,"title":"From conventional to computer-adaptive testing of ESL reading comprehension.","authors":[],"date":"1996","doi":"10.1016\/0346-251x(95)00051-k","raw":"YOUNG, R., SHERMIS, M. D, BRUTTEN, S. R. & PERKINS, K. (1996). From conventional to computer-adaptive testing of ESL reading comprehension. System, 24(1), 23-40.","cites":null},{"id":16365641,"title":"From text to test, automatically - an evaluation of a computer cloze-test generator.","authors":[],"date":"1998","doi":null,"raw":"CONIAM, D. (1998). From text to test, automatically - an evaluation of a computer cloze-test generator. Hong Kong Journal of Applied Linguistics, 3(1), 41-60.","cites":null},{"id":16365733,"title":"Grading classroom oral activities: effects on motivation and proficiency.","authors":[],"date":"1989","doi":"10.1111\/j.1944-9720.1989.tb02743.x","raw":"HAHN, S., STASSEN,T. & DESCHKE, C. (1989). Grading classroom oral activities: effects on motivation and proficiency. Foreign Language Annals, 22(3), 241-52.","cites":null},{"id":16365625,"title":"How does washback influence teaching? Implications for Hong Kong.","authors":[],"date":"1997","doi":"10.1080\/09500789708666717","raw":"CHENG, L. (1997). How does washback influence teaching? Implications for Hong Kong. Language and Education 11(1), 38-54.","cites":null},{"id":16365707,"title":"I didn't get the grade I need.Where's my solicitor?","authors":[],"date":"1996","doi":"10.1016\/s0346-251x(96)00040-1","raw":"FULCHER, G. & BAMFORD, R. (1996). I didn't get the grade I need.Where's my solicitor? System, 24(4), 437-48.","cites":null},{"id":16365762,"title":"Identification and differential diagnosis of phonological disorder in bilingual children.","authors":[],"date":"1999","doi":"10.1177\/026553229901600303","raw":"HOLM, A., DODD, B., STOW, C. & PERT, S. (1999). Identification and differential diagnosis of phonological disorder in bilingual children. LanguageTesting, 16(3), 271-92.","cites":null},{"id":16365536,"title":"ILTA language testing bibliography 1990-1999, First edition.","authors":[],"date":"1999","doi":null,"raw":"BANERJEE, J., CLAPHAM, C, CLAPHAM, P. & WALL, D. (Eds.) (1999). ILTA language testing bibliography 1990-1999, First edition. Lancaster, UK: Language Testing Update.","cites":null},{"id":16365824,"title":"In search of the ethical test.","authors":[],"date":"1997","doi":"10.1177\/026553229701400308","raw":"LYNCH, B. (1997). In search of the ethical test. Language Testing, 14(3), 315-27.","cites":null},{"id":16365880,"title":"in the IELTS academic module writing test: a comparative study of task 2 items and university assignments.","authors":[],"date":"1999","doi":null,"raw":"MOORE.T. & MORTONJ. (1999).Authenticity in the IELTS academic module writing test: a comparative study of task 2 items and university assignments. In R.Tulloh (Ed.), IELTS Research Reports 1999 (Vol. 2, 64-106). Canberra: IELTS Australia Pty Limited.","cites":null},{"id":16365444,"title":"Innovations in language testing?","authors":[],"date":"1986","doi":null,"raw":"ALDERSON, J. C. (1986b). Innovations in language testing? In M.","cites":null},{"id":16365576,"title":"Interviewer variability in specific-purpose language performance tests. In","authors":[],"date":"1997","doi":null,"raw":"BROWN.A. & LUMLEY,T. (1997). Interviewer variability in specific-purpose language performance tests. In A. Huhta, V. Kohonen, L. Kurki-Suonio & S. Luoma (Eds.), Current developments and alternatives in language assessment (137-50).Jyvaskyla: Centre for Applied Language Studies, University of Jyvaskyla.","cites":null},{"id":16366007,"title":"Introducing new tests into traditional systems: Insights from general education and from innovation theory.","authors":[],"date":"1996","doi":"10.1177\/026553229601300307","raw":"WALL, D. (1996). Introducing new tests into traditional systems: Insights from general education and from innovation theory. LanguageTesting, 13(3),334-54.","cites":null},{"id":16365792,"title":"Investigating authenticity in language testing.","authors":[],"date":"1997","doi":"10.1177\/026553220001700102","raw":"LEWKOWICZ, J. A. (1997). Investigating authenticity in language testing. Unpublished PhD dissertation, Lancaster University, Lancaster.","cites":null},{"id":16365586,"title":"IP address: 194.80.32.9 Language testing and assessment","authors":[],"date":"2009","doi":null,"raw":"235http:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9 Language testing and assessment (Part 1) ROY, M.-J. (1988). Writing in the GCSE - modern languages. British Journal of Language Teaching, 26(2), 99-102.","cites":null},{"id":16365447,"title":"IP address: 194.80.32.9 Language testing and assessment (Part 1) Portal (Ed.), Innovations in language testing","authors":[],"date":"2009","doi":null,"raw":"231http:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9 Language testing and assessment (Part 1) Portal (Ed.), Innovations in language testing (pp. 93-105). Windsor: NFER\/Nelson.","cites":null},{"id":16365562,"title":"Joint declaration of the European Ministers of Education convened in Bologna on the 19th of","authors":[],"date":"1999","doi":null,"raw":"BOLOGNA DECLARATION (1999) Joint declaration of the European Ministers of Education convened in Bologna on the 19th of June 1999. http:\/\/europa.eu.int\/comm\/education\/ socrates\/erasmus\/bologna.pdf BOYLE, J., GlLLHAM, B. & SMITH, N. (1996). Screening for early language delay in the 18-36 month age-range: the predictive validity of tests of production and implications for practice. Child Language Teaching and Tlierapy, 12(2), 113-27.","cites":null},{"id":16365729,"title":"L'evaluation ministerielle en classe de seconde en anglais. Les Langues Modernes,","authors":[],"date":"1997","doi":null,"raw":"GuiLLON, M. (1997). L'evaluation ministerielle en classe de seconde en anglais. Les Langues Modernes, 2,32-39.","cites":null},{"id":16365573,"title":"Language background and item difficulty: the development of a computer-adaptive test ofjapanese.","authors":[],"date":"1996","doi":"10.1016\/0346-251x(96)00004-8","raw":"BROWN, A. & IWASHITA, N. (1996). Language background and item difficulty: the development of a computer-adaptive test ofjapanese. System, 24(2), 199-206.","cites":null},{"id":16366023,"title":"Language examinations in Dutch secondary schools from","authors":[],"date":"1999","doi":null,"raw":"WELLING-SLOOTMAEKERS, M. (1999). Language examinations in Dutch secondary schools from 2000 onwards. Levende Talen, 542,488-90.","cites":null},{"id":16365677,"title":"Language for Specific Purposes assessment criteria: where do they come from?","authors":[],"date":"2001","doi":"10.1177\/026553220101800204","raw":"DOUGLAS, D. (2001b). Language for Specific Purposes assessment criteria: where do they come from? Language Testing, 18(2), 171-85.","cites":null},{"id":16365666,"title":"Language for specific purposes testing.","authors":[],"date":"1997","doi":"10.1007\/978-1-4020-4489-2_11","raw":"DOUGLAS, D. (1997). Language for specific purposes testing. In C. Clapham & D. Corson (Eds.), Language testing and assessment (Vol. 7, 111-19). Dordrecht, The Netherlands: Kluwer Academic Publishers.","cites":null},{"id":16365493,"title":"Language test construction and evaluation. Cambridge:","authors":[],"date":"1995","doi":null,"raw":"ALDERSONJ. C, CLAPHAM, C. & WALL, D. (1995). Language test construction and evaluation. Cambridge: Cambridge University Press.","cites":null},{"id":16365525,"title":"Language testing in practice. Oxford:","authors":[],"date":"1996","doi":"10.2307\/328718","raw":"BACHMAN, L. F. & PALMER,A. S. (1996). Language testing in practice. Oxford: Oxford University Press.","cites":null},{"id":16365719,"title":"Language testing in research and education: the need for standards.","authors":[],"date":"1990","doi":null,"raw":"GROOT, P. J. M. (1990). Language testing in research and education: the need for standards. AILA Review, 7,9-23.","cites":null},{"id":16365453,"title":"Language testing in the 1990s: How far have we got? How much further have we to go?","authors":[],"date":"1991","doi":null,"raw":"ALDERSON, J. C. (1991). Language testing in the 1990s: How far have we got? How much further have we to go? In S. Anivan (Ed.), Current developments in language testing (Vol. 25, pp. 1-26). Singapore: SEAMEO Regional Language Centre.","cites":null},{"id":16365659,"title":"Language testing: survey articles 1 and 2.","authors":[],"date":"1978","doi":null,"raw":"DAVIES, A. (1978). Language testing: survey articles 1 and 2. Language Teaching and Linguistics Abstracts, 11, 145-59 and 215-31.","cites":null},{"id":16365750,"title":"Learner autonomy by remote control.","authors":[],"date":"1994","doi":"10.1016\/0346-251x(94)90002-7","raw":"HARGAN, N. (1994). Learner autonomy by remote control. System, 22(4), 455-62.","cites":null},{"id":16365780,"title":"Leistungsmessung im Fach Englisch: PraktischeVorschlage zur Konzeption von Lernzielkontrollen.","authors":[],"date":"1992","doi":null,"raw":"KlEWEG, W. (1992). Leistungsmessung im Fach Englisch: PraktischeVorschlage zur Konzeption von Lernzielkontrollen. Fremdsprachenunterricht, 45(6), 321-32.","cites":null},{"id":16365472,"title":"Levels of performance. In","authors":[],"date":"2000","doi":null,"raw":"ALDERSON, J. C. (2000a). Levels of performance. In J. C. Alderson, E. Nagy, & E. Oveges (Eds.), English language education in Hungary, Part II: Examining Hungarian learners' achievements in English. Budapest: The British Council.","cites":null},{"id":16365622,"title":"Linguistic and cultural bias in language proficiency tests.","authors":[],"date":"1985","doi":"10.1177\/026553228500200204","raw":"CHEN, Z. & HENNING, G. (1985). Linguistic and cultural bias in language proficiency tests. Language Testing, 2(2), 155-63.","cites":null},{"id":16365982,"title":"Listening Summary Translation Exam (LSTE)","authors":[],"date":"1997","doi":"10.1177\/026553229601300106","raw":"STANSFIELD, C. W, WU, W. M. & Liu, C. C. (1997). Listening Summary Translation Exam (LSTE) in Taiwanese, akak Minnan (Final Project Report. ERIC Document Reproduction Service, ED 413 788). N. Bethesda, MD: Second Language Testing, Inc.","cites":null},{"id":16365979,"title":"Listening summary translation exam (LSTE) \u2014 Spanish (Final Project Report. ERIC Document Reproduction Service, ED 323 786).Washington DC: Centre for Applied Linguistics.","authors":[],"date":"1990","doi":null,"raw":"STANSFIELD, C. W., SCOTT, M. L. & KENYON, D. M. (1990). Listening summary translation exam (LSTE) \u2014 Spanish (Final Project Report. ERIC Document Reproduction Service, ED 323 786).Washington DC: Centre for Applied Linguistics.","cites":null},{"id":16365995,"title":"Managing the assessment process. A framework for measuring student attainment of the ESL standards.","authors":[],"date":"1998","doi":null,"raw":"TESOL (1998). Managing the assessment process. A framework for measuring student attainment of the ESL standards. Alexandria, VA:TESOL.","cites":null},{"id":16365782,"title":"Methodologie devaluation dans des contextes d'apprentissage des langages assistes par des environnements informatiques multimedias. Etudes de Linguistique Appliquee,","authors":[],"date":"1998","doi":null,"raw":"LAURIER, M. (1998). Methodologie devaluation dans des contextes d'apprentissage des langages assistes par des environnements informatiques multimedias. Etudes de Linguistique Appliquee, 110,247-55.","cites":null},{"id":16365965,"title":"methods, testing consequences: are they ethical?","authors":[],"date":"1997","doi":null,"raw":"SriOHAMY, E. (1997a).Testing methods, testing consequences: are they ethical? Language Testing, 14(3), 340-9.","cites":null},{"id":16365855,"title":"Modelling performance: opening Pandora's box.","authors":[],"date":"1995","doi":"10.1093\/applin\/16.2.159","raw":"MCNAMARA, T. F. (1995). Modelling performance: opening Pandora's box. Applied Linguistics, 16(2), 159-75.","cites":null},{"id":16366039,"title":"Monitoring language skills in Austrian primary (elementary) schools: a case study.","authors":[],"date":"2000","doi":"10.1177\/026553220001700208","raw":"ZANGL, R. (2000). Monitoring language skills in Austrian primary (elementary) schools: a case study. Language Testing, 77(2), 250-60.","cites":null},{"id":16365540,"title":"New approaches to evaluation in selfaccess learning (trans, from French). Etudes de Linguistique Appliquee,","authors":[],"date":"1991","doi":null,"raw":"BARBOT, M.-J. (1991). New approaches to evaluation in selfaccess learning (trans, from French). Etudes de Linguistique Appliquee, 79,77-94.","cites":null},{"id":16365555,"title":"New exams in secondary education, new question types. An investigation into the reliability of the evaluation of open-ended questions in foreignlanguage exams.","authors":[],"date":"1999","doi":null,"raw":"BHGEL, K. & LEIJN, M. (1999). New exams in secondary education, new question types. An investigation into the reliability of the evaluation of open-ended questions in foreignlanguage exams. LevendeTalen, 537,173-81.","cites":null},{"id":16365531,"title":"Normative testing and bilingual populations.","authors":[],"date":"1988","doi":"10.1080\/01434632.1988.9994345","raw":"BAKER, C. (1988). Normative testing and bilingual populations. Journal of Multilingual and Multicultural Development, 9(5), 399-409.","cites":null},{"id":16365657,"title":"Norms appropriacy of achievement tests: Spanish-speaking children and English children's norms.","authors":[],"date":"1994","doi":"10.1177\/026553229401100107","raw":"DAVIDSON, F. (1994). Norms appropriacy of achievement tests: Spanish-speaking children and English children's norms. Language Testing, 11(1), 83-95.","cites":null},{"id":16365842,"title":"On ESL standards for school-age learners.","authors":[],"date":"2000","doi":"10.1177\/026553220001700205","raw":"MCKAY, P. (2000). On ESL standards for school-age learners. Language Testing, 17(2), 185-214.","cites":null},{"id":16366003,"title":"Open questions: answers in the foreign language? Toegepaste Taalwetenschap in","authors":[],"date":"1998","doi":"10.1075\/ttwia.58.19elm","raw":"VAN ELMPT, M. & LOONEN, P. (1998). Open questions: answers in the foreign language? Toegepaste Taalwetenschap in Artikelen, 58, 149-54.","cites":null},{"id":16365567,"title":"Outcomes-based assessment and reporting in language learning programmes: a review of the issues.","authors":[],"date":"1998","doi":"10.1177\/026553229801500103","raw":"BRINDLEY, A. (1998). Outcomes-based assessment and reporting in language learning programmes: a review of the issues. LanguageTesting, 35(1),45-85.","cites":null},{"id":16365569,"title":"Outcomes-based assessment in practice: some examples and emerging insights.","authors":[],"date":"2001","doi":"10.1177\/026553220101800405","raw":"BRINDLEY, G. (2001). Outcomes-based assessment in practice: some examples and emerging insights. Language Testing, 18(4), 393-407.","cites":null},{"id":16365828,"title":"Peer evaluation in practice.","authors":[],"date":"1988","doi":null,"raw":"LYNCH, T. (1988). Peer evaluation in practice. ELT Documents, 131,119-25.","cites":null},{"id":16365809,"title":"Perceptions of language-trained raters and occupational experts in a test of occupational English language proficiency. English for Specific Purposes,","authors":[],"date":"1998","doi":"10.1016\/s0889-4906(97)00016-1","raw":"LUMLEY, T. (1998). Perceptions of language-trained raters and occupational experts in a test of occupational English language proficiency. English for Specific Purposes, 17(4), 347-67.","cites":null},{"id":16365852,"title":"Policy and social considerations in language assessment.","authors":[],"date":"1998","doi":"10.1017\/s0267190500003603","raw":"MCNAMARA, T. (1998). Policy and social considerations in language assessment. Annual Review of Applied Linguistics, 18, 304-19.","cites":null},{"id":16366036,"title":"Predicting success for international teaching assistants in a US university.","authors":[],"date":"1990","doi":"10.2307\/3586900","raw":"YULE, G. (1990). Predicting success for international teaching assistants in a US university. TESOL Quarterly, 24(2),227-43.","cites":null},{"id":16365495,"title":"preparation courses: a study of washback.","authors":[],"date":"1996","doi":"10.1177\/026553229601300304","raw":"ALDERSONJ. C. & HAMP-LYONS, L. (1996).TOEFL preparation courses: a study of washback. Language Testing, 13(3), 280-97.","cites":null},{"id":16365935,"title":"Probing above the ceiling in oral interviews: what's up there? In","authors":[],"date":"1997","doi":null,"raw":"REED, D. J. & HALLECK, G. B. (1997). Probing above the ceiling in oral interviews: what's up there? In A. Huhta,V. Kohonen, L. Kurki-Suonio & S. Luoma (Eds.), Current developments and alternatives in language assessment. Jyvaskyla: University of Jyvaskyla'.","cites":null},{"id":16365674,"title":"problems in testing language for specific purposes: authenticity, specificity and inseparability. In","authors":[],"date":"2001","doi":null,"raw":"DOUGLAS, D. (2001a).Three problems in testing language for specific purposes: authenticity, specificity and inseparability. In C. Elder, A. Brown, E. Grove, K. Hill, N. Iwashita.T. Lumley.T. F. McNamara & K. O'Loughlin (Eds.), Experimenting with uncertainty: essays in honour of Alan Davies (Studies in Language Testing Series, Vol. 11, 45\u201451). Cambridge: University of Cambridge Local Examinations Syndicate and Cambridge University Press.","cites":null},{"id":16365717,"title":"Process assessment in ESP: input, throughput and output. English for Specific Purposes,","authors":[],"date":"1996","doi":"10.1016\/0889-4906(96)00007-5","raw":"GIMENEZJ. C. (1996). Process assessment in ESP: input, throughput and output. English for Specific Purposes, 15(3), 233-41. GlNTHER, A. (forthcoming). Context and content visuals and performance on listening comprehension stimuli. Language Testing.","cites":null},{"id":16365563,"title":"Profiling ESL children: how teachers interpret and use national and state assessment frameworks (Vol. 1). Commonwealth of Australia: Department of Employment, Education, Training and Youth Affairs.","authors":[],"date":"1997","doi":null,"raw":"BREEN, M. P., BARRATT-PUGH, C, DEREWIANKA, B., HOUSE, H., HUDSON, C, LUMLEY,T., & ROHL, M. (Eds.) (1997). Profiling ESL children: how teachers interpret and use national and state assessment frameworks (Vol. 1). Commonwealth of Australia: Department of Employment, Education, Training and Youth Affairs.","cites":null},{"id":16365932,"title":"Providing relevant content in an EAP writing test, English for Specific Purposes 1,243-68.","authors":[],"date":"1990","doi":"10.1016\/0889-4906(90)90002-t","raw":"READ, J. (1990) Providing relevant content in an EAP writing test, English for Specific Purposes 1,243-68.","cites":null},{"id":16365747,"title":"Questioning assumptions about portfolio-based assessment.","authors":[],"date":"1993","doi":"10.2307\/358837","raw":"HAMP-LYONS, L. & CONDON, W. (1993). Questioning assumptions about portfolio-based assessment. College Composition and Communication, 44(2), 176-90.","cites":null},{"id":16365998,"title":"Raising silent voices: educating the linguistic minorities for the twenty-first century.","authors":[],"date":"1989","doi":null,"raw":"TRUEBA, H. T. (1989). Raising silent voices: educating the linguistic minorities for the twenty-first century. New York: Newbury House.","cites":null},{"id":16365819,"title":"Rater characteristics and rater bias: implications for training.","authors":[],"date":"1995","doi":"10.1177\/026553229501200104","raw":"LUMLEY,T. & MCNAMARA.T. F. (1995). Rater characteristics and rater bias: implications for training. Language Testing, 12(1), 54-71.","cites":null},{"id":16365740,"title":"Rating scales and native speaker performance on a communicatively oriented EAP test.","authors":[],"date":"1993","doi":"10.1177\/026553229301000307","raw":"HAMILTON.J., LOPES, M., MCNAMARA.T. & SHERIDAN, E. (1993). Rating scales and native speaker performance on a communicatively oriented EAP test. LanguageTesting, 10(3), 337-53.","cites":null},{"id":16365619,"title":"Recent developments in IELTS.","authors":[],"date":"1997","doi":"10.1093\/elt\/51.4.374","raw":"CHARGE, N. & TAYLOR, L. B. (1997). Recent developments in IELTS. ELTJournal, 51(4), 374-80.","cites":null},{"id":16365552,"title":"Reinventing assessment: speculations on the future of large-scale educational testing.","authors":[],"date":"1998","doi":null,"raw":"BENNETT, R. E. (1998). Reinventing assessment: speculations on the future of large-scale educational testing. Princeton, New Jersey: Educational Testing Service.","cites":null},{"id":16365937,"title":"Reliability and validity in the GCSE oral examination.","authors":[],"date":"1996","doi":"10.1080\/09571739685200351","raw":"RICHARDS, B. & CHAMBERS, F. (1996). Reliability and validity in the GCSE oral examination. Language Learning Journal, 14, 28-34.","cites":null},{"id":16365685,"title":"Researching languages at primary school: some European perspectives.","authors":[],"date":"1996","doi":null,"raw":"EDELENBOS, P. & JOHNSTONE, R. (Eds.). (1996). Researching languages at primary school: some European perspectives. London: CILT, in collaboration with Scottish CILT and GION.","cites":null},{"id":16365888,"title":"Scaling descriptors for language proficiency scales.","authors":[],"date":"1998","doi":"10.1177\/026553229801500204","raw":"NORTH, B. & SCHNEIDER, G. (1998) Scaling descriptors for language proficiency scales. LanguageTesting, 15 (2), 217\u201462.","cites":null},{"id":16365561,"title":"Self assessment: the limits of learner independence.","authors":[],"date":"1988","doi":null,"raw":"BLUE, G. M. (1988). Self assessment: the limits of learner independence. ELT Documents, 131,100-18.","cites":null},{"id":16365940,"title":"Self-assessment in second language testing: a meta-analysis of experiential factors.","authors":[],"date":"1998","doi":"10.1177\/026553229801500101","raw":"Ross, S. (1998). Self-assessment in second language testing: a meta-analysis of experiential factors. Language Testing, 15{\\), 1-20.","cites":null},{"id":16365901,"title":"Self-assessment of foreign and second language proficiency. In","authors":[],"date":"1997","doi":"10.1007\/978-1-4020-4489-2_17","raw":"OSCARSON, M. (1997). Self-assessment of foreign and second language proficiency. In C. Clapham & D. Corson (Eds.), Language testing and assessment (Vol. 7,175-87). Dordrecht.The Netherlands: Kluwer Academic Publishers.","cites":null},{"id":16365893,"title":"Self-assessment of foreign language skills: a survey of research and development work.","authors":[],"date":"1984","doi":null,"raw":"OSCARSON, M. (1984). Self-assessment of foreign language skills: a survey of research and development work. Strasbourg, France: Council of Europe, Council for Cultural Co-operation.","cites":null},{"id":16365559,"title":"Self-assessment of foreign language skills: implications for teachers and researchers.","authors":[],"date":"1989","doi":"10.1111\/j.1467-1770.1989.tb00595.x","raw":"BLANCHE, P. & MERINO, B. J. (1989). Self-assessment of foreign language skills: implications for teachers and researchers. Language Learning, 39(3), 313-40.","cites":null},{"id":16365894,"title":"Self-assessment of language proficiency: rationale and applications.","authors":[],"date":"1989","doi":"10.1177\/026553228900600103","raw":"OSCARSON, M. (1989). Self-assessment of language proficiency: rationale and applications. LanguageTesting, 6(1), 1-13.","cites":null},{"id":16365758,"title":"Self-assessment of second language ability: the role of response effects.","authors":[],"date":"1990","doi":"10.1177\/026553229000700204","raw":"HEILENMAN, L. K. (1990). Self-assessment of second language ability: the role of response effects. Language Testing, 7(2), 174-201.","cites":null},{"id":16365601,"title":"Self-evaluation at the heart of learning. Le Francais dans le Monde (special number),","authors":[],"date":"1993","doi":null,"raw":"CARTON, F. (1993). Self-evaluation at the heart of learning. Le Francais dans le Monde (special number), 28-35.","cites":null},{"id":16365957,"title":"self-reported language proficiency by testing performance in an immigrant community: the Wellington Indo-Fijans.","authors":[],"date":"1998","doi":"10.1177\/026553229801500104","raw":"SHAMEEM, N. (1998).Validating self-reported language proficiency by testing performance in an immigrant community: the Wellington Indo-Fijans. Language Testing, 15(1), 86\u2014108.","cites":null},{"id":16365498,"title":"Sequencing as an item type.","authors":[],"date":"2000","doi":"10.1177\/026553220001700403","raw":"ALDERSON.J.C, PERCSICH, R. & SZABO, G. (2000b). Sequencing as an item type. Language Testing, 17 (4), 423\u201447.","cites":null},{"id":16365752,"title":"Small words and good testing.","authors":[],"date":"1998","doi":null,"raw":"HASSELGREN.A. (1998). Small words and good testing. Unpublished PhD dissertation, University of Bergen, Bergen.","cites":null},{"id":16365929,"title":"Snares or silver bullets: disentangling the construct of formative assessment.","authors":[],"date":"2000","doi":"10.1177\/026553220001700206","raw":"REA-DICKINS, P. & GARDNER, S. (2000). Snares or silver bullets: disentangling the construct of formative assessment. Language Testing, 17(2), 215-43.","cites":null},{"id":16365926,"title":"So why do we need relationships with stakeholders in language testing? A view from the UK.","authors":[],"date":"1997","doi":"10.1177\/026553229701400307","raw":"REA-DICKINS, P. (1997). So why do we need relationships with stakeholders in language testing? A view from the UK. LanguageTesting, 14(3), 304-14.","cites":null},{"id":16365664,"title":"Sprogtest': a smart test (or how to develop a reliable and anonymous EFL reading test).","authors":[],"date":"1994","doi":"10.1177\/026553229401100106","raw":"DOLLERUP, C, GLAHN, E.& ROSENBERG HANSEN, C. (1994). 'Sprogtest': a smart test (or how to develop a reliable and anonymous EFL reading test). Language Testing, 11(1), 65-81.","cites":null},{"id":16365491,"title":"Standards in testing: a study of the practice of UK examination boards in EFL\/ESL testing.","authors":[],"date":"1993","doi":"10.1177\/026553229301000101","raw":"ALDERSON.J. C. & BUCK, G. (1993). Standards in testing: a study of the practice of UK examination boards in EFL\/ESL testing. LanguageTesting, 20(1), 1-26.","cites":null},{"id":16365974,"title":"State of the art: language testing, part I. Language Teaching,","authors":[],"date":"1988","doi":"10.1017\/s0261444800005218","raw":"SKEHAN, P. (1988). State of the art: language testing, part I. Language Teaching, 211-21.","cites":null},{"id":16365975,"title":"State of the art: language testing, part II.","authors":[],"date":"1989","doi":"10.1017\/s0261444800005346","raw":"SKEHAN, P. (1989). State of the art: language testing, part II.","cites":null},{"id":16365906,"title":"target language and examinations.","authors":[],"date":"1993","doi":"10.1080\/09571739385200271","raw":"PAGE, B. (1993).The target language and examinations. Language Learning Journal, 8,6\u20147.","cites":null},{"id":16365993,"title":"Teacher assessment and psychometric theory: a case of paradigm crossing?","authors":[],"date":"2000","doi":"10.1177\/026553220001700204","raw":"TEASDALE, A. & LEUNG, C. (2000). Teacher assessment and psychometric theory: a case of paradigm crossing? Language Testing, 17(2), 163-84.","cites":null},{"id":16365592,"title":"Technologies for language assessment. Annual Review of Applied Linguistics,","authors":[],"date":"1996","doi":"10.1017\/s0267190500001537","raw":"BURSTEIN, J., FRASE, L. T., GINTHER, A. & GRANT, L. (1996). Technologies for language assessment. Annual Review of Applied Linguistics, 16,240-60.","cites":null},{"id":16365478,"title":"Technology in testing: the present and the future.","authors":[],"date":"2000","doi":"10.1016\/s0346-251x(00)00040-3","raw":"ALDERSON, J. C. (2000c). Technology in testing: the present and the future. System 28 (4) 593-603.","cites":null},{"id":16365972,"title":"Test impact revisited: washback effect over time.","authors":[],"date":"1996","doi":"10.1177\/026553229601300305","raw":"SHOHAMY, E., DONITSA-SCHMIDT, S. & FERMAN, I. (1996). Test impact revisited: washback effect over time. Language Testing, 13(3), 298-317.","cites":null},{"id":16365918,"title":"testing and evaluation of international teaching assistants. In","authors":[],"date":"1990","doi":null,"raw":"PLAKANS, B. & ABRAHAM, R. G. (1990).The testing and evaluation of international teaching assistants. In D. Douglas (Ed.), English language testing in U.S. colleges and universities (68-81). Washington DC: NAFSA.","cites":null},{"id":16365451,"title":"Testing English for Specific Purposes: how specific can we get? ELTDocuments,","authors":[],"date":"1988","doi":null,"raw":"ALDERSON, J. C. (1988). Testing English for Specific Purposes: how specific can we get? ELTDocuments, 127,16-28.","cites":null},{"id":16365736,"title":"Testing language and teaching skills of international teaching assistants: the limits of compensatory strategies.","authors":[],"date":"1995","doi":"10.2307\/3588172","raw":"HALLECK, G. B. & MODER, C. L. (1995). Testing language and teaching skills of international teaching assistants: the limits of compensatory strategies. TESOL Quarterly, 29(4), 733-57.","cites":null},{"id":16365754,"title":"The assessment of the English ability of young learners in Norwegian schools: an innovative approach.","authors":[],"date":"2000","doi":"10.1177\/026553220001700209","raw":"HASSELGR\u00a3N,A. (2000). The assessment of the English ability of young learners in Norwegian schools: an innovative approach. LanguageTesting, 17(2), 261-77.","cites":null},{"id":16365915,"title":"The development of the Canadian Language Benchmarks Assessment.","authors":[],"date":"1997","doi":null,"raw":"PEIRCE, B. N. & STEWART, G. (1997). The development of the Canadian Language Benchmarks Assessment. TESL Canada Journal, 14(2), 17-31.","cites":null},{"id":16365604,"title":"The effect of background disciplines on IELTS scores.","authors":[],"date":"1999","doi":null,"raw":"CELESTINE, C. & CHEAH, S. M. (1999). The effect of background disciplines on IELTS scores. In R. Tulloh (Ed.), 1ELTS Research Reports 1999 (Vol. 2, 36-51). Canberra: IELTS Australia Pty Limited.","cites":null},{"id":16365775,"title":"The effect of prior knowledge on EAP listening-test performance,","authors":[],"date":"1995","doi":"10.1177\/026553229501200106","raw":"JENSEN, C. & HANSEN, C. (1995) The effect of prior knowledge on EAP listening-test performance, Language Testing, 12(\\), 99-119.","cites":null},{"id":16365571,"title":"The effect of rater variables in the development of an occupation-specific language performance test.","authors":[],"date":"1995","doi":"10.1177\/026553229501200101","raw":"BROWN, A. (1995). The effect of rater variables in the development of an occupation-specific language performance test. LanguageTesting, 32(1), 1-15.","cites":null},{"id":16365909,"title":"The effect of topic variation in performance testing: the case of the chemistry TEACH test for international teaching assistants.","authors":[],"date":"1999","doi":"10.1177\/026553229901600104","raw":"PAPAJOHN, D. (1999). The effect of topic variation in performance testing: the case of the chemistry TEACH test for international teaching assistants. LanguageTesting, 16(1), 52-81.","cites":null},{"id":16366010,"title":"The impact of high-stakes testing on teaching and learning: can this be predicted or controlled?","authors":[],"date":"2000","doi":"10.1016\/s0346-251x(00)00035-x","raw":"WALL, D. (2000). The impact of high-stakes testing on teaching and learning: can this be predicted or controlled? System, 28, 499-509.","cites":null},{"id":16365864,"title":"The interplay of evidence and consequences in the validation of performance assessments.","authors":[],"date":"1994","doi":"10.3102\/0013189x023002013","raw":"MESSICK, S. (1994). The interplay of evidence and consequences in the validation of performance assessments. Educational Researcher, 23(2), 13-23.","cites":null},{"id":16365798,"title":"The labyrinth of exit standard controls.","authors":[],"date":"1997","doi":null,"raw":"Li, K. C. (1997). The labyrinth of exit standard controls. Hong Kongjournal of Applied Linguistics, 2(1), 23\u201438.","cites":null},{"id":16365801,"title":"The Language Profile: oral interaction.","authors":[],"date":"1996","doi":null,"raw":"LIDDICOAT, A. (1996). The Language Profile: oral interaction. Babel, 31(2), 4-7,35.","cites":null},{"id":16365488,"title":"The lift is being fixed. You will be unbearable today (Or why we hope that there will not be translation on the new English erettsegi). Paper presented at the Magyar Macmillan Conference,","authors":[],"date":"2001","doi":null,"raw":"ALDERSON, J. C. (2001b). The lift is being fixed. You will be unbearable today (Or why we hope that there will not be translation on the new English erettsegi). Paper presented at the Magyar Macmillan Conference, Budapest, Hungary.","cites":null},{"id":16365662,"title":"The logic of testing Languages for Specific Purposes.","authors":[],"date":"2001","doi":"10.1177\/026553220101800202","raw":"DAVIES, A. (2001). The logic of testing Languages for Specific Purposes. Language Testing, 18(2), 133-47.","cites":null},{"id":16366004,"title":"The National Core French Assessment Project: design and field test of formative evaluation instruments at the intermediate level. The Canadian Modern Language Review,","authors":[],"date":"1998","doi":"10.3138\/cmlr.54.4.553","raw":"VANDERGRIFT, L. & BELANGER, C. (1998). The National Core French Assessment Project: design and field test of formative evaluation instruments at the intermediate level. The Canadian Modern Language Review, 54(4), 553\u201478.","cites":null},{"id":16365646,"title":"The pilot speaking examinations. In","authors":[],"date":"2000","doi":null,"raw":"CSEPES, I., SULYOK, A. & OVEGES, E. (2000). The pilot speaking examinations. In J. C. Alderson, E. Nagy & E. Oveges (Eds.), English language education in Hungary, Part II: Examining Hungarian learners' achievements in English. Budapest:The British Council.","cites":null},{"id":16365757,"title":"The political dimension of language testing in Australia.","authors":[],"date":"1997","doi":"10.1177\/026553229701400303","raw":"HAWTHORNE, L. (1997). The political dimension of language testing in Australia. Language Testing, 14(3), 248-60.","cites":null},{"id":16365962,"title":"The power oftests:Tlie impact of language tests on teaching and learning NFLC Occasional Papers.","authors":[],"date":"1993","doi":null,"raw":"SHOHAMY, E. (1993). The power oftests:Tlie impact of language tests on teaching and learning NFLC Occasional Papers. Washington, D.C.: The National Foreign Language Center.","cites":null},{"id":16365774,"title":"The test-takers' choice: an investigation of the effect of topic on language-test performance.","authors":[],"date":"1999","doi":"10.1177\/026553229901600402","raw":"JENNINGS, M., FOX.J., GRAVES, B. & SHOHAMY, E. (1999). The test-takers' choice: an investigation of the effect of topic on language-test performance. LanguageTesting, 16(4), 426\u201456.","cites":null},{"id":16366000,"title":"The Threshold Level for modern language learning in schools.","authors":[],"date":"1997","doi":null,"raw":"VAN EK,J.A. (1997). The Threshold Level for modern language learning in schools. London: Longman.","cites":null},{"id":16365943,"title":"The use of CLBA scores in LINC program placement practices in Western Canada.","authors":[],"date":"1999","doi":null,"raw":"ROSSITER, M. & PAWLIKOWSSKA-SMITH, G. (1999). The use of CLBA scores in LINC program placement practices in Western Canada. TESL Canada Journal, 16(2),39-52.","cites":null},{"id":16365779,"title":"The washback effect of a textbookbased test. Edinburgh Working Papers in","authors":[],"date":"1990","doi":null,"raw":"KHANIYAH,T. R. (1990b). The washback effect of a textbookbased test. Edinburgh Working Papers in Applied Linguistics, 1, 48-58.","cites":null},{"id":16365589,"title":"The'use of the target language at GCSE.","authors":[],"date":"1999","doi":"10.1080\/09571739985200031","raw":"BUCKBY, M. (1999). The'use of the target language at GCSE. Language Learning Journal, 19,4-11.","cites":null},{"id":16365627,"title":"Tlie development of IELTS: a study of the effect of background knowledge on reading comprehension (Studies","authors":[],"date":"1996","doi":"10.1017\/s0272263198231056","raw":"CLAPHAM, C. (1996). Tlie development of IELTS: a study of the effect of background knowledge on reading comprehension (Studies in Language Testing Series, Vol. 4). Cambridge: University of Cambridge Local Examinations Syndicate and Cambridge University Press.","cites":null},{"id":16365847,"title":"Tlie ESL Framework of Stages.","authors":[],"date":"1991","doi":null,"raw":"MCKAY, P. & SCARINO, A. (1991). Tlie ESL Framework of Stages. Melbourne: Curriculum Corporation.","cites":null},{"id":16365969,"title":"Tlie power of tests.","authors":[],"date":null,"doi":null,"raw":"SHOHAMY.E. (2001a). Tlie power of tests. London: Longman.","cites":null},{"id":16365549,"title":"Tlie Primary Language Record: A handbook for teachers. London: Centre for Language in Primary Education.","authors":[],"date":"1988","doi":null,"raw":"BARRS, M., ELLIS, S., HESTER, H. & THOMAS, A. (1988). Tlie Primary Language Record: A handbook for teachers. London: Centre for Language in Primary Education.","cites":null},{"id":16365636,"title":"Towards a common ability scale for Hong Kong English secondary-school forms.","authors":[],"date":"1995","doi":"10.1177\/026553229501200203","raw":"CONIAM, D. (1995). Towards a common ability scale for Hong Kong English secondary-school forms. Language Testing, 12(2), 182-93.","cites":null},{"id":16365802,"title":"Trialling the languages profile in the A.C.T.","authors":[],"date":"1998","doi":null,"raw":"LIDDICOAT, A. J. (1998). Trialling the languages profile in the A.C.T. Babel, 33(2), 14-38.","cites":null},{"id":16365731,"title":"Using a videocamera and task-based activities to make classroom oral testing a more realistic communicative experience.","authors":[],"date":"1994","doi":"10.1111\/j.1944-9720.1994.tb01199.x","raw":"HAGGSTROM, M. (1994). Using a videocamera and task-based activities to make classroom oral testing a more realistic communicative experience. Foreign Language Annals, 27(2), 161-75.","cites":null},{"id":16365772,"title":"Using dictionaries with national foreign-language examinations for reading comprehension. Levende Talen,","authors":[],"date":"1999","doi":null,"raw":"JANSEN, H. & PEER, C. (1999). Using dictionaries with national foreign-language examinations for reading comprehension. Levende Talen, 544,639-41.","cites":null},{"id":16365558,"title":"Using standardised achievement and oral proficiency tests for self-assessment purposes: the DLIFLC study.","authors":[],"date":"1990","doi":"10.1177\/026553229000700205","raw":"BLANCHE, P. (1990). Using standardised achievement and oral proficiency tests for self-assessment purposes: the DLIFLC study. Language Testing, 7(2), 202\u201429.","cites":null},{"id":16365868,"title":"Validity and vvashback in language testing.","authors":[],"date":"1996","doi":"10.1177\/026553229601300302","raw":"MESSICK, S. (1996). Validity and vvashback in language testing. LanguageTesting, 13(3), 241-56.","cites":null},{"id":16365614,"title":"Validity in language assessment.","authors":[],"date":"1999","doi":"10.1017\/s0267190599190135","raw":"CHAPELLE, C. (1999). Validity in language assessment. Annual Review of Applied Linguistics, 19,254-72.","cites":null},{"id":16365655,"title":"Wanted: A theoretical framework for relating language proficiency to academic achievement among bilingual students. In","authors":[],"date":"1984","doi":null,"raw":"CUMMINS, J. (1984b). Wanted: A theoretical framework for relating language proficiency to academic achievement among bilingual students. In C. Rivera (Ed.), Language proficiency and academic achievement (Vol. 10). Clevedon, England: Multilingual Matters.","cites":null},{"id":16365744,"title":"Washback, impact and validity: ethical concerns.","authors":[],"date":"1997","doi":"10.1177\/026553229701400306","raw":"HAMP-LYONS, L. (1997). Washback, impact and validity: ethical concerns. Language Testing, 14(3), 295-303.","cites":null},{"id":16365790,"title":"What do teachers mean by speaking and listening: a contextualised study of assessment in the English National Curriculum. In","authors":[],"date":"1997","doi":null,"raw":"LEUNG, C. & TEASDALE, A. (1997). What do teachers mean by speaking and listening: a contextualised study of assessment in the English National Curriculum. In A. Huhta,V. Kohonen, L. Kurki-Suonio & S. Luoma (Eds.), New contexts,goals and alternatives in language assessment (291-324). Jyvaskyla: University ofjyvaskyla.","cites":null},{"id":16365467,"title":"What does PESTI have to do with us testers? Paper presented at the International Language Education Conference, Hong Kong.","authors":[],"date":"1999","doi":null,"raw":"ALDERSON, J. C. (1999). What does PESTI have to do with us testers? Paper presented at the International Language Education Conference, Hong Kong.","cites":null},{"id":16365689,"title":"What does test bias have to do with fairness?","authors":[],"date":"1997","doi":"10.1177\/026553229701400304","raw":"ELDER, C. (1997). What does test bias have to do with fairness? LanguageTesting, 14(3), 261-77.","cites":null},{"id":16365528,"title":"Working for washback: A review of the washback concept in language testing.","authors":[],"date":"1996","doi":"10.1177\/026553229601300303","raw":"BAILEY, K. (1996). Working for washback: A review of the washback concept in language testing. Language Testing, 13(3), 257-79.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2001-10","abstract":null,"downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/71410.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/1036\/1\/displayFulltext2.pdf","pdfHashValue":"fb46a7ff42160c5590a2b52e9c8834d568e25823","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:1036<\/identifier><datestamp>\n      2018-01-24T00:01:30Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D50:5031<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Language testing and assessment (Part 1):state-of-the-art review<\/dc:title><dc:creator>\n        Alderson, J. Charles<\/dc:creator><dc:creator>\n        Banerjee, Jayanti<\/dc:creator><dc:subject>\n        P Philology. Linguistics<\/dc:subject><dc:date>\n        2001-10<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1017\/S0261444800014464<\/dc:relation><dc:identifier>\n        Alderson, J. Charles and Banerjee, Jayanti (2001) Language testing and assessment (Part 1):state-of-the-art review. Language Teaching, 34 (4). pp. 213-236. ISSN 0261-4448<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/1036\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1017\/S0261444800014464","http:\/\/eprints.lancs.ac.uk\/1036\/"],"year":2001,"topics":["P Philology. Linguistics"],"subject":["Journal Article","PeerReviewed"],"fullText":"http:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nState-of-the-Art Review\nLanguage testing and assessment (Part I)\nJ Charles Alderson and Jayanti Banerjee Lancaster University, UK\nIntroduction\nThis is the third in a series of State-of-the-Art\nreview articles in language testing in this journal, the\nfirst having been written by Alan Davies in 1978 and\nthe second by Peter Skehan in 1988\/1989. Skehan\nremarked that testing had witnessed an explosion of\ninterest, research and publications in the ten years\nsince the first review article, and several commenta-\ntors have since made similar remarks. We can only\nconcur, and for quantitative corroboration would\nrefer the reader to Alderson (1991) and to the\nInternational Language Testing Association (ILTA)\nBibliography 1990-1999 (Banerjee et al., 1999). In\nthe latter bibliography, there are 866 entries, divided\ninto 15 sections, from Testing Listening to Ethics and\nStandards.The field has become so large and so active\nthat it is virtually impossible to do justice to it, even\nin a multi-part 'State-of-the-Art' review like this, and\nit is changing so rapidly that any prediction of trends\nis likely to be outdated before it is printed.\nIn this review, therefore, we not only try to avoid\nanything other than rather bland predictions, we also\nacknowledge the partiality of our choice of topics\nand trends, as well, necessarily, of our selection of\npublications.We have tried to represent the field fair-\nly, but have tended to concentrate on articles rather\nthan books, on the grounds that these are more likely\nJ Charles Alderson is Professor of Linguisics and\nEnglish Language Education at Lancaster University.\nHe holds an MA in German and French from Oxford\nUniversity and a PhD in Applied Linguistics from\nEdinburgh University. He is co-editor of the journal\nLanguage Testing (Edward Arnold), and co-editor\nof the Cambridge Language Assessment Series\n(C. UP), and has published many books and articles on\nlanguage testing, reading in a foreign language, and\nevaluation of language education.\nJayanti Banerjee is a PhD student in the\nDepartment of Linguistics and Modern English\nLanguage at Lancaster University. She has been\ninvolved in a number of test development and research\nprojects and has taught on introductory testing courses.\nShe has also been involved in teaching English for\nAcademic Purposes (EAP) at Lancaster University. Her\nresearch interests include the teaching and assessment of\nEAP as well as qualitative research methods. She is par-\nticularly interested in issues related to the interpretation\nand use of test scores.\nto reflect the state of the art than are full-length\nbooks. We have also referred to other similar reviews\npublished in the last 10 years or so, where we judged\nit relevant. We have usually begun our review with\narticles printed in or around 1988, the date of the last\nreview, aware that this is now 13 years ago, but also\nconscious of the need to cover the period since the\nlast major review in this journal. However, we have\nalso, where we felt it appropriate, included articles\npublished somewhat earlier.\nThis review is divided into two parts, each of\nroughly equal length. The bibliography for works\nreferred to in each part is published with the relevant\npart, rather than in a complete bibliography at the\nend. Therefore, readers wishing to have a complete\nbibliography will have to put both parts together.\nThe rationale for the organisation of this review is\nthat we wished to start with a relatively new concern\nin language testing, at least as far as publication of\nempirical research is concerned, before moving on to\nmore traditional ongoing concerns and ending with\naspects of testing not often addressed in international\nreviews, and remaining problems. Thus, we begin\nwith an account of research into washback, which\nthen leads us to ethics, politics and standards. We then\nexamine trends in testing on a national level,\nfollowed by testing for specific purposes. Next, we\nsurvey developments in computer-based testing\nbefore moving on to look at self-assessment and\nalternative assessment. Finally in this first part, we\nsurvey a relatively new area: the assessment of young\nlearners.\nIn the second part, we address new concerns in\ntest validity theory, which argues for the inclusion of\ntest consequences in what is now generally referred\nto as a unified theory of construct validity. Thereafter\nwe deal with issues in test validation and test devel-\nopment, and examine in some detail more traditional\nresearch into the nature of the constructs (reading,\nlistening, grammatical abilities, etc.) that underlie\ntests. Finally we discuss a number of remaining con-\ntroversies and puzzles that we call, following\nMcNamara (1995),'Pandora's Boxes'.\nWe are very grateful to many colleagues for their\nassistance in helping us draw up this review, but in\nparticular we would like to acknowledge the help,\nadvice and support of the Lancaster Language Testing\nResearch Group, above all of Dianne Wall and\nCaroline Clapham, for their invaluable and insightful\ncomments. All faults that remain are entirely our\nresponsibility.\nLang.Teach. 34,213-236. DOI: 10.1017\/S0261444801001707 Printed in the United Kingdom \u00a9 2001 Cambridge University Press 2 1 3\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\nWashback\nThe term 'washback' refers to the impact that tests\nhave on teaching and learning. Such impact is usually\nseen as being negative: tests are said to force teachers\nto do things they do not necessarily wish to do.\nHowever, some have argued that tests are potentially\nalso 'levers for change' in language education: the\nargument being that if a bad test has negative impact,\na good test should or could have positive washback\n(Alderson, 1986b; Pearson, 1988).\nInterestingly, Skehan, in the last review of the State\nof the Art in Language Testing (Skehan, 1988,1989),\nmakes only fleeting reference to washback, and even\nthen, only to assertions that communicative language\ntesting and criterion-referenced testing are likely to\nlead to better washback - with no evidence cited.\nNor is research into washback signalled as a likely\nimportant future development within the language\ntesting field. Let those who predict future trends do\nso at their peril!\nIn the Annual Review of Applied Linguistics series,\nequally, the only substantial reference to washback is\nby McNamara (1998) in a chapter entitled: 'Policy\nand social considerations in language assessment'.\nEven the chapter entitled 'Developments in language\ntesting' by Douglas (1995) makes no reference to\nwashback. Given the importance assigned to conse-\nquential validity and issues of consequences in the\ngeneral assessment literature, especially since the\npopularisation of the Messickian view of an all-\nencompassing construct validity (see Part Two), this is\nremarkable, and shows how much the field has\nchanged in the last six or seven years. However, a\nrecent review of validity theory (Chapelle, 1999)\nmakes some reference to washback under construct\nvalidity, reflecting the increased interest in the topic.\nAlthough the notion that tests have impact on\nteaching and learning has a long history, there was\nsurprisingly little empirical evidence to support such\nnotions until recently. Alderson and Wall (1993) were\namong the first to problematise the notion of test\nwashback in language education, and to call for\nresearch into the impact of tests. They list a number\nof'Washback Hypotheses' in an attempt to develop a\nresearch agenda. One Washback Hypothesis, for\nexample, is that tests will have washback on what\nteachers teach (the content agenda), whereas a sepa-\nrate washback hypothesis might posit that tests also\nhave impact on how teachers teach (the methodology\nagenda). Alderson and Wall also hypothesise that\nhigh-stakes tests - tests with important consequences\n- would have more impact than low-stakes tests.They\nurge researchers to broaden the scope of their\nenquiry, to include not only attitude measurement\nand teachers' accounts of washback but also classs-\nroom observation. They argue that the study of wash-\nback would benefit from a better understanding of\nstudent motivation and of the nature of innovation in\n214\neducation, since the notion that tests will automati-\ncally have an impact on the curriculum and on learn-\ning has been advocated atheoretically. Following on\nfrom this suggestion, Wall (1996) reviews key con-\ncepts in the field of educational innovation and shows\nhow they might be relevant to an understanding of\nwhether and how tests have washback. Lynch and\nDavidson (1994) describe an approach to criterion-\nreferenced testing which involves practising teachers\nin the translation of curricular goals into test specifi-\ncations. They claim that this approach can provide a\nlink between the curriculum, teacher experience and\ntests and can therefore, presumably, improve the\nimpact of tests on teaching.\nRecently, a number of empirical washback studies\nhave been carried out (see, for example, Khaniyah,\n1990a, 1990b; Shohamy, 1993; Shohamy et al, 1996;\nWall & Alderson, 1993; Watanabe, 1996; Cheng,\n1997) in a variety of settings. There is general agree-\nment among these that high-stakes tests do indeed\nimpact on the content of teaching and on the nature\nof the teaching materials. However, the evidence that\nthey impact on how teachers teach is much scarcer\nand more complicated. Wall and Alderson (1993)\nfound no evidence for any change in teachers'\nmethodologies before and after the introduction of a\nnew style school-leaving examination in English in\nSri Lanka. Alderson and Hamp-Lyons (1996) show\nthat teachers may indeed change the way they teach\nwhen teaching towards a test (in this case, the\nTOEFL \u2014Test of English as a Foreign Language), but\nthey also show that the nature of the change and the\nmethodology adopted varies from teacher to teacher,\na conclusion supported by Watanabe's 1996 findings.\nAlderson and Hamp-Lyons argue that it is not\nenough to describe whether and how teachers might\nadapt their teaching and the content of their teaching\nto suit the test. They believe that it is important to\nexplain why teachers do what they do, if we are to\nunderstand the washback effect. Alderson (1998) sug-\ngests that testing researchers should explore the litera-\nture on teacher cognition and teacher thinking to\nunderstand better what motivates teacher behaviour.\nCheng (1997) shows that teachers only adapt their\nmethodology slowly, reluctantly and with difficulty,\nand suggests that this may relate to the constraints on\nteachers and teaching from the educational system\ngenerally. Shohamy et al. (1996) show that the nature\nof washback varies according to factors such as the\nstatus of the language being tested, and the uses of the\ntest. In short, the phenomenon of washback is slowly\ncoming to be recognised as a complex matter, influ-\nenced by many factors other than simply the exis-\ntence of a test or the nature of that test. Nevertheless,\nno major studies have yet been carried out into the\neffect of test preparation on test performance, which\nis remarkable, given the prevalence, for high-stakes\ntests at least, of test preparation courses.\nHahn et al. (1989) conducted a small-scale study\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\nof the effects on beginning students of German of\nwhether they were or were not graded on their oral\nperformance in the first six months of instruction.\nAlthough no effects on developing oral proficiency\nwere found, attitudes in the two groups were differ-\nent: those who had been graded considered the\nexperience stressful and unproductive, whereas the\ngroup that had not been graded would like to have\nbeen graded. Moeller and Reschke (1993) also found\nno effect whatsoever of the formal scoring of class-\nroom performance on student proficiency or achieve-\nment. More studies are needed of learners' views of\ntests and test preparation.\nThere are in fact remarkably few studies of the\nimpact of tests on motivation or of motivation on\ntest preparation or test performance. A recent excep-\ntion is Watanabe (2001). Watanabe calls his study a\nhypothesis-generating exercise, acknowledging that\nthe relationship between motivation and test prepa-\nration is likely to be complex. He interviewed\nJapanese university students about their test prepara-\ntion practices. He found that attitudes to test prepa-\nration varied and that impact was far from uniform,\nalthough those exams which the students thought\nmost important for their future university careers\nusually had more impact than those perceived as less\ncritical. Thus, if an examination for a university\nwhich was the student's first choice contained gram-\nmar-translation tasks, the students reported that they\nhad studied grammar-translation exercises, whereas if\na similar examination was offered by a university\nwhich was their second choice, they were much less\nlikely to study translation exercises. Interestingly, stu-\ndents studied in particular those parts of the exam\nthat they perceived to be more difficult, and more\ndiscriminating. Conversely those sections perceived\nto be easy had less impact on their test preparation\npractices: far fewer students reported preparing for\neasy or non-discriminating exam sections. However,\nthose students who perceived an exam section to be\ntoo difficult did not bother preparing for it.\nWatanabe concludes that washback is caused by the\ninterplay between the test and the test taker in a\ncomplex manner, and he emphasises that what may\nbe most important is not the objective difficulty of\nthe test, but the students' perception of difficulty.\nWall (2000) provides a very useful overview and\nup-date of studies of the impact of tests on teaching,\nfrom the field of general education as well as in lan-\nguage education. She summarises research findings\nwhich show that test design is only one of the factors\naffecting washback, and lists as factors influencing the\nnature of test washback:\nteacher ability, teacher understanding of the test and the\napproach it was based on, classroom conditions, lack of resources,\nmanagement practices within the school... the status of the sub-\nject within the curriculum, feedback mechanisms between the\nschools and the testing agency, teacher style, commitment and\nwillingness to innovate, teacher background, the general social\nand political context, the time that has elapsed since the test was\nintroduced, and the role of publishers in materials design and\nteacher training (2000: 502).\nIn other words, test washback is far from being\nsimply a technical matter of design and format, and\nneeds to be understood within a much broader\nframework. Wall suggests that such a framework\nmight usefully come from studies and theories of\neducational change and innovation, and she sum-\nmarises the most important findings from these areas.\nShe develops a framework derived from Hen-\nrichsen (1989), and owing something to the work\nof Hughes (1993) and Bailey (1996), and shows\nhow such a framework might be applied to under-\nstanding better the causes and nature of washback.\nShe makes a number of recommendations about the\nsteps that test developers might take in the future in\norder to assess the amount of risk involved in\nattempting to bring about change through testing.\nThese include assessing the feasibility of examination\nreform by studying the 'antecedent' conditions \u2014\nwhat is increasingly referred to as a 'baseline study'\n(Weir & Roberts, 1994, Fekete et al, 1999); involving\nteachers at all stages of test development; ensuring\nthe participation of other key stakeholders including\npolicy-makers and key institutions; ensuring clarity\nand acceptability of test specifications, and clear\nexemplification of tests, tasks, and scoring criteria;\nfull piloting of tests before implementation; regular\nmonitoring and evaluation not only of test perfor-\nmance but also of classrooms; and an understanding\nthat change takes time. Innovating through tests is\nnot a quick fix if it is to be beneficial.'Policy makers\nand test designers should not expect significant\nimpact to occur immediately or in the form they\nintend. They should be aware that tests on their own\nwill not have positive impact if the materials and\npractices they are based on have not been effective.\nThey may, however, have negative impact and the sit-\nuation must be monitored continuously to allow\nearly intervention if it takes an undesirable turn'\n(2000:507).\nSimilar considerations of the potential complexity\nof the impact of tests on teaching and learning\nshould also inform research into the washback of\nexisting tests. Clearly this is a rich field for further\ninvestigation. More sophisticated conceptual frame-\nworks, which are slowly developing in the light of\nresearch findings and related studies into innovation,\nmotivation theory and teacher thinking, are likely to\nprovide better understanding of the reasons for\nwashback and an explanation of how tests might be\ndeveloped to contribute to the engineering of desir-\nable change.\nEthics in language testing\nWhilst Alderson (1997) and others have argued that\ntesters have long been concerned with matters of\n215\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\nfairness (as expressed in their ongoing interest in\nvalidity and reliability), and that striving for fairness\nis an aspect of ethical behaviour, others have separat-\ned the issue of ethics from validity, as an essential part\nof the professionalising of language testing as a disci-\npline (Davies, 1997). Messick (1994) argues that all\ntesting involves making value judgements, and there-\nfore language testing is open to a critical discussion\nof whose values are being represented and served;\nthis in turn leads to a consideration of ethical con-\nduct. Messick (1994, 1996) has redefined the scope\nof validity to include what he calls consequential\nvalidity - the consequences of test score interpreta-\ntion and use. Hamp-Lyons (1997) argues that the\nnotion of washback is too narrow and should be\nbroadened to cover 'impact', defined as the effect of\ntests on society at large, not just on individuals or on\nthe educational system. In this, she is expressing a\nconcern that has grown in recent years with the\npolitical and related ethical issues which surround\ntest use.\nBoth McNamara (1998) and Hamp-Lyons (1998)\nsurvey the emerging literature on the topic of ethics,\nand highlight the need for the development of\nlanguage testing standards (see below). Both com-\nment on a draft Code of Practice sponsored by the\nInternational Language Testing Association (ILTA,\n1997), but where Hamp-Lyons sees it as a possible\nway forward, McNamara is more critical of what he\ncalls its conservatism, and this inadequate acknowl-\nedgement of the force of current debates on the\nethics of language testing. Davies (1997) argues that,\nsince tests often have a prescriptive or normative\nrole, their social consequences are potentially far-\nreaching. He argues for a professional morality\namong language testers, both to protect the profes-\nsion's members, and to protect individuals from the\nmisuse and abuse of tests. However, he also argues\nthat the morality argument should not be taken too\nfar, lest it lead to professional paralysis, or cynical\nmanipulation of codes of practice.\nSpolsky (1997) points out that tests and examina-\ntions have always been used as instruments of social\npolicy and control, with the gate-keeping function\nof tests often justifying their existence. Shohamy\n(1997a) claims that language tests which contain\ncontent or employ methods which are not fair to all\ntest-takers are not ethical, and discusses ways of\nreducing various sources of unfairness. She also\nargues that uses of tests which exercise control and\nmanipulate stakeholders rather than providing infor-\nmation on proficiency levels are also unethical, and\nshe advocates the development of'critical language\ntesting' (Shohamy, 1997b). She urges testers to exer-\ncise vigilance to ensure that the tests they develop are\nfair and democratic, however that may be defined.\nLynch (1997) also argues for an ethical approach to\nlanguage testing and Rea-Dickins (1997) claims that\ntaking full account of the views and interests of vari-\n216\nous stakeholder groups can democratise the testing\nprocess, promote fairness and therefore enhance an\nethical approach.\nA number of case studies have been presented\nrecently which illustrate the use and misuse of\nlanguage tests. Hawthorne (1997) describes two\nexamples of the misuse of language tests: the use of\nthe access test to regulate the flow of migrants into\nAustralia, and the step test, allegedly designed to play\na central role in the determining of asylum seekers'\nresidential status. Unpublished language testing lore\nhas many other examples, such as the misuse of the\nGeneral Training component of the International\nEnglish Language Testing System (IELTS) test with\napplicants for immigration to New Zealand, and the\nuse of the TOEFL test and other proficiency tests to\nmeasure achievement and growth in instructional\nprogrammes (Alderson, 2001a). It is to be hoped that\nthe new concern for ethical conduct will result in\nmore accounts of such misuse.\nNorton and Starfield (1997) claim, on the basis of\na case study in South Africa, that unethical conduct is\nevident when second language students' academic\nwriting is implicitly evaluated on linguistic grounds\nwhilst ostensibly being assessed for the examinees'\nunderstanding of an academic subject. They argue\nthat criteria for assessment should be made explicit\nand public if testers are to behave ethically. Elder\n(1997) investigates test bias, arguing that statistical\nprocedures used to detect bias such as DIF\n(Differential Item Functioning) are not neutral since\nthey do not question whether the criterion used to\nmake group comparisons is fair and value-free.\nHowever, in her own study she concludes that what\nmay appear to be bias may actually be construct-rele-\nvant variance, in that it indicates real differences in\nthe ability being measured. One similar study was\nChen and Henning (1985), who compared inter-\nnational students' performance on the UCLA\n(University of California, Los Angeles) English as a\nSecond Language Placement Test, and discovered\nthat a number of items were biased in favour of\nSpanish-speaking students and against Chinese-\nspeaking students. The authors argue, however, that\nthis 'bias' is relevant to the construct since Spanish is\nmuch closer to English typologically and therefore\nbiased in favour of speakers of Spanish, who would\nbe expected to find many aspects of English much\neasier to learn than speakers of Chinese would.\nReflecting this concern for ethical test use,\nCumming (1995) reviews the use in four Canadian\nsettings of assessment instruments to monitor learn-\ners' achievements or the efFectiveness of pro-\ngrammes, and concludes that this is a misuse of such\ninstruments, which should be used mainly for plac-\ning students onto programmes. Cumming (1994)\nasks whether use of language assessment instruments\nfor immigrants to Canada facilitates their successful\nparticipation in Canadian society. He argues that\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\nsuch a criterion should be used to evaluate whether\nassessment practices are able to overcome institutional\nor systemic barriers that immigrants may encounter,\nto account for the quality of language use that may\nbe fundamental to specific aspects of Canadian life,\nand to prompt majority populations and instruments\nto better accommodate minority populations.\nIn the academic context, Pugsley (1988) prob-\nlematises the assessment of the need of international\nstudents for pre- and in-sessional linguistic training\nin the light of test results. Decisions on whether a\nstudent should receive the benefit of additional lan-\nguage instruction are frequently made at the last\nminute, and in the light of competing demands on\nthe student and on finance. Language training may\nbe the victim of reduced funding, and many aca-\ndemics downplay on importance of language in\nacademic performance. Often, teachers and students\nperceive students' language related problems differ-\nently, and the question of the relevance or influence\nof the test result is then raised.\nIn another investigation of score interpretation\nand use, Yule (1990) analyses the performance of\ninternational teaching assistants, attempting to pre-\ndict on the basis of TOEFL and Graduate Record\nExaminations Program scores whether the subjects\nshould have received positive or negative recom-\nmendations to be teaching assistants. Students who\nreceived negative recommendations did indeed have\nlower scores on both tests than those with positive\nrecommendations, but the relationship between\nsubsequent grade point average (GPA) and positive\nrecommendations only held during the first year of\ngraduate study, not thereafter. The implications for\nmaking decisions about the award of teaching assist-\nantships are discussed, and there are obvious ethical\nimplications about the length of time a test score\nshould be considered to be valid.\nBoth these case studies show the difficulty in inter-\npreting language test results, and the complexity of the\nissues that surround gate-keeping decisions. They also\nemphasise that there must be a limit on what informa-\ntion one can ethically expect a language test to deliver,\nand what decisions test results can possibly inform.\nPartly as a result of this heightened interest in\nethics and the role of tests in society, McNamara\n(1998:313) anticipates in the future:\n1. a renewed awareness ... of the socially constructed nature of\ntest performance and test score interpretation;\n2. an awareness of the issues raised for testing in the context of\nEnglish as an International Language;\n3. a reconsideration of the social impact of technology in the\ndelivery of tests;\n4. an explicit consideration of issues of fairness at every stage of\nthe language testing cycle, and\n5. an expanded agenda for research on fairness accompanying\ntest development.\nHe concludes that we are likely to see 'a broaden-\ning of the range of issues involved in language testing\nresearch, drawing on, at least, the following disci-\nplines and fields: philosophy, especially ethics and the\nepistemology of social science; critical theory; policy\nanalysis; program evaluation, and innovation theory'\n(loc cit).\nThe International Language Testing Association\n(ILTA) has recently developed a Code of Ethics\n(rather than finalising the draft Code of Practice\nreferred to above), which is 'a set of principles which\ndraws upon moral philosophy and strives to guide\ngood professional conduct ... All professional codes\nshould inform professional conscience and judge-\nment ... Language testers are independent moral\nagents, and they are morally entitled to refuse to par-\nticipate in procedures which would violate personal\nmoral belief. Language testers accepting employment\npositions where they foresee they may be called on\nto be involved in situations at variance with their\nbeliefs have a responsibility to acquaint their employ-\ner or prospective employer with this fact. Employers\nand colleagues have a responsibility to ensure that\nsuch language testers are not discriminated against\nin their workplace.' [http:\/\/www.surrey.ac.uk\/ELI\/\nltrfile\/ltrframe.html]\nThese are indeed fine words and the moral tone\nand intent of this Code is clear: testers should follow\nethical practices, and have a moral responsibility to\ndo so. Whether this Code of Ethics will be acceptable\nin the diverse environments in -which language\ntesters work around the world remains to be seen.\nSome might even see this as the imposition of\nWestern cultural or even political values.\nPolitics\nTests are frequently used as instruments of educa-\ntional policy, and they can be very powerful \u2014 as\nattested by Shohamy (2001a). Inevitably, therefore,\ntesting - especially high-stakes testing - is a political\nactivity, and recent publications in language testing\nhave begun to address the relation between testing\nand politics, and the politics of testing, perhaps rather\nbelatedly, given the tradition in educational assess-\nment in general.\nBrindley (1998,2001) describes the political use of\ntest-based assessment for reasons of public account-\nabilty, often in the context of national frameworks,\nstandards or benchmarking. However, he points out\nthat political rather than professional concerns are\nusually behind such initiatives, and are often in con-\nflict with the desire for formative assessment to be\nclosely related to the learning process. He addresses a\nnumber of political as well as technical and practical\nissues in the use of outcomes-based assessment for\naccountability purposes, and argues for the need for\nincreased consultation between politicians and pro-\nfessionals and for research into the quality of associ-\nated instruments.\nPolitics can be defined as action, or activities, to\n217\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessnnent (Part 1)\nachieve power or to use power, and as beliefs about\ngovernment, attitudes to power, and to the use of\npower. But this need not only be at the macro-politi-\ncal level of national or local government. National\neducational policy often involves innovations in test-\ning in order to influence the curriculum, or in order\nto open up or restrict access to education and\nemployment - and even, as we have seen in the cases\nof Australia and New Zealand, to influence immigra-\ntion opportunities. But politics can also operate at\nlower levels, and can be a very important influence\non test development and deployment. Politics can be\nseen as methods, tactics, intrigue, manoeuvring,\nwithin institutions which are themselves not politi-\ncal, but commercial, financial and educational.\nIndeed, Alderson (1999) argues that politics with a\nsmall 'p' includes not only institutional politics, but\nalso personal politics: the motivation of the actors\nthemselves and their agendas. And personal politics\ncan influence both test development and test use.\nExperience shows that, in most institutions, test\ndevelopment is a complex matter where individual\nand institutional motives interact and are interwo-\nven. Yet the language testing literature has virtually\nnever addressed such matters, until very recently.\nThe literature, when it deals with test development\nmatters at all, which is not very often, gives the\nimpression that testing is basically a technical matter,\nconcerned with the development of appropriate\nspecifications, the creation and revision of appropri-\nate test tasks and scoring criteria, and the analysis of\nresults from piloting. But behind that facade is a\ncomplex interplay of personalities, of institutional\nagendas, and of intrigue. Although the macro-politi-\ncal level of testing is certainly important, one also\nneeds to understand individual agendas, prejudices\nand motivations. However, this is an aspect of lan-\nguage testing which rarely sees the light of day, and\nwhich is part of the folklore of language testing.\nExploring such issues is difficult because of the\nsensitivities involved, and it is difficult to publish any\naccount of individual motivations for proposing or\nresisting test use and misuse. However, that does not\nmake it any the less important. Alderson (2001a) has\nthe title: 'Testing is too important to be left to\ntesters', and he argues that language testers need to\ntake account of the different perspectives of various\nstakeholders: not only classroom teachers, who are all\ntoo often left out of consideration in test develop-\nment, but also educational policy makers and poli-\nticians more generally. Although there are virtually\nno studies in this area at present (exceptions being\nAlderson et al, 2000a, Alderson, 1999, 2001b, and\nShohamy, 2001), it is to be hoped that the next\ndecade will see such matters discussed much more\nopenly in language testing, since politics, ethics and\nfairness are rather closely related. Shohamy (2001b)\ndescribes and discusses the potential abuse of tests as\ninstruments of power by authoritarian agencies, and\n218\nargues for more democratic and accountable testing\npractice.\nAs an example of the influence of politics, it is\ninstructive to consider Alderson (2001b). In Hungary\ntranslation is still used as a testing technique in\nthe current school-leaving exams, and in the tests\nadministered by the State Foreign Language\nExaminations Board (SFLEB), a quasi-commercial\nconcern. Language teachers have long expressed\ntheir concern at the continued use of a test method\nwhich has uncertain validity (this has not been estab-\nlished to date in Hungary), where the marking of\ntranslations is felt to be subjective and highly vari-\nable, where no marking criteria or scales exist, and\nwhere the washback effect is felt to be negative\n(Fekete et al, 1999). New school-leaving examina-\ntions are due to be introduced in 2005, and the\nintention is not to use translation as a test method in\nfuture. However, many people, including teachers,\nand also Ministry officials, have resisted such a pro-\nposal, and it has recently been declared that the\nMinister himself will take the decision on this mat-\nter. Yet the Minister is not a language expert, knows\nnothing about language testing, and is therefore not\ntechnically competent to judge. Many suspect that\nthe SFLEB, which wishes to retain translation, is\nlobbying the Minister to insist that translation be\nretained as a test method. Furthermore, many suspect\nthat the SFLEB fears that foreign language examina-\ntions, which necessarily do not use translation as a\ntest method, might take over the language test mar-\nket in Hungary if translation is no longer required\n(by law) as a testing technique. Alderson (2001b)\nsuggests that translation may be being used as a\nweapon in the cause of commercial protectionism.\nStandards in testing\nOne area of increasing concern in language testing\nhas been that of standards. The word 'standards' has\nvarious meanings in the literature, as the Task Force on\nLanguage Testing Standards set up by ILTA discovered\n(http:\/\/www.surrey.ac.uk\/ELI\/ilta\/tfts_report.pdf).\nOne common meaning used by respondents to the\nILTA survey was that of procedures for ensuring\nquality, standards to be upheld or adhered to, as in\n'codes of practice'. A second meaning was that of\n'levels of proficiency' - 'what standard have you\nreached?'A related, third meaning was that contained\nin the phrase 'standardised test', which typically\nmeans a test whose difficulty level is known, which\nhas been adequately piloted and analysed, the results\nof which can be compared with those of a norming\npopulation: standardised tests are typically norm-\nreferenced tests. In the latter context 'standards' is\nequivalent to 'norms'.\nIn recent years, language testing has sought to\nestablish standards in the first sense (codes of prac-\ntice) and to investigate whether tests are developed\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\nfollowing appropriate professional procedures. Groot\n(1990) argues that the standardisation of procedures\nfor test construction and validation is crucial to the\ncomparability and exchangeability of test results\nacross different education settings. Alderson and\nBuck (1993) and Alderson et al. (1995) describe\nwidely accepted procedures for test development and\nreport on a survey of the practice of British EFL\nexamining boards. The results showed that current\n(in the early 1990s) practice was wanting. Practice\nand procedures among boards varied greatly, yet\n(unpublished) information was available which could\nhave attested to the quality of examinations. Exam\nboards appeared not to feel obliged to follow or\nindeed to understand accepted procedures, nor did\nthey appear to be accountable to the public for the\nquality of the tests they produced. Fulcher and\nBamford (1996) argue that testing bodies in the USA\nconduct and report reliability and validity studies\npartly because of a legal requirement to ensure that\nall tests meet technical standards.They conclude that\nBritish examination boards should be subject to sim-\nilar pressures of litigation on the grounds that their\ntests are unreliable, invalid or biased. In the German\ncontext, Kieweg (1999) makes a plea for common\nstandards in examining EFL, claiming that within\nschools there is litde or no discussion of appropriate\nmethods of testing or of procedures for ensuring the\nquality of language tests.\nPossibly as a result of such pressures and publica-\ntions, things appear to be changing in Europe, an\nexample of this being the publication of the ALTE\n(Association of Language Testers in Europe) Code of\nPractice, which is intended to ensure quality work in\ntest development throughout Europe. 'In order to\nestablish common levels of proficiency, tests must be\ncomparable in terms of quality as well as level, and\ncommon standards need, therefore, to be applied to\ntheir production' (ALTE, 1998).To date, no mecha-\nnism exists for monitoring whether such standards\nare indeed being applied, but the mere existence of\nsuch a Code of Practice is a step forward in establish-\ning the public accountability of test developers.\nExamples of how such standards are applied in prac-\ntice are unfortunately rare, one exception being\nAlderson et al. (2000a), which presents an account of\nthe development of new school-leaving examina-\ntions in Hungary.\nWork on standards in the third sense, namely\n'norms' for different test populations, was less com-\nmonly published in the last decade. Baker (1988) dis-\ncusses the problems and procedures of producing test\nnorms for bilingual school populations, challenging\nthe usual a priori procedure of classifying populations\ninto mother tongue and second language groups.\nEmploying a range of statistical measures, Davidson\n(1994) examines the appropriacy of the use of a\nnationally standardised test normed on native English\nspeakers, when used with non-English speaking\nstudents. Although he concludes that such a use of\nthe test might be defensible statistically, additional\nmeasures might nevertheless be necessary for a pop-\nulation different from the norming group.\nThe meaning of'standards' as 'levels of proficiency'\nor 'levels certified by public examinations' has been\nan issue for some considerable time, but has received\nnew impetus, both with recent developments in\nCentral Europe and with the publication of the\nCouncil of Europe's Common European Framework\n(Council of Europe, 2001). Work in the 1980s by\nWest and Carroll led to the development of the\nEnglish Speaking Union's Framework (Carroll &\nWest, 1989), but this was not widely accepted,\nprobably because of commercial rivalries within the\nBritish EFL examining industry. Milanovic (1995)\nreports on work towards the establishment of com-\nmon levels of proficiency by ALTE, which has devel-\noped its own definitions of five levels of proficiency,\nbased upon an inspection and comparison of the\nexaminations of its members. This has had more\nacceptability, possibly because it was developed by\ncooperating examination bodies, rather than for\ncompeting bodies. However, such a framework of\nlevels is still not seen by many as being neutral: it is,\nafter all, associated with the main European com-\nmercial language test providers. The Council of\nEurope's Common European Framework, on the\nother hand, is not only seen as independent of any\npossible vested interest, it also has a long pedigree,\noriginating over 25 years ago in the development of\nthe Threshold level (van Ek, 1977), and thus broad\nacceptability across Europe is guaranteed. In addi-\ntion, the scales of various aspects of language profi-\nciency that are associated with the Framework\nhave been extensively researched and validated by\nthe Swiss Language Portfolio Project (North &\nSchneider, 1998).\nde Jong (1992) predicted that international stan-\ndards for language tests and assessment procedures,\nand internationally interpretable standards of profi-\nciency would be developed, with the effect that\ninternationally comparable language tests would be\nestablished. In the 21st century, that prediction is\ncoming true. It is now clear that the Common\nEuropean Framework will become increasingly\ninfluential because of the growing need for interna-\ntional recognition of certificates in Europe, in order\nto guarantee educational and employment mobility.\nNational language qualifications, be they provided by\nthe state or by quasi-private organisations, presently\nvary in their standards - both quality standards and\nstandards as levels.Yet international comparability of\ncertificates has become an economic as well as an\neducational imperative, especially after the Bologna\nDeclaration of 1999 (http:\/\/europa.eu.int\/comm\/\neducation\/socrates\/erasmus\/bologna.pdf), and the\navailability of a transparent, independent framework\nlike the Common European framework is crucial to\n219\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\nthe attempt to establish a common scale of reference\nand comparison. Moreover, the Framework is not\njust a set of scales, it is also a compendium of what is\nknown about language learning, language use and\nlanguage proficiency. As an essential guide to syllabus\nconstruction, as well as to the development of test\nspecifications and rating criteria, it is bound to be\nused for materials design and textbook production, as\nwell as in teacher education. The Framework is also\nthe anchor point for the European Language\nPortfolio, and for new diagnostic tests like DIALANG\n(see below).\nThe Framework is particularly relevant to countries\nin East and Central Europe, where many educational\nsystems are currently revising their assessment proce-\ndures. The intention is that the reformed exam-\ninations should have international recognition,\nunlike the current school-leaving exams. Calibrating\nthe new tests against the Framework is essential, and\nthere is currently a great deal of activity in the devel-\nopment of school-leaving achievement tests in the\nregion (for one account of such development, see\nAlderson et ah, 2000a).We are confident that we will\nhear much more about the Common European\nFramework in the coming years, and it will increas-\ningly become a point of reference for language\nexaminations across Europe and beyond.\nNational tests\nThe development of national language tests contin-\nues to be the focus of many publications, although\nmany are either simply descriptions of test develop-\nment or discussions of controversies, rather than\nreports on research done in connection with test\ndevelopment.\nIn the UK context, Neil (1989) discusses what\nshould be included in an assessment system for foreign\nlanguages in the UK secondary system but reports\nno research. Roy (1988) claims that writing tasks\nfor modern languages should be more relevant, task-\nbased and authentic, yet criticises an emphasis on\nletter writing, and argues for other forms of writing,\nlike paragraph writing. Again, no research is report-\ned. Page (1993) discusses the value and validity of\nhaving test questions and rubrics in the target\nlanguage and asserts that the authenticity of such\ntasks is in doubt. He argues that the use of the target\nlanguage in questions makes it more difficult to\nsample the syllabus adequately, and claims that the\nmore communicative and authentic the tasks in\nexaminations become, the more English (the mother\ntongue) has to be used on the examination paper in\norder to safeguard both the validity and the authen-\nticity of the task. No empirical research into this\nissue is reported. Richards and Chambers (1996) and\nChambers and Richards (1992) examine the reliabil-\nity and validity of teacher assessments in oral produc-\ntion tasks in the school-leaving GCSE (General\n220\nCertificate in Secondary Education) French exami-\nnation, and find problems particularly in the rating\ncritera, which they hold should be based on a princi-\npled model of language proficiency and be informed\nby an analysis of communicative development.\nHurman (1990) is similarly critical of the imprecise\nspecifications of objectives, tasks and criteria for\nassessing speaking ability in French at GCSE level.\nBarnes and Pomfrett (1998) find that teachers need\ntraining in order to conform to good practice in\nassessing German for pupils at Key Stage 3 (age 14).\nBuckby (1999) reports an empirical comparison of\nrecent and older GCSE examinations, to determine\nwhether standards of achievement are falling, and\nconcludes that although the evidence is that stan-\ndards are indeed being maintained, there is a need for\na range of different question types in order to enable\ncandidates to demonstrate their competencies.\nBarnes et al. (1999) consider the recent introduction\nof the use of bilingual dictionaries in school exami-\nnations, report teachers' positive reactions to this\ninnovation, but call for more research into the use\nand impact of dictionaries on pupil performance in\nexaminations.\nSimilar research in the Netherlands (Jansen &\nPeer, 1999) reports a study of the recently introduced\nuse of dictionaries in Dutch foreign language exami-\nnations and shows that dictionary use does not have\nany significant effect on test scores. Nevertheless,\npupils are very positive about being allowed to use\ndictionaries, claiming that it reduces anxiety and\nenhances their text comprehension. Also in the\nNetherlands, Welling-Slootmaekers (1999) describes\nthe introduction of a range of open-ended questions\ninto national examinations of reading ability in\nforeign languages, arguing that these will improve\nthe assessment of language ability (the questions\nare to be answered in Dutch, not the target foreign\nlanguage), van Elmpt and Loonen (1998) question\nthe assumption that answering test questions in the\ntarget language is a handicap, and report research that\nshows results to be similar, regardless of whether\ncandidates answered comprehension questions in\nDutch (the mother tongue) or in English (the target\nlanguage). However, Bhgel and Leijn (1999) report\nresearch that showed low interrater reliability in\nmarking these new item types and they call for\nimproved assessment practice.\nGuillon (1997) evaluates the assessment of English\nin French secondary schools, criticises the time taken\nby test-based assessment and the technical quality\nof the tests, and makes suggestions for improved\npupil profiling. Mundzeck (1993) similarly criticises\nmany of the objective tests in use in Germany for\nofficial school assessment of modern languages,\narguing that they do not reflect the communicative\napproach to language required by the syllabus. He\nrecommends that more open-ended tasks be used,\nand that teachers be trained in the reliable use of\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\nvalid criteria for subjective marking, instead of their\ncurrent practice of merely counting errors in pro-\nduction. Kieweg (1992) makes proposals for the\nimprovement of English assessment in German\nschools, and for the comparability of standards with-\nin and across schools.\nDollerup et al. (1994) describe the development in\nDenmark of an English language reading proficiency\ntest which is claimed to help diagnose reading weak-\nnesses in undergraduates. Further afield, in Australia,\nLiddicoat (1996) describes the Language Profile oral\ninteraction component which sees listening and\nspeaking as interdependent skills and assesses school\npupils' ability to participate successfully in sponta-\nneous conversation. Liddicoat (1998) criticises the\nAustralian Capital Territory's guidelines for the\nassessment of proficiency in languages like Chinese,\nJapanese and Indonesian, as well as French, German,\nSpanish and Italian. He argues that empirically-based\ndescriptions of the achievement of learners of\nsuch different languages should inform the revision\nof the descriptors of different levels in profiles of\nachievement.\nIn Hong Kong, dissatisfaction with graduating\nstudents' levels of language proficiency has resulted\nin plans for tertiary institution exit controls of lan-\nguage. Li (1997) describes these plans and discusses a\nrange of problematic issues that need resolving\nbefore valid measures can be introduced. Coniam\n(1994, 1995) describes the construction of a com-\nmon scale which attempts to cover the range of\nEnglish language ability of Hong Kong secondary\nschool pupils in English. An Item Response Theory-\nbased test bank - the TeleNex - has been construct-\ned to provide teachers both with reference points for\nability levels and help in school-based testing.\nA similar concern with levels or standards of profi-\nciency is evinced by Peirce and Stewart (1997), who\ndescribe the development of the Canadian Language\nBenchmarks Assessment (CLBA), which is intended\nto be used across Canada to place newcomers into\nappropriate English instructional programmes, as\npart of a movement to establish a common frame-\nwork for the description of adult ESL language pro-\nficiency. The authors give an account of the history\nof the project and the development of the instru-\nments. However, Rossiter and Pawlikowsska-Smith\n(1999) are critical of the usefulness of the CLBA\nbecause it is based on very broad-band differences in\nproficiency among individuals and is insensitive to\nsmaller, but important, differences in proficiency.\nThey argue that the CLBA should be supplemented\nby more appropriate placement instruments.\nVandergrift and Belanger (1998) describe the back-\nground to and development of formative instru-\nments to evaluate achievement in Canadian National\nCore French programmes, and argue that research\nshows that reactions to the instruments are positive.\nBoth teachers and pupils regard them as beneficial\nfor focusing and organising learning activities and\nfind them motivating and useful for the feedback\nthey provide to learners.\nIn the USA, one example of concern with school-\nbased assessment is Manley (1995) who describes a\nproject in a large Texas school district to develop\ntape-mediated tests of oral language proficiency in\nFrench, German, Spanish and Japanese, with positive\noutcomes.\nThese descriptive accounts of local and national\ntest development contrast markedly with the litera-\nture surrounding international language proficiency\nexaminations, like TOEFL, TWE (Test of Written\nEnglish), IELTS and some Cambridge exams.\nAlthough some reports of the development of inter-\nnational proficiency tests are merely descriptive (for\nexample, Charge & Taylor, 1997, and Kalter &\nVossen, 1990), empirical research into various aspects\nof the validity and reliability of such tests is common-\nplace, often revealing great sophistication in analytic\nmethodology.\nThis raises a continuing problem: language testing\nresearchers tend to research and write about large-\nscale international tests, and not about more localised\ntests (including school-leaving achievement tests\nwhich are clearly relatively high-stakes). Thus, the\nlanguage testing and more general educational com-\nmunities lack empirical evidence about the value of\nmany influential assessment instruments, and research\noften fails to address matters of educational political\nimportance.\nHowever, there are exceptions. For example, in\nconnection with examination reform in Hungary,\nresearch studies have addressed issues like the use\nof sequencing as a test method (Alderson et al.,\n2000b), the pairing of candidates in oral tests (Csepes\net al., 2000), experimentation with procedures for\nstandard setting (Alderson, 2000a), and evidence\ninforming ongoing debates about how many hours\nper week should be devoted to foreign language\neducation in the secondary school system (Alderson,\n2000b).\nIn commenting on the lack of international dis-\nsemination of national or regional test development\nwork, we do not wish to deny the value of local\ndescriptive publications. Indeed, such descriptions\ncan serve many needs, including necessary publicity\nfor reform work, helping teachers to understand\ndevelopments, their rationale and the need for them,\npersuading authorities about a desired course of\naction or counselling against other possible actions.\nPublication can serve political as well as professional\nand academic purposes. Standard setting data can\nreveal what levels are achieved by the school popula-\ntion, including comparisons of those who started\nlearning the language early with late-starters, those\nstudying a first foreign language with those studying\nthe same language as their second or third foreign\nlanguage, and so on.\n221\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\nLanguage testing can inform debates in language\neducation more generally. Examples of this include\nbaseline studies associated with examination reform\nwhich attempt to describe current practice in lan-\nguage classrooms (Fekete et al, 1999). What such\nstudies have revealed has been used in in-service and\npre-service teacher education and baseline studies\ncan also be referred to in impact studies to show the\neffect of innovations, and to help language educators\nto understand how to do things more effectively.\nWashback studies have also been used in teacher\ntraining, both in order to influence test preparation\npractices, but also to encourage teachers to reflect on\nthe reasons for their and others' practices.\nLSP Testing\nThe development of specific purpose testing, i.e.,\ntests in which the test content and test method are\nderived from a particular language use context rather\nthan more general language use situations, can be\ntraced back to the Temporary Registration Assess-\nment Board (TRAB), introduced by the British\nGeneral Medical Council in 1976 (see Rea-Dickins,\n1987) and the development of the English Lan-\nguage Testing Development Unit (ELTDU) scales\n(Douglas, 2000).The 1980s saw the introduction of\nEnglish for Academic Purposes (EAP) tests and it is\nthese that have subsequently dominated the research\nand development agenda. It is important to note,\nhowever, that Language for Specific Purposes (LSP)\ntests are not the diametric opposite of general pur-\npose tests. Rather, they typically fall along a continu-\num between general purpose tests and those for\nhighly specialised contexts and include tests for\nacademic purposes (e.g., the International English\nLanguage Testing System, IELTS) and for occupa-\ntional or professional purposes (e.g., the Occupational\nEnglish Test, OET).\nDouglas (1997, 2000) identifies two aspects that\ntypically distinguish LSP testing from general purpose\ntesting.The first is the authenticity of the tasks, i.e., the\ntest tasks share key features with the tasks that a test\ntaker might encounter in the target language use sit-\nuation. The assumption here is that the more closely\nthe test and 'real-life' tasks are linked, the more likely\nit is that the test takers' performance on the test task\nwould reflect their performance in the target situa-\ntion.The second distinguishing feature of LSP testing\nis the interaction between language knowledge and specific\ncontent knowledge.This is perhaps the most crucial dif-\nference between general purpose testing and LSP\ntesting, for in the former, any sort of background\nknowledge is considered to be a confounding vari-\nable that contributes construct-irrelevant variance to\nthe test score. However, in the case of LSP testing,\nbackground knowledge constitutes an integral part\nof what is being tested, since it is hypothesised that\ntest takers' language knowledge has developed within\n222\nthe context of their academic or professional field\nand that they would be disadvantaged by taking a test\nbased on content outside that field.\nThe development of an LSP test typically begins\nwith an in-depth analysis of the target language use\nsituation, perhaps using genre analysis (see Tarone,\n2001). Attention is paid to general situational features\nsuch as topics, typical lexis and grammatical struc-\ntures. Specifications are then developed that take into\naccount the specific language characteristics of the\ncontext as well as typical scenarios that occur (e.g.,\nPlakans & Abraham, 1990; Stansfield et al, 1990;\nScott et al, 1996; Stansfield et al, 1997; Stansfield et\nal., 2000). Particular areas of concern, quite under-\nstandably, tend to relate to issues of background\nknowledge and topic choice (e.g.,Jensen & Hansen,\n1995; Clapham, 1996; Fox et al, 1997; Celestine &\nCheah, 1999; Jennings et al, 1999; Papajohn, 1999;\nDouglas, 2001a) and authenticity of task, input or,\nindeed, output (e.g., Lumley & Brown, 1998; Moore\n& Morton, 1999; Lewkowicz, 2000; Elder, 2001;\nDouglas, 2001a; Wu & Stansfield; 2001) and these\nareas of concern have been a major focus of research\nattention in the last decade.\nResults, though somewhat mixed (cf. Jensen &\nHansen, 1995 and Fox et al, 1997), suggest that back-\nground knowledge and language knowledge interact\ndifferently depending on the language proficiency of\nthe test taker. Clapham's (1996) research into sub-\nject-specific reading tests (research she conducted\nduring and after the ELTS revision project) shows\nthat, at least in the case of her data, the scores of nei-\nther lower nor higher proficiency test takers seemed\ninfluenced by their background knowledge. She\nhypothesises that for the former this was because\nthey were most concerned with decoding the text\nand for the latter it was because their linguistic\nknowledge was sufficient for them to be able to\ndecode the text with that alone. However, the scores\nof medium proficiency test takers were affected by\ntheir background knowledge. On the basis of these\nfindings she argues that subject-specific tests are not\nequally valid for test takers at different levels of lan-\nguage proficiency.\nFox et al. (1997), examining the role of back-\nground knowledge in the context of the listening\nsection of an integrated test of English for Academic\nPurposes (the Carleton Academic English Test,\nCAEL), report a slight variation on this finding. They\ntoo find a significant interaction between language\nproficiency and background knowledge with the\nscores of low proficiency test takers showing no ben-\nefit from background knowledge. However, the\nscores of the high proficiency candidates and analysis\nof their verbal protocols indicate that they did make\nuse of their background knowledge to process the\nlistening task.\nClapham (1996) has further shown that back-\nground knowledge is an extremely complex con-\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessnnent (Part 1)\ncept. She reveals dilemmas including the difficulty of\nidentifying with any precision the absolute specifici-\nty of an input passage and the nigh impossibility of\nbeing certain about test takers' background knowl-\nedge (particularly given that test takers often read\noutside their chosen academic field and might even\nhave studied in a different academic area in the past).\nThis is of particular concern when tests are topic-\nbased and all the sub-tests and tasks relate to a single\ntopic area. Jennings et al. (1999) and Papajohn (1999)\nlook at the possible effect of topic, in the case of the\nformer, for the CAEL and, in the case of the latter, in\nthe chemistry TEACH test for international teaching\nassistants. They argue that the presence of topic effect\nwould compromise the construct validity of the test\nwhether test takers are offered a choice of topic dur-\ning test administration (as with the CAEL) or not.\nPapajohn finds that topic does play a role in chem-\nistry TEACH test scores and warns of the danger of\nassuming that subject-specificity automatically guar-\nantees topic equivalence. Jennings et al. are relieved\nto report that choice of topic does not seem to affect\ntest taker performance on the CAEL. However, they\ndo note that there is a pattern in the choices made by\ntest takers of different proficiency levels and suggest\nthat more research is needed into the implications of\nthese patterns for test performance.\nAnother particular concern of LSP test developers\nhas been authenticity (of task, input and\/or output),\none example of the care taken to ensure that the test\nmaterials are authentic being Wu and Stansfield's\n(2001) description of the test construction procedure\nfor the LSTE-Taiwanese (listening summary transla-\ntion exam). Yet Lewkowicz (1997) somewhat puts\nthe cat among the pigeons when she demonstrates\nthat it is not always possible accurately to identify\nauthentic texts from those specially constructed for\ntesting purposes. She further problematises the\nvaluing of authenticity in her study of a group of test\ntakers' perceptions of an EAP test, finding that they\nseemed unconcerned about whether the test materi-\nals were situationally authentic or not. Indeed, they\nmay even consider multiple-choice tests to be\nauthentic tests of language, as opposed to tests of\nauthentic language (Lewkowicz, 2000). (For further\ndiscussion of this topic, see Part Two of this review.)\nOther test development concerns, however, are\nvery much like those of researchers developing tests\nin different sub-skills. Indeed, researchers working on\nLSP tests have contributed a great deal to our under-\nstanding of a number of issues related to the testing\nof reading, writing, speaking and listening. Apart\nfrom being concerned with how best to elicit sam-\nples of language for assessment (Read, 1990), they\nhave investigated the influence of interlocutor\nbehaviour on test takers' performance in speaking\ntests (e.g., Brown & LunJey, 1997; McNamara &\nLumley, 1997; Reed & Halleck, 1997). They have\nalso studied the assumptions underpinning rating\nscales (Hamilton et al., 1993) as well as the effect of\nrater variables on test scores (Brown, 1995; Lumley &\nMcNamara, 1995) and the question of who should\nrate test performances \u2014 language specialists or sub-\nject specialists (Lumley, 1998).\nThere have also been concerns related to the\ninterpretation of test scores. Just as in general pur-\npose testing, LSP test developers are concerned with\nminimising and accounting for construct-irrelevant\nvariables. However, this can be a particularly thorny\nissue in LSP testing since construct irrelevant vari-\nables can be introduced as a result of the situational\nauthenticity of the test tasks. For instance, in his\nstudy of the chemistry TEACH test, Papajohn (1999)\ndescribes the difficulty of identifying when a teach-\ning assistant's teaching skills (rather than language\nskills) are contributing to his\/her test performance.\nHe argues that test behaviours such as the provision\nof accessible examples or good use of the blackboard\nare not easily distinguished as teaching or language\nskills and this can result in construct-irrelevant vari-\nance being introduced into the test score. He suggests\nthat test takers should be given specific instructions\non how to present their topics, i.e., teaching tips so\nthat teaching skills do not vary widely across perfor-\nmances. Stansfield et al. (2000) have taken a similar\napproach in their development of the LSTE-\nTaiwanese. The assessment begins with an instruction\nsection on the summary skills needed for the test\nwith the aim of ensuring that test performances are\nnot unduly influenced by a lack of understanding of\nthe task requirements.\nIt must be noted, however, that, because of the\nneed for in-depth analysis of the target language use\nsituation, LSP tests are time-consuming and expen-\nsive to produce. It is also debatable whether English\nfor Specific Purposes (ESP) tests are more informa-\ntive than a general purpose test. Furthermore, it is\nincreasingly unclear just how 'specific' an LSP test is\nor can be. Indeed, more than a decade has passed\nsince Alderson (1988) first asked the crucial question\nof how specific ESP testing could get. This question\nis recast in Elder's (2001) work on LSP tests for\nteachers when she asks whether for all their 'teacher-\nliness' these tests elicit language that is essentially dif-\nferent from that elicited by a general language test.\nAn additional concern is the finding that con-\nstruct relevant variables such as background knowl-\nedge and compensatory strategies interact differently\nwith language knowledge depending on the lan-\nguage proficiency of the test taker (e.g., Halleck &\nModer, 1995; Clapham, 1996). As a consequence of\nClapham's (1996) research, the current IELTS test\nhas no subject-specific reading texts and care is taken\nto ensure that the input materials are not biased for\nor against test takers of different disciplines. Though\nthe extent to which this lack of bias has been\nachieved is debatable (see Celestine & Cheah, 1999),\nit can still be argued that the attempt to make texts\n223\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessnnent (Part 1)\naccessible regardless of background knowledge has\nresulted in the IELTS test being very weakly specific.\nIts claims to specificity (and indeed similar claims by\nmany EAP tests) rest entirely on the fact that it is\ntesting the generic language skills needed in academ-\nic contexts.This leaves it unprotected against sugges-\ntions like Clapham's (2000a) when she questions the\ntheoretical soundness of assessing discourse knowl-\nedge that the test taker, by registering for a degree\ntaught in English, might arguably be hoping to learn\nand that even a native speaker of English might lack.\nRecently the British General Medical Council has\nabandoned its specific purpose test, the Professional\nand Linguistic Assessment Board (PLAB, a revised\nversion of theTRAB), replacing it with a two-stage\nassessment process that includes the use of the IELTS\ntest to assess linguistic proficiency. These develop-\nments represent the thin end of the wedge. Though\nthe IELTS is still a specific purpose test, it is itself less\nso than its precursor the English Language Testing\nSystem (ELTS) and it is certainly less so than the\nPLAB. And so the questioning continues. Davies\n(2001) has joined the debate, debunking the theoret-\nical justifications typically put forward to explain\nLSP testing, in particular the principle that different\nfields demand different language abilities. He argues\nthat this principle is based far more on differences of\ncontent rather than on differences of language (see\nalso Fulcher, 1999a). He also questions the view that\ncontent areas are discrete and heterogeneous.\nDespite all the rumblings of discontent, Douglas\n(2000) stands firmly by claims made much earlier in\nthe decade that in highly field-specific language con-\ntexts, a field-specific language test is a better predic-\ntor of performance than a general purpose test\n(Douglas & Selinker, 1992). He concedes that many\nof these contexts will be small-scale educational, pro-\nfessional or vocational programmes in which the\nnumber of test takers is small but maintains (Douglas,\n2000:282):\nif we want to know how well individuals can use a language in\nspecific contexts of use, we will require a measure that takes into\naccount both their language knowledge and their background\nknowledge, and their use of strategic competence in relating the\nsalient characteristics of the target language use situation to their\nspecific purpose language abilities. It is only by so doing ... that\nwe can make valid interpretations of test performances.\nHe also suggests that the problem might not be\nwith the LSP tests or with their specification of the\ntarget language use domain but with the assessment\ncriteria applied. He argues (Douglas, 2001b) that just\nas we analyse the target language use situation in\norder to develop the test content and methods, we\nshould exploit that source when we develop the\nassessment criteria. This might help us to avoid\nexpecting a perfection of the test taker that is not\nmanifested in authentic performances in the target\nlanguage use situation.\nBut perhaps the real challenge to the field is in\nidentifying when it is absolutely necessary to know\nhow well someone can communicate in a specific\ncontext or if the information being sought is equally\nobtainable through a general-purpose language test.\nThe answer to this challenge might not be as easily\nreached as is sometimes presumed.\nComputer-based testing\nComputer-based testing has witnessed rapid growth\nin the past decade and computers are now used to\ndeliver language tests in many settings. A computer-\nbased version of the TOEFL was introduced on a\nregional basis in the summer of 1998, tests are now\navailable on CD ROM, and the Internet is increas-\ningly used to deliver tests to users. Alderson (1996)\npoints out that computers have much to offer\nlanguage testing: not just for test delivery, but also for\ntest construction, test compilation, response capture,\ntest scoring, result calculation and delivery, and test\nanalysis. They can also, of course, be used for storing\ntests and details of candidates.\nIn short, computers can be used at all stages in the\ntest development and administration process. Most\nwork reported in the literature, however, concerns\nthe compilation, delivery and scoring of tests by\ncomputer. Fulcher (1999b) describes the delivery of\nan English language placement test over the Web and\nGervais (1997) reports the mixed results of transfer-\nring a diagnostic paper-and-pencil test to the com-\nputer. Such articles set the scene for studies of\ncomputer-based testing which compare the accuracy\nof the computer-based test with a traditional paper-\nand-pencil test, addressing the advantages of a com-\nputer-delivered test in terms of accessibility and\nspeed of results, and possible disadvantages in terms\nof bias against those with no computer familiarity, or\nwith negative attitudes to computers.\nThis concern with bias is a recurrent theme in the\nliterature, and it inspired a large-scale study by the\nEducational Testing Service (ETS), the developers of\nthe computer-based version of the TOEFL, who\nneeded to show that such a test would not be biased\nagainst those with no computer literacy. Jamieson et\nah (1998) describe the development of a computer-\nbased tutorial intended to train examinees to take\nthe computerised TOEFL. Taylor et al. (1999) exam-\nine the relationship between computer familiarity\nand TOEFL scores, showing that those with high\ncomputer familiarity tend to score higher on the\ntraditional TOEFL. They compare examinees with\nhigh and low computer familiarity in terms of their\nperformance on the computer tutorial and on com-\nputerised TOEFL-like tasks.They claim that no rela-\ntionship was found between computer familiarity\nand performance on the computerised tasks after\ncontrolling for English language proficiency. They\nconclude that there is no evidence of bias against\ncandidates with low computer familiarity, but also\n224\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessnnent (Part 1)\ntake comfort in the fact that all candidates will be\nable to take the computer tutorial before taking an\noperational computer-based TOEFL.\nThe commonest use of computers in language\ntesting is to deliver tests adaptively (e.g.,Young et al.,\n1996). This means that the computer adjusts the\nitems to be delivered to a candidate in the light of\nthat candidates success or failure on previous items.\nIf the candidate fails a difficult item, s\/he is presented\nwith an easier item, and if s\/he gets an item correct,\ns\/he is presented with a more difficult item. This has\nadvantages: firstly, candidates are presented with\nitems at their level of ability, and are not faced with\nitems that are either too easy or too difficult, and sec-\nondly, computer-adaptive tests (CATs) are typically\nquicker to deliver, and security is less of a problem\nsince different candidates are presented with different\nitems. Many authors discuss the advantages of CATs\n(Laurier, 1998; Brown, 1997; Chalhoub-Deville &\nDeville, 1999;Dunkel, 1999), but they also emphasise\nissues that test developers and score users must\naddress when developing or using CATs. When\ndesigning such tests, developers have to take a num-\nber of decisions: what should the entry level be, and\nhow is this best determined for any given popula-\ntion? At what point should testing cease (the so-\ncalled exit point) and what should the criteria be\nthat determine this? How can content balance best\nbe assured in tests where the main principle for\nadaptation is psychometric? What are the conse-\nquences of not allowing users to skip items, and can\nthese consquences be ameliorated? How to ensure\nthat some items are not presented much more fre-\nquendy than others (item exposure), because of their\nfacility, or their content? Brown and Iwashita (1996)\npoint out that grammar items in particular will vary\nin difficulty according to the language background\nof candidates, and they show how a computer-adap-\ntive test of Japanese resulted in very different item\ndifficulties for speakers of English and Chinese. Thus\na CAT may also need to take account of the lan-\nguage background of candidates when deciding\nwhich items to present, at least in grammar tests, and\nconceivably also in tests of vocabulary.\nChalhoub-Deville and Deville (1999) point out\nthat, despite the apparent advantages of computer-\nbased tests, computer-based testing relies over-\nwhelmingly on selected response (typically multiple-\nchoice questions) discrete-point tasks rather than\nperformance-based items, and thus computer-based\ntesting may be restricted to testing linguistic knowl-\nedge rather than communicative skills. However,\nmany computer-based tests include tests of reading,\nwhich is surely a communicative skill. The question\nis whether computer-based testing offers any added\nvalue over paper-and-pencil reading tests: adaptivity\nis one possibility, although some test developers are\nconcerned that since reading tests typically present\nseveral items on one text \u2014 what is known in the\njargon as a testlet \u2014 they may not be suitable for\ncomputer-adaptivity. This concern for the inherent\nconservatism of computer-based testing has a long\nhistory (see Alderson, 1986a, 1986b, for example), and\nsome claimed innovations, for example, computer-\ngenerated cloze and multiple-choice tests (Coniam,\n1997, 1998) were actually implemented as early as\nthe 1970s, and were often criticised in the literature\nfor risking the assumption of automatic validity. But\nrecent developments offer some hope. Burstein et al.\n(1996) argue for the relevance of new technologies\nin innovation in test design, construction, trialling,\ndelivery, management, scoring, analysis and report-\ning. They review ways in which new input devices\n(e.g., voice and handwriting recognition), output\ndevices (e.g., video, virtual reality), software such as\nauthoring tools, and knowledge-based systems for\nlanguage analysis could be used, and explore\nadvances in the use of new technologies in comput-\ner-assisted learning materials. However, as they point\nout, 'innovations applied to language assessment lag\nbehind their instructional counterparts ... the situa-\ntion is created in which a relatively rich language\npresentation is followed by a limited productive\nassessment.'(1996:245).\nNo doubt, this is largely due to the fact that com-\nputer-based tests require the computer to score\nresponses. However, Burstein et al. (1996) argue that\nhuman-assisted scoring systems could reduce this\ndependency. (Human-assisted scoring systems are\ncomputer-based systems where most scoring of\nresponses is done by computer but responses that the\nprograms are unable to score are given to humans for\ngrading.) They also give details of free-response scor-\ning tools which are capable of scoring responses up\nto 15 words long which correlate highly with human\njudgements (coefficients of between .89 and .98 are\nreported). Development of such systems for short-\nanswer questions and for essay questions has since\ngone on apace. For example, ETS has developed an\nautomated system for assessing productive language\nabilities, called 'e-rater'. e-rater uses natural language\nprocessing techniques to duplicate the performance\nof humans rating open-ended essays. Already, the\nsystem is used to rate GMAT (Graduate Management\nAdmission Test) essays and research is ongoing\nfor other programmes, including second\/foreign\nlanguage testing situations. Burstein et al. conclude\nthat 'the barriers to the successful use of technology\nfor language testing are less technical than conceptu-\nal' (1996: 253), but progress since that article was\npublished is extremely promising.\nAn example of the use of IT to assess aspects of\nthe speaking ability of second\/foreign language\nlearners of English is PhonePass. PhonePass (www.\nordinate.org) is delivered over the telephone, and\ncandidates are asked to read texts aloud, repeat heard\nsentences, say words opposite in meaning to heard\nwords, and give short answers to questions. The sys-\n225\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\ntern uses speech recognition technology to rate\nresponses, by comparing candidate performance to\nstatistical models of native and non-native perfor-\nmance on the tasks. The system gives a score that\nreflects a candidate's ability to understand and\nrespond appropriately to decontextualised spoken\nmaterial, with 40% of the evaluation reflecting the\nfluency and pronunciation of the responses. Alderson\n(2000c) reports that reliability coefficients of 0.91\nhave been found as well as correlations with the Test\nof Spoken English (TSE) of 0.88 and with an ILR\n(Inter-agency Language Roundtable) Oral\nProficiency Interview (OPI) of 0.77. An interesting\nfeature is that the scored sample is retained on a data-\nbase, classified according to the various scores\nassigned. This enables users to access the speech sam-\nple, in order to make their own judgements about\nthe performance for their particular purposes, and to\ncompare how their candidate has performed with\nother speech samples that have been rated either the\nsame, or higher or lower.\nIn addition to e-rater and PhonePass there are a\nnumber of promising initiatives in the use of com-\nputers in testing. The listening section of the com-\nputer-based TOEFL uses photos and graphics to\ncreate context and support the content of the mini-\nlectures, producing stimuli that more closely approx-\nimate 'real world' situations in which people do more\nthan just listen to voices. Moreover, candidates wear\nheadphones, can adjust the volume control, and are\nallowed to control how soon the next question is\npresented. One innovation in test method is that\ncandidates are required to select a visual or part of a\nvisual; in some questions candidates must select two\nchoices, usually out of four, and in others candidates\nare asked to match or order objects or texts.\nMoreover, candidates see and hear the test questions\nbefore the response options appear. (Interestingly,\nGinther, forthcoming, suggests, however, that the use\nof visuals in the computer-based TOEFL listening\ntest depresses scores somewhat, compared with tradi-\ntionally delivered tests. More research is clearly need-\ned.)\nIn the Reading section candidates are required to\nselect a word, phrase, sentence or paragraph in the\ntext itself, and other questions ask candidates to\ninsert a sentence where it fits best. Although these\ntechniques have been used elsewhere in paper-and-\npencil tests, one advantage of their computer format\nis that the candidate can see the result of their choice\nin context, before making a final decision. Although\nthese innovations may not seem very exciting,\nBennett (1998) claims that the best way to innovate\nin computer-based testing is first to mount on com-\nputer what can already be done in paper-and-pencil\nformat, with possible minor improvements allowed\nby the medium, in order to ensure that the basic soft-\nware works well, before innovating in test method\nand construct. Once the delivery mechanisms work,\n226\nit is argued, then computer-based deliveries can be\ndeveloped that incorporate desirable innovations.\nDIALANG (http:\/\/www.dialang.org) is a suite\nof computer-based diagnostic tests (funded by the\nEuropean Union) which are available over the Inter-\nnet, thus capitalising on the advantages of Internet-\nbased delivery (see below). DIALANG uses self-\nassessment as an integral part of diagnosis. Users'\nself-ratings are combined with objective test results\nin order to identify a suitably difficult test for the\nuser. DIALANG gives users feedback immediately,\nnot only on their test scores, but also on the relation-\nship between their test results and their self-assess-\nment. DIALANG also gives extensive advice to users\non how they can progress from their current level to\nthe next level of language proficiency, basing this\nadvice on the Common European Framework\n(Council of Europe, 2001).The interface and support\nlanguage, and the language of self-assessment and of\nfeedback, can be chosen by the test user from a list of\n14 European languages. Users can decide which skill\nor language aspect (reading, writing, listening, gram-\nmar and vocabulary) they wish to be tested in, in any\none of the same 14 European languages. Currently\navailable test methods consist of multiple-choice, gap-\nfilling and short-answer questions, but DIALANG\nhas already produced CD-based demonstrations of 18\ndifferent experimental item types which could be\nimplemented in the future, and the CD demonstrates\nthe use of help, clue, dictionary and multiple-attempt\nfeatures.\nAlthough DIALANG is limited in its ability to\nassess users' productive language abilities, the experi-\nmental item types include a promising combination\nof self-assessment and benchmarking. Tasks for the\nelicitation of speaking and writing performances are\nadministered to pilot candidates and performances are\nrated by human judges.Those performances on which\nraters achieve the greatest agreement are chosen as\n'benchmarks'. A DIALANG user is presented with the\nsame task, and, in the case of a writing task, responds\nvia the keyboard. The user's performance is then pre-\nsented on screen alongside the pre-rated benchmarks.\nThe user can compare their own performance with\nthe benchmarks. In addition, since the benchmarks are\npre-analysed, the user can choose to see raters' com-\nments on various features of the benchmarks, in\nhypertext form, and consider whether they could pro-\nduce a similar quality of such features. In the case of\nSpeaking tasks, the candidate is simply asked to imag-\nine how they would respond to the task, rather than\nactually to record their performance. They are then\npresented with recorded benchmark performances,\nand are asked to estimate whether they could do bet-\nter or worse than each performance. Since the perfor-\nmances are graded, once candidates have self-assessed\nthemselves against a number of performances, the sys-\ntem can tell them roughly what level their own (imag-\nined) performance is likely to be.\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\nThese developments illustrate some of the advan-\ntages of computer-based assessment, which make\ncomputer-based testing not only more user-friendly,\nbut also more compatible with language pedagogy.\nHowever, Alderson (2000c) argues the need for a\nresearch agenda, which would address the challenge\nof the opportunities afforded by computer-based\ntesting and the data that can be amassed. Such an\nagenda would investigate the comparative advantages\nand added value of each form of assessment \u2014 IT-\nbased or not IT-based. This includes issues like the\neffect of providing immediate feedback, support\nfacilities, second attempts, self-assessment, confidence\ntesting, and the like. Above all, it would seek to throw\nmore light onto the nature of the constructs that can\nbe tested by computer-based testing:\nWhat is needed above all is research that will reveal more about\nthe validity of the tests, that will enable us to estimate the effects\nof the test method and delivery medium; research that will pro-\nvide insights into the processes and strategies test-takers use;\nstudies that will enable the exploration of the constructs that are\nbeing measured, or that might be measured ... And we need\nresearch into the impact of the use of the technology on learn-\ning, on learners and on the curriculum. (Alderson, 2000c: 603)\nSelf-assessment\nThe previous section has shown how computer-\nbased testing can incorporate test takers' self-assess-\nment of their abilities in the target language. Until\nthe 1980s references to self-assessment were rare but\nsince then interest in self-assessment has increased.\nThis increase can at least in part be attributed to an\nincreased interest in involving the learner in all phas-\nes of the learning process and in encouraging learner\nautonomy and decision making in (and outside) the\nlanguage classroom (e.g., Blanche & Merino, 1989).\nThe introduction of self-assessment was viewed as\npromising by many, especially in formative assess-\nment contexts (Oscarson, 1989). It was considered to\nencourage increasing sophistication in learner aware-\nness, helping learners to: gain confidence in their\nown judgement; acquire a view of evaluation that\ncovers the whole learning process; and see errors as\nsomething helpful. It was also seen to be potentially\nuseful to teachers, providing information on learning\nstyles, on areas needing remediation and feedback on\nteaching (Barbot, 1991).\nHowever, self-assessment also met with consider-\nable scepticism, largely due to concerns about the\nability of learners to provide accurate judgements of\ntheir achievement and proficiency. For instance, Blue\n(1988), while acknowledging that self-assessment is\nan important element in self-directed learning and\nthat learners can play an active role in the assessment\nof their own language learning, argues that learners\ncannot self-assess unaided. Taking self-assessment\ndata gathered from students on a pre-sessional EAP\nprogramme, he reports a poor correlation between\nteachers' assessments of the students and their own\nself-assessments. He also shows that in multicultural\ngroups such as those typical of pre-sessional EAP\ncourses, overestimates of language proficiency are\nmore common than underestimates. Finally, he\nargues that learners'lack of familiarity with metalan-\nguage and with the practice of discussing language\nproficiency in terms of its composite skills impairs\ntheir capacity for identifying their precise language\nlearning needs.\nSuch concerns, however, did not dampen enthusi-\nasm for investigations in this area and research in the\n1980s was concerned with the development of self-\nassessment instruments and their validation (e.g.,\nOscarson, 1984; Lewkowicz & Moon, 1985). Con-\nsequently, a variety of approaches were developed\nincluding pupil progress cards, learning diaries, log\nbooks, rating scales and questionnaires. In the last\ndecade the research focus has shifted towards\nenhancing our understanding of the evaluation tech-\nniques that were already in existence through\ncontinued validation exercises and by applying self-\nassessment in new contexts or in new ways.\nFor instance, Blanche (1990) uses standardised\nachievement and oral proficiency tests both for test-\ning and for self-assessment purposes, arguing that this\napproach helps to circumvent the problems of train-\ning that are associated with self-assessment question-\nnaires. Hargan (1994) documents the use of a\n'do-it-yourself instrument for placement purposes,\nreporting that it results in much the same placement\nlevels as suggested by a traditional multiple-choice\ntest. Hargun argues that placement testing for large\nnumbers in her context has resulted in the imple-\nmentation of a traditional multiple-choice grammar-\nbased placement test and a consequent emphasis on\nteaching analytic grammar skills. She believes that\nthe 'do-it-yourself-placement' instrument might help\nto redress the emphasis on grammar and stem the\nneglect of reading and writing skills in the classroom.\nCarton (1993) discusses how self-assessment can\nbecome part of the learning process. He describes his\nuse of questionnaires to encourage learners to reflect\non their learning objectives and preferred modes of\nlearning. He also presents an approach to monitoring\nlearning that involves the learners in devising their\nown criteria, an approach that he argues helps learn-\ners to become more aware of their own cognitive\nprocesses.\nA typical approach to validating self-assessment\ninstruments has been to obtain concurrent validity\nstatistics by correlating the self-assessment measure\nwith one or more external measures of student per-\nformance (e.g., Shameem, 1998; Ross, 1998). Other\napproaches have included the use of multi-trait\nmulti-method (MTMM) designs and factor analysis\n(Bachman & Palmer, 1989) and a split-ballot tech-\nnique (Heilenman, 1990). In general, these studies\nhave found self-assessment to be a robust method for\n227\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessnnent (Part 1)\ngathering information about learner proficiency and\nthat the risk of cheating is low (see Barbot, 1991).\nHowever, they also indicate that some approaches to\ngathering self-assessment data are more effective than\nothers. Bachman and Palmer (1989) report that\nlearners were more able to identify what they found\ndifficult to do in a language than what they found\neasy. Therefore, 'Can-do' questions were the least\neffective question type of the three they used in their\nMTMM study, while the most effective question\ntype appeared to be that which asked about the\nlearners' perceived difficulties with aspects of the\nlanguage.\nAdditionally, learner experience of the self-\nassessment procedure and\/or the language skill being\nassessed has been found to affect self-assessments.\nHeilenman (1990), in a study of the role of response\neffects, reports both an acquiescence effect (the ten-\ndency to respond positively to an item regardless of\nits content) and a tendency to overestimate ability,\nthese tendencies being more marked among less\nexperienced learners. Ross (1998) has found that the\nreliability of learners' self-assessments is affected\nby their experience of the skill being assessed. He\nsuggests that when learners do not have memory of a\ncriterion, they resort to recollections of their general\nproficiency in order to make their judgement. This\nprocess is more likely to be affected by the method\nof the self-assessment instrument and by factors such\nas self-flattery. He argues, therefore, for the design of\ninstruments that are cast in terms which offer learn-\ners a reference point such as specific curricular con-\ntent. In a similar finding Shameem (1998) reports\nthat respondents' self-assessments of their oral profi-\nciency in Fijian Hindi are less reliable at the highest\nlevels of the self-assessment scale. Like Ross, he\nattributes this slip in accuracy to the respondents'\nlack of familiarity with the criterion measure.\nOscarson (1997) sums up progress in this area by\nreminding us that research in self-assessment is still\nrelatively new. He acknowledges that conundrums\nremain. For instance, learner goals and interpreta-\ntions need to be reconciled with external impera-\ntives. Also self-assessment is not self-explanatory; it\nmust be introduced slowly and learners need to be\nguided and supported in their use of the instruments.\nFurthermore, particularly when using self-assessment\nin multicultural groups, it is important to consider\nthe cultural influences on self-assessment. Never-\ntheless, he considers the research so far to be promis-\ning. Despite residual concerns about the accuracy of\nself-assessment, the majority of studies report favour-\nable results and we have already learned a great deal\nabout the appropriate methodology to use for cap-\nturing self-assessments. However, as Oscarson points\nout, more work is needed, both in the study of fac-\ntors that influence self-assessment ratings in various\ncontexts and in the selection and design of materials\nand methods for self-assessment.\n228\nAlternative assessnnent\nSelf-assessment is one example of what is increasingly\ncalled 'alternative assessment'. 'Alternative assessment'\nis usually taken to mean assessment procedures\nwhich are less formal than traditional testing, which\nare gathered over a period of time rather than being\ntaken at one point in time, which are usually forma-\ntive rather than summative in function, are often\nlow-stakes in terms of consequences, and are claimed\nto have beneficial washback effects. Although such\nprocedures may be time-consuming and not very\neasy to administer and score, their claimed advantages\nare that they provide easily understood information,\nthey are more integrative than traditional tests and\nthey are more easily integrated into the classroom.\nMcNamara (1998) makes the point that alternative\nassessment procedures are often developed in an\nattempt to make testing and assessment more respon-\nsive and accountable to individual learners, to pro-\nmote learning and to enhance access and equity in\neducation (1998: 310). Hamayan (1995) presents a\ndetailed rationale for alternative assessment, describes\ndifferent types of such assessment, and discusses pro-\ncedures for setting up alternative assessment. She also\nprovides a very useful bibliography for further refer-\nence.\nA recent special issue of Language Testing, guest-\nedited by McNamara (Vol 18, 4, October 2001)\nreports on a symposium to discuss challenges to the\ncurrent mainstream in language testing research,\ncovering issues like assessment as social practice,\ndemocratic assessment, the use of outcomes based\nassessment and processes of classroom assessment.\nSuch discussions of alternative perspectives are close-\nly linked to so-called critical perspectives (what\nShohamy calls critical language testing).\nThe alternative assessment movement, if it may be\ntermed such, probably began in writing assessment,\nwhere the limitations of a one-off impromptu single\nwriting task are apparent. Students are usually given\nonly one, or at most two tasks, yet generalisations\nabout writing ability across a range of genres are\noften made. Moreover, it is evidently the case that\nmost writing, certainly for academic purposes but\nalso in business settings, takes place over time,\ninvolves much planning, editing, revising and redraft-\ning, and usually involves the integration of input\nfrom a variety of (usually written) sources. This is in\nclear contrast with the traditional essay which usually\nhas a short prompt, gives students minimal input,\nminimal time for planning and virtually no opportu-\nnity to redraft or revise what they have produced\nunder often stressful, time-bound circumstances. In\nsuch situations, the advocacy of portfolios of pieces\nof writing became a commonplace, and a whole\nportfolio assessment movement has developed, espe-\ncially in the USA for first language writing (Hamp-\nLyons & Condon, 1993, 1999) but also increasingly\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\nfor ESL writing assessment (Hamp-Lyons, 1996) and\nalso for the assessment of foreign languages (French,\nSpanish, German, etc.) writing assessment.\nAlthough portfolio assessment in other subject\nareas (art, graphic design, architecture, music) is not\nnew, in foreign language education portfolios have\nbeen hailed as a major innovation, supposedly over-\ncoming the drawbacks of traditional assessment. A\ntypical example is Padilla et al. (1996) who describe\nthe design and implementation of portfolio assess-\nment in Japanese, Chinese, Korean and Russian, to\nassess growth in foreign language proficiency. They\nmake a number of practical recommendations to\nassist teachers wishing to use portfolios in progress\nassessment.\nHughes Wilhelm (1996) describes how portfolio\nassessment was integrated with criterion-referenced\ngrading in a pre-university English for academic\npurposes programme, together with the use of con-\ntract grading and collaborative revision of grading\ncriteria. It is claimed that such an assessment scheme\nencourages learner control whilst maintaining\nstandards of performance.\nShort (1993) discusses the need for better assessment\nmodels for instruction where content and language\ninstruction are integrated. She describes examples\nof the implementation of a number of alternative\nassessment measures, such as checklists, portfolios,\ninterviews and performance-tasks, in elementary and\nsecondary school integrated content and language\nclasses.\nAlderson (2000d) describes a number of alterna-\ntive procedures for assessing reading, including\nchecklists, teacher-pupil conferences, learner diaries\nand journals, informal reading inventories, classroom\nreading aloud sessions, portfolios of books read, self-\nassessments of progress in reading, and the like.\nMany of the accounts of alternative assessment are\nfor classroom-based assessment, often for assessing\nprogress through a programme of instruction.\nGimenez (1996) gives an account of the use of\nprocess assessment in an ESP course; Bruton (1991)\ndescribes the use of continuous assessment over a full\nschool year in Spain, to measure achievement of\nobjectives and learner progress. Haggstrom (1994)\ndescribes ways she has successfully used a video\ncamera and task-based activities to make classroom-\nbased oral testing more communicative and realistic,\nless time-consuming for the teacher, and more\nenjoyable and less stressful for students. Lynch (1988)\ndescribes an experimental system of peer evaluation\nusing questionnaires in a pre-sessional EAP summer\nprogramme, to assess speaking abilities. He concludes\nthat this form of evaluation had a marked effect on\nthe extent to which speakers took their audience\ninto account. Lee (1989) discusses how assessment\ncan be integrated with the learning process, illustrat-\ning her argument with an example where pupils pre-\npare, practise and perform a set task in Spanish\ntogether. She offers practical tips for how teachers\ncan reduce the amount of paperwork involved in\nclassroom assessment of this sort. Sciarone (1995) dis-\ncusses the difficulties of monitoring learning with\nlarge groups of students (in contrast with that of\nindividuals) and describes the use, with 200 learners\nof Dutch, of a simple monitoring tool (a personal\ncomputer) to keep track of the performance of indi-\nvidual learners on a variety of learning tasks.\nTypical of these accounts, however, is the fact that\nthey are descriptive and persuasive, rather than\nresearch-based, or empirical studies of the advantages\nand disadvantages of'alternative assessment'. Brown\nand Hudson (1998) present a critical overview of\nsuch approaches, criticising the evangelical way in\nwhich advocates assert the value and indeed validity\nof their procedures without any evidence to support\ntheir assertions. They point out that there is no such\nthing as automatic validity, a claim all too often made\nby the advocates of alternative assessment. Instead\nof 'alternative assessment', they propose the term\n'alternatives in assessment', pointing out that there\nare many different testing methods available for\nassessing student learning and achievement. They\npresent a description of these methods, including\nselected-response techniques, constructed-response\ntechniques and personal-response techniques.\nPortfolio and other forms of'alternative assessment'\nare classified under the latter category, but Brown\nand Hudson emphasise that they should be subject to\nthe same criteria of reliability, validity and practicali-\nty as any other assessment procedure, and should be\ncritically evaluated for their 'fitness for purpose',\nwhat Bachman and Palmer (1996) called'usefulness'.\nHamp-Lyons (1996) concludes that portfolio scoring\nis less reliable than traditional writing rating; little\ntraining is given and raters may be judging the writer\nas much as the writing. Brown and Hudson empha-\nsise that decisions for use of any assessment proce-\ndure should be informed by considerations of\nconsequences (washback), the significance and need\nfor, and value of, feedback based on the assessment\nresults, and the importance of using multiple sources\nof information when making decisions based on\nassessment information.\nClapham (2000b) makes the point that many\nalternative assessment procedures are not pre-tested\nand trialled, their tasks and mark schemes are there-\nfore of unknown or even dubious quality, and despite\nface validity, they may not tell the user very much at\nall about learners' abilities.\nIn short, as Hamayan (1995) admits, alternative\nassessment procedures have yet to 'come of age', not\nonly in terms of demonstrating beyond doubt their\nusefulness, in Bachman and Palmer's terms, but\nalso in terms of being implemented in mainstream\nassessment, rather than in informal class-based assess-\nment. She argues that consistency in the application\nof alternative assessment is still a problem, that mech-\n229\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\nanisms for thorough self-criticism and evaluation of\nalternative assessment procedures are lacking, that\nsome degree of standardisation of such procedures\nwill be needed if they are to be used for high-stakes\nassessment, and that the financial and logistic viability\nof such procedures remains to be demonstrated.\nAssessing young learners\nFinally, in this first part of our review, we consider\nrecent developments in the assessment of young\nlearners, an area where it is often argued that alterna-\ntive assessment procedures are more appropriate than\nformal testing procedures. Typically considered to\napply to the assessment of children between the ages\nof 5 and 12 (but also including much younger and\nslightly older children), the assessment of young\nlearners dates back to the 1960s. However, research\ninterest in this area is relatively new and the last\ndecade has witnessed a plethora of studies (e.g., Low\net ah, 1993; McKay et al, 1994; Edelenbos &\nJohnstone, 1996; Breen et al, 1997; Leung &\nTeasdale, 1997;TESOL, 1998; Blondin et al, 1998).\nThis trend can be largely attributed to three factors.\nFirstly, second language teaching (particularly\nEnglish) to children in the pre-primary and primary\nage groups both within mainstream education and\nby commercial organisations, has mushroomed.\nSecondly, it is recognised that classrooms have\nbecome increasingly multi-cultural and, particularly\nin the context of Australia, Canada, the United States\nand the UK, many learners are speakers of English as\nan additional\/second language (rather than heritage\nspeakers of English). Thirdly, the decade has seen an\nincreased proliferation, within mainstream education,\nof teaching and learning standards (e.g., the National\nCurriculum Guidelines in England and Wales) and\ndemands for accountability to stakeholders.\nThe research that has resulted falls broadly into\nthree areas: the assessment of language delay and\/or\nimpairment, the assessment of young learners with\nEnglish as an additional\/second language, and the\nassessment of foreign languages in primary\/elemen-\ntary school.\nChanges in the measurement of language delay\nand\/or impairment have been attributed to theoreti-\ncal and practical advances in speech and language\ntherapy. It is claimed that these advances have, in\nturn, wrought changes in the scope of what is\ninvolved in language assessment and in the methods\nby which it takes place (Howard et ah, 1995).\nResulting research has included reflection on the\npredictive validity of tests involving language pro-\nduction that are used as standard screening for lan-\nguage delay in children as young as 18 months\n(particularly in the light of research evidence that\nproduction and comprehension are not functionally\ndiscrete before 28 months) (Boyle et ah, 1996). Other\nresearch, however, has looked at the nature of the\n230\nlanguage disorder. Windsor (1999) investigates the\neffect of semantic inconsistency on sentence gram-\nmaticality judgements for children with and without\nlanguage-learning disabilities (LD), finding that chil-\ndren with LD differed most from their chronological\nage-group peers in the identification of ungrammati-\ncal sentences and that it is important to consider the\neffect on performance of competing linguistic infor-\nmation in the task. Holm et al (1999) have developed\na phonological assessment procedure for bilingual\nchildren, using this assessment to describe the\nphonological development, in each language, of\nnormally developing bilingual children as well as of\ntwo bilingual children with speech disorders. They\nconclude that the normal phonological development\nof bilingual children differs from monolingual\ndevelopment in each of the languages and that the\nphonological output of bilingual children with\nspeech disorders reflects a single underlying deficit.\nThe findings of these studies have implications for\nthe design of assessment tools as well as for the need\nto identify appropriate norms against which to\nmeasure performance on the assessments.\nSuch issues, particularly the identification of\nappropriate norms of performance, are also impor-\ntant in studies of young learners' readiness to access\nmainstream education in a language other than their\nheritage language. Recent research involving learn-\ners of English as an additional or second language\n(EAL\/ESL) has benefited from work in the 1980s\n(e.g., Stansfield, 1981; Cummins, 1984a, 1984b; Barrs\net ah, 1988;Trueba, 1989) which problematised the\nuse of standardised tests that had been normed on\nmonolingual learners of English. The equity consid-\nerations they raised, particularly the false positive diag-\nnosis of EAL\/ESL learners as having learning\ndisabilities, has resulted in the development of\nEAL\/ESL learner 'profiles' (also called standards\/\nbenchmarks\/scales) (see NLLIA, 1993; Australian\nEducation Council, 1994;TESOL, 1998). Research\nhas also focused on the provision of guidance for\nteachers when monitoring and reporting on learner\nprogress (see McKay & Scarino, 1991; Genesee &\nHamayan, 1994; Law & Eckes, 1995). Curriculum-\nbased age-level tasks have also been developed to\nhelp teachers observe performance and place learn-\ners on a common framework\/standard (Lumley et\nah, 1993).\nHowever, these directions, though productive,\nhave not been unproblematic, not least because they\nimply (and indeed encourage) differential assessment\nfor EAL\/ESL learners in order for individual\nstudents' needs to be identified and addressed. This\ncan result in tension between the concerns of the\neducational system for ease of administration, appear-\nances of equity and accountability and those of\nteachers for support in teaching and learning (see\nBrindley, 1995). Indeed, Australia and England and\nWales have now introduced standardised testing\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\nfor all learners regardless of language background.\nThe latter two countries are purportedly follow-\ning a policy of entitlement for all but, as McKay\n(2000) argues, their motives are far more likely to\nbe to simplify\/rationalise reporting in order to\nmake comparisons across schools and on which\nto predicate funding. Furthermore, and somewhat\nparadoxically, as Leung and Teasdale (1996) have\nestablished, the use of standardised attainment targets\ndoes not result in more equitable treatment of learners,\nbecause teachers implicitly apply native-speaker\nnorms in making judgements of EAL\/ESL learner\nperformances.\nLatterly, research has focused on classroom-based\nteacher assessment, looking, in the case of Rea-\nDickins and Gardner (2000), at the constructs under-\nlying formative and summative assessment and, in the\ncase of Teasdale and Leung (2000), at the epistemic\nand practical challenges for alternative assessment.\nThe overriding conclusion of both studies is that\n'insufficient research has been done to establish\nwhat, if any, elements of assessment for learning and\nassessment as measurement are compatible' (Teasdale\n& Leung, 2000: 180), a concern no doubt shared by\nresearchers studying the introduction of assessment\nof foreign languages in primary\/elementary schools.\nIndeed, the growing tendency to introduce a\nforeign language at the primary school level has\nresulted in a parallel growth in interest in how this\nearly learning might be assessed. This research focuses\non both formative (e.g., Hasselgren, 1998; Gattullo,\n2000; Hasselgren, 2000; Zangl, 2000) and summative\nassessment (Johnstone, 2000; Edelenbos & Vinje,\n2000) and is primarily concerned with how young\nlearners' foreign language skills might be assessed,\nwith an emphasis on identifying what learners can\ndo. Motivated in many cases by a need to evaluate\nthe effectiveness of language programmes (e.g.,\nCarpenter et al., 1995; Edelenbos & Vinje, 2000),\nthese studies document the challenges of designing\ntests for young learners. In doing so they cite, among\nother factors: the learners' need for fantasy and fun,\nthe potentially detrimental effect of perceived 'fail-\nure' on future language learning, the need to design\ntasks that are developmentally appropriate and com-\nparable for children of different language abilities\nwho have studied in different schools\/language pro-\ngrammes and the potential problem inherent in tasks\nwhich encourage children to interact with an unfa-\nmiliar adult in the test situation (see Carpenter et al,\n1995; Hasselgren, 1998,2000).The studies also reflect\na desire to understand how teachers implement\nassessment (Gatullo, 2000) as well as a need for\ninducting teachers into assessment practices in con-\ntexts where there is no tradition of assessment\n(Hasselgren, 1998).\nRecent years have also seen a phenomenal\nincrease in the number of commercial language\nclasses for young learners with a consequent market\nfor certification of progress. The latest additions to the\ncertificates available are the Saxoncourt Tests forYoung\nLearners of English (STYLE) (http:\/\/www.saxon-\ncourt.com\/publishing.htm) and a suite of tests for\nyoung learners developed by the University of\nCambridge Local Examinations Syndicate (UCLES):\nStarters, Movers and Flyers (http:\/\/www.cambridge-\nefl.org\/exam\/young\/bg_yle.htm)\nIn the development of the latter, the cognitive\ndevelopment of young learners has purportedly been\ntaken into account and though certificates are issued,\nthese are intended to reward young learners for what\nthey can do. By adopting this approach it is hoped\nthat the tests will be used to find out what the learn-\ners already know\/have learned and to check if teach-\ning objectives have been achieved (Wilson, 2001).\nIt is clear that, despite an avowed preference for\nteacher-based formative assessment, recent research\non assessing young learners documents a growth in\nformal assessment and ongoing research exemplifies\nthe movement towards greater standardisation of\nassessment activities and measures of attainment.\nFurthermore, the expansion in formal assessment has\nled to increased specification of the language targets\nyoung learners might plausibly be expected to reach\nand indicates the spread of centrally specified\ncurriculum goals. It seems that the field has moved\nforward in its understanding of the assessment needs\nof young learners yet has been pressed back by eco-\nnomic considerations. The challenge in the next\ndecade will perhaps lie in addressing the tension\nbetween these competing agendas.\nIn this first part of the two-part review of lan-\nguage testing and assessment, we have reviewed rela-\ntively new concerns in language testing, beginning\nwith an account of research into washback, and then\nmoving on to discuss issues in the ethics and politics\nof language testing and the development of standards\nfor language tests. After describing trends in testing\non a national level and developments in testing\nfor specific purposes, we surveyed developments\nin computer-based testing before discussing self-\nassessment and alternative assessment. Finally we\nreviewed the assessment of young learners.\nIn the second part of this review, to appear in April\n2002, we describe developments in what are basically\nrather traditional concerns in language testing\nresearch, looking at the major language constructs\n(reading, listening, and so on) but in the context of\na new approach to validity and validation, some-\ntimes known as the Messick approach, or construct\nvalidation.\nReferences\nALDERSON, J. C. (1986a). Computers in language testing. In\nG. N. Leech & C. N. Candlin (Eds.), Computers in English\nlanguage education and research (pp. 99-111). London: Longman.\nALDERSON, J. C. (1986b). Innovations in language testing? In M.\n231\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\nPortal (Ed.), Innovations in language testing (pp. 93-105).\nWindsor: NFER\/Nelson.\nALDERSON, J. C. (1988). Testing English for Specific Purposes:\nhow specific can we get? ELTDocuments, 127,16-28.\nALDERSON, J. C. (1991). Language testing in the 1990s: How far\nhave we got? How much further have we to go? In S. Anivan\n(Ed.), Current developments in language testing (Vol. 25, pp. 1-26).\nSingapore: SEAMEO Regional Language Centre.\nALDERSON, J. C. (1996). Do corpora have a role in language\nassessment? In J. Thomas & M. Short (Eds.), Using corpora for\nlanguage research (pp. 248\u201459). Harlow:Longman.\nALDERSON,J. C. (1997). Ethics and language testing. Paper present-\ned at the annualTESOL Convention, Orlando, Florida.\nALDERSON, J. C. (1998).Testing and teaching: the dream and the\nreality. novELTy, 5(4), 23-37.\nALDERSON, J. C. (1999). What does PESTI have to do with us\ntesters? Paper presented at the International Language\nEducation Conference, Hong Kong.\nALDERSON, J. C. (2000a). Levels of performance. In J. C.\nAlderson, E. Nagy, & E. Oveges (Eds.), English language educa-\ntion in Hungary, Part II: Examining Hungarian learners' achieve-\nments in English. Budapest: The British Council.\nALDERSON, J. C. (2000b). Exploding myths: Does the number of\nhours per week matter? novELTy, 7(1), 17-32.\nALDERSON, J. C. (2000c). Technology in testing: the present and\nthe future. System 28 (4) 593-603.\nALDERSON, J. C. (2000d). Assessing reading. Cambridge:\nCambridge University Press.\nALDERSONj. C. (2001a).Testing is too important to be left to the\ntester. Paper presented at the 3rd Annual Language Testing\nSymposium, Dubai, United Arab Emirates.\nALDERSON, J. C. (2001b). The lift is being fixed. You will be un-\nbearable today (Or why we hope that there will not be translation\non the new English erettsegi). Paper presented at the Magyar\nMacmillan Conference, Budapest, Hungary.\nALDERSON.J . C. & BUCK, G. (1993). Standards in testing: a study\nof the practice of UK examination boards in EFL\/ESL testing.\nLanguageTesting, 20(1), 1-26.\nALDERSONJ. C , CLAPHAM, C. & WALL, D. (1995). Language test con-\nstruction and evaluation. Cambridge: Cambridge University Press.\nA L D E R S O N J . C. & HAMP-LYONS, L. (1996).TOEFL preparation\ncourses: a study of washback. Language Testing, 13(3), 280-97.\nA L D E R S O N J . C , NAGY, E. & OVEGES^E. (Eds.) (2000a). English\nlanguage education in Hungary, Part II: Examining Hungarian\nlearners' achievements in English. Budapest: The British Council.\nALDERSON.J.C, PERCSICH, R. & SZABO, G. (2000b). Sequencing\nas an item type. Language Testing, 17 (4), 423\u201447.\nALDERSON, J. C. & WALL, D. (1993). Does washback exist?\nApplied Linguistics, 14(2), 115-29.\nALTE (1998)MLTJ5 handbook ojEuropean examinations and exam-\nination systems. Cambridge: UCLES.\nAUSTRALIAN EDUCATION COUNCIL (1994). ESL Scales.\nMelbourne: Curriculum Corporation.\nBACHMAN, L. F. & PALMER, A. S. (1989).The construct validation\nof self-ratings of communicative language ability. Language\nTesting, 6(1), 14-29.\nBACHMAN, L. F. & PALMER,A. S. (1996). Language testing in practice.\nOxford: Oxford University Press.\nBAILEY, K. (1996). Working for washback: A review of the wash-\nback concept in language testing. Language Testing, 13(3),\n257-79.\nBAKER, C. (1988). Normative testing and bilingual populations.\nJournal of Multilingual and Multicultural Development, 9(5),\n399-409.\nBANERJEE, J., CLAPHAM, C , CLAPHAM, P. & WALL, D. (Eds.)\n(1999). ILTA language testing bibliography 1990-1999, First edi-\ntion. Lancaster, UK: Language Testing Update.\nBARBOT, M.-J. (1991). New approaches to evaluation in self-\naccess learning (trans, from French). Etudes de Linguistique\nAppliquee, 79,77-94.\nBARNES, A., H U N T , M. & POWELL, B. (1999). Dictionary use in\nthe teaching and examining of MFLs at GCSE. Language\nLearning Journal, 19,19-27.\nBARNES, A. & POMFRETT, G. (1998). Assessment in German at\nKS3: how can it be consistent, fair and appropriate? Deutsch:\nLehren und Lernen, 17,2-6.\nBARRS, M., ELLIS, S., HESTER, H. & THOMAS, A. (1988). Tlie\nPrimary Language Record: A handbook for teachers. London:\nCentre for Language in Primary Education.\nBENNETT, R. E. (1998). Reinventing assessment: speculations on the\nfuture of large-scale educational testing. Princeton, New Jersey:\nEducational Testing Service.\nBHGEL, K. & LEIJN, M. (1999). New exams in secondary educa-\ntion, new question types. An investigation into the reliability\nof the evaluation of open-ended questions in foreign-\nlanguage exams. LevendeTalen, 537,173-81.\nBLANCHE, P. (1990). Using standardised achievement and oral\nproficiency tests for self-assessment purposes: the DLIFLC\nstudy. Language Testing, 7(2), 202\u201429.\nBLANCHE, P. & M E R I N O , B. J. (1989). Self-assessment of foreign\nlanguage skills: implications for teachers and researchers.\nLanguage Learning, 39(3), 313-40.\nBLONDIN, C , CANDELIER, M., EDELENBOS, P., JOHNSTONE, R.,\nKUBANEK-GERMAN, A. & TAESCHNER, T. (1998). Foreign\nlanguages in primary and preschool education: context and outcomes.\nA review of recent research within the European Union. London:\nCILT.\nBLUE, G. M. (1988). Self assessment: the limits of learner inde-\npendence. ELT Documents, 131,100-18.\nBOLOGNA DECLARATION (1999) Joint declaration of the\nEuropean Ministers of Education convened in Bologna on the\n19th of June 1999. http:\/\/europa.eu.int\/comm\/education\/\nsocrates\/erasmus\/bologna.pdf\nBOYLE, J., GlLLHAM, B. & SMITH, N. (1996). Screening for early\nlanguage delay in the 18-36 month age-range: the predictive\nvalidity of tests of production and implications for practice.\nChild Language Teaching and Tlierapy, 12(2), 113-27.\nBREEN, M. P., BARRATT-PUGH, C , DEREWIANKA, B., HOUSE, H.,\nHUDSON, C , LUMLEY,T., & R O H L , M. (Eds.) (1997). Profiling\nESL children: how teachers interpret and use national and state\nassessment frameworks (Vol. 1). Commonwealth of Australia:\nDepartment of Employment, Education, Training and Youth\nAffairs.\nBRINDLEY, G. (1995). Assessment and reporting in language learning\nprograms: Purposes, problems and pitfalls. Plenary presentation at\nthe International Conference on Testing and Evaluation in\nSecond Language Education, Hong Kong University of\nScience and Technology, 21-24 June 1995.\nBRINDLEY, A. (1998). Outcomes-based assessment and reporting\nin language learning programmes: a review of the issues.\nLanguageTesting, 35(1),45-85.\nBRINDLEY, G. (2001). Outcomes-based assessment in practice:\nsome examples and emerging insights. Language Testing, 18(4),\n393-407.\nBROWN, A. (1995). The effect of rater variables in the develop-\nment of an occupation-specific language performance test.\nLanguageTesting, 32(1), 1-15.\nBROWN, A. & IWASHITA, N. (1996). Language background and\nitem difficulty: the development of a computer-adaptive test\nofjapanese. System, 24(2), 199-206.\nB R O W N . A . & LUMLEY,T. (1997). Interviewer variability in specif-\nic-purpose language performance tests. In A. Huhta, V.\nKohonen, L. Kurki-Suonio & S. Luoma (Eds.), Current develop-\nments and alternatives in language assessment (137-50).Jyvaskyla:\nCentre for Applied Language Studies, University of Jyvaskyla.\nBROWN, J. D. (1997). Computers in language testing: present\nresearch and some future directions. Language Learning and\nTechnology, 3(1), 44-59.\nBROWN, J. D. & HUDSON,T. (1998).The alternatives in language\nassessment. TESOL Quarterly, 32(4), 653-75.\n232\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\nBRUTON, A. (1991). Continuous assessment in Spanish state\nschools. Language Testing Update, 10,14\u201420.\nBUCKBY, M. (1999). The'use of the target language at GCSE.\nLanguage Learning Journal, 19,4-11.\nBURSTEIN, J., FRASE, L. T., GINTHER, A. & GRANT, L. (1996).\nTechnologies for language assessment. Annual Review of\nApplied Linguistics, 16,240-60.\nCARPENTER, K., FUJII, N. & KATAOKA, H. (1995). An oral inter-\nview procedure for assessing second language abilities in chil-\ndren. Language Testing, 12(2), 157-81.\nCARROLL, B. J. & WEST, R. (1989). ESU Framework: Performance\nscales for English language examinations. Harlow: Longman.\nCARTON, F. (1993). Self-evaluation at the heart of learning. Le\nFrancais dans le Monde (special number), 28-35.\nCELESTINE, C. & CHEAH, S. M. (1999). The effect of background\ndisciplines on IELTS scores. In R. Tulloh (Ed.), 1ELTS\nResearch Reports 1999 (Vol. 2, 36-51). Canberra: IELTS\nAustralia Pty Limited.\nCHALHOUB-DEVILLE, M. & DEVILLE, C. (1999). Computer-\nadaptive testing in second language contexts. Annual Review of\nApplied Linguistics, 19,273-99.\nCHAMBERS, F. & RICHARDS, B. (1992). Criteria for oral assess-\nment. Latiguage Learning Journal, 6, 5-9.\nCHAPELLE, C. (1999). Validity in language assessment. Annual\nReview of Applied Linguistics, 19,254-72.\nCHARGE, N. & TAYLOR, L. B. (1997). Recent developments in\nIELTS. ELTJournal, 51(4), 374-80.\nCHEN, Z. & HENNING, G. (1985). Linguistic and cultural bias in\nlanguage proficiency tests. Language Testing, 2(2), 155-63.\nCHENG, L. (1997). How does washback influence teaching?\nImplicat ions for H o n g K o n g . Language and Education 11(1),\n38-54.\nCLAPHAM, C. (1996). Tlie development of IELTS: a study of the effect\nof background knowledge on reading comprehension (Studies in\nLanguage Testing Series, Vol. 4). Cambridge: University of\nCambridge Local Examinations Syndicate and Cambridge\nUniversity Press.\nCLAPHAM, C. (2000a). Assessment for academic purposes: where\nnext? System, 28,511-21.\nCLAPHAM, C. (2000b). Assessment and testing. Annual Review of\nApplied Linguistics, 20,147-61.\nCONIAM, D. (1994). Designing an ability scale for English across\nthe range of secondary school forms. Hong Kong Papers in\nLinguistics and Language Teaching, 17,55-61.\nCONIAM, D. (1995). Towards a common ability scale for Hong\nKong English secondary-school forms. Language Testing, 12(2),\n182-93.\nCONIAM, D. (1997). A computerised English language proofing\ncloze program. Computer-Assisted Language Learning, 10(1),\n83-97.\nCONIAM, D. (1998). From text to test, automatically - an evalua-\ntion of a computer cloze-test generator. Hong Kong Journal of\nApplied Linguistics, 3(1), 41-60.\nCOUNCIL OF EUROPE (2001). A Common European Framework of\nreference for learning, teaching and assessment. Cambridge:\nCambridge University Press.\nCSEPES, I., SULYOK, A. & OVEGES, E. (2000). The pilot speaking\nexaminations. In J. C. Alderson, E. Nagy & E. Oveges (Eds.),\nEnglish language education in Hungary, Part II: Examining Hungarian\nlearners' achievements in English. Budapest:The British Council.\nCUMMING, A. (1994). Does language assessment facilitate recent\nimmigrants' participation in Canadian society? TESL Canada\nJournal, 11 (2), 117-33.\nCUMMING, A. (1995). Changing definitions of language profi-\nciency: functions of language assessment in educational pro-\ngrammes for recent immigrant learners of English in Canada.\nJournal of the CAAL, J 7(1), 35-48.\nCUMMINS, J. (1984a). Bilingualism and special education: Issues in\nassessment and pedagogy. Clevedon, England: Multilingual\nMatters.\nCUMMINS, J. (1984b). Wanted: A theoretical framework for relat-\ning language proficiency to academic achievement among\nbilingual students. In C. Rivera (Ed.), Language proficiency and\nacademic achievement (Vol. 10). Clevedon, England: Multilingual\nMatters.\nDAVIDSON, F. (1994). Norms appropriacy of achievement tests:\nSpanish-speaking children and English children's norms.\nLanguage Testing, 11(1), 83-95.\nDAVIES, A. (1978). Language testing: survey articles 1 and 2.\nLanguage Teaching and Linguistics Abstracts, 11, 145-59 and\n215-31.\nDAVIES, A. (1997). Demands of being professional in language\ntesting. Language Testing, 14(3), 328-39.\nDAVIES, A. (2001). The logic of testing Languages for Specific\nPurposes. Language Testing, 18(2), 133-47.\nDE JONG,J. H. A. L. (1992). Assessment of language proficiency in\nthe perspective of the 21st century. AILA Review, 9,39-45.\nDOLLERUP, C , GLAHN, E.& ROSENBERG HANSEN, C. (1994).\n'Sprogtest': a smart test (or how to develop a reliable and\nanonymous EFL reading test). Language Testing, 11(1), 65-81.\nDOUGLAS, D. (1995). Developments in language testing. Animal\nReview of Applied Linguistics, 15,167-87.\nDOUGLAS, D. (1997). Language for specific purposes testing. In\nC. Clapham & D. Corson (Eds.), Language testing and assessment\n(Vol. 7, 111-19). Dordrecht, The Netherlands: Kluwer\nAcademic Publishers.\nDOUGLAS, D. (2000). Assessing languages for specific purposes.\nCambridge: Cambridge University Press.\nDOUGLAS, D. (2001a).Three problems in testing language for spe-\ncific purposes: authenticity, specificity and inseparability. In C.\nElder, A. Brown, E. Grove, K. Hill, N. Iwashita.T. Lumley.T. F.\nMcNamara & K. O'Loughlin (Eds.), Experimenting with uncer-\ntainty: essays in honour of Alan Davies (Studies in Language Testing\nSeries, Vol. 11, 45\u201451). Cambridge: University of Cambridge\nLocal Examinations Syndicate and Cambridge University Press.\nDOUGLAS, D. (2001b). Language for Specific Purposes assessment\ncriteria: where do they come from? Language Testing, 18(2),\n171-85.\nDOUGLAS, D. & SELINKER, L. (1992). Analysing oral proficiency\ntest performance in general and specific-purpose contexts.\nSystem, 20(3), 317-28.\nDUNKEL, P. (1999). Considerations in developing or using sec-\nond\/foreign language proficiency computer-adaptive tests.\nLanguage Learning and Technology, 2(2), 77\u201493.\nEDELENBOS, P. & JOHNSTONE, R. (Eds.). (1996). Researching lan-\nguages at primary school: some European perspectives. London:\nCILT, in collaboration with Scottish CILT and GION.\nEDELENBOS, P. &VlNJE, M. P. (2000).The assessment of a foreign\nlanguage at the end of primary (elementary) education.\nLanguageTesting, 17(2), 144-62.\nELDER, C. (1997). What does test bias have to do with fairness?\nLanguageTesting, 14(3), 261-77.\nELDER, C. (2001). Assessing the language proficiency of teachers:\nare there any border controls? LanguageTesting, 18(2), 149-70.\nFEKETE, H., MAJOR, E. & NIKOLOV, M. (Eds.) (1999). English\nlanguage education in Hungary: A baseline study. Budapest: The\nBritish Council.\nFox, J., PYCHYL, T. & ZUMBO, B. (1997). An investigation of\nbackground knowledge in the assessment of language profi-\nciency. In A. Huhta,V. Kohonen, L. Kurki-Suonio & S. Luoma\n(Eds.), Current developments and alternatives in language assessment\n(367-83).Jyvaskyla: University ofjyva'skyla.\nFULCHER, G. (1999a). Assessment in English for Academic\nPurposes: putting content validity in its place. Applied\nLinguistics, 20(2), 221-36.\nFULCHER, G. (1999b). Computerising an English language\nplacement test. ELTJournal, 53(4), 289-99.\nFULCHER, G. & BAMFORD, R. (1996). I didn't get the grade I\nneed.Where's my solicitor? System, 24(4), 437-48.\nGATTULLO, F. (2000). Formative assessment in ELT primary\n233\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\n(elementary) classrooms: an Italian case study. Language Testing,\n77(2), 278-88.\nGENESEE, F. & HAMAYAN, E.V. (1994). Classroom-based assess-\nment. In F. Genesee (Ed.), Educating second language children.\nCambridge: Cambridge University Press.\nGERVAIS, C. (1997). Computers and language testing: a harmo-\nnious relationship? Francophonie, 16,3-7.\nGIMENEZJ. C. (1996). Process assessment in ESP: input, through-\nput and output. English for Specific Purposes, 15(3), 233-41.\nGlNTHER, A. (forthcoming). Context and content visuals and\nperformance on listening comprehension stimuli. Language\nTesting.\nGROOT, P. J. M. (1990). Language testing in research and educa-\ntion: the need for standards. AILA Review, 7,9-23.\nGuiLLON, M. (1997). L'evaluation ministerielle en classe de\nseconde en anglais. Les Langues Modernes, 2,32-39.\nHAGGSTROM, M. (1994). Using a videocamera and task-based\nactivities to make classroom oral testing a more realistic com-\nmunicative experience. Foreign Language Annals, 27(2),\n161-75.\nHAHN, S., STASSEN,T. & DESCHKE, C. (1989). Grading classroom\noral activities: effects on motivation and proficiency. Foreign\nLanguage Annals, 22(3), 241-52.\nHALLECK, G. B. & MODER, C. L. (1995). Testing language and\nteaching skills of international teaching assistants: the limits of\ncompensatory strategies. TESOL Quarterly, 29(4), 733-57.\nHAMAYAN, E. (1995). Approaches to alternative assessment.\nAnnual Review of Applied Linguistics, 15,212-26.\nHAMILTON.J., LOPES, M., MCNAMARA.T. & SHERIDAN, E. (1993).\nRating scales and native speaker performance on a commu-\nnicatively oriented EAP test. LanguageTesting, 10(3), 337-53.\nHAMP-LYONS, L. (1996). Applying ethical standards to portfolio\nassessment of writing in English as a second language. In M.\nMilanovic & N. Saville (Eds.), Performance testing, cognition and\nassessment: Selected papers from the 15th Language Testing Research\nColloquium (Studies in LanguageTesting Series, Vol. 3,151-64).\nCambridge: Cambridge University Press.\nHAMP-LYONS, L. (1997). Washback, impact and validity: ethical\nconcerns. Language Testing, 14(3), 295-303.\nHAMP-LYONS, L. (1998). Ethics in language testing. In C. M.\nClapham & D. Corson (Eds.), Language testing and assessment\n(Vol. 7). Dordrecht, The Netherlands: Kluwer Academic\nPublishing.\nHAMP-LYONS, L. & CONDON, W. (1993). Questioning assump-\ntions about portfolio-based assessment. College Composition and\nCommunication, 44(2), 176-90.\nHAMP-LYONS, L. & CONDON, W (1999). Assessing college writing\nportfolios: principles for practice, theory, research. Cresskill, NJ:\nHampton Press.\nHARGAN, N. (1994). Learner autonomy by remote control.\nSystem, 22(4), 455-62.\nHASSELGREN.A. (1998). Small words and good testing. Unpublished\nPhD dissertation, University of Bergen, Bergen.\nHASSELGR\u00a3N,A. (2000). The assessment of the English ability of\nyoung learners in Norwegian schools: an innovative approach.\nLanguageTesting, 17(2), 261-77.\nHAWTHORNE, L. (1997). The political dimension of language\ntesting in Australia. Language Testing, 14(3), 248-60.\nHEILENMAN, L. K. (1990). Self-assessment of second language\nability: the role of response effects. Language Testing, 7(2),\n174-201.\nHENRICHSEN, L. E. (1989). Diffusion of innovations in English lan-\nguage teaching: The ELEC effort in fapan, 1956-1968. New\nYork: Greenwood Press.\nHOLM, A., DODD, B., STOW, C. & PERT, S. (1999). Identification\nand differential diagnosis of phonological disorder in bilingual\nchildren. LanguageTesting, 16(3), 271-92.\nHOWARD, S., HARTLEYJ. & MUELLER, D. (1995).The changing\nface of child language assessment: 1985-1995. Child Language\nTeaching and Therapy, 11(1), 7-22.\nHUGHES, A. (1993). Backwash and TOEFL 2000. Unpublished\nmanuscript, University of Reading.\nHUGHES WILHELM, K. (1996). Combined assessment model for\nEAP writing workshop: portfolio decision-making, criterion-\nreferenced grading and contract negotiation. TESL Canada\nJournal, 14(1), 21-33.\nHURMANJ. (1990). Deficiency and development. Francophonie, 1,\n8-12.\nILTA - INTERNATIONAL LANGUAGE TESTING ASSOCIATION\n(1997). Code of practice for foreign\/ second language testing.\nLancaster: ILTA. [Draft,March, 1997].\nILTA - INTERNATIONAL LANGUAGE TESTING ASSOCIATION.\nCode of Ethics. [http:\/\/www.surrey.ac.uk\/ELI\/ltrfile\/ltr-\nframe.html]\nJAMIESON, J., TAYLOR, C , KIRSCH, I. & EIGNOR, D. (1998).\nDesign and evaluation of a computer-based TOEFL tutorial.\nSystem, 26(4), 485-513.\nJANSEN, H. & PEER, C. (1999). Using dictionaries with national\nforeign-language examinations for reading comprehension.\nLevende Talen, 544,639-41.\nJENNINGS, M., FOX.J., GRAVES, B. & SHOHAMY, E. (1999). The\ntest-takers' choice: an investigation of the effect of topic on\nlanguage-test performance. LanguageTesting, 16(4), 426\u201456.\nJENSEN, C. & HANSEN, C. (1995) The effect of prior knowledge\non EAP listening-test performance, Language Testing, 12(\\),\n99-119.\nJOHNSTONE, R. (2000). Context-sensitive assessment of modern\nlanguages in primary (elementary) and early secondary educa-\ntion: Scotland and the European experience. Language Testing,\n17(2), 123-43.\nKALTER, A. O. & VOSSEN, P. W. J. E. (1990). EUROCERT: an\ninternational standard for certification of language proficiency.\nAILA Review, 7,91-106.\nKHANIYAH.T. R. (1990a). Examinations as instruments for education-\nal change: Investigating the washback effect of the Nepalese English\nexams. Unpublished PhD dissertation, University of Edinburgh,\nEdinburgh.\nKHANIYAH,T. R. (1990b). The washback effect of a textbook-\nbased test. Edinburgh Working Papers in Applied Linguistics, 1,\n48-58.\nKlEWEG, W. (1992). Leistungsmessung im Fach Englisch:\nPraktischeVorschlage zur Konzeption von Lernzielkontrollen.\nFremdsprachenunterricht, 45(6), 321-32.\nKIEWEG, W (1999). Allgemeine Giitekriterien fiir Lernziel-\nkontrollen (Common standards for the control of learning).\nDer Fremdsprachliche Unterricht Englisch, 3 7(1), 4\u201411.\nLAURIER, M. (1998). Methodologie devaluation dans des\ncontextes d'apprentissage des langages assistes par des environ-\nnements informatiques multimedias. Etudes de Linguistique\nAppliquee, 110,247-55.\nLAW, B. & ECKES, M. (1995). Assessment and ESL. Winnipeg,\nCanada: Peguis.\nLEE, B. (1989). Classroom-based assessment - why and how?\nBritish Journal of Language Teaching, 27(2), 73\u20146.\nLEUNG, C. &TEASDALE, A. (1996). English as an additional lan-\nguage within the National Curriculum: A study of assessment\npractices. Prospect, 12(2), 58-68.\nLEUNG, C. & TEASDALE, A. (1997). What do teachers mean by\nspeaking and listening: a contextualised study of assessment in\nthe English National Curriculum. In A. Huhta,V. Kohonen, L.\nKurki-Suonio & S. Luoma (Eds.), New contexts,goals and alter-\nnatives in language assessment (291-324). Jyvaskyla: University\nofjyvaskyla.\nLEWKOWICZ, J. A. (1997). Investigating authenticity in language\ntesting. Unpublished PhD dissertation, Lancaster University,\nLancaster.\nLEWKOWICZ, J. A. (2000). Authenticity in language testing: some\noutstanding questions. Language Testing, 17(1), 43-64.\nLEWKOWICZ, J. A., & MOON, J. (1985). Evaluation, a way of\ninvolving the learner. In J. C. Alderson (Ed.), Lancaster Practical\n234\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\nPapers in English Language Education (Vol. 6: Evaluation),\n45-80. Oxford: Pergamon Press.\nLi, K. C. (1997). The labyrinth of exit standard controls. Hong\nKongjournal of Applied Linguistics, 2(1), 23\u201438.\nLIDDICOAT, A. (1996). The Language Profile: oral interaction.\nBabel, 31(2), 4-7,35.\nLIDDICOAT, A. J. (1998). Trialling the languages profile in the\nA.C.T. Babel, 33(2), 14-38.\nLow, L., DUFFIELD, J., BROWN, S. & JOHNSTONE, R. (1993).\nEvaluating foreign languages in Scottish primary schools: report to\nScottish Office. Stirling: University of Stirling: Scottish CILT.\nLUMLEY, T. (1998). Perceptions of language-trained raters\nand occupational experts in a test of occupational English\nlanguage proficiency. English for Specific Purposes, 17(4),\n347-67.\nLUMLEY, T. & BROWN, A. (1998). Authenticity of discourse in a\nspecific purpose test. In E. Li & G.James (Eds.), Testing and\nevaluation in second language education (22-33). Hong Kong: The\nLanguage Centre.The University of Science and Technology.\nLUMLEY,T. & MCNAMARA.T. F. (1995). Rater characteristics and\nrater bias: implications for training. Language Testing, 12(1),\n54-71.\nLUMLEY, T., RASO, E. & MINCHAM, L. (1993). Exemplar assess-\nment activities. In NLLIA (Ed.), NLLIA ESL Development:\nLanguage and Literacy in Schools. Canberra: National Languages\nand Literacy Institute of Australia.\nLYNCH, B. (1997). In search of the ethical test. Language Testing,\n14(3), 315-27.\nLYNCH, B. & DAVIDSON, F. (1994). Criterion-referenced test\ndevelopment: linking curricula, teachers and tests. TESOL\nQuarterly, 28(4), 727-43.\nLYNCH, T. (1988). Peer evaluation in practice. ELT Documents,\n131,119-25.\nMANLEY, J. H. (1995). Assessing students' oral language: one\nschool district's response. Foreign Language Annals, 28(1),\n93-102.\nMCKAY, P. (2000). On ESL standards for school-age learners.\nLanguage Testing, 17(2), 185-214.\nMCKAY, P., HUDSON, C. & SAPUPPO, M. (1994). ESL bandscales,\nNLLIA ESL development: language and literacy in schools project.\nCanberra: National Languages and Literacy Institute of\nAustralia.\nMCKAY, P. & SCARINO, A. (1991). Tlie ESL Framework of Stages.\nMelbourne: Curriculum Corporation.\nMCNAMARA, T. (1998). Policy and social considerations in\nlanguage assessment. Annual Review of Applied Linguistics, 18,\n304-19.\nMCNAMARA, T. F. (1995). Modelling performance: opening\nPandora's box. Applied Linguistics, 16(2), 159-75.\nMcNAMARA.T. F. & LUMLEY,T. (1997).The effect of interlocutor\nand assessment mode variables in overseas assessments of\nspeaking skills in occupational settings. Language Testing, 14(2),\n140-56.\nMESSICK, S. (1994). The interplay of evidence and consequences\nin the validation of performance assessments. Educational\nResearcher, 23(2), 13-23.\nMESSICK, S. (1996). Validity and vvashback in language testing.\nLanguageTesting, 13(3), 241-56.\nMILANOVIC, M. (1995). Comparing language qualifications in\ndifferent languages: a framework and code of practice. System,\n23(4), 467-79.\nMOELLER,A.J. & RESCHKE, C. (1993). A second look at grading\nand classroom performance: report of a research study. Modern\nLanguage Journal, 77(2), 163-9.\nMOORE.T. & MORTONJ. (1999).Authenticity in the IELTS aca-\ndemic module writing test: a comparative study of task 2 items\nand university assignments. In R.Tulloh (Ed.), IELTS Research\nReports 1999 (Vol. 2, 64-106). Canberra: IELTS Australia Pty\nLimited.\nMUNDZECK, F. (1993). Die Problematik objektiver Leistungs-\nmessung in einem kommunikativen Fremdsprachenun-\nterricht: am Beispiel des Franzosischen. Fremdsprachenunterricht,\n46($), 449-54.\nNLLIA (NATIONAL LANGUAGES AND LITERACY INSTITUTE OF\nAUSTRALIA) (1993). NLLIA ESL Development: Language and\nLiteracy in Schools, Canberra: National Languages and Literacy\nInstitute ofAustralia.\nNEIL, D. (1989). Foreign languages in the National Curriculum\n\u2014 what to teach and how to test? A proposal for the Languages\nTask Group. Modern Languages, 70(1), 5\u20149.\nNORTH, B. & SCHNEIDER, G. (1998) Scaling descriptors for lan-\nguage proficiency scales. LanguageTesting, 15 (2), 217\u201462.\nNORTON, B. & STARFIELD, S. (1997). Covert language assessment\nin academic writing. Language Testing, 14(3), 278\u201494.\nOSCARSON, M. (1984). Self-assessment of foreign language skills: a\nsurvey of research and development work. Strasbourg, France:\nCouncil of Europe, Council for Cultural Co-operation.\nOSCARSON, M. (1989). Self-assessment of language proficiency:\nrationale and applications. LanguageTesting, 6(1), 1-13.\nOSCARSON, M. (1997). Self-assessment of foreign and second\nlanguage proficiency. In C. Clapham & D. Corson (Eds.),\nLanguage testing and assessment (Vol. 7,175-87). Dordrecht.The\nNetherlands: Kluwer Academic Publishers.\nPADILLA.A. M., ANINAO.J. C. & SUNG, H. (1996). Development\nand implementation of student portfolios in foreign language\nprograms. Foreign Language Annals, 29(3), 429-38.\nPAGE, B. (1993).The target language and examinations. Language\nLearning Journal, 8,6\u20147.\nPAPAJOHN, D. (1999). The effect of topic variation in perfor-\nmance testing: the case of the chemistry TEACH test for\ninternational teaching assistants. LanguageTesting, 16(1), 52-81.\nPEARSON, I. (1988).Tests as levers for change. In D. Chamberlain\n& R. Baumgardner (Eds.), ESP in the classroom: Practice and\nevaluation (Vol. 128, 98-107). London: Modern English\nPublications.\nPEIRCE, B. N. & STEWART, G. (1997). The development of the\nCanadian Language Benchmarks Assessment. TESL Canada\nJournal, 14(2), 17-31.\nPLAKANS, B. & ABRAHAM, R. G. (1990).The testing and evalua-\ntion of international teaching assistants. In D. Douglas (Ed.),\nEnglish language testing in U.S. colleges and universities (68-81).\nWashington D C : NAFSA.\nPUGSLEY,J. (1988). Autonomy and individualisation in language\nlearning: institutional implications. ELT Documents, 131,\n54-61.\nREA-DICKINS, P. (1987).Testing doctors' written communicative\ncompetence: an experimental technique in English for spe-\ncialist purposes. Quantitative Linguistics, 34,185-218.\nREA-DICKINS, P. (1997). So why do we need relationships with\nstakeholders in language testing? A view from the UK.\nLanguageTesting, 14(3), 304-14.\nREA-DICKINS, P. & GARDNER, S. (2000). Snares or silver bullets:\ndisentangling the construct of formative assessment. Language\nTesting, 17(2), 215-43.\nREAD, J. (1990) Providing relevant content in an EAP writing\ntest, English for Specific Purposes 1,243-68.\nREED, D. J. & HALLECK, G. B. (1997). Probing above the ceiling\nin oral interviews: what's up there? In A. Huhta,V. Kohonen,\nL. Kurki-Suonio & S. Luoma (Eds.), Current developments and\nalternatives in language assessment. Jyvaskyla: University of\nJyvaskyla'.\nRICHARDS, B. & CHAMBERS, F. (1996). Reliability and validity in\nthe GCSE oral examination. Language Learning Journal, 14,\n28-34.\nRoss, S. (1998). Self-assessment in second language testing: a\nmeta-analysis of experiential factors. Language Testing, 15{\\),\n1-20.\nROSSITER, M. & PAWLIKOWSSKA-SMITH, G. (1999). The use of\nCLBA scores in LINC program placement practices in\nWestern Canada. TESL Canada Journal, 16(2),39-52.\n235\nhttp:\/\/journals.cambridge.org Downloaded: 26 Mar 2009 IP address: 194.80.32.9\nLanguage testing and assessment (Part 1)\nROY, M.-J. (1988). Writing in the GCSE - modern languages.\nBritish Journal of Language Teaching, 26(2), 99-102.\nSCIARONE.A. G. (1995). A fully automatic homework checking\nsystem. IRAL, 33(1), 35^6.\nSCOTT, M. L., STANSFIELD, C. W. & KENYON, D. M. (1996).\nExamining validity in a performance test: the listening sum-\nmary translation exam (LSTE). Language Testing, 13,83-109.\nSHAMEEM, N. (1998).Validating self-reported language proficien-\ncy by testing performance in an immigrant community: the\nWellington Indo-Fijans. Language Testing, 15(1), 86\u2014108.\nSHOHAMY, E. (1993). The power oftests:Tlie impact of language tests\non teaching and learning NFLC Occasional Papers. Washington,\nD.C.: The National Foreign Language Center.\nSriOHAMY, E. (1997a).Testing methods, testing consequences: are\nthey ethical? Language Testing, 14(3), 340-9.\nSHOHAMY, E. (1997b). Critical language testing and beyond,\nplenary paper presented at the American Association for\nApplied Linguistics, Orlando, Florida. 8-11 March.\nSHOHAMY.E. (2001a). Tlie power of tests. London: Longman.\nSHOHAMY, E. (2001b). Democratic assessment as an alternative.\nLanguage Testing, 18(4), 373-92.\nSHOHAMY, E., DONITSA-SCHMIDT, S. & FERMAN, I. (1996). Test\nimpact revisited: washback effect over time. Language Testing,\n13(3), 298-317.\nSHORT, D. (1993). Assessing integrated language and content\ninstruction. TESOL Quarterly, 27(4), 627-56.\nSKEHAN, P. (1988). State of the art: language testing, part I.\nLanguage Teaching, 211-21.\nSKEHAN, P. (1989). State of the art: language testing, part II.\nLanguage Teaching, 1-13.\nSPOLSKY,B. (1997).The ethics of gatekeeping tests: what have we\nlearned in a hundred years? LanguageTesting, 14(3), 242-7.\nSTANSFIELD, C.W. (1981).The assessment of language proficiency\nin bilingual children: An analysis of theories and instrumenta-\ntion. In R.V Padilla (Ed.), Bilingual education and technology.\nSTANSFIELD, C. W., SCOTT, M. L. & KENYON, D. M. (1990).\nListening summary translation exam (LSTE) \u2014 Spanish (Final\nProject Report. ERIC Document Reproduction Service, ED\n323 786).Washington DC: Centre for Applied Linguistics.\nSTANSFIELD, C. W, WU, W. M. & Liu, C. C. (1997). Listening\nSummary Translation Exam (LSTE) in Taiwanese, akak Minnan\n(Final Project Report. ERIC Document Reproduction\nService, ED 413 788). N. Bethesda, MD: Second Language\nTesting, Inc.\nSTANSFIELD, C. W . , W U , W . M. & VAN DER HEIDE, M. (2000). A\njob-relevant listening summary translation exam in Minnan.\nIn A. J. Kunnan (Ed.), Fairness and validation in language assess-\nment (Studies in Language Testing Series, Vol. 9, 177-200).\nCambridge: University of Cambridge Local Examinations\nSyndicate and Cambridge University Press.\nTARONE, E. (2001). Assessing language skills for specific purpos-\nes: describing and analysing the 'behaviour domain'. In C.\nElder, A. Brown, E. Grove, K. Hill, N. Iwashita.T. Lumley,T. F.\nMcNamara & K. O'Loughlin (Eds.), Experimenting with uncer-\ntainty: essays in honour of Alan Davies (Studies in Language\nTesting Series, Vol. 11, 53-60). Cambridge: University of\nCambridge Local Examinations Syndicate and Cambridge\nUniversity Press.\nTAYLOR, C , KIRSCH, I., EIGNOR, D. & JAMIESON, J. (1999).\nExamining the relationship between computer familiarity and\nperformance on computer-based language tasks. Language\nLearning, 49(2), 219-74.\nTEASDALE, A. & LEUNG, C. (2000). Teacher assessment and\npsychometric theory: a case of paradigm crossing? Language\nTesting, 17(2), 163-84.\nTESOL (1998). Managing the assessment process. A framework for\nmeasuring student attainment of the ESL standards. Alexandria,\nVA:TESOL.\nTRUEBA, H. T. (1989). Raising silent voices: educating the linguistic\nminorities for the twenty-first century. New York: Newbury House.\nVAN EK,J .A . (1997). The Threshold Level for modern language\nlearning in schools. London: Longman.\nVAN ELMPT, M. & LOONEN, P. (1998). Open questions: answers in\nthe foreign language? Toegepaste Taalwetenschap in Artikelen, 58,\n149-54.\nVANDERGRIFT, L. & BELANGER, C. (1998). The National Core\nFrench Assessment Project: design and field test of formative\nevaluation instruments at the intermediate level. The Canadian\nModern Language Review, 54(4), 553\u201478.\nWALL, D. (1996). Introducing new tests into traditional systems:\nInsights from general education and from innovation theory.\nLanguageTesting, 13(3),334-54.\nWALL, D. (2000). The impact of high-stakes testing on teaching\nand learning: can this be predicted or controlled? System, 28,\n499-509.\nWALL, D. & ALDERSON, J. C. (1993). Examining washback: The\nSri Lankan impact study. LanguageTesting, 10(1), 41-69.\nWATANABE,Y. (1996). Does Grammar-Translation come from the\nEntrance Examination? Preliminary findings from classroom-\nbased research. LanguageTesting, 13(3), 319-33.\nWATANABE,Y. (2001). Does the university entrance examination\nmotivate learners? A case study of learner interviews. Akita\nAssociation of English Studies (ed.). Trans-equator exchanges:\nA collection of acadmic papers in honour of Professor David\nIngram, 100-10.\nWEIR, C. J. & ROBERTS, J. (1994). Evaluation in ELT. Oxford:\nBlackwell Publishers.\nWELLING-SLOOTMAEKERS, M. (1999). Language examinations in\nDutch secondary schools from 2000 onwards. Levende Talen,\n542,488-90.\nWILSON, J. (2001). Assessing young learners: what makes a good test?\nPaper presented at the Association of Language Testers in\nEurope (ALTE) Conference, Barcelona, 5-7 July 2001.\nWINDSORJ. (1999). Effect of semantic inconsistency on sentence\ngrammaticality judgements for children with and without lan-\nguage-learning disabilities. LanguageTesting, 16(3), 293-313.\nWu,W. M. & STANSFIELD, C.W. (2001).Towards authenticity of\ntask in test development. Language Testing, 18(2), 187-206.\nYOUNG, R., SHERMIS, M. D, BRUTTEN, S. R. & PERKINS, K.\n(1996). From conventional to computer-adaptive testing of\nESL reading comprehension. System, 24(1), 23-40.\nYULE, G. (1990). Predicting success for international teaching\nassistants in a US university. TESOL Quarterly, 24(2),227-43.\nZANGL, R. (2000). Monitoring language skills in Austrian prima-\nry (elementary) schools: a case study. Language Testing, 77(2),\n250-60.\n236\n"}