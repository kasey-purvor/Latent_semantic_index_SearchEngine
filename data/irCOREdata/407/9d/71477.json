{"doi":"10.1016\/j.jeconom.2006.10.002","coreId":"71477","oai":"oai:eprints.lancs.ac.uk:868","identifiers":["oai:eprints.lancs.ac.uk:868","10.1016\/j.jeconom.2006.10.002"],"title":"Generalized R-estimators under Conditional heteroscedasticity.","authors":["Mukherjee, Kanchan"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2007-12","abstract":"In this paper, we extend the classical idea of Rank estimation of parameters from homoscedastic problems to heteroscedastic problems. In particular, we define a class of rank estimators of the parameters associated with the conditional mean function of an autoregressive model through a three-steps procedure and then derive their asymptotic distributions. The class of models considered includes Engel's ARCH model and the threshold heteroscedastic model. The class of estimators includes an extension of Wilcoxon-type rank estimator. The derivation of the asymptotic distributions depends on the uniform approximation of a randomly weighted empirical process by a perturbed empirical process through a very general weight-dependent partitioning argument","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/71477.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/868\/1\/Final_Version(T).pdf","pdfHashValue":"1ecd543fa58e82ac86742b37c687084889b47bf4","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:868<\/identifier><datestamp>\n      2018-01-24T03:18:00Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Generalized R-estimators under Conditional heteroscedasticity.<\/dc:title><dc:creator>\n        Mukherjee, Kanchan<\/dc:creator><dc:subject>\n        QA Mathematics<\/dc:subject><dc:description>\n        In this paper, we extend the classical idea of Rank estimation of parameters from homoscedastic problems to heteroscedastic problems. In particular, we define a class of rank estimators of the parameters associated with the conditional mean function of an autoregressive model through a three-steps procedure and then derive their asymptotic distributions. The class of models considered includes Engel's ARCH model and the threshold heteroscedastic model. The class of estimators includes an extension of Wilcoxon-type rank estimator. The derivation of the asymptotic distributions depends on the uniform approximation of a randomly weighted empirical process by a perturbed empirical process through a very general weight-dependent partitioning argument.<\/dc:description><dc:date>\n        2007-12<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/868\/1\/Final_Version(T).pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1016\/j.jeconom.2006.10.002<\/dc:relation><dc:identifier>\n        Mukherjee, Kanchan (2007) Generalized R-estimators under Conditional heteroscedasticity. Journal of Econometrics, 141 (2). pp. 383-415. ISSN 0304-4076<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/868\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":null,"relations":["http:\/\/dx.doi.org\/10.1016\/j.jeconom.2006.10.002","http:\/\/eprints.lancs.ac.uk\/868\/"],"year":2007,"topics":["QA Mathematics"],"subject":["Journal Article","PeerReviewed"],"fullText":"Generalized R-estimators under Conditional\nHeteroscedasticity\nKanchan Mukherjee\nThe University of Liverpool\nEmail: k.mukherjee@liverpool.ac.uk\nAbstract\nIn this paper, we extend the classical idea of Rank-estimation of parameters from\nhomoscedastic problems to heteroscedastic problems. In particular, we define a class\nof rank estimators of the parameters associated with the conditional mean function of\nan autoregressive model through a three-steps procedure and then derive their asymp-\ntotic distributions. The class of models considered includes Engel\u2019s ARCH model and\nthe threshold heteroscedastic model. The class of estimators includes an extension of\nWilcoxon-type rank estimator. The derivation of the asymptotic distributions depends\non the uniform approximation of a randomly weighted empirical process by a perturbed\nempirical process through a very general weight-dependent partitioning argument.\nKeywords: Rank estimation; heteroscedastic model; weighted empirical process; uniform\napproximation.\nJEL Classifications: C14, C22.\nKanchan Mukherjee\nDepartment of Mathematical Sciences\nThe University of Liverpool\nLiverpool, L69 7ZL, UK\nE-mail: k.mukherjee@liverpool.ac.uk\nJune 8, 2006\n1\n1 Introduction\nSince the introduction of the autoregressive conditional heteroscedastic (ARCH) time series\nmodel of Engle (1982), there have been huge developments on the theory and application of\nthis model and its various generalizations to economics and finance. ARCH models have been\nused to represent the volatility, i.e, the strong dependence of the instantaneous variability of\na time series on its own past, in numerous economic and financial data sets. For a literature\nreview, see Bollerslev, Chou, and Kroner (1992), Shephard (1996), and Gourie\u00b4roux (1997),\namong others. Most of the existing methodological literature have focused on developing\nestimation procedures for the parameters associated with the conditional variability using\npseudo-likelihood methods. However, development of the estimation methods associated\nwith the conditional mean component of a heteroscedastic problem is also important from\nthe application point of view and this has been largely overlooked. In this paper, we aim to\nfill that gap by developing a rank-based robust procedure for estimating the mean parameter\nof an autoregressive model with conditional heteroscedastic errors.\nIn a parametric formulation, linearity of regression, independence and normality of er-\nrors, homoscedasticity or form of heteroscedasticity etc. are typically assumed for drawing\nconclusions about parameters of interest. However, there is no guarantee that such regularity\nassumptions will be valid in a given situation and therefore it is natural to investigate alter-\nnative procedures that can perform well under probable departures from model assumptions.\nAmong different types of such robust procedures, estimators based on ranks or the so-called\nR-estimators are sometimes preferable to their other competitors for their global robustness\nproperty as they generally demand much less restrictive assumptions on the underlying dis-\ntributions; see, for example, Jurec\u02c7kova\u00b4 and Sen (1996, Section 3.4) for a discussion on this.\nThe need for using such robust estimators is even more for financial data due to the empirical\nfinding that \u2018outliers\u2019 appear more often in asset returns than that implied by white noises\n2\nhaving normal distribution. For more on this, see Tsay (2002, Section 3.3) and Engle and\nGonzalez-Rivera (1991) who quantified the loss of efficiency resulting from the use of estima-\ntors arising from the first-order conditions for the normal MLE (called the quasi maximum\nlikelihood estimator or the QMLE) on non-normal distributions and concluded that \u2018it is\nworthwhile searching for estimators that can improve on QMLE\u2019.\nThere is a vast literature on the R-estimation of parameters in homoscedastic regression\nand autoregression models. For a glimpse, see Koul (1992, Section 4.4), Jurec\u02c7kova\u00b4 and Sen\n(1996, Section 3.4, Chapter 6) and Ha\u00b4jek, S\u02c7ida\u00b4k and Sen (1999, Section 10.3), among others.\nIn linear regression model with i.i.d. or homoscedastic long memory errors, R-estimators are\nknown to have highly desirable efficiency; see, e.g., Jurec\u02c7kova\u00b4 (1971), Koul (1971), Jaeckel\n(1972) and Koul and Mukherjee (1993). In the homoscedastic autoregressive time series\nmodel (1.1) with \u03c3 \u2261 1, analogs of the R-estimators are known to have similar efficiency and\nrobustness properties as investigated by Koul and Ossiander (1994) and Mukherjee and Bai\n(2002). It is thus natural to investigate their behavior in the heteroscedastic set up.\nAccordingly, consider the following autoregressive model with heteroscedastic error where\nfor known integers s, p and r, {Xi, 1\u2212 s \u2264 i \u2264 n} is an observable time series. SetW i\u22121 :=\n(Xi\u22121, Xi\u22122, . . . , Xi\u2212s)\u2032 and Y i\u22121 = c(W i\u22121), 1 \u2264 i \u2264 n, where c : IRs \u2192 IRp is a known\nfunction. Let \u2126j, j = 1, 2, be open subsets of IR\np, IRr, respectively with \u2126 := \u21261\u00d7\u21262 \u2282 IRm,\nwhere m = p + r. Let \u03c3 be a known function from IRp \u00d7 \u21262 to IR+ := (0,\u221e), differentiable\nin its second argument. Consider the model\nXi = Y\n\u2032\ni\u22121\u03b1+ \u03c3(Y i\u22121,\u03b2) \u03b7i, 1 \u2264 i \u2264 n, (1.1)\nwhere \u03b1 \u2208 \u21261, \u03b2 \u2208 \u21262 are the unknown parameters, and the unobservable errors {\u03b7i, i \u2265 1}\nare i.i.d. with zero mean and finite variance having a distribution function (d.f.) G and\nprobability density function (p.d.f.) g. Throughout, we also assume that {\u03b7i, i \u2265 1} are\nindependent of W 0 := (X0, X\u22121, . . . , X1\u2212s)\u2032 and hence independent of Y 0; for each y \u2208 IRp,\n3\n\u03c3\u02d9(y, t) is the derivative of \u03c3(y, t) with respect to t; and {Xi} is strictly stationary and ergodic.\nAll of these assumptions will be referred to as the model assumptions in the sequel. Although\nsome sufficient conditions for the stationarity and ergodicity of {Xi} in the full generality of\nthe model (1.1) may not be possible at this stage, we discuss the relevant sufficient conditions\nfor particular examples cited below. Our goal here is to develop the asymptotics of the R-\nestimators of the parameter \u03b1 in addition to the estimation of the entire parameter vector\n\u03b8 := (\u03b1\u2032,\u03b2\u2032)\u2032 based on the data W 0, X1, X2, . . . , Xn.\nNote in this connection that model (1.1) is not the \u2018pure ARCH\u2019 model since the condi-\ntional variance depends on a lag of the observed dependent variable, rather than a lag of the\nerror term. In the following, we cite some examples of (1.1).\nExample 1. (Engle\u2019s ARCH model). In the ARCH model introduced by Engle (1982),\none observes {Zi, 1\u2212 s \u2264 i \u2264 n} such that\nZi = (\u03b10 + \u03b11Z\n2\ni\u22121 + . . .+ \u03b1sZ\n2\ni\u2212s)\n1\/2\u000fi, 1 \u2264 i \u2264 n, (1.2)\nwhere \u03b1 = (\u03b10, \u03b11, . . . , \u03b1s)\n\u2032 \u2208 IR+(s+1) := (0,\u221e)(s+1) is the unknown parameter and {\u000fi; 1 \u2264\ni \u2264 n} are unobservable i.i.d. with mean zero, variance 1 and finite fourth moment.\nSquaring both sides of (1.2) and writing \u03b7i := \u03b5\n2\ni\u22121, Xi = Z2i ,W i\u22121 = [Xi\u22121, . . . , Xi\u2212s]\u2032 =\n[Z2i\u22121, . . . , Z\n2\ni\u2212s]\n\u2032, and Y \u2032i\u22121 = [1,W\n\u2032\ni\u22121], model (1.2) can be recast as\nXi = Y\n\u2032\ni\u22121\u03b1+ (Y\n\u2032\ni\u22121\u03b1) \u03b7i, 1 \u2264 i \u2264 n. (1.3)\nThis is an example of the model (1.1) with \u03b1 = \u03b2, c(w) = [1,w]\u2032, w \u2208 [0,\u221e)s, p = s + 1,\nr = s+1, and \u03c3(y, t) = t\u2032y. For various sufficient conditions related to the strict stationarity\nand ergodicity of the process {Zi; 1\u2212 s \u2264 i}, see Nelson (1990), Bougerol and Picard (1992)\nand Giraitis, Kokoszka and Lepius (2000).\nExample 2. (Autoregressive Linear Square Conditional Heteroscedastic model)\n(ARLSCH). Consider the first order autoregressive model with heteroscedastic errors where\n4\none observes {Xi; 0 \u2264 i \u2264 n} such that the conditional variance of the i-th observation Xi\ndepends linearly on the squares of past as follows:\nXi = \u03b1Xi\u22121 + {\u03b20 + \u03b21X2i\u22121}1\/2 \u03b7i, 1 \u2264 i \u2264 n, (1.4)\nwhere \u03b1 \u2208 IR, \u03b2 = (\u03b20, \u03b21)\u2032 \u2208 (0,\u221e)2 and {\u03b7i}\u2019s are i.i.d. with zero mean and unit variance.\nWith the identification s = 1 = p, c(w) = w, r = 2, and\n\u03c3(y, t) = (t0 + t1y\n2)1\/2, y \u2208 IR,\nmodel (1.4) can be seen as an example of (1.1).\nThe assumption needed on the parameters under which the process {Xi; i \u2265 0} of (1.4)\nis strictly stationary and ergodic is as follows:\n|\u03b1|+ E|\u03b71|max{\u03b21\/20 , \u03b21\/21 } < 1. (1.5)\nThis follows by using Lemma 3.1 of Ha\u00a8rdle and Tsybakov (1997, p 227) with C1 = |\u03b1| and\nC2 = max{\u03b21\/20 , \u03b21\/21 } = sup{(\u03b20 + \u03b21x2)1\/2\/(1 + |x|);x \u2208 IR}.\nExample 3. (Autoregressive Threshold Conditional Heteroscedastic model)\n(ARTCH). Consider an s-th order autoregressive model with self exciting threshold het-\neroscedastic errors where the conditional standard deviation of the i-th observation Xi is\npiecewise linear on the past as follows:\nXi = (\u03b11Xi\u22121 + . . . \u03b1sXi\u2212p) +\n{\n\u03b21Xi\u22121I(Xi\u22121 > 0)\u2212 \u03b22Xi\u22121I(Xi\u22121 \u2264 0)\n+ . . .+ \u03b22s\u22121Xi\u2212sI(Xi\u2212s > 0)\u2212 \u03b22sXi\u2212sI(Xi\u2212s \u2264 0)\n}\n\u03b7i, 1 \u2264 i \u2264 n,\nwhere all \u03b2j\u2019s are positive and {\u03b7i}\u2019s are i.i.d. with zero mean and unit variance. For\napplications and many probabilistic properties of this model including conditions on the\nstationarity and ergodicity, see Rabemananjara and Zakoian (1993). For a discussion on the\ndifficulties associated with the asymptotics of the robust estimation in this model due to the\nlack of differentiability caused by threshold, see Rabemananjara and Zakoian (1993, p 38).\n5\nWith the identification p = s, c(w) = w, r = 2p, and\n\u03c3(y, t) =\np\u2211\nj=1\nt2j\u22121yjI(yj \u2265 0) +\np\u2211\nj=1\nt2j(\u2212yj)I(yj < 0), y \u2208 IRp, t \u2208 (0,\u221e)2p,\nthis can be seen as an example of (1.1).\nSome of the important findings on R-estimation under the model (1.1) are as follows.\nIt turns out that efficiency properties similar to homoscedastic models continue to hold for\nthe heteroscedastic setup also; see Remark 3.3 for details. In particular, for every fixed\ninnovation density g satisfying some conditions, optimal R-estimator based on suitable score\nfunction exists. Also, the Wilcoxon R-estimator have asymptotic relative efficiency (ARE)\nof at least 0.864 with respect to the quasi maximum likelihood estimator for a large class of\ninnovation density. Our simulation results reported in Tables 1 and 2 also confirm some of\nthese theoretical efficiency results for a variety of innovation distributions. Moreover, using\nthree well-known real data examples, the robustness of R-estimators against misspecified\nform of the heteroscedasticity is exhibited.\nFor estimation of the conditional mean parameters using the MLE and the least squares\nmethod in an autoregressive model with errors generated by an ARCH process itself, see\nPantula (1988). See also Koenker and Zhao (1996) and Koul and Mukherjee (2002) for\nrelated work on the least absolute deviation and M-estimators.\nThe paper is organized as follows. The class of R-estimators is defined in Section 2. Sec-\ntion 3 states all distributional results and compares R-estimators with least squares estimator\nbased on their asymptotic efficiencies. In Section 4, we verify that conditions of the theorems\nof Section 3 are satisfied for each of the above examples. Analysis of simulated and real data\nare reported in Section 5. Section 6 gives detail proofs of the theoretical results of Section 3.\n6\n2 Generalized R-estimators\nTo define the class of R-estimators, we proceed in three steps. First we estimate \u03b1 in (1.1) by\na preliminary consistent estimator \u03b1\u0302p which only considers the linear additive autoregressive\nstructure of (1.1) but does not take into account the conditional heteroscedasticity of the\nmodel. Next, we use \u03b1\u0302p to construct an estimator \u03b2\u0302 of the parameter \u03b2. Finally, an\nestimator \u03b1\u0302 of \u03b1 based on the estimator \u03b2\u0302 of \u03b2 is defined which does take into account the\nheteroscedastic structure of the model (1.1). Throughout, u\u02d9 will denote the derivative of a\nfunction u.\nStep 1: Define H(\u03c4 1) := n\u22121\/2\u2211ni=1 Y i\u22121(Xi \u2212 Y \u2032i\u22121\u03c4 1). Since E[H(\u03b1)] = 0, a pre-\nliminary least squares estimator of \u03b1 is defined as a solution of H(\u03c4 1) = 0 and is given\nby\n\u03b1\u0302p := [\nn\u2211\ni=1\nY i\u22121Y \u2032i\u22121]\n\u22121[\nn\u2211\ni=1\nXiY i\u22121].\nStep 2: For \u03c4 := (\u03c4 \u20321, \u03c4\n\u2032\n2)\n\u2032 \u2208 \u2126 := \u21261 \u00d7 \u21262, let \u03b7i(\u03c4 ) := [Xi \u2212 Y \u2032i\u22121\u03c4 1]\/\u03c3(Y i\u22121, \u03c4 2)\ndenote the i-th residual, 1 \u2264 i \u2264 n. Let \u03ba be a nondecreasing right continuous functions\non IR such that E{\u03b71\u03ba(\u03b71)} = 1. This is automatically satisfied, for example, when the\ninnovations have unit variance and \u03ba is the identity function (\u03ba(x) \u2261 x) or when it is the\nscore function for location of the maximum likelihood estimator at the error distribution G\ni.e., \u03ba(x) \u2261 \u2212g\u02d9(x)\/g(x). Consider the statistic\nMs(\u03c4 ) := n\n\u22121\/2\nn\u2211\ni=1\n\u03c3\u02d9(Y i\u22121, \u03c4 2)\n\u03c3(Y i\u22121, \u03c4 2)\n[\n\u03b7i(\u03c4 )\u03ba(\u03b7i(\u03c4 ))\u2212 1\n]\n.\nSince E[Ms(\u03b1,\u03b2)] = 0, an estimator of the scale parameter \u03b2 is defined by the relation\n\u03b2\u0302 := argmin{\nr\u2211\nj=1\n|Msj(\u03b1\u0302p, \u03c4 2)|; \u03c4 2 \u2208 \u21262},\nwhere Msj(\u03b1\u0302p, \u03c4 2) is the j-th coordinate of the vector Ms(\u03c4 ), 1 \u2264 j \u2264 r. This definition is\nmotivated by the discussion in Huber (1981, Ch. 7, Eqns. 7.3-7.7) pertaining to the linear\n7\nregression model. The idea is to obtain estimates of the location and concomitant scale\nparameters by solving a simultaneous system of equations. Estimates of the scale parameters\nare obtained by substituting those of the location parameters.\nStep 3: Finally, based on \u03b2\u0302, an improved estimator of \u03b1 can be motivated as follows.\nNote that (1.1) can be written as\nXi\/\u03c3(Y i\u22121,\u03b2) = Y \u2032i\u22121\u03b1\/\u03c3(Y i\u22121,\u03b2) + \u03b7i.\nThis in turn can be approximated by\nXi\/\u03c3(Y i\u22121, \u03b2\u0302) \u2248 {Y i\u22121\/\u03c3(Y i\u22121, \u03b2\u0302)}\u2032\u03b1+ \u03b7i. (2.1)\nThis can be thought as a linear autoregressive model with homoscedastic errors. Hence, ex-\ntending Koul and Ossiander (1994), a class of R-estimators generalized to the heteroscedastic\nmodel can be defined as follows. For 1 \u2264 i \u2264 n, let\nai(\u03c4 2) := Xi\/\u03c3(Y i\u22121, \u03c4 2),\nand\nZi\u22121(\u03c4 2) := Y i\u22121\/\u03c3(Y i\u22121, \u03c4 2).\nLet \u03d5 : [0, 1]\u2192 IR be a (score) function belonging to the class\nF = {\u03d5;\u03d5 : [0, 1]\u2192 IR is right continuous, non-decreasing, with\n\u03d5(1)\u2212 \u03d5(0) = 1}.\nThe function \u03d5(u) = u\u2212 1\/2 in this class corresponds to the Wilcoxon rank score.\nDefine a rank statistic as\nS\u03d5(\u03c4 ) = n\n\u22121\/2\nn\u2211\ni=1\n{Zi\u22121(\u03c4 2)\u2212 Z\u00af(\u03c4 2)}\u03d5\n(\nRi\u03c4\nn+ 1\n)\n, \u03c4 \u2208 \u2126,\n8\nwhere Ri\u03c4 =\n\u2211n\nj=1 I{aj(\u03c4 2) \u2212 \u03c4 \u20321Zj\u22121(\u03c4 2) \u2264 ai(\u03c4 2) \u2212 \u03c4 \u20321Zi\u22121(\u03c4 2)} (the \u03c4 -residual rank of\nthe i-th residual), 1 \u2264 i \u2264 n and Z\u00af(\u03c4 2) = \u2211ni=1Zi\u22121(\u03c4 2)\/n.\nNote that Ri\u03c4 is also the rank of \u03b7i(\u03c4 ) among {\u03b7j(\u03c4 ); 1 \u2264 j \u2264 n}. Hence, E[S\u03d5(\u03b1,\u03b2)] = 0\nand so a generalized R-estimator of \u03b1 corresponding to the score function \u03d5 is defined as\n\u03b1\u0302 = argmin{\np\u2211\nj=1\n|S\u03d5j(\u03c4 1, \u03b2\u0302)|; \u03c4 1 \u2208 \u21261},\nwhere S\u03d5j(\u03c4 ) is the j-th coordinate of the vector S\u03d5(\u03c4 ), 1 \u2264 j \u2264 p.\nSee Section 5 of this paper and Mukherjee (2006 b) for some algebraic expressions for R-\nestimators based on Wilcoxon and the sign score function for simple linear model. Although\nan algebraic expression for an R-estimator for more complex models may not exist in general,\nfast computational algorithms for ranking are available. Using the initial estimator \u03b1\u0302p of \u03b1,\na Newton-Raphson type method can be used to solve this minimization problem. For more\non the existence of the solution to the above minimization problem and computation in the\nanalogous setup, see Jaeckel (1972), Huber (1981, Section 7.3) and Koul (1992, Section 7.3b).\nNote that this minimization problem may not always have unique solution. However, as in\nJurec\u02c7kova\u00b4 (1971, Section 4) for the analogous case of linear regression models, it can be\nshown using the asymptotic uniform linearity result (AUL) of Lemma 3.3 that all solutions\nare asymptotically equivalent.\nRemark 2.1 Strictly speaking, these estimators are not functions of the ranks of the \u03c4 -\nresiduals only. However, we borrow the terminology from the regression and the homoscedastic-\nautoregression settings and still call them (generalized) R-estimators. When, for example,\n\u03d5(u) = u\u2212 1\n2\n, \u03b1\u0302 = \u03b1\u0302\u03d5 is an analogue of the Wilcoxon type R-estimator.\n3 Main results\nOur first result is on the asymptotic distribution of \u03b1\u0302p. Here and in the sequel, the expecta-\ntion of a random matrix is defined as the matrix of entry-wise expectations.\n9\nTheorem 3.1 In the model (1.1), assume that E[Y 0Y\n\u2032\n0\u03c3(Y 0,\u03b2)\n2] <\u221e. Then\nn1\/2(\u03b1\u0302p \u2212\u03b1) =\u21d2 N\n[\n0, E(\u03b721)[E(Y 0Y\n\u2032\n0)]\n\u22121[E(\u03c32(Y 0,\u03b2)Y 0Y \u20320)][E(Y 0Y\n\u2032\n0)]\n\u22121\n]\n. (3.1)\nFor the subsequent results, we need some additional notations and assumptions. Because\nof (2.1), we standardize the mean, variance and various other quantities by \u03c3(Y i\u22121,\u03b2).\nAccordingly, for t1 \u2208 IRp, t2 \u2208 IRr, 1 \u2264 i \u2264 n, let\n\u00b5ni(t1) :=\nY \u2032i\u22121(\u03b1+ n\n\u22121\/2t1)\n\u03c3(Y i\u22121,\u03b2)\n, \u00b5\u02d9ni(t1) :=\nY i\u22121\n\u03c3(Y i\u22121,\u03b2)\n,\n\u03c3ni(t2) :=\n\u03c3(Y i\u22121,\u03b2 + n\u22121\/2t2)\n\u03c3(Y i\u22121,\u03b2)\n, \u03c3\u02d9ni(t2) :=\n\u03c3\u02d9(Y i\u22121,\u03b2 + n\u22121\/2t2)\n\u03c3(Y i\u22121,\u03b2)\n,\nsni(t1, t2) =\n\u00b5\u02d9ni(t1)\n\u03c3ni(t2)\n=\nY i\u22121\n\u03c3(Y i\u22121,\u03b2 + n\u22121\/2t2)\n, rni(t2) :=\n\u03c3\u02d9ni(t2)\n\u03c3ni(t2)\n=\n\u03c3\u02d9(Yi\u22121,\u03b2 + n\u22121\/2t2)\n\u03c3ni(Yi\u22121,\u03b2 + n\u22121\/2t2)\n.\nNote that some of the above quantities, e.g., \u00b5\u02d9ni(t1), are free from both t1 and n; nevertheless,\nwe retain these arguments for consistency. In the sequel, \u00b5\u02d9i, \u00b5i, \u03c3\u02d9i, ri will stand for \u00b5\u02d9ni(0),\n\u00b5ni(0), \u03c3\u02d9ni(0) and rni(0) respectively, as they also do not depend on n. Also, the probability\nand expectation are taken under the model (1.1) under \u03b8 := (\u03b1\u2032,\u03b2\u2032)\u2032. We assume the\nexistence of the following limiting matrices as a consequence of the stationarity and ergodicity,\nwhere \u2192 denotes the convergence in probability. Also condition (3.4) below is a smoothness\ncondition related to the heteroscedasticity.\nThere exist positive definite matrices M(\u03b8), \u03a3\u02d9(\u03b8) and matrices G(\u03b8) and Gc(\u03b8) such that\nn\u22121\nn\u2211\ni=1\n[(\u00b5\u02d9i \u2212 n\u22121\nn\u2211\ni=1\n\u00b5\u02d9i)(\u00b5\u02d9i \u2212 n\u22121\nn\u2211\ni=1\n\u00b5\u02d9i)\n\u2032]\u2192\nE\n{[\nY 0\n\u03c3(Y 0,\u03b2)\n\u2212 E( Y 0\n\u03c3(Y 0,\u03b2)\n)\n][\nY 0\n\u03c3(Y 0,\u03b2)\n\u2212 E( Y 0\n\u03c3(Y 0,\u03b2)\n)\n]\u2032}\n= M(\u03b8), say, (3.2)\nn\u22121\nn\u2211\ni=1\n\u03c3\u02d9i\u03c3\u02d9\n\u2032\ni \u2192 E\n{[\n\u03c3\u02d9(Y 0,\u03b2)\n\u03c3(Y 0,\u03b2)\n][\n\u03c3\u02d9(Y 0,\u03b2)\n\u03c3(Y 0,\u03b2)\n]\u2032}\n= \u03a3\u02d9(\u03b8), say,\nn\u22121\nn\u2211\ni=1\n\u03c3\u02d9i\u00b5\u02d9\n\u2032\ni \u2192 E\n{[\n\u03c3\u02d9(Y 0,\u03b2)\n\u03c3(Y 0,\u03b2)\n][\nY 0\n\u03c3(Y 0,\u03b2)\n]\u2032}\n= G(\u03b8), say, and\n10\nn\u22121\nn\u2211\ni=1\n[(\u00b5\u02d9i \u2212 n\u22121\nn\u2211\ni=1\n\u00b5\u02d9i)\u03c3\u02d9\n\u2032\ni]\u2192\nE\n{[\nY 0\n\u03c3(Y 0,\u03b2)\n\u2212 E( Y 0\n\u03c3(Y 0,\u03b2)\n)\n][\n\u03c3\u02d9(Y 0,\u03b2)\n\u03c3(Y 0,\u03b2)\n]\u2032}\n= Gc(\u03b8), say.\nThere exists a matrix-valued (of order r \u00d7 r) function R\u02d9 on IRp \u00d7 \u21262 such that\nE\u2016R\u02d9(Y 0,\u03b2)\u2016 <\u221e, (3.3)\nand for every \u000f > 0, k > 0, s \u2208 \u21262,\nlim sup\nn\nP\n\uf8eb\uf8ed sup\n1\u2264i\u2264n,n1\/2\u2016t\u2212s\u2016\u2264k\n\u2016rni(t)\u2212 rni(s)\u2212 R\u02d9(Y i\u22121, s)n\u22121\/2(t\u2212 s)\u2016\n\u2016t\u2212 s\u2016 > \u000f\n\uf8f6\uf8f8 = 0. (3.4)\nThe next theorem gives a one-step Taylor-type expansion of Ms around the true parameter\n\u03b8, uniformly on its compact neighbourhood.\nLemma 3.1 Suppose that in the model (1.1) assumptions (3.3) and (3.4) hold. Also,\nlet \u03ba be a nondecreasing twice differentiable function satisfying (i)\n\u222b\nx\u03ba(x)G(dx) = 1, (ii)\u222b\nx2|\u03ba\u02d9(x)|G(dx) <\u221e, and (iii) the second derivative of \u03ba is bounded.\nThen, for every 0 < b <\u221e,\nsup\n\u2016t\u2016\u2264b\n\u2225\u2225\u2225\u2225\u2225Ms(\u03b8 + n\u22121\/2t)\u2212Ms(\u03b8) +\n[\u222b\n\u03ba(x)G(dx) +\n\u222b\nx\u03ba\u02d9(x)G(dx)\n]\nG(\u03b8) t1\n+\n[\u222b\nx\u03ba(x)G(dx) +\n\u222b\nx2\u03ba\u02d9(x)G(dx)\n]\n\u03a3\u02d9(\u03b8) t2\n\u2225\u2225\u2225\u2225\u2225 = op(1).\nTherefore, substituting t1 = n\n1\/2(\u03b1\u0302p \u2212 \u03b1) and t2 = n1\/2(\u03b2\u0302 \u2212 \u03b2), and using the uniform\nconvergence over compacta, we have the following theorem.\nTheorem 3.2 In addition to the assumptions of Theorem 3.1 and Lemma 3.1, assume\nthat\n\u2016n1\/2(\u03b2\u0302 \u2212 \u03b2)\u2016 = Op(1). (3.5)\nThen [\u222b\nx\u03ba(x)G(dx) +\n\u222b\nx2\u03ba\u02d9(x)G(dx)\n]\n\u03a3\u02d9(\u03b8)n1\/2(\u03b2\u0302 \u2212 \u03b2)\n= Ms(\u03b8)\u2212\n[\u222b\n\u03ba(x)G(dx) +\n\u222b\nx\u03ba\u02d9(x)G(dx)\n]\nG(\u03b8) n1\/2(\u03b1\u0302p \u2212\u03b1) + op(1).\n11\nIf \u222b\n\u03ba(x)G(dx) = 0 =\n\u222b\nx\u03ba\u02d9(x)G(dx) (3.6)\nalso, then\n[\u222b\nx\u03ba(x)G(dx) +\n\u222b\nx2\u03ba\u02d9(x)G(dx)\n]\nn1\/2(\u03b2\u0302 \u2212 \u03b2) = (\u03a3\u02d9(\u03b8))\u22121Ms(\u03b8) + op(1).\nNote that under (3.6), the asymptotic distribution of \u03b2\u0302 does not depend on the preliminary\nestimator \u03b1\u0302p used in defining \u03b2\u0302.\nRemark 3.1. Conditions (i)-(iii) of Lemma 3.1 and (3.6) are satisfied by \u03ba(x) \u2261 x when\nE(\u03b721) = 1. Another possible candidate is \u03ba(x) = \u2212g\u02d9(x)\/g(x). In this case\n\u222b\nx\u03ba(x)G(dx) = 1\nand when g is symmetric,\n\u222b\n\u03ba(x)G(dx) = 0. Also for such choices conditions (i)-(iii) and\n(3.6) does not impose any extra moment condition for normal, logistic or double-exponential\nerror densities since they are automatically satisfied.\nThe derivation of the asymptotic results on R-estimators depends on the uniform ap-\nproximation of a randomly weighted empirical process by a perturbed empirical process. We\ndefine these processes under the following probabilistic framework.\nProbabilistic framework: Let {\u03b7i, 1 \u2264 i \u2264 n} be i.i.d. with the d.f. G, {lni, vni, uni; 1 \u2264\ni \u2264 n} be an array of measurable functions from IRm to IR such that for every t \u2208 IRm, and\n1 \u2264 i \u2264 n, (lni(t), vni(t), uni(t)) are independent of \u03b7i. For x \u2208 IR and t \u2208 IRm, let\nV\u02dc(x, t) := n\u22121\/2\nn\u2211\ni=1\nlni(t)I\n(\n\u03b7i < x+ xvni(t) + uni(t)\n)\n,\nJ\u02dc (x, t) := n\u22121\/2\nn\u2211\ni=1\nlni(t)G\n(\nx+ xvni(t) + uni(t)\n)\n,\nU\u02dc(x, t) := V\u02dc(x, t)\u2212 J\u02dc (x, t),\nU\u2217(x, t) := n\u22121\/2\nn\u2211\ni=1\nlni(t)\n[\nI(\u03b7i < x)\u2212G(x)\n]\n.\nHere U\u2217(., .) is a sequence of ordinary weighted empirical processes with weights {lni(.)} and\nU\u02dc(., .) is a sequence of perturbed weighted empirical processes with location perturbations\n12\n{uni(.)} and scale perturbations {vni(.)}. In Lemma 3.2 below it is shown that U\u02dc can be\nuniformly approximated by U\u2217 and this, in turn, will be applied to Lemma 3.3 to approximate\nempirical processes based on residuals that are different from actual errors by location and\nscale factors.\nThe following conditions (3.7)-(3.15) will be referred to as Condition luv. Here in\n(3.7)-(3.13), the assumptions\/convergence hold pointwise for each fixed t \u2208 IRm.\nThere exist numbers q > 2 and \u000f (both free from t) satisfying 0 < \u000f < q\/2 such that with\nCn(t) :=\n\u2211n\ni=1E|lni(t)|q,\nCn(t)\/n\nq\/2\u2212\u000f = o(1), for each t \u2208 IRm. (3.7)\nFor some positive random process `(t),(\nn\u22121\nn\u2211\ni=1\nl2ni(t)\n)1\/2\n= `(t) + op(1), t \u2208 IRm. (3.8)\nE\n(\nn\u22121\nn\u2211\ni=1\nl2ni(t)\n)q\/2\n= O(1), t \u2208 IRm. (3.9)\nmax\n1\u2264i\u2264n\nn\u22121\/2|lni(t)| = op(1), t \u2208 IRm. (3.10)\nmax\n1\u2264i\u2264n\n{|vni(t)|+ |uni(t)|} = op(1), t \u2208 IRm. (3.11)\nnq\/2\u2212\u000f\nCn(t)\nE\n[\nn\u22121\nn\u2211\ni=1\nl2ni(t){|uni(t)|+ |vni(t)|}\n]q\/2\n= o(1), t \u2208 IRm. (3.12)\nn\u22121\/2\nn\u2211\ni=1\n|lni(t)| [|vni(t)|+ |uni(t)|] = Op(1), t \u2208 IRm. (3.13)\n\u2200 b and \u000f > 0, \u2203 \u03b4 > 0, and an n1 3 whenever \u2016s\u2016 \u2264 b, andn > n1, (3.14)\nP\n(\nn\u22121\/2\nn\u2211\ni=1\n|lni(s)|\n{\nsup\n\u2016t\u2212s\u2016<\u03b4\n|vni(t)\u2212 vni(s)|\n+ sup\n\u2016t\u2212s\u2016<\u03b4\n|uni(t)\u2212 uni(s)|\n}\n\u2264 \u000f\n)\n> 1\u2212 \u000f.\n\u2200 b and \u000f > 0, \u2203 a \u03b4 > 0, and an n2, 3 whenever \u2016s\u2016 \u2264 b, andn > n2, (3.15)\nP\n\uf8eb\uf8ed sup\n\u2016t\u2212s\u2016\u2264\u03b4\nn\u22121\/2\nn\u2211\ni=1\n|lni(t)\u2212 lni(s)| \u2264 \u000f\n\uf8f6\uf8f8 > 1\u2212 \u000f.\nConditions (3.7)-(3.15) are regularity conditions on the weights and perturbations of the\n13\ntwo-parameters empirical processes. Conditions (3.14)-(3.15) are smoothness conditions on\nthe weights and perturbations. Under stationarity and ergodicity, many of these conditions\nreduce to much simpler conditions based on existence of the moments. These conditions will\nbe verified for particular examples in Section 4.\nWe also make the following additional assumptions on the error d.f. G.\n\u2022 (G.1) The d.f. G has Lebesgue density g satisfying the following: g is positive on the\nset {x : 0 < G(x) < 1}, g(x) and xg(x) are bounded in x \u2208 IR, and the functions\nu 7\u2192 g(G\u22121(u)) and u 7\u2192 G\u22121(u)g(G\u22121(u)) are uniformly continuous on [0, 1].\n\u2022 (G.2) The d.f. G is uniformly Lipschitz in scale: For some constant 0 < C < \u221e and\nfor every s \u2208 IR, supx\u2208IR |G(x+ xs)\u2212G(x)| \u2264 C |s|.\n\u2022 (G.3) lim\u03b4\u21920 sup{|x| \u222b 10 |g(x)\u2212 g(x+ tx\u03b4)|dt;x \u2208 IR} = 0.\nWe remark here that if the error density g has decreasing tails, then (G.2) is implied by\nsupx\u2208IR |x|g(x) < \u221e, which in turn, is guaranteed by E\u03b72 < \u221e. In this case, more easily\nverifiable conditions ensuring (G.3) can also be obtained. For example, if g is differentiable\nwith the derivative g\u02d9 satisfying sup[x2 sup{|g\u02d9(y)|; x(1 \u2212 \u03b4) < y < x(1 + \u03b4)}, x \u2208 IR] < \u221e,\nfor some \u03b4 > 0, then (G.3) holds. In particular, (G.1)-(G.3) hold for standardized normal,\ndouble-exponential logistic and t-distributions with degrees of freedom more than 2.\nThe following lemma is used for proving the needed result.\nLemma 3.2 Under the above framework, suppose that Condition luv and assumptions\n(G.1)-(G.3) hold. Then for every 0 < b <\u221e,\nsup\nx\u2208IR,\u2016t\u2016\u2264b\n|U\u02dc(x, t)\u2212 U\u2217(x, t)| = op(1). (3.16)\nBased on this lemma, the next result gives a Taylor-type expansion for the R-scores.\nLemma 3.3 Suppose that the assumptions of Lemma 3.2 hold with lni(t) equal to the j-th\ncoordinate (1 \u2264 j \u2264 p) of sni(t), uni(t) := \u00b5ni(t1) \u2212 \u00b5i and vni(t) := \u03c3ni(t2) \u2212 1, 1 \u2264 i \u2264 n.\n14\nThen\nsup\n\u2016t\u2016\u2264b\n\u2225\u2225\u2225\u2225\u2225S\u03d5(\u03b8 + n\u22121\/2t)\u2212 S\u03d5(\u03b8)\n\u2212\n(\u222b\ng(x)\u03d5(G(dx))M(\u03b8)t1 +\n\u222b\nxg(x)\u03d5(G(dx))Gc(\u03b8)t2\n) \u2225\u2225\u2225\u2225\u2225 = op(1).\nTherefore, we have the following theorem on the asymptotic distribution of the R-estimator.\nNote that here the condition n1\/2(\u03b1\u0302\u2212\u03b1) = Op(1) is automatically satisfied as in Jurec\u02c7kova\u00b4\n(1971, Theorem 1.1) and Koul (1996, Corollary 1.1, Remark 1.2) since the mean function in\n(1.1) is a linear function of the parameters.\nTheorem 3.3 In addition to the assumptions of Lemma 3.3, assume that (3.5) holds.\nThen\n(i)\n\u222b\ng(x)\u03d5(G(dx))n1\/2(\u03b1\u0302\u2212\u03b1)\n= \u2212(M(\u03b8))\u22121\n[\nS\u03d5(\u03b8) +Gc(\u03b8)n\n1\/2(\u03b2\u0302 \u2212 \u03b2)\n\u222b\nxg(x)\u03d5(G(dx))\n]\n+ op(1). (3.17)\n(ii) If, in addition to (i), either\n\u222b\nxg(x)\u03d5(G(dx)) = 0 or Gc(\u03b8) = 0, then\nn1\/2(\u03b1\u0302\u2212\u03b1) = \u2212{\n\u222b\ng(x)\u03d5(G(dx))M(\u03b8)}\u22121S\u03d5(\u03b8) + op(1).\nIn order to get the asymptotic normality of the R-estimator \u03b1\u0302, we need to establish the\nsame for\nS\u03d5(\u03b8) = n\n\u22121\/2\nn\u2211\ni=1\n{Zi\u22121(\u03b2)\u2212 Z\u00af(\u03b2)}\u03d5\n(\nRi\nn+ 1\n)\n,\nwhere Ri is the rank of \u03b7i among {\u03b7j; 1 \u2264 j \u2264 n}. But, this is a randomly weighted sum\nof rank scores. Moreover, the random weights\n{\nZi\u22121(\u03b2) \u2212 Z\u00af(\u03b2); 1 \u2264 i \u2264 n\n}\nas well as\n{R1, . . . , Rn} are dependent. However, extending an argument of Koul and Ossiander (1994,\nTheorem 1.2, Remark 1.1 and Lemma 1.2), S\u03d5(\u03b8) can be approximated by a randomly\nweighted sum of independent random variables defined by\nS\u0302\u03d5 = n\n\u22121\/2\nn\u2211\ni=1\n{Zi\u22121(\u03b2)\u2212 Z\u00af(\u03b2)}\u03d5(G(\u03b7i))\n15\n= n\u22121\/2\nn\u2211\ni=1\n{Zi\u22121(\u03b2)\u2212 Z\u00af(\u03b2)}{\u03d5(G(\u03b7i))\u2212 E[\u03d5(G(\u03b71))]}.\nThen the asymptotic normality of S\u0302\u03d5 can be established by using multivariate martingale\ncentral limit theorem on S\u0302\u03d5. We state that formally in the following proposition whose proof\nis similar to Koul and Ossiander (1994, Lemma 1.2).\nProposition 3.1 Under the model (1.1),\nS\u03d5(\u03b8)\u2212 S\u0302\u03d5 = op(1).\nMoreover\nS\u0302\u03d5 \u21d2N p[0, \u03c32\u03d5M(\u03b8)],\nwhere \u03c32\u03d5 = V ar[\u03d5(G(\u03b71))]. Hence under the assumptions of Theorem 3.3(ii)\nn\n1\n2 (\u03b1\u0302\u2212\u03b1) =\u21d2 Np[0,\u03a3(\u03b8)], (3.18)\nwhere \u03a3(\u03b8) := (M(\u03b8))\u22121J(\u03d5,G) with J(\u03d5,G) :=\n\u222b\n\u03d52(u)du\u2212(\n\u222b\n\u03d5(u)du)2\n(\n\u222b\ng(x)\u03d5(G(dx)))2\n.\nRemark 3.2. The conditions of Theorem 3.3(ii) ensures that the preliminary estimator\nand the scale estimator have no effect on the asymptotics of the final estimator. A sufficient\ncondition for\n\u222b\nxg(x)\u03d5(G(dx)) = 0 is that g is symmetric i.e., g(\u2212x) = g(x) and \u03d5 is skew\nsymmetric, i.e., \u03d5(u) = \u2212\u03d5(1 \u2212 u), \u2200u \u2208 [0, 1]. Therefore, in practice, we recommend to\nuse a skew symmetric \u03d5 to ensure that Theorem 3.3(ii) holds when the innovations are\nsymmetrically distributed. For some model, e.g., in ARLSCH of Example 2, Gc(\u03b8) = 0 when\nX0 is symmetrically distributed around zero. However, for Example 1 (Engle\u2019s ARCH) and\nExample 3 (ARTCH), Gc(\u03b8) 6= 0 and the use of a skew symmetric score function is essential.\nIf the conditions of Theorem 3.3(ii) are not satisfied, then there will be extra terms in the\nvariance-covariance matrix of the asymptotic distribution of \u03b1\u0302 that depend on \u03b1, \u03b2 and \u03ba\nin a complex manner.\n16\nUnder Theorem 3.3(ii), the asymptotic distribution of \u03b1\u0302 is the same as that of an R-\nestimator of \u03b1 for the model\nXi\n\u03c3(Y i\u22121,\u03b2)\n=\nY \u2032i\u22121\u03b1\n\u03c3(Y i\u22121,\u03b2)\n+ \u03b7i, (3.19)\nwith \u03b2 known. In general, an R-estimator is location invariant. However, since we compute\nR-estimator basically for the model (3.19), even though the original model (1.1) may have a\nlocation parameter like the ARCH model, (3.19) need not have that, unless \u03c3 is a constant.\nThus we can estimate the intercept parameter of the original model through R-estimation.\nRemark 3.3. Comparison with other estimators. (i) Relative efficiency of an R-\nestimator with respect to (wrt) the optimal R-estimator: From (3.18) it follows that for a\nfixed score function \u03d5, the asymptotic dispersion of the standardized R-estimator is a scalar\nJ(\u03d5,G) that depends only on the underlying error distribution, multiplied by a matrix which\ndepends only on \u03b8 and the error distribution. Hence, for a given innovation density g,\nthe optimal R-estimator based on the score function \u03d5\u2217g(u) = \u2212g\u02d9(G\u22121(u))\/g(G\u22121(u)) exists,\nprovided that \u03d5g \u2208 F . In particular, when g is the logistic density, \u03d5\u2217g(u) = u \u2212 1\/2 and\nwhen g is the double-exponential density, \u03d5\u2217g(u) = (1\/2) sign (u\u2212 1\/2). Also\nJ(\u03d5\u2217g, G) = 1\/Ig, (3.20)\nwhere Ig is the Fisher\u2019s information for g. See Jurec\u02c7kova\u00b4 and Sen (1996, Display 3.4.30) for a\nsimilar result under homoscedastic linear model. Note also that for the Wilcoxon R-estimator\n\u03b1\u0302W corresponding to the score function \u03d5(u) = u\u2212 1\/2,\nJ(\u03d5,G) = 1\/{12(\n\u222b\ng2(x)dx)2} (3.21)\nand for the R-estimator \u03b1\u0302S based on the signed-score function \u03d5(u) = (1\/2) sign {u\u2212(1\/2)},\nJ(\u03d5,G) = 1\/{4g2(0)}. (3.22)\n17\nIt is of natural interest to compare the performance of an R-estimator with the optimal R-\nestimator \u03d5\u2217g. Accordingly, one can define the absolute relative efficiency of an R-estimator\nbased on \u03d5 as 1\/[IgJ(\u03d5,G)] which will be bounded above by one. From Mukherjee (2006\nb), the absolute relative efficiency does not depend on the variance of G. Hence, from\n(3.18) and Lehmann (1983, Section 2.6, Table 6.2 and Section 5.6, Table 6.2), the absolute\nrelative efficiencies of \u03b1\u0302W are 3\/pi = 0.955, 1 and 0.75 at the normal, logistic and the double-\nexponential density, respectively. Also, from (3.18), Mukherjee (2006 b) and Lehmann (1983,\nSection 5.4, Table 4.4), the absolute relative efficiencies of \u03b1\u0302S are 2\/pi = 0.637, 0.75 and 1 at\nthe normal, logistic and the double-exponential density, respectively.\n(ii) Relative efficiency of an R-estimator wrt the quasi maximum likelihood estimator\n(QMLE): From (2.1) and (3.19), a maximum likelihood estimator of \u03b1 based on the normal\ndistribution of the errors can be defined as a minimizer \u03b1\u0302QMLE of\nn\u2211\ni=1\n[Xi\/\u03c3(Y i\u22121, \u03b2\u0302)\u2212 {Y i\u22121\/\u03c3(Y i\u22121, \u03b2\u0302)}\u2032\u03c4 1]2\nwith respect to \u03c4 1. This yields\n\u03b1\u0302QMLE = [\nn\u2211\ni=1\nY i\u22121Y \u2032i\u22121\/\u03c3\n2(Y i\u22121, \u03b2\u0302)]\u22121[\nn\u2211\ni=1\nY i\u22121Xi\/\u03c32(Y i\u22121, \u03b2\u0302)]. (3.23)\nThe estimator \u03b1\u0302QMLE can also be termed as the least squares estimator (LSE) and using\nstandard techniques, its asymptotic distribution can be obtained as\nn\n1\n2 (\u03b1\u0302QMLE \u2212\u03b1) =\u21d2 Np[0, (E[Y 0Y \u20320\/\u03c32(Y 0,\u03b2)])\u22121]. (3.24)\nWhen, for example, E[Y 0\/\u03c3(Y 0,\u03b2)] = 0, we can use (3.18) and (3.24) to define the ARE of\nan R-estimator based on \u03d5, with respect to the QMLE as 1\/J(\u03d5,G). Therefore from (3.21),\nthe asymptotic relative efficiency (ARE) of the Wilcoxon R-estimator with respect to the\nQMLE is 12(\n\u222b\ng2(x)dx)2 which is at least 0.864 for a large class of symmetric standardized\nerror densities g; see, for example, Lehmann (1983, Section 5.6) for similar result under\n18\nthe location model. In particular, for the standardized normal, logistic and the double-\nexponential g, ARE equals 3\/pi = 0.955, pi2\/9 = 1.10 and 1.50, respectively. In a similar\nfashion, from (3.22), the ARE of the R-estimator based on signed score with respect to the\nQMLE is 4g2(0) which is at least 1\/3 for symmetric unimodal error densities g (with variance\n1); see, for example, Lehmann (1983, Section 5.3) for similar result under the location model.\nIn particular, for the standardized normal, logistic and double-exponential g, ARE equals\n2\/pi = 0.637, pi2\/12 = 0.82 and 2, respectively.\nA classical result due to Chernoff-Savage (1958), translated to our setup, asserts that there\nexists R-estimator that can ensure the ARE with respect to the QMLE to be at least one; in\nother words, such estimator is even better than the Wilcoxon-type R-estimator for which the\nminimum ARE is 0.864. Such R-estimator based on the unbounded normal score function\n(van der Waerden type R-estimator) is asymptotically efficient at the normal errors and\nhas the ARE of at least 1 for all other error densities. In the homoscedastic autoregressive\nmodel with \u03c3 \u2261 1, Mukherjee and Bai (2002) derived (3.18) for unbounded but square-\nintegrable score function and showed consequently that the Chernoff-Savage phenomenon\nholds for the autoregressive models. We conjecture that (3.18) holds for the unbounded score\nfunction under the heteroscedastic setup also, which, if proved, should give more motivation\nfor considering the R-estimators.\n(iii) Relative efficiency of the optimal R-estimator wrt the QMLE: Note from (3.20) and\n(3.24) that the ARE of the optimal R-estimator based on \u03d5\u2217g with respect the QMLE at the\nerror density g is given by\n1\/(1\/Ig) = Ig. (3.25)\nIn particular, for the standardized normal, logistic and double-exponential g, this efficiency\nequals 1, pi2\/9 = 1.10 and 2, respectively. However, in order to use the optimal estimator,\nthe form of g should be known.\n19\nRemark 3.4. In order to use the result of Proposition 3.1 to construct, for example,\nconfidence intervals, we need to estimate\n\u222b\ng(x)\u03d5(G(dx)) appearing in J(\u03d5,G). For the\nR-estimation in the homoscedastic autoregressive model with \u03c3 \u2261 1 the same factor arise\nand an estimate can be obtained by replacing g and G by a kernel density estimator and the\nempirical distribution function based on the estimated residuals; see, for example, Koul (1992,\nSection 7.3c). In a similar fashion, we can obtain an estimate\n\u222b\ng(x)\u03d5(G(dx)) by replacing\ng and G by a kernel density estimator and the empirical distribution function based on the\nestimated residuals {\u03b7j(\u03b1\u0302, \u03b2\u0302); 1 \u2264 j \u2264 n}; however, the performance of such estimator has\nbeen investigated here neither theoreticaly nor empirically. In the empirical study we use\nthe Wilcoxon score function for which\n\u222b\ng(x)\u03d5(G(dx)) =\n\u222b\ng2(x)dx and there we use simple\nhistogram estimator of g which performs very well; see Section 5 for details.\n4 Examples\nThis section contains some details for verifying the general conditions of the previous section\nin three examples. Here we check Condition luv with\nuni(t) =\nn\u22121\/2Y \u2032i\u22121t1\n\u03c3(Y i\u22121,\u03b2)\n, vni(t) =\n\u03c3(Y i\u22121,\u03b2 + n\u22121\/2t2)\u2212 \u03c3(Y i\u22121,\u03b2)\n\u03c3(Y i\u22121,\u03b2)\n,\nand\nlni(t) =\nj-th coordinate of Y i\u22121\n\u03c3(Y i\u22121,\u03b2 + n\u22121\/2t2)\n, 1 \u2264 j \u2264 p.\nWe will also use the following fact repeatedly which states that if U = [u1, . . . , uk]\n\u2032, V =\n[v1, . . . , vk]\n\u2032 and W are vectors with all entries nonnegative, then\nW \u2032V \/W \u2032U \u2264 1 + (v1\/u1) + . . . (vk\/uk), (4.1)\nwhere we define vj\/uj = 0 if uj = 0 = vj. See, for example, Mukherjee (2006 a, Lemma 2).\nExample 1. (ARCH model). In this example, \u03b1 = \u03b2, \u03c3\u02d9(Y i\u22121, t) = Y i\u22121 and (3.3)-(3.4)\n20\nare satisfied with R\u02d9(Y i\u22121, t) = \u2212Y i\u22121Y \u2032i\u22121\/(Y \u2032i\u22121t)2. Now we check Condition luv with\nlni(t) = either\n1\nY \u2032i\u22121(\u03b1+ n\u22121\/2t2)\n, or\nXi\u2212j\nY \u2032i\u22121(\u03b1+ n\u22121\/2t2)\n, 1 \u2264 j \u2264 s,\nuni(t) =\nn\u22121\/2Y \u2032i\u22121t1\nY \u2032i\u22121\u03b1\nand vni(t) =\nn\u22121\/2Y \u2032i\u22121t2\nY \u2032i\u22121\u03b1\n.\nUsing (4.1), all coordinates of the vectors \u00b5\u02d9i and \u03c3\u02d9i are uniformly bounded and consequently\nthe existence of all the matrices in (3.2) is guaranteed. Also, there is a compact neighbourhood\ncontaining zero on which {lni(t)}\u2019s are all uniformly bounded and by the stationarity, \u2200t,\nCn(t) = O(n). Any choice of q > 2 and 0 < \u000f < q\/2 with 1 < q\/2\u2212 \u000f will satisfy (3.7).\nBy the stationarity and boundedness, n\u22121\n\u2211n\ni=1E{lni(t)\u2212 lni(0)}2 = o(1). Therefore\n`(t) = either\n{\nE\n[\n1\nY \u20320\u03b1\n]2}1\/2\n, or\n{\nE\n[\nX\u2212j\nY \u20320\u03b1\n]2}1\/2\n, 1 \u2264 j \u2264 s,\nand hence, condition (3.8) is satisfied. Conditions (3.9) and (3.10) are satisfied by bounded-\nness which is a consequence (4.1). Condition (3.11) is also a consequence of (4.1) and so is\n(3.13) after taking expectation and using the stationarity.\nFor (3.12), the left hand side is bounded by a constant times n\nq\/2\u2212\u000f\nn\n[n\u22121\/2]q\/2 which is o(1)\nif 0 < q\/2\u2212 \u000f\u2212 1 < q\/4. In other words, any choice of q and \u000f satisfying q\/4 < 1 + \u000f < q\/2\nwill satisfy (3.7) and (3.12). Verification of (3.14) and (3.15) are immediate by writing down\nthe corresponding expressions.\nSince here \u03b1 = \u03b2, for estimation in this model, we use just a two-step procedure, i.e., use\n\u03b1\u0302p instead of \u03b2\u0302 to define final \u03b1\u0302. Therefore, from (3.18), if either\n\u222b\nxg(x)\u03d5(G(dx)) = 0 or\nGc(\u03b8) = 0, then\nn1\/2(\u03b1\u0302\u2212\u03b1) =\u21d2 Np(0,\u03a3(\u03b1)), \u03a3(\u03b1) := M\u22121(\u03b8)J(\u03d5,G).\nDenote the estimator in (3.23) under a two-step procedure by \u03b1\u0302QMLE which is the most\ncommonly-used estimator for this model. Introduced by Engle (1982), it is a maximizer of\n21\nthe normal likelihood\n\u2212(1\/2)\nn\u2211\ni=1\n[{X2i \/(Y \u2032i\u22121\u03c4 )}+ log(Y \u2032i\u22121\u03c4 )].\nWeiss (1986) proved that under the stationarity of {Xi}\u2019s and the finite fourth moment\nassumption on the i.i.d. errors \u000fi (which is the same as finiteness of the second moment of\n\u03b7i), the asymptotic distribution of \u03b1\u0302QMLE is as follows:\nn1\/2(\u03b1\u0302QMLE \u2212\u03b1) =\u21d2 Np(0,\u03a3QMLE), where \u03a3QMLE :=\n(\nE\n[\nY 0Y\n\u2032\n0\/(\u03b1\n\u2032Y 0)2\n])\u22121\nV ar(\u03b7).\nSince in this example E[Y 0\/\u03c3(Y 0,\u03b2)] is non-null, computation of the ARE of a rank-\nestimator \u03b1\u0302, relative to the commonly-used quasi maximum likelihood estimator in Engle\u2019s\nARCH model is not straight-forward. However, the ratio of the scalar-factors is exactly\nthe same as that of the rank-estimator relative to the least squares estimator in the linear\nregression model; see Remark 3.3 for more on this.\nExample 2. (ARLSCH model). Letting Z\u02dci\u22121 = (1, X2i\u22121)\n\u2032, \u03c3\u02d9(Y i\u22121, t) = Z\u02dci\u22121\/{2(Z\u02dc \u2032i\u22121t)1\/2}.\nAlso, with R\u02d9(Y i\u22121, t) = \u2212Z\u02dci\u22121Z\u02dc \u2032i\u22121\/{2(Z\u02dc\n\u2032\ni\u22121t)\n2}, (3.3)-(3.4) are satisfied. Now we check\nCondition luv with\nlni(t) =\nXi\u22121{\n(\u03b2 + n\u22121\/2t2)\u2032Z\u02dci\u22121\n}1\/2 ,\nuni(t) =\nn\u22121\/2t\u20321Xi\u22121{\n\u03b2\u2032Z\u02dci\u22121\n}1\/2 , vni(t) = {(\u03b2 + n\u22121\/2t2)\u2032Z\u02dci\u22121}1\/2 \u2212 (\u03b2\u2032Z\u02dci\u22121)1\/2(\u03b2\u2032Z\u02dci\u22121)1\/2 .\nUsing the boundedness of the function x\u2192 x\/(\u03b20 + \u03b21x2)1\/2 on [0,\u221e) and the stationarity\nand the ergodicity of {Xi}, (3.2) holds. To verify (3.8), note that by the stationarity\nn\u22121\nn\u2211\ni=1\nE{lni(t)\u2212 lni(0)}2 = E\n[ X0{\n(\u03b2 + n\u22121\/2t2)\u2032Z\u02dc0\n}1\/2 \u2212 X0{\n\u03b2\u2032Z\u02dc0\n}1\/2 ]2\n= E\n[{ X0{\n\u03b2\u2032Z\u02dc0\n}1\/2}{ (\u03b2\u2032Z\u02dc0)1\/2{(\u03b2 + n\u22121\/2t2)\u2032Z\u02dc0}1\/2 \u2212 1\n}]2\n.\n22\nBy (4.1), the sequence of r.v.\u2019s under the expectation is bounded and tends to 0, a.s. Therefore\nthe above is o(1) by the bounded Convergence Theorem. Also\n`(t) =\n{\nE\n[\nX20\n(Z\u02dc\n\u2032\n0\u03b2)\n]}1\/2\n,\nand hence, condition (3.8) is satisfied. Conditions (3.9) and (3.10) are satisfied by bounded-\nness which is a consequence (4.1). Condition (3.11) is also a consequence of (4.1)\nNext, we verify (3.13). Taking expectation, it is easy to see that n\u22121\/2\n\u2211n\ni=1 |lni(t)uni(t)| =\nOp(1); next we check that n\n\u22121\/2\u2211n\ni=1 |lni(t)vni(t)| = Op(1). First note that\nn\u22121\/2\nn\u2211\ni=1\n|lnivni| \u2264 n\u22121\/2\nn\u2211\ni=1\n|Xi\u22121|\n(\u03b2\u2032Z\u02dci\u22121)1\/2\n\u2223\u2223\u2223\u2223\u2223\u2223\n{\n\u03b2\u2032Z\u02dci\u22121\n(\u03b2 + n\u22121\/2t2)\u2032Z\u02dci\u22121\n}1\/2\n\u2212 1\n\u2223\u2223\u2223\u2223\u2223\u2223\nNext note that the derivative of the function s 7\u2192 [x\/(x+s)]1\/2 at s = 0 is \u22121\/(2x). Therefore\nabove is bounded by\nn\u22121\/2\nn\u2211\ni=1\n|Xi\u22121|\n(\u03b2\u2032Z\u02dci\u22121)1\/2\n\u2223\u2223\u2223\u2223\u2223\u2223\n{\n\u03b2\u2032Z\u02dci\u22121\n(\u03b2 + n\u22121\/2t2)\u2032Z\u02dci\u22121\n}1\/2\n\u2212 1 + n\n\u22121\/2Z\u02dc\n\u2032\ni\u22121t2\n2\u03b2\u2032Z\u02dci\u22121\n\u2223\u2223\u2223\u2223\u2223\u2223\n+\n1\n2\nn\u22121\nn\u2211\ni=1\n|Xi\u22121|\n(\u03b2\u2032Z\u02dci\u22121)1\/2\nZ\u02dc\n\u2032\ni\u22121t2\nZ\u02dc\n\u2032\ni\u22121\u03b2\n.\nAssuming E\u2016X0\u20164 < \u221e, we have E\u2016Z\u02dc0\u20162 < \u221e, and hence max1\u2264i\u2264n |n\u22121\/2Z\u02dc \u2032i\u22121t2| = op(1).\nUsing a two-step Taylor-type expansion of the function s 7\u2192 [x\/(x + s)]1\/2 at s = 0, we get\na factor of n\u22121\/2 \u00d7 n\u22121 at the first term which together with the stationarity and ergodicity\nforces the first term to go to zero in probability. The n\u22121 factor implies that the r.v.\u2019s in\nthe second term converges in probability to E[{|Xi\u22121|Z\u02dc \u2032i\u22121t2}\/{(\u03b2\u2032Z\u02dci\u22121)1\/2Z\u02dc\n\u2032\ni\u22121\u03b2}], thereby\nverifying (3.13) here.\nFinally, we can verify (3.7) and (3.12) an in Example 1 since all the underlying quantities\nare bounded. Verification of (3.14) and (3.15) can be done by writing down the corresponding\nexpressions and invoking the smoothness of the derivatives.\nTherefore, to summarize, we obtain that if either\n\u222b\nxg(x)\u03d5(G(dx)) = 0 or Gc(\u03b8) = 0,\n23\nthen\nn1\/2(\u03b1\u0302\u2212\u03b1) =\u21d2 N1(0, \u03c4 2(\u03b8)J(\u03d5,G)), \u03c4 2(\u03b8) := [Var{X0\/(\u03b20 + \u03b21X20 )1\/2}]\u22121.\nExample 3. (ARTCH model). To verify the assumptions in this model, let\nZ\u02dci\u22121 = [Xi\u22121I(Xi\u22121 > 0),\u2212Xi\u22121I(Xi\u22121 \u2264 0), . . . , Xi\u2212pI(Xi\u2212p > 0),\u2212Xi\u2212pI(Xi\u2212p \u2264 0)]\u2032.\nThen, in this example, \u03c3\u02d9(Y i\u22121, t) = Z\u02dci\u22121 and (3.3)-(3.4) are satisfied with R\u02d9(Y i\u22121, t) =\n\u2212Z\u02dci\u22121Z\u02dc \u2032i\u22121(Z\u02dc\n\u2032\ni\u22121t)\n\u22122. Next we can check Condition luv with\nlni(t) =\nXi\u2212j\nZ\u02dc\n\u2032\ni\u22121(\u03b2 + n\u22121\/2t2)\n, 1 \u2264 j \u2264 p,\nuni(t) =\nn\u22121\/2Y \u2032i\u22121t1\nZ\u02dc\n\u2032\ni\u22121\u03b2\nand vni(t) =\nn\u22121\/2Z\u02dc\n\u2032\ni\u22121t2\nZ\u02dc\n\u2032\ni\u22121\u03b2\n.\nThe details are similar to those of Example 1 since the standard deviation is a linear function\nof the parameters; here one needs to use the fact that the functions x \u2192 x\/(\u03b22j\u22121xI(x \u2265\n0)\u2212 \u03b22jxI(x < 0)) are bounded. Hence, from Proposition 3.1, if\n\u222b\nxg(x)\u03d5(G(dx)) = 0, then\nn1\/2(\u03b1\u0302\u2212\u03b1) =\u21d2 Np(0,\u03a3(\u03b8)). (4.2)\n5 Empirical study\nIn this section we first report Monte Carlo study comparing the Wilcoxon R-estimator (\u03b1\u0302W ),\nthe R-estimator based on the signed score (\u03b1\u0302S) and the QMLE (\u03b1\u0302QMLE) at three error\ndensities in terms of their average squared deviations from the true parameter. Consequently,\nthe performance of some optimal R-estimators at certain error densities are compared with\nthe Gaussian likelihood based MLE . Next we consider three important real data sets in the\nfinancial time series and study the robustness of R-estimators against misspecified form of\nthe heteroscedasticity.\n24\nModel. Among many different models, we choose the ARTCH model of Example 3 with\np = s = 1 and the ARLSCH model of Example 2 with p = s = 1, r = 2 with specific value\nof the underlying true parameters, when the errors are simulated from the standardized (i)\nnormal (N), (ii) logistic (L) and (iii) double-exponential (D) distribution. For results with\ndifferent combinations of the underlying true parameters for which the model could be even\nnonstationary, see Mukherjee (2006 b). To estimate the scale parameters, we use the score\nfunction \u03ba(u) = u. The computations become relatively simpler under such choice of the\nscore function with even closed form expressions for the scale estimators in the ARTCH\nmodel. For each model, we compute (i) the preliminary estimator \u03b1\u0302p, (ii) the MLE based\non the normal distribution \u03b1\u0302QMLE, (iii) the Wilcoxon R-estimator \u03b1\u0302W based on the score\nfunction \u03d5(u) = u \u2212 (1\/2) and (iv) the R-estimator \u03b1\u0302S based on the signed-score function\n\u03d5(u) = sign {u\u2212 (1\/2)}.\nFormulae for the ARTCH model. From\nXi = \u03b1Xi\u22121 + {\u03b21Xi\u22121I(Xi\u22121 > 0)\u2212 \u03b22Xi\u22121I(Xi\u22121 \u2264 0)} \u03b7i, 1 \u2264 i \u2264 n,\nnote that \u03b1\u0302p =\n\u2211n\ni=1XiXi\u22121\/\n\u2211n\ni=1X\n2\ni\u22121. Write Ms(\u03c4 ) = [p(\u03c41), n(\u03c42)]\n\u2032, where, for example,\np(\u03c41) = n\n\u22121\/2 \u2211\ni;Xi\u22121>0\nXi\u22121{(\u03b7i(\u03c41))2 \u2212 1}\/(\u03c41Xi\u22121),\nwith \u03b7i(\u03c41) = (Xi \u2212 \u03b1\u0302pXi\u22121)\/(\u03c41Xi\u22121). After some simplifications,\np(\u03c41) = c(n, \u03c41)\n[ \u2211\ni;Xi\u22121>0\n{(Xi \u2212 \u03b1\u0302pXi\u22121)\/Xi\u22121}2\/\u03c4 21 \u2212 np\n]\n,\nwhere c(n, \u03c41) is a constant and np is the total number of positive Xi\u22121\u2019s. Hence p(\u03c41) = 0\nhas the solution \u03b2\u02c61 =\n{\u2211\ni;Xi\u22121>0{(Xi \u2212 \u03b1\u0302pXi\u22121)\/Xi\u22121}2\/np\n}1\/2\nwhich estimates \u03b21. Sim-\nilarly, n(\u03c42) = c(n, \u03c42)\n[\u2211\ni;Xi\u22121<0{(Xi \u2212 \u03b1\u0302pXi\u22121)\/Xi\u22121}2\/\u03c4 22 \u2212 (n \u2212 np)\n]\n, which gives \u03b2\u02c62 ={\u2211\ni;Xi\u22121<0{(Xi \u2212 \u03b1\u0302pXi\u22121)\/Xi\u22121}2\/(n\u2212 np)\n}1\/2\n.\n25\nTo compute the Wilcoxon R-estimator, we apply the Hodges-Lehmann\/Jaeckel (1972)\u2019s\nformula in the approximating ARTCH model\nXi\n\u03b2\u03021Xi\u22121\n\u2248 1\n\u03b2\u03021\n\u00d7 \u03b1+ \u03b7i, when Xi\u22121 > 0,\nand\nXi\n\u2212\u03b2\u03022Xi\u22121\n\u2248 \u22121\n\u03b2\u03022\n\u00d7 \u03b1+ \u03b7i, when Xi\u22121 < 0,\nto get\n\u03b1\u0302W = median\n{XiI(Xi\u22121 > 0)\n\u03b2\u03021Xi\u22121\n+\nXjI(Xj\u22121 < 0)\n\u03b2\u03022Xj\u22121\n}\n\/{(\u03b2\u03021)\u22121 + (\u03b2\u03022)\u22121}. (5.1)\nFor \u03b1\u0302S, first orderm number of {Xi\/\u03b2\u03022Xi\u22121}\u2019s corresponding to negativeXi\u22121\u2019s and call them\n{y1, y2, \u00b7 \u00b7 \u00b7 ym}; here we assume that all of {y1, y2, \u00b7 \u00b7 \u00b7 ym} are distinct and m equals n \u2212 np\nwith probability one. Next order np number of {Xi\/\u03b2\u03021Xi\u22121}\u2019s corresponding to positive\nXi\u22121\u2019s and call them {ym+1, ym+2, \u00b7 \u00b7 \u00b7 yn}. Then from Mukherjee (2006 b), we get that if n is\nodd,\n\u03b1\u0302S = median {(yj+yi)\/[(\u03b2\u03021)\u22121+(\u03b2\u03022)\u22121]; i+j = (n+1)\/2+m+1, 1 \u2264 i \u2264 m,m+1 \u2264 j \u2264 n},\n(5.2)\nwhereas if n is even,\n\u03b1\u0302S = median {(yj + yi)\/[(\u03b2\u03021)\u22121 + (\u03b2\u03022)\u22121]; i+ j = (n)\/2 +m+ 1, 1 \u2264 i \u2264 m,m+ 1 \u2264 j \u2264 n}.\nFinally, from (3.23), the QMLE for the ARTCH model is obtained as\n\u03b1\u0302QMLE = {np(\u03b2\u03021)\u22122 + (n\u2212 np)(\u03b2\u03022)\u22122}\u22121\n[ \u2211\ni;Xi\u22121>0\n{ Xi\nXi\u22121\u03b2\u030221\n}+ \u2211\nj;Xj\u22121<0\n{ Xj\nXj\u22121\u03b2\u030222\n}\n]\n. (5.3)\nFormulae for the ARLSCH model. From\nXi = \u03b1Xi\u22121 + {\u03b20 + \u03b21X2i\u22121}1\/2 \u03b7i, 1 \u2264 i \u2264 n,\n26\nnote that \u03b1\u0302p =\n\u2211n\ni=1XiXi\u22121\/\n\u2211n\ni=1X\n2\ni\u22121. To estimate the scale parameters \u03b20 and \u03b21, write\nMs(\u03c4 ) = [m1(\u03c4 ),m2(\u03c4 )]\n\u2032, where \u03c4 = [\u03c40, \u03c41]\u2032 and with ci = (Xi \u2212 \u03b1\u0302pXi\u22121)2,\nm1(\u03c4 ) =\nn\u2211\ni=1\n{ci\/(\u03c40 + \u03c41X2i\u22121)\u2212 1}\/(\u03c40 + \u03c41X2i\u22121),\nand\nm2(\u03c4 ) =\nn\u2211\ni=1\nX2i\u22121{ci\/(\u03c40 + \u03c41X2i\u22121)\u2212 1}\/(\u03c40 + \u03c41X2i\u22121).\nWrite r\u02dc = \u03c41\/\u03c40. Then the equations m1(\u03c4) = 0 = m2(\u03c4) can be rewritten as\nn\u2211\ni=1\nci\n(1 + r\u02dcX2i\u22121)2\n= \u03c40\nn\u2211\ni=1\n1\n(1 + r\u02dcX2i\u22121)\nand\nn\u2211\ni=1\nciX\n2\ni\u22121\n(1 + r\u02dcX2i\u22121)2\n= \u03c40\nn\u2211\ni=1\nX2i\u22121\n(1 + r\u02dcX2i\u22121)\n.\nNow eliminating \u03c40 one can get an equation in r\u02dc which can be solved using numerical method.\nTo compute the Wilcoxon R-estimator, we apply Jaeckel (1972)\u2019s formula in the approx-\nimating ARLSCH model\nXi\n(\u03b2\u03020 + \u03b2\u03021X2i\u22121)1\/2\n\u2248 Xi\u22121\n(\u03b2\u03020 + \u03b2\u03021X2i\u22121)1\/2\n\u00d7 \u03b1+ \u03b7i\nto get \u03b1\u0302W as the median of the set of numbers {\u03b1ij} with corresponding probability propor-\ntional to {pij} where\n\u03b1ij =\nYi \u2212 Yj\ndi \u2212 dj and pij = di \u2212 dj, (5.4)\nwith Yi = Xi\/(\u03b2\u03020 + \u03b2\u03021X\n2\ni\u22121)\n1\/2, di = Xi\u22121\/(\u03b2\u03020 + \u03b2\u03021X2i\u22121)\n1\/2; here pij\u2019s are defined only for\nthose {(i, j)} for which di \u2212 dj > 0.\nFor computing \u03b1\u0302S we obtain from Mukherjee (2006 b) that it is the median of the set of\nnumbers {\u03b1ij} with corresponding probability proportional to {pij} where pij\u2019s are defined\npositive only for those 1 \u2264 i, j \u2264 n for which di\u2212dj > 0 and for which Yi\u2212di\u03b1ij (also equal to\nYj\u2212dj\u03b1ij by the definition of \u03b1ij) is the \u201cmedian\u201d of the n numbers {Yu\u2212du\u03b1ij;u 6= i, j, Yi\u2212\n27\ndi\u03b1ij, Yj \u2212 dj\u03b1ij}; for this later \u201cmedian\u201d, the definition is the n\/2-th ordered observation\nwhen n is even and as usual the (n+ 1)\/2-th ordered observation when n is even.\nSimulation results and analysis. For simulation, we use r = 100 replications. For each\nof the k-th replication (1 \u2264 k \u2264 r), we generate a sample of size n = 100 from the underlying\nmodel with parameters \u03b1 = 0.1, \u03b21 = 0.2, \u03b22 = 0.3 for the ARTCH model and \u03b1 = 0.1, \u03b20 =\n0.2, \u03b21 = 0.3 for the ARLSCH model and compute \u03b1\u0302p(k) = \u03b1\u0302p, \u03b1\u0302W , \u03b1\u0302S and \u03b1\u0302QMLE. For\neach estimator (denoted generically by \u03b1\u0302(k)), we also compute r\u22121\n\u2211r\nk=1(\u03b1\u0302(k)\u2212\u03b1)2 which is\nthe average (over all replications) squared deviation of the estimate from the true parameter\nvalue \u03b1 and this is an estimate of mean squared error (MSE) of \u03b1\u0302. These are reported in\ncolumns (2)-(5) in Tables 1 and 2 below. Columns (6) and (8) are obtained from dividing\nColumn (5) by Columns (3) and (4) respectively and represent the estimated ARE of \u03b1\u0302W\nand \u03b1\u0302S with respect to \u03b1\u0302QMLE (denoted by E(\u03b1\u0302W ) etc.); Columns (7) and (9) represent the\ncorresponding theoretical ARE of \u03b1\u0302W and \u03b1\u0302S as explained in Remark 3.3(ii) (denoted by\nT(\u03b1\u0302W ) etc.). For each scenario (corresponding to a particular row in the tables), we have run\nsimulations five times under identical setup and have reported the result of that simulation\nwhich has best estimated ARE (in the sense that it is either more than or the closest to\nthe theoretical ARE); for simulation results of all five runs and also the results when the\nobservations were generated under different true parameters, see Mukherjee (2006 b).\nTable 1 : Estimated MSE\u2019s and ARE\u2019s of the different estimators of \u03b1 (ARTCH model)\ng MSE(\u03b1\u0302p) MSE(\u03b1\u0302W ) MSE(\u03b1\u0302S) MSE(\u03b1\u0302QMLE) E(\u03b1\u0302W ) T(\u03b1\u0302W ) E(\u03b1\u0302S) T(\u03b1\u0302S)\nN 0.0544951888 0.0005477203 0.0005744816 0.0005382787 0.983 .96 0.940 .64\nL 0.0458744400 0.0006679956 0.0006526314 0.0007891252 1.181 1.1 1.209 .82\nD 0.0415501346 0.0004704636 0.0004387167 0.0007328313 1.558 1.5 1.670 2\n28\nTable 2 : Estimated MSE\u2019s and ARE\u2019s of the different estimators of \u03b1 (ARLSCH model)\ng MSE(\u03b1\u0302p) MSE(\u03b1\u0302W ) MSE(\u03b1\u0302S) MSE(\u03b1\u0302QMLE) E(\u03b1\u0302W ) T(\u03b1\u0302W ) E(\u03b1\u0302S) T(\u03b1\u0302S)\nN 0.01827685 0.02077171 0.02907107 0.01875711 0.903 .96 0.645 .64\nL 0.02324641 0.01353362 0.02136932 0.01540861 1.139 1.1 0.721 .82\nD 0.02168655 0.01279244 0.01333624 0.01732438 1.354 1.5 1.300 2\nSimulation results as well as several histograms conform with our theoretical finding on the\nasymptotic distributions of the different estimators. In several cases, the estimated ARE is\nmore than the theoretical ARE even at much smaller value of n. In particular, the estimated\nAREs of \u03b1\u0302W at the logistic density are 1.181 and 1.139 for the ARTCH and ARLSCH\nmodels respectively, exceeding the theoretical ARE of 1.10 which, from (3.25), represents\nthe relative efficiency of the optimal R-estimator with respect to the QMLE. However, the\nestimated AREs of \u03b1\u0302S at the double-exponential density are 1.670 and 1.300 for the ARTCH\nand ARLSCH models respectively which are far below the theoretical relative efficiency of 2\nof the optimal R-estimator with respect to the QMLE. A plausible reason for this could be\nthat n = 100 may not be \u2018large enough\u2019 for asymptotics to hold at the double-exponential\ndensity.\nIn many other simulations not reported here with different combinations of the underlying\nparameters, it was observed that the ARE-results for \u03b1\u0302W and \u03b1\u0302S approximately hold even\nwhen the models are nonstationary. In general, to a practitioner, we recommend the use of\n\u03b1\u0302W as a good alternative to the QMLE which has high ARE for a wide number of distributions\nwith a \u2018small sacrifice\u2019 at the normal distribution. Hence, in the real data examples below,\nwe use only \u03b1\u0302W and \u03b1\u0302QMLE for our analysis.\nFinancial Data. Tsay (2002, Chapter 3 on Conditional Heteroscedastic Models) have\nanalyzed three important data sets, namely, (A) The monthly log stock returns of the Intel\nCorporation from 1973 to 1997 (300 observations with first value 0.010050 and last value\n\u22120.095008), (B) The monthly excess returns of S & P 500 from 1926 to 1991 (792 observations\n29\nwith first value 0.0225 and last value 0.1116) and (C) The monthly log returns of IBM stock\nfrom 1926 to 1999 (888 observations with first value 1.0434 and last value 4.5633) and fitted\nvarious types of conditional heteroscedastic models to them. These data can be found in\nhttp:\/\/www.gsb.uchicago.edu\/fac\/ruey.tsay\/teaching\/fts\/m-intc.dat\nFor Data A, denoted by {Xi; 0 \u2264 i \u2264 n = 299}, Tsay\u2019s analysis of the autocorrelation function\n(ACF) of log returns, absolute log returns and squared log returns suggests that monthly\nreturns are serially uncorrelated but dependent. The mean, median, standard deviation\nand kurtosis of {Xi} are 0.0286162, 0.019202, 0.1297513 and 3.370, respectively. Other\nexploratory analysis show presence of heavy tails.\nNext we fitted the centered {Xi} with the ARLSCH model. We use (5.4) and other related\nformulae from the previous subsection to compute \u03b1\u0302W . For estimating its standard error (SE),\nwe estimate (i) M(\u03b8) using the lhs of (3.2) with \u03b2 replaced by \u03b2\u0302 and (ii)\n\u222b\ng2(x)dx using\nthe standardized residuals {\u03b7i(\u03b1\u0302, \u03b2\u0302); 1 \u2264 i \u2264 n}. For the integral, we use simple histogram\nestimator of g by dividing [min{\u03b7i(\u03b1\u0302, \u03b2\u0302)},max{\u03b7i(\u03b1\u0302, \u03b2\u0302)}] into an ad hoc choice of m = 15\nequal intervals over each of which the estimate of g is constant and then estimate the integral\nbased on the integral of the step function. For estimating the SE of \u03b1\u0302QMLE using (3.24), we\nuse a formula similar to (3.2). The efficiency of the R-estimator is defined as the square of\nthe ratio of two estimated SE\u2019s. The estimates are reported in Table 3 below.\nTable 3 : Estimates of \u03b1 for the Intel Corporation data based on the ARLSCH model.\nAuxiliary Estimates \u03b1\u0302QMLE \u03b1\u0302 Efficiency\n\u03b1\u0302p = 0.05654418 0.05174328 0.05043456 1.18223914\n\u03b2\u02c60 = 0.01052003 SE=0.05779476 SE=0.05315395\n\u03b2\u02c61 = 0.4322009\nTsay (2002, Example 3.1) used a standard ARCH model (where p = 1) with intercept\nto analyze this data. Using Xi = \u00b5 + ai with ai = \u03c3i\u22121(\u03b2)\u03b5t, where \u03c32i\u22121(\u03b2) = \u03b20 + \u03b21a\n2\ni\u22121,\n30\n1 \u2264 i \u2264 n = 299, Tsay (2002) obtained \u00b5\u02c6 = 0.0213, \u03b2\u02c60 = 0.00998 and \u03b2\u02c61 = 0.4437 using\nthe QMLE. Note that our estimates \u03b2\u02c60 and \u03b2\u02c61 of the variance parameters are quite close to\nthose of Tsay. The differences are due to the fact that we used centered (mean-subtracted)\nobservations and used preliminary estimate of \u03b1 before estimating the variance parameters\nwith \u03ba(x) = x. Introduction of the autoregressive term \u2018\u03b1\u2019 seems to have misspecified the\nmodel for this data. This is reflected in the studentized ratio of \u03b1\u02c6W which equals 0.95 and\nhence the null hypothesis \u03b1 = 0 is not significant. The conclusion remains same using the\nstudentized ratio of \u03b1\u02c6QMLE also.\nAsymmetry is an inherent feature in the financial market as the market seems to be more\nsensitive to a negative news. Usual ARCH model of volatility may not capture this feature\nbecause of its symmetric dependence on the past values in the form of squares. Sometimes\nan ARTCH model with {\u03b22j\u22121 6= \u03b22j; 1 \u2264 j \u2264 p} may be a reasonable model to capture\nsuch asymmetry. Hence we now fit an ARTCH model with p = s = 1 to Data A where we\nestimate the parameter \u03b1 using \u03b1\u02c6W and \u03b1\u02c6QMLE. Formulae (5.1) and (5.3) yield the following\nestimates.\nTable 4 : Estimates of \u03b1 for the Intel Corporation data based on the ARTCH model.\nAuxiliary Estimates \u03b1\u0302QMLE \u03b1\u0302W Efficiency\n\u03b1\u0302p = 0.05654418 0.40840697 0.03742153 8.73970912\n\u03b2\u02c61 = 8.84610615 SE=0.57839845 SE=0.19564945\n\u03b2\u02c62 = 11.46261069\nThe asymmetric feature of the data set is reflected by the fact that \u03b2\u02c61 < \u03b2\u02c62. Since\n\u03b2\u02c62\/\u03b2\u02c61 = 1.296, impact of a negative shock is about 29.6% higher than that of a positive\nshock of the same magnitude. Also, similar to the ARLSCH fitting, \u03b1 is not significant using\nboth \u03b1\u0302W and \u03b1\u0302QMLE as the model is misspecified. For both models, the R-estimator turned\nout to be much more efficient (in the sense of smaller estimated MSE) than the commonly-\nused \u03b1\u0302QMLE.\n31\nNext consider Data B denoted by {Xi; 0 \u2264 i \u2264 n = 791}. Similar analysis with the\nARLSCH and ARTCH models yields the following estimates of the parameters reported in\nTables 5 and 6. Tsay (2002, Example 3.3) fitted an AR(3)-GARCH(1, 1) model to this data\nand the joint estimation of the parameters in the model yields 0.021 as the estimate of the\nintercept at lag 1. Clearly, in the ARLSCH model, \u03b1\u0302W is closer to this estimate than \u03b1\u0302QMLE.\nHowever, as in Tsay, the coefficient is insignificant using both \u03b1\u0302W and \u03b1\u0302QMLE.\nTable 5 : Estimates of \u03b1 for the S & P 500 data based on the ARLSCH model.\nAuxiliary Estimates \u03b1\u0302QMLE \u03b1\u0302W Efficiency\n\u03b1\u0302p = 0.09023211 0.03311225 0.01982906 1.27527764\n\u03b2\u02c60 = 0.002768820 SE=0.03558038 SE=0.03150709\n\u03b2\u02c61 = 0.1657376\nTable 6 : Estimates of \u03b1 for the S & P 500 data based on the ARTCH model.\nAuxiliary Estimates \u03b1\u0302QMLE \u03b1\u0302W Efficiency\n\u03b1\u0302p = 0.09023211 -0.52288761 0.04611074 2.31178836\n\u03b2\u02c61 = 12.51359399 SE= 0.51621701 SE=0.33951446\n\u03b2\u02c62 = 18.27277865\nFor both Data sets A and B, we observe that the R-estimate and the QMLE of the au-\ntoregressive parameter \u03b1 are small and turned out to be \u2018not significant\u2019 while fitting the\nARLSCH model; hence there was little for the R-estimator to target other than concluding\nthat the model is misspecified. However, under the ARTCH model, the absolute values of\nthe QMLE are higher than \u03b1\u0302W for both data sets. As the inclusion of the autoregressive\nparameter seems to have misspecified the model, the R-estimate resulted in a small value\nrightfully while the QMLE resulted in high value. Moreover, R-estimators are highly efficient\ncompared to the QMLE in terms of smaller estimated MSE for both models and data sets\nwith estimated relative efficiency well above one.\nFinally, we consider Data (C). Tsay (2002, Example 3.5) fitted an AR(1) model with\nGARCH error to this data to obtain the estimate of the autoregressive parameter as 0.099\n32\nwith SE 0.037 and the model seemed to be adequate. We use the ARLSCH model to get\nthe preliminary estimate \u03b1\u0302p = 0.10601551 and the R-estimate \u03b1\u0302W = 0.10864080 with SE\n0.01903097. Therefore the intercept parameter is close to Tsay\u2019s estimate and is significant\nin accord with Tsay\u2019s result. However, the QMLE turns out to be \u03b1\u0302QMLE = 0.31733076 with\nSE 0.09571206 and is very different than the estimate obtained by Tsay using the QMLE of\nAR(1)-GARCH model. This shows that \u03b1\u0302W is more robust to the specification between the\nARCH or GARCH model than \u03b1\u0302QMLE. Moreover, the estimated ARE of the R-estimator\nwrt the QMLE is as high as 25.29363788.\nLet L(k) denote the Ljung-Box statistic with lag k for the portmanteau test of the random-\nness of the residuals. Using the R-estimate for residuals, the Ljung-Box statistics turn out to\nbe L(10) = 6.8387 and L(20) = 15.0339 while using the QMLE for residuals, L(10) = 6.9607\nand L(20) = 14.7694. Since the Ljung-Box statistics have high p-values, the ARLSCH model\nseems to be adequate using both R-estimate and the QMLE.\nNext we appeal to the asymmetric feature of Data C. Tsay (2002, Section 3.7.2) fitted an\nAR(1)-EGARCH model to this data to obtain the estimate of the autoregressive parameter as\n0.092. Fitting an ARTCH model to this data, we obtain the preliminary estimate 0.10601551\nand \u03b1\u0302W = 0.09289947 with SE 0.14118706. However, the QMLE is very different from\nthe R-estimate and Tsay\u2019s comparable estimate with value \u03b1\u0302QMLE = 0.41444369 and SE\n0.26747658. Note that the intercept parameter appears to be not significant using both\nestimates. Using the Ljung-Box statistics, with rank-estimate for residuals L(10) = 7.0857\nand L(20) = 31.7230 while with the QMLE, L(10) = 7.4309 and L(20) = 31.3810 and\nthe ARTCH model seems to be adequate. This shows, as before, that the R-estimator\nperforms better with model misspecification between the ARTCH and the EGARCH models.\nMoreover, the estimated ARE of the R-estimator is 3.58906858.\n33\n6 Proofs\nProof of Theorem 3.1. Clearly\nn1\/2(\u03b1\u0302p \u2212\u03b1) =\n[\nn\u22121\nn\u2211\ni=1\nY i\u22121Y \u2032i\u22121\n]\u22121 [\nn\u22121\/2\nn\u2211\ni=1\nY i\u22121\u03c3(Y i\u22121,\u03b2)\u03b7i\n]\n,\nand so the result follows by applying the martingale CLT on the second term. \u22a5\u22a5\nIn the following, for two sequences of vector-valued stochastic processes {Xn(.)} and\n{Yn(.)}, we write Xn(t) = up(1), if \u2200b > 0 \u000f > 0, P [sup{||Xn(t)||; ||t|| \u2264 b} > \u000f] = o(1) and\nXn(t) = Yn(t) + up(1) if Xn(t)\u2212 Yn(t) = up(1).\nProof of Lemma 3.1. The proof of this uses a simple Taylor expansion of the function\nx\u03ba(x) as follows. Fix a 0 < b <\u221e. Let h(x) = x\u03ba(x), and for a t = (t\u20321, t\u20322)\u2032 \u2208 IRm, \u2016t\u2016 \u2264 b,\nlet\n\u03b7\u02dci(t) := \u03b7i(\u03b8 + n\n\u22121\/2t) = [Xi \u2212 Y \u2032i\u22121(\u03b1+ n\u22121\/2t1)]\/\u03c3(Y i\u22121,\u03b2 + n\u22121\/2t2).\nRecall that rni(t2) = \u03c3\u02d9ni(t2)\/\u03c3ni(t2). Then\nMs(\u03b8 + n\n\u22121\/2t)\u2212Ms(\u03b8)\n= n\u22121\/2\nn\u2211\ni=1\nrni(t2)[h(\u03b7\u02dci(t))\u2212 h(\u03b7i)] + n\u22121\/2\nn\u2211\ni=1\n[rni(t2)\u2212 ri][h(\u03b7i)\u2212 1]\n= M1(t) +M2(t), say.\nUsing the second differentiability of \u03ba, M1(t) = n\n\u22121\/2\u2211n\ni=1 rni(t2)[\u03b7\u02dci(t) \u2212 \u03b7i]h\u02d9(\u03b7i) + up(1),\nwhere h\u02d9(\u03b7i) = \u03b7i\u03ba\u02d9(\u03b7i) + \u03ba(\u03b7i). Next, using \u03c3ni(0) = 1, rewrite\n\u03b7i(t)\u2212 \u03b7i = [\u03b7i \u2212 (\u00b5ni(t1)\u2212 \u00b5i(0))]\n\u03c3ni(t2)\n\u2212 \u03b7i\n= \u2212\u03c3ni(t2)\u2212 1\n\u03c3ni(t2)\n\u03b7i \u2212 \u00b5ni(t1)\u2212 \u00b5i(0)\n\u03c3ni(t2)\n.\nTherefore, the leading term in the above approximation of M1 can be further rewritten a\nM11(t) +M12(t), where\nM11(t) = \u2212n\u22121\/2\nn\u2211\ni=1\nrni(t2)\n\u03c3ni(t2)\u2212 1\n\u03c3ni(t2)\n\u03b7ih\u02d9(\u03b7i) = \u2212\u03a3\u02d9(\u03b8) t2E[\u03b7h\u02d9(\u03b7)] + up(1),\n34\nM12(t) = \u2212n\u22121\/2\nn\u2211\ni=1\nrni(t2)\n\u00b5ni(t1)\u2212 \u00b5i\n\u03c3ni(t2)\nh\u02d9(\u03b7i) = \u2212G(\u03b8) t1E[h\u02d9(\u03b7)] + up(1).\nIn the above approximations, the conditions (3.3)-(3.4) are used. Similarly, one obtains\nM2(t) = up(1), thereby completing the proof of the Lemma. \u22a5\u22a5\nThe proof of Lemma 3.2 depends on the following technical result.\nLet {(\u03b7i, \u03b3ni, \u03b4ni, \u03beni), 1 \u2264 i \u2264 n} be an array of 4-tuple r.v.\u2019s defined on a probability\nspace such that {\u03b7i, 1 \u2264 i \u2264 n} are i.i.d. according to a d.f. G, and for each 1 \u2264 i \u2264 n,\n\u03b7i is independent of (\u03b3ni, \u03b4ni, \u03beni). Let {Ani; 1 \u2264 i \u2264 n} be an array of sub-\u03c3-fields such\nthat Ani \u2282 Ani+1, Ani \u2282 An+1i, 1 \u2264 i \u2264 n, n \u2265 1; (\u03b3n1, \u03b4n1, \u03ben1) is An1 measurable,\nand {{(\u03b3ni, \u03b4ni, \u03beni); 1 \u2264 i \u2264 j}, \u03b71, \u03b72, . . . , \u03b7j\u22121} are Anj measurable, 2 \u2264 j \u2264 n. Define the\nfollowing processes for x \u2208 IR.\nV\u02dcn(x) := n\n\u22121\/2\nn\u2211\ni=1\n\u03b3niI(\u03b7i < x+ x\u03b4ni + \u03beni), (6.1)\nJ\u02dcn(x) := n\n\u22121\/2\nn\u2211\ni=1\n\u03b3niG(x+ x\u03b4ni + \u03beni),\nVn(x) := n\n\u22121\/2\nn\u2211\ni=1\n\u03b3niI(\u03b7i < x+ \u03beni), Jn(x) := n\n\u22121\/2\nn\u2211\ni=1\n\u03b3niG(x+ \u03beni),\nV \u2217n (x) := n\n\u22121\/2\nn\u2211\ni=1\n\u03b3niI(\u03b7i \u2264 x), J\u2217n(x) := n\u22121\/2\nn\u2211\ni=1\n\u03b3niG(x),\nU\u02dcn(x) := V\u02dcn(x)\u2212 J\u02dcn(x), Un(x) := Vn(x)\u2212 Jn(x), U\u2217n(x) := V \u2217n (x)\u2212 J\u2217n(x).\nWe assume that the following conditions are satisfied by the weights {\u03b3ni} and the pertur-\nbations {\u03b4ni, \u03beni}.\nLet Cn :=\n\u2211\nE|\u03b3ni|q. Then for some q > 2 and \u000f, with 0 < \u000f < q\/2,\nCn\/n\nq\/2\u2212\u000f = o(1). (6.2)(\nn\u22121\nn\u2211\ni=1\n\u03b32ni\n)1\/2\n= \u03b3 + op(1), \u03b3 a positive r.v. (6.3)\nE\n(\nn\u22121\nn\u2211\ni=1\n\u03b32ni\n)q\/2\n= O(1). (6.4)\nmax\n1\u2264i\u2264n\nn\u22121\/2|\u03b3ni| = op(1). (6.5)\n35\n(a) max\n1\u2264i\u2264n\n|\u03beni| = op(1), (b) max\n1\u2264i\u2264n\n|\u03b4ni| = op(1). (6.6)\nnq\/2\u2212\u000f\nCn\nE\n[\nn\u22121\nn\u2211\ni=1\n{\u03b32ni(|\u03beni|+ |\u03b4ni|)}\n]q\/2\n= o(1). (6.7)\n(a) n\u22121\/2\nn\u2211\ni=1\n|\u03b3ni\u03beni| = Op(1), (b) n\u22121\/2\nn\u2211\ni=1\n|\u03b3ni\u03b4ni| = Op(1). (6.8)\nThe following theorem states that uniformly over the entire real line, the perturbed process\nU\u02dcn can be approximated by U\n\u2217\nn.\nTheorem 6.1. Under the above setup and under the assumptions (6.2)-(6.8) and (G.1)-\n(G.3),\nsup\nx\u2208IR\n|U\u02dcn(x)\u2212 Un(x)| = op(1), (6.9)\nsup\nx\u2208IR\n|U\u02dcn(x)\u2212 U\u2217n(x)| = op(1). (6.10)\nProof. The proof of such uniform approximation theorem depends on efficient partitioning\nof the real line; here pointwise convergence can be shown easily and then we invoke the\nmonotone structure of the empirical processes to achieve the uniform convergence. The\nuniform closeness of the processes Un and U\n\u2217\nn was proved in Koul and Ossiander (1994,\nTheorem 1.1 ), under the assumption that G has uniformly continuous positive density g,\nand under (6.3), (6.5), (6.6)(a) and (6.8)(a). Thus, the claim (6.10) is a consequence of that\ntheorem and (6.9).\nTo prove (6.9), assume without loss of generality that all \u03b3ni are non-negative. Next,\nwrite U\u02dcn(x) = U\u02dc\n+\nn (x) + U\u02dc\n\u2212\nn (x), where U\u02dc\n+\nn (x), U\u02dc\n\u2212\nn (x) correspond to that part of the sum in\nU\u02dcn(x) which has \u03b4ni \u2265 0, \u03b4ni < 0, respectively. Decompose Un(x) similarly. It thus suffices\nto show that\nsup\nx\u2208IR\n|U\u02dc+n (x)\u2212 U+n (x)| = op(1), (6.11)\nsup\nx\u2208IR\n|U\u02dc\u2212n (x)\u2212 U\u2212n (x)| = op(1). (6.12)\nDetails will be given only for (6.11), they being similar for (6.12).\n36\nFix a \u03b4 > 0 and let Cn = Cn(q) :=\n\u2211n\ni=1E\u03b3\nq\nni. Let \u2212\u221e = x0 < x1 \u2264 . . . \u2264 xrn\u22121 \u2264 xrn =\n\u221e be a weight-dependent partition of IR where xj = G\u22121(j\u03b4Cn\/n(q\/2)\u2212\u000f), 0 \u2264 j \u2264 rn \u2212 1 and\nrn := [n\nq\/2\u2212\u000f\/(Cn\u03b4)] + 1, with [x] denoting the integer part of x. Note that\n[G(xj)\u2212G(xj\u22121)] \u2264 \u03b4Cn\/nq\/2\u2212\u000f, \u2200 1 \u2264 j \u2264 rn. (6.13)\nThe dependence of xj\u2019s on n, \u03b4 and q is suppressed for the sake of convenience.\nUsing the monotonicity of the indicator function and the d.f. G, we obtain that for\nxj\u22121 < x \u2264 xj,\n|U\u02dc+n (x)\u2212 U+n (x)|\n\u2264 |U\u02dc+n (xj)\u2212 U+n (xj\u22121)|+ |U\u02dc+n (xj\u22121)\u2212 U+n (xj)|\n+2 |J\u02dc+n (xj)\u2212 J\u02dc+n (xj\u22121)|+ 2 |J+n (xj)\u2212 J+n (xj\u22121)|\n= |Anj,1|+ |Anj,2|+ 2|Anj,3|+ 2|Anj,4|, say. (6.14)\nNote that the number of partitions varies with n; nevertheless, intuitively, we show the\nconvergence of the j-th partition and consequently, the uniform convergence over it. First,\nconsider Anj,1. For the sake of brevity, let tni = \u03b4ni + 1. Then, one can rewrite Anj,1 as\nn\u22121\/2\nn\u2211\ni=1\n\u03b3ni\n{\nI(\u03b7i < xjtni + \u03beni)\u2212 I(\u03b7i < xj\u22121 + \u03beni)\u2212G(xjtni + \u03beni) +G(xj\u22121 + \u03beni)\n}\n,\nwhich is a sum of martingale differences. We need the following inequality on the tail proba-\nbility of a sum of martingale differences; see Hall and Heyde (1980, Corollary 2.1 and Theorem\n2.12).\nRosenthal Inequality. Suppose Mj =\n\u2211j\ni=1Di is a sum of martingale differences with\nrespect to the underlying increasing filtration {Di} and q \u2265 2. Then, there exists a constant\nC = C(q) such that for any \u000f > 0,\nP [|Mn| > \u000f] \u2264 P\n[\nmax\n1\u2264j\u2264n\n|Mj| > \u000f\n]\n\u2264 C\u000f\u2212q\n\uf8ee\uf8f0 n\u2211\ni=1\nE|Di|q + E\n{\nn\u2211\ni=1\nE(D2i |Di\u22121)\n}q\/2\uf8f9\uf8fb .\n37\nApply the above inequality with D0 = \u03c3 < \u03b3n1, \u03b4n1, \u03ben1 > and for 2 \u2264 i \u2264 n, Di\u22121 =\n\u03c3 < \u03b71, . . . , \u03b7i\u22121; (\u03b3nj, \u03b4nj, \u03benj), 1 \u2264 j \u2264 i >; Di = n\u22121\/2\u03b3ni\n{\nI(xj\u22121 + \u03beni \u2264 \u03b7i < xjtni +\n\u03beni) \u2212 G(xjtni + \u03beni) + G(xj\u22121 + \u03beni)\n}\n. Use |Di| \u2264 n\u22121\/2|\u03b3ni|, and the fact E(D2i |Di\u22121) \u2264\nn\u22121\u03b32ni{|G(xjtni + \u03beni)\u2212G(xj\u22121 + \u03beni)|}, to obtain\nP [|Anj,1| > \u000f]\n\u2264 C\u000f\u2212qn\u2212q\/2Cn + C\u000f\u2212qE\n[\nn\u22121\nn\u2211\ni=1\n\u03b32ni{|G(xjtni + \u03beni)\u2212G(xj\u22121 + \u03beni)|}\n]q\/2\n.\nThe first term in the above inequality is free from j. Next, we shall obtain an upper-bound\n(free of j) for the second term using (i) the Taylor expansion of G and the boundedness of\ng, and (ii) assumptions (G.2) as follows.\nn\u2211\ni=1\n\u03b32ni{|G(xjtni + \u03beni)\u2212G(xj\u22121 + \u03beni)|}\n\u2264\nn\u2211\ni=1\n\u03b32ni{|G(xj)\u2212G(xj\u22121)|}+\nn\u2211\ni=1\n\u03b32ni{|G(xjtni + \u03beni)\u2212G(xjtni)|}\n+\nn\u2211\ni=1\n\u03b32ni{|G(xjtni)\u2212G(xj)|}+\nn\u2211\ni=1\n\u03b32ni{|G(xj\u22121 + \u03beni)\u2212G(xj\u22121)|}\n\u2264 K1\nn\u2211\ni=1\n\u03b32ni\n[\n\u03b4 Cnn\n\u2212(q\/2\u22121) + 2 |\u03beni| + |\u03b4ni|\n]\n.\nThe above bound is obtained by using (6.13) for the first term, the boundedness of g for the\n2nd and 4th terms, and (G.2) for the 3rd term. Now using the so called \u2018Cr\u2019-inequality\n[\nn\u22121\nn\u2211\ni=1\n\u03b32ni{|G(xjtni + \u03beni)\u2212G(xj\u22121 + \u03beni)|}\n]q\/2\n\u2264 K2[Cnn\u2212(q\/2\u2212\u000f)\u03b4n\u22121\nn\u2211\ni=1\n\u03b32ni]\nq\/2 +K2\n[\nn\u22121\nn\u2211\ni=1\n\u03b32ni{|\u03beni| + |\u03b4ni|}\n]q\/2\n.\nHence, using rn = O(n\nq\/2\u2212\u000f\/Cn), for some constant C(\u03b4) > 0,\nP\n(\nmax\n1\u2264j\u2264rn\n|Anj,1| > \u000f\n)\n\u2264 C(\u03b4)\n[\nC\u000f\u2212qn\u2212q\/2Cn \u00d7 n\nq\/2\u2212\u000f\nCn\n+\n{\nCn\nn(q\/2\u2212\u000f)\n}q\/2\u22121\n(\u03b4)q\/2E[n\u22121\nn\u2211\ni=1\n\u03b32ni]\nq\/2\n+\nnq\/2\u2212\u000f\nCn\nE\n{\nn\u22121\nn\u2211\ni=1\n\u03b32ni(|\u03beni|+ |\u03b4ni|)\n}q\/2]\n= o(1), (6.15)\n38\nusing (6.2), (6.4) and (6.7). This implies that max1\u2264j\u2264r |Anj,1| = op(1). Note that for (6.15)\nto hold, the order of the total number of partitions rn is carefully chosen. A similar statement\nholds for Anj,2. Next,\nAnj,3 = n\u22121\/2\nn\u2211\ni=1\n\u03b3ni\n[\nG(xjtni + \u03beni)\u2212G(xj\u22121tni + \u03beni)\n]\n=\n{\nn\u22121\/2\nn\u2211\ni=1\n\u03b3ni[G(xj)\u2212G(xj\u22121)] + n\u22121\/2\nn\u2211\ni=1\n\u03b3ni[G(xjtni + \u03beni)\u2212G(xjtni)]\n+n\u22121\/2\nn\u2211\ni=1\n\u03b3ni[G(xjtni)\u2212G(xj)]\u2212 n\u22121\/2\nn\u2211\ni=1\n\u03b3ni[G(xj\u22121tni)\u2212G(xj\u22121)]\n\u2212n\u22121\/2\nn\u2211\ni=1\n\u03b3ni[G(xj\u22121tni + \u03beni)\u2212G(xj\u22121tni)]\n}\nHence\n|Anj,3| \u2264\n{\n(n\u22121\/2\nn\u2211\ni=1\n\u03b3ni)\nCn\nnq\/2\u2212\u000f\n\u03b4 + n\u22121\/2\nn\u2211\ni=1\n\u03b3ni|G(xjtni + \u03beni)\u2212G(xjtni)\u2212 \u03benig(xjtni)|\n+n\u22121\/2\nn\u2211\ni=1\n\u03b3ni|G(xjtni)\u2212G(xj)\u2212 \u03b4nixjg(xj)|\n+n\u22121\/2\nn\u2211\ni=1\n\u03b3ni|G(xj\u22121tni)\u2212G(xj\u22121)\u2212 \u03b4nixj\u22121g(xj\u22121)|\n+n\u22121\/2\nn\u2211\ni=1\n\u03b3ni|G(xj\u22121tni + \u03beni)\u2212G(xj\u22121tni)\u2212 \u03benig(xj\u22121tni)|\n+n\u22121\/2\nn\u2211\ni=1\n\u03b3ni\u03beni|g(xjtni)\u2212 g(xj\u22121tni)|\n+n\u22121\/2\nn\u2211\ni=1\n\u03b3ni\u03b4ni|xjg(xj)\u2212 xj\u22121g(xj\u22121)|\n}\n.\nNow, letmn := max1\u2264i\u2264n |\u03beni|, \u00b5n := max1\u2264i\u2264n |\u03b4ni|. Note that the sum of the absolute values\nof the second and fifth term in the right hand side of the above equation is bounded above\nby\nn\u22121\/2\nn\u2211\ni=1\n|\u03b3ni\u03beni| sup\n|x\u2212y|\u2264mn\n|g(x)\u2212 g(y)| = op(1),\nuniformly in j = 1, . . . ,m, by the uniform continuity of g and (6.8)(a).\nNext we handle the third term; the fourth term can be handled similarly. By the one-\nstep Taylor expansion of G with remainder in the integral form, for all large n such that\n39\nmax{|\u03b4ni|; 1 \u2264 i \u2264 n} is sufficiently small, the absolute value of the third term is bounded by\nn\u22121\/2\nn\u2211\ni=1\n|\u03b3ni\u03b4nixj|\n\u222b 1\n0\n|g(xj + txj\u03b4ni)\u2212 g(xj)|dt\n\u2264 n\u22121\/2\nn\u2211\ni=1\n|\u03b3ni\u03b4ni| sup{|x|\n\u222b 1\n0\n|g(x+ tx\u03b4)\u2212 g(x)|dt;x \u2208 IR} = op(1),\nby (G.3) and (6.8)(b).\nFinally, consider the sixth term; the seventh one can be dealt with similarly. To begin\nwith observe that by (G.2), max1\u2264i\u2264n,1\u2264j\u2264rn |G(xjtni)\u2212G(xj)| \u2264 C max1\u2264i\u2264n |\u03b4ni|, and hence\nby (G.1), (6.6)(b) and (6.8)(b), max1\u2264i\u2264n,1\u2264j\u2264rn |g(xjtni) \u2212 g(xj)| = op(1). Upon combining\nall these bounds and using E(n\u22121\n\u2211n\ni=1 \u03b3ni) = O(1), we obtain\nmax\n1\u2264j\u2264m\n|Anj,3| \u2264 Op(1) o(1) \u03b4 + op(1).\nA similar result holds for Anj,4. All the above facts together with the arbitrariness of \u03b4 thus\nimply (6.9), thereby completing the proof of the lemma. \u22a5\u22a5\nRemark 6.1. Boldin (1998) proved an analog of (6.10) for the ordinary residual empirical\nprocesses in Engle\u2019s ARCH model with p = 1, using a different method of proof. Koul and\nMukherjee (2002) also proved an anologous result using more stringent moment assumptions.\nProof of Lemma 3.2.. Fix a 0 < b <\u221e. Observe that if in (6.1), we take\n\u03b3ni = lni(t), \u03b4ni = vni(t), \u03beni = uni(t), 1 \u2264 i \u2264 n, (6.16)\nthen, U\u02dcn(x) and U\n\u2217\nn(x) are, respectively equal to U\u02dc(x, t) and U\u2217(x, t), for all x \u2208 IR, t \u2208 IRm.\nClearly the assumptions (3.7)-(3.13) for each fixed t imply (6.2)-(6.8). Hence, (6.10) implies\nthat for each t \u2208 IRm,\nsup\nx\u2208IR\n|U\u02dc(x, t)\u2212 U\u2217(x, t)| = op(1). (6.17)\nThe uniform convergence with respect to t over compact sets can be proved as in Koul (1996)\nand Koul and Mukherjee (2002) using the last two assumptions (3.14) and (3.15) which are\nrelated to the smoothness assumptions on the weights. \u22a5\u22a5\n40\nProof of Lemma 3.3. Using \u03d5(y)\u2212\u03d5(0) = \u222b 10 I(y \u2265 u)\u03d5(du), and Ri\u03c4 = nGn\u03c4 {\u03b7i(\u03c4 )},\nwhere Gn\u03c4 is the empirical distribution function based on {\u03b7j(\u03c4 ), 1 \u2264 j \u2264 n} we get\n\u03d5\n(\nRi\u03c4\nn+ 1\n)\n\u2212 \u03d5(0) =\n\u222b 1\n0\nI\n(\nRi\u03c4\nn+ 1\n\u2265 u\n)\n\u03d5(du) =\n\u222b 1\n0\nI\n(\nGn\u03c4 (\u03b7i(\u03c4 )) \u2265 (n+ 1)u\/n\n)\n\u03d5(du)\n=\n\u222b 1\n0\nI\n(\n\u03b7i(\u03c4 ) \u2265 G\u22121n\u03c4 {(n+ 1)u\/n}\n)\n\u03d5(du),\nwhere for any distribution function H, H\u22121(u) = inf{x;u \u2264 H(x)}, 0 < u < 1. In the\nfollowing, suppressing the dependence of Zi\u22121 on \u03c4 2, we get\nn1\/2S\u03d5(\u03c4 )\n=\nn\u2211\ni=1\n(Zi\u22121(\u03c4 )\u2212 Z\u00af(\u03c4 ))\u03d5\n(\nRi\u03c4\nn+ 1\n)\n=\nn\u2211\ni=1\n(Zi\u22121 \u2212 Z\u00af){\u03d5\n(\nRi\u03c4\nn+ 1\n)\n\u2212 \u03d5(0)}\n=\n\u222b n\u2211\ni=1\n(Zi\u22121 \u2212 Z\u00af)I\n(\n\u03b7i(\u03c4 ) \u2265 G\u22121n\u03c4 {(n+ 1)u\/n}\n)\n\u03d5(du)\n=\n\u222b n\u2211\ni=1\n(Zi\u22121 \u2212 Z\u00af)I\n(\nXi \u2265 \u03c3(Y i\u22121, \u03c4 2)G\u22121n\u03c4 {(n+ 1)u\/n}+ Y \u2032i\u22121\u03c4 1\n)\n\u03d5(du)\n=\n\u222b n\u2211\ni=1\n(Zi\u22121 \u2212 Z\u00af)I\n(\nY \u2032i\u22121\u03b1+ \u03c3(Yi\u22121,\u03b2)\u03b7i \u2265 \u03c3(Yi\u22121, \u03c4 2)G\u22121n\u03c4 {(n+ 1)u\/n}+ Y \u2032i\u22121\u03c4 1\n)\n\u03d5(du)\n=\n\u222b n\u2211\ni=1\n(Zi\u22121 \u2212 Z\u00af)I\n(\n\u03b7i \u2265 G\u22121n\u03c4 {(n+ 1)u\/n}\n\u03c3(Y i\u22121, \u03c4 2)\n\u03c3(Yi\u22121,\u03b2)\n+\n(\u03c4 1 \u2212\u03b1)\u2032Y i\u22121\n\u03c3(Yi\u22121,\u03b2)\n)\n\u03d5(du)\n= \u2212\n\u222b n\u2211\ni=1\n(Zi\u22121 \u2212 Z\u00af)I\n(\n\u03b7i < G\n\u22121\nn\u03c4 {(n+ 1)u\/n}\n\u03c3(Y i\u22121, \u03c4 2)\n\u03c3(Yi\u22121,\u03b2)\n+\n(\u03c4 1 \u2212\u03b1)\u2032Y i\u22121\n\u03c3(Yi\u22121,\u03b2)\n)\n\u03d5(du).\nSubstituting \u03c4 = \u03b8n in the above where \u03b8n = \u03b8 + n\n\u22121\/2t, and using Lemma 3.2, and the\nboundedness of \u03d5,\nS\u03d5(\u03b8 + n\n\u22121\/2t)\n= n\u22121\/2\nn\u2211\ni=1\n{Zi\u22121(\u03b2 + n\u22121\/2t2)\u2212 Z\u00af(\u03b2 + n\u22121\/2t2)}\u03d5\n(\nR\ni(\u03b8+n\u22121\/2t)\nn+ 1\n)\n= \u2212n\u22121\/2\n\u222b n\u2211\ni=1\n(\nZi\u22121(\u03b8n)\u2212 Z\u00af(\u03b8n)\n)\nI\n(\n\u03b7i < G\n\u22121\nn\u03b8n\n{(n+ 1)u\/n}\u03c3(Y i\u22121,\u03b2 + n\n\u22121\/2t2)\n\u03c3(Yi\u22121,\u03b2)\n+\nn\u22121\/2t\u20321Y i\u22121\n\u03c3(Yi\u22121,\u03b2)\n)\n\u03d5(du)\n41\n= \u2212n\u22121\/2\n\u222b n\u2211\ni=1\n(\nZi\u22121(\u03b8n)\u2212 Z\u00af(\u03b8n)\n)\nI\n(\n\u03b7i < G\n\u22121\nn\u03b8n\n{(n+ 1)u\/n}+G\u22121\nn\u03b8n\n{(n+ 1)u\/n}(\u03c3ni(t2)\u2212 1) + (\u00b5ni(t1)\u2212 \u00b5i)\n)\n\u03d5(du)\n= \u2212n\u22121\/2\n\u222b n\u2211\ni=1\n(\nZi\u22121(\u03b8n)\u2212 Z\u00af(\u03b8n)\n)\nG\n(\nG\u22121\nn\u03b8n\n{(n+ 1)u\/n}+G\u22121\nn\u03b8n\n{(n+ 1)u\/n}(\u03c3ni(t2)\u2212 1) + (\u00b5ni(t1)\u2212 \u00b5i)\n)\n\u03d5(du)\n+ n\u22121\/2\n\u222b n\u2211\ni=1\n(\nZi\u22121(\u03b8n)\u2212 Z\u00af(\u03b8n)\n)\n{\nI(\u03b7i < G\n\u22121\nn\u03b8n\n{(n+ 1)u\/n})\u2212G(G\u22121\nn\u03b8n\n{(n+ 1)u\/n})\n}\n\u03d5(du) + up(1)\n= \u2212T1 + T2 + up(1), say.\nSimilarly, substituting \u03c4 = \u03b8, we have\nS\u03d5(\u03b8) = \u2212n\u22121\/2\n\u222b n\u2211\ni=1\n(\nZi\u22121(\u03b8)\u2212 Z\u00af(\u03b8)\n)\nG(G\u22121\nn\u03b8{(n+ 1)u\/n})\u03d5(du)\n+n\u22121\/2\n\u222b n\u2211\ni=1\n(\nZi\u22121(\u03b8)\u2212 Z\u00af(\u03b8)\n)\n{I(\u03b7i < G\u22121\nn\u03b8{(n+ 1)u\/n})\u2212G(G\n\u22121\nn\u03b8{(n+ 1)u\/n})}\u03d5(du) + op(1) = \u2212T3 + T4 + op(1), say.\nFor the terms T2 and T4 involving centered empirical processes, note that T2 \u2212 T4 equals\nn\u22121\/2\n\u222b n\u2211\ni=1\n(Zi\u22121(\u03b8n)\u2212 Z\u00af(\u03b8n))\n{\nI(\u03b7i < G\n\u22121\nn\u03b8n\n{(n+ 1)u\/n})\u2212G(G\u22121\nn\u03b8n\n{(n+ 1)u\/n})\n}\n\u03d5(du)\n\u2212 n\u22121\/2\n\u222b n\u2211\ni=1\n(Zi\u22121(\u03b8)\u2212 Z\u00af(\u03b8)){I(\u03b7i < G\u22121n\u03b8{(n+ 1)u\/n})\u2212G(G\n\u22121\nn\u03b8{(n+ 1)u\/n})}\u03d5(du)\n= n\u22121\/2\n\u222b n\u2211\ni=1\n{\n(Zi\u22121(\u03b8n)\u2212 Z\u00af(\u03b8n))\u2212 (Zi\u22121(\u03b8)\u2212 Z\u00af(\u03b8))\n}\n{\nI(\u03b7i < G\n\u22121\nn\u03b8n\n{(n+ 1)u\/n})\u2212G(G\u22121\nn\u03b8n\n{(n+ 1)u\/n})\n}\n\u03d5(du)\n\u2212 n\u22121\/2\n\u222b n\u2211\ni=1\n(Zi\u22121(\u03b8)\u2212 Z\u00af(\u03b8)){\nI(\u03b7i < G\n\u22121\nn\u03b8{(n+ 1)u\/n})\u2212 I(\u03b7i < G\n\u22121\nn\u03b8n\n{(n+ 1)u\/n})\n+G(G\u22121\nn\u03b8n\n{(n+ 1)u\/n})\u2212G(G\u22121\nn\u03b8{(n+ 1)u\/n})\n}\n\u03d5(du) = T5 \u2212 T6, say.\n42\nSince |I(\u03b7i < G\u22121n\u03b8n{(n + 1)u\/n}) \u2212 G(G\n\u22121\nn\u03b8n\n{(n + 1)u\/n})| \u2264 1, T5 is up(1) by assumption\n(3.15) with lni(t) = sni(t). Next we heavily use the result of Koul and Ossiander (1994,\nTheorem 1.1) on the tightness of U\u2217n and the following fact from Koul and Ossiander (1994,\nEqns (3.11), (3.12))\nsup{|G(G\u22121\nn\u03b8+n\u22121\/2t{(n+ 1)u\/n})\u2212 u|;u \u2208 [0, 1], \u2016t\u2016 \u2264 b} = op(1), (6.18)\nwhich entails sup{|G(G\nn\u03b8n{(n+1)u\/n})\u2212G(Gn\u03b8{(n+1)u\/n})|;u \u2208 [0, 1], \u2016t\u2016 \u2264 b} = op(1).\nTherefore T6 = op(1).\nNext, it remains to examine T1 \u2212 T3 which equals\nn\u22121\/2\n\u222b n\u2211\ni=1\n(Zi\u22121(\u03b8n)\u2212 Z\u00af(\u03b8n))\nG\n(\nG\u22121\nn\u03b8n\n{(n+ 1)u\/n}+G\u22121\nn\u03b8n\n{(n+ 1)u\/n}(\u03c3ni(t2)\u2212 1) + (\u00b5ni(t1)\u2212 \u00b5i)\n)\n\u03d5(du)\n\u2212 n\u22121\/2\n\u222b n\u2211\ni=1\n(Zi\u22121(\u03b8)\u2212 Z\u00af(\u03b8))G(G\u22121\nn\u03b8{(n+ 1)u\/n})\u03d5(du).\nNow subtracting and adding\nn\u22121\/2(Zi\u22121(\u03b8)\u2212 Z\u00af(\u03b8))G(G\u22121n\u03b8n{(n+1)u\/n}+G\n\u22121\nn\u03b8n\n{(n+1)u\/n}(\u03c3ni(t2)\u22121)+(\u00b5ni(t1)\u2212\u00b5i))\nto the i-th summand, and using\nn\u22121\/2\n\u222b n\u2211\ni=1\n(\n(Zi\u22121(\u03b8n)\u2212 Z\u00af(\u03b8n)\u2212Zi\u22121(\u03b8) + Z\u00af(\u03b8)\n)\n\u00d7G\n(\nG\u22121\nn\u03b8n\n{(n+ 1)u\/n}+G\u22121\nn\u03b8n\n{(n+ 1)u\/n}(\u03c3ni(t2)\u2212 1) + (\u00b5ni(t1)\u2212 \u00b5i)\n)\n\u03d5(du) = up(1),\nand\nn\u22121\/2\n\u222b n\u2211\ni=1\n(Zi\u22121(\u03b8)\u2212 Z\u00af(\u03b8)){G(G\u22121\nn\u03b8n\n{(n+ 1)u\/n})\u2212G(G\u22121\nn\u03b8{(n+ 1)u\/n})}\u03d5(du) = up(1),\nT1 \u2212 T3 equals\nn\u22121\/2\n\u222b n\u2211\ni=1\n(\nZi\u22121(\u03b8)\u2212 Z\u00af(\u03b8)\n)\n43\n\u00d7G\n(\nG\u22121\nn\u03b8n\n{(n+ 1)u\/n}+G\u22121\nn\u03b8n\n{(n+ 1)u\/n}(\u03c3ni(t2)\u2212 1) + (\u00b5ni(t1)\u2212 \u00b5i)\n)\n\u03d5(du)\n\u2212n\u22121\/2\n\u222b n\u2211\ni=1\n(Zi\u22121(\u03b8)\u2212 Z\u00af(\u03b8))G(G\u22121n\u03b8{(n+ 1)u\/n})\u03d5(du) + up(1).\nNext we use the mean value theorem on G around the point G\u22121\nn\u03b8n\n{(n + 1)u\/n}, and write\ng(G\u22121\nn\u03b8n\n{(n+1)u\/n}+\u03beinut) = gG\u22121\n(\nG(G\u22121\nn\u03b8n\n{(n+1)u\/n}+\u03beinut)\n)\n. We also use the uniform\ncontinuity of the function gG\u22121 and G(G\u22121\nn\u03b8n\n{(n+1)u\/n}+ \u03beinut) = g(G\u22121n\u03b8n{(n+1)u\/n}) +\n\u03beinut), (6.18) and\nsup\n\u2016t\u2016\u2264b\n\u2225\u2225\u2225n\u22121\/2 n\u2211\ni=1\n{\n\u00b5\u02d9ni(t1)\n\u03c3ni(t2)\n\u2212 n\u22121\nn\u2211\ni=1\n(\n\u00b5\u02d9ni(t1)\n\u03c3ni(t2)\n)\n}\n{\u03c3ni(t2)\u2212 1} \u2212Gc(\u03b8)t2\n\u2225\u2225\u2225 = op(1)\nsup\n\u2016t\u2016\u2264b\n\u2225\u2225\u2225n\u22121\/2 n\u2211\ni=1\n{\n\u00b5\u02d9ni(t1)\n\u03c3ni(t2)\n\u2212 n\u22121\nn\u2211\ni=1\n(\n\u00b5\u02d9ni(t1)\n\u03c3ni(t2)\n)\n}\n{\u00b5ni(t1)\u2212 \u00b5i} \u2212M(\u03b8)t1\n\u2225\u2225\u2225 = op(1),\nwe get that T1 \u2212 T3 equals\n\u222b\nG\u22121(u)g(G\u22121(u))\u03d5(du)Gc(\u03b8)t2 +\n\u222b\ng(G\u22121(u))\u03d5(du)M(\u03b8)t1 + up(1).\nHence Lemma 3.3 follows. \u22a5\u22a5\nAcknowledgment. I am grateful to the Editor, the Associate Editor, and the two\nanonymous referees for making many insightful and constructive comments. Their extraor-\ndinarily careful reading and suggestions have changed and improved the paper to a great\nextent compared to the original manuscript.\nThe research was supported partly by the Association of Commonwealth Universities\nduring my visit to the London School of Economics and Political Sciences under the Com-\nmonwealth Fellowship.\nREFERENCES\nBoldin, M. V., 1998, On residual empirical distribution functions in ARCH models with\napplications to testing and estimation. Mathem. Seminar Giessen, 49-66.\nBollerslev, T., Chou, R. Y. and K. F. Kroner, 1992, ARCH modeling in finance; a review of\nthe theory and empirical evidence. Journal of Econometrics 52, 115-127.\n44\nBougerol, P. and N. Picard, 1992, Stationarity of GARCH processes and of some nonnegative\ntime series. Journal of Econometrics 52, 115-127.\nChernoff, H. and I. R. Savage, 1958, Asymptotic normality and efficiency of certain nonpara-\nmetric test statistics. Annals of Mathematical Statistics 29, 972-994.\nEngle, R.F., 1982, Autoregressive conditional heteroscedasticity and estimates of the variance\nof UK inflation. Econometrica 50, 987-1008.\nEngle, R.F. and G. Gonzalez-Rivera, 1991, Semiparametric ARCH models. Journal of Busi-\nness and Economic Statistics 9, 345-349.\nGiraitis, L., Kokoszka, P. and R. Lepius, 2000, Stationary ARCH models: dependence struc-\nture and central limit theorem. Econometric theory 16, 3-22.\nGourie\u00b4roux, C., 1997, ARCH models and Financial Applications. Springer-Verlag, New York.\nHa\u00b4jek, J., S\u02c7ida\u00b4k, Z. and P. Sen, 1999, Theory of Rank Tests. Academic Press, San Diego.\nHall P. and C. Heyde, 1980, Martingale Limit Theory and its Application. Academic Press,\nNew York.\nHa\u00a8rdle, W. and A. Tsybakov, 1997, Local polynomial estimators of the volatility function in\nnonparametric autoregressive. Journal of Econometrics 81, 223-242.\nHuber, P.J., 1981, Robust Statistics. John Wiley and Sons Inc., New York.\nJaeckel, L. A., 1972, Estimating regression coefficients by minimizing the dispersion of the\nresiduals. Annals of Mathematical Statistics 43, 1449-1458.\nJurec\u02c7kova\u00b4, J., 1971, Nonparametric estimates of regression coefficients. Annals of Mathe-\nmatical Statistics 42, 1328-1338.\nJurec\u02c7kova\u00b4, J. and P. Sen, 1996, Robust Statistical Procedures: Asymptotics and Interrela-\ntions. Wiley, New York.\nKoenker, R. and Q. Zhao, 1996, Conditional quantile estimation and inference for ARCH\nmodels. Econometric Theory 12, 793-813.\nKoul, H., 1971, Asymptotic behavior of a class of confidence region based on rank in regres-\n45\nsion. Annals of Mathematical Statistics 42, 466-476.\nKoul H., 1992, Weighted empiricals and linear models. IMS Lecture Notes-Monograph Ser.\n21, Hayward, CA.\nKoul, H., 1996, Asymptotics of some estimators and sequential residual empiricals in non-\nlinear time series. Annals of Statistics 24, 380-404.\nKoul, H. and K. Mukherjee, 1993, Asymptotics of R-, MD- and LAD-estimators in linear\nregression models with long range dependent errors. Probability Theory and Related Fields\n95, 535-553.\nKoul, H. and K. Mukherjee, 2002, Some estimation procedures in ARCH models. Technical\nReport 9-2000. National University of Singapore.\nKoul H.L. and M. Ossiander, 1994, Weak convergence of randomly weighted dependent resid-\nual empiricals with applications to autoregression. Annals of Statistics 22, 540-562.\nLehmann, E. L., 1983, Theory of Point Estimation. Wiley, New York.\nMukherjee, K. (2006 a). Pseudo-likelihood estimation in ARCH models. To appear in Cana-\ndian Journal of Statistics, 34,?-?.\nMukherjee, K. (2006 b). Computation of R-estimators and some related topics. Unpublished\nmanuscript with link from http:\/\/www.liv.ac.uk\/maths\/SP\/HOME\/K.Mukherjee.html.\nMukherjee, K. and Z. D. Bai, 2002, R-estimation in autoregression with square-integrable\nscore function. Journal of Multivariate Analysis 81, 167-186.\nNelson, D. B., 1990, Stationarity and persistence in the GARCH (1, 1) model. Econometric\nTheory 6, 318-334.\nPantula, S. G., 1988, Estimation of autoregressive models with ARCH errors. Sankhya\u00af, Ser\nB 50, 119-138.\nRabemananjara, R. and J. M. Zakoian, 1993, Threshold ARCH models and asymmetry in\nvolatility. Journal of Applied Econometrics 8, 31-49.\nShephard, N., 1996, Statistical aspects of ARCH and stochastic volatility, in: D. Cox, D.\n46\nHinkley and O. Barndorff-Nielsen, (Eds.), Time Series Models in Econometric, Finance and\nother fields, Chapman and Hall Ltd., London, pp. 1-67.\nTsay, R. S., 2002, Analysis of Financial Time Series. Wiley, New York.\nWeiss, A. A., 1986, Asymptotic Theory for ARCH models: estimation and testing. Econo-\nmetric Theory 2, 107-131.\n47\n"}