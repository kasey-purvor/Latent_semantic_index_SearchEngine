{"doi":"10.1109\/TMM.2005.858388","coreId":"66654","oai":"oai:dro.dur.ac.uk.OAI2:614","identifiers":["oai:dro.dur.ac.uk.OAI2:614","10.1109\/TMM.2005.858388"],"title":"Multi-server support for large scale distributed virtual environments.","authors":["Ng, B.","Lau, R. W. H.","Si, A.","Li, F."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2005-12","abstract":"CyberWalk is a distributed virtual walkthrough system that we have developed. It allows users at different geographical locations to share information and interact within a shared virtual environment (VE) via a local network or through the Internet. In this paper, we illustrate that as the number of users exploring the VE increases, the server will quickly become the bottleneck. To enable good performance, CyberWalk utilizes multiple servers and employs an adaptive region partitioning technique to dynamically partition the whole VE into regions. All objects within each region will be managed by one server. Under normal circumstances, when a viewer is exploring a region, the server of that region will be responsible for serving all requests from the viewer. When a viewer is crossing the boundary of two or more regions, the servers of all the regions involved will be serving requests from the viewer since the viewer might be able to view objects within all these regions. This is analogous to evaluating a database query using a parallel database server, which could improve the performance of serving a viewer's request tremendously. We evaluate the performance of this multiserver architecture of CyberWalk via a detail simulation model","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66654.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/614\/1\/614.pdf","pdfHashValue":"546df26a3dc21fac63aa196a69bdc5511ee36ea4","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:614<\/identifier><datestamp>\n      2011-06-15T15:51:33Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Multi-server support for large scale distributed virtual environments.<\/dc:title><dc:creator>\n        Ng, B.<\/dc:creator><dc:creator>\n        Lau, R. W. H.<\/dc:creator><dc:creator>\n        Si, A.<\/dc:creator><dc:creator>\n        Li, F.<\/dc:creator><dc:description>\n        CyberWalk is a distributed virtual walkthrough system that we have developed. It allows users at different geographical locations to share information and interact within a shared virtual environment (VE) via a local network or through the Internet. In this paper, we illustrate that as the number of users exploring the VE increases, the server will quickly become the bottleneck. To enable good performance, CyberWalk utilizes multiple servers and employs an adaptive region partitioning technique to dynamically partition the whole VE into regions. All objects within each region will be managed by one server. Under normal circumstances, when a viewer is exploring a region, the server of that region will be responsible for serving all requests from the viewer. When a viewer is crossing the boundary of two or more regions, the servers of all the regions involved will be serving requests from the viewer since the viewer might be able to view objects within all these regions. This is analogous to evaluating a database query using a parallel database server, which could improve the performance of serving a viewer's request tremendously. We evaluate the performance of this multiserver architecture of CyberWalk via a detail simulation model.<\/dc:description><dc:subject>\n        Multi-server architecture<\/dc:subject><dc:subject>\n         Adaptive region partitioning<\/dc:subject><dc:subject>\n         Distributed virtual environments.<\/dc:subject><dc:publisher>\n        IEEE<\/dc:publisher><dc:source>\n        IEEE transactions on multimedia, 2005, Vol.7(6), pp.1054-1065 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2005-12<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:614<\/dc:identifier><dc:identifier>\n        issn:1520-9210<\/dc:identifier><dc:identifier>\n        doi:10.1109\/TMM.2005.858388<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/614\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1109\/TMM.2005.858388<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/614\/1\/614.pdf<\/dc:identifier><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:1520-9210","1520-9210"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2005,"topics":["Multi-server architecture","Adaptive region partitioning","Distributed virtual environments."],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n04 July 2008\nVersion of attached file:\nPublished Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nNg, B. and Lau, R. W. H. and Si, A. and Li, F. (2005) \u2019Multi-server support for large scale distributed virtual\nenvironments.\u2019, IEEE transactions on multimedia., 7 (6). pp. 1054-1065.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1109\/TMM.2005.858388\nPublisher\u2019s copyright statement:\nAdditional information:\n2005 IEEE. Personal use of this material is permitted. However, permission to reprint\/republish this material for\nadvertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists,\nor to reuse any copyrighted component of this work in other works must be obtained from the IEEE.\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\n1054 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 7, NO. 6, DECEMBER 2005\nMultiserver Support for Large-Scale\nDistributed Virtual Environments\nBeatrice Ng, Rynson W. H. Lau, Antonio Si, and Frederick W. B. Li\nAbstract\u2014CyberWalk is a distributed virtual walkthrough\nsystem that we have developed. It allows users at different ge-\nographical locations to share information and interact within a\nshared virtual environment (VE) via a local network or through\nthe Internet. In this paper, we illustrate that as the number of\nusers exploring the VE increases, the server will quickly become\nthe bottleneck. To enable good performance, CyberWalk utilizes\nmultiple servers and employs an adaptive region partitioning\ntechnique to dynamically partition the whole VE into regions. All\nobjects within each region will be managed by one server. Under\nnormal circumstances, when a viewer is exploring a region, the\nserver of that region will be responsible for serving all requests\nfrom the viewer. When a viewer is crossing the boundary of two or\nmore regions, the servers of all the regions involved will be serving\nrequests from the viewer since the viewer might be able to view\nobjects within all these regions. This is analogous to evaluating a\ndatabase query using a parallel database server, which could im-\nprove the performance of serving a viewer\u2019s request tremendously.\nWe evaluate the performance of this multiserver architecture of\nCyberWalk via a detail simulation model.\nIndex Terms\u2014Adaptive region partitioning, distributed virtual\nenvironments, multiserver architecture.\nI. INTRODUCTION\nCyberWalk [7], [8] is a distributed virtual walkthrough pro-totype system that we have developed. It allows users at\ndifferent geographical locations to share information and in-\nteract within a shared virtual environment (VE) via a local net-\nwork or through the Internet. This VE may represent a physical\nmuseum or place of interest. It may also represent a place to be\nconstructed, such as a planned theme park. CyberWalk employs\na standard client-server architecture. Information on virtual ob-\njects, including their locations and shapes, are maintained in a\ncentral database server. When a viewer (user) walks through a\nVE, geometry information of the virtual objects located within a\nvisible distance from the viewer will be conveyed to the viewer\u2019s\nclient machine. The information will then be processed and ren-\ndered into images to be viewed in a timely fashion. In gen-\neral, the virtual objects could be dynamic, changing their loca-\nManuscript received December 11, 2002; revised August 23, 2004. This work\nwas supported in part by the Research Grants Council of Hong Kong under\nCERG grants CityU 1080\/00E and CityU 1308\/03E. The associate editor co-\nordinating the review of this manuscript and approving it for publication was\nDr. Wen-Tsung Chang.\nB. Ng and R. W. H. Lau are with Department of Computer Science, City Uni-\nversity of Hong Kong, Kowloon, Hong Kong (e-mail: beatrice@cs.cityu.edu.hk;\nrynson@cs.cityu.edu.hk).\nA. Si is with the Oracle Corporation, Redwood Shores, CA 94065 USA\n(e-mail: antonio.si@oracle.com).\nF. W. B. Li is with Department of Computing, The Hong Kong Polytechnic\nUniversity, Hung Hom, Hong Kong (e-mail: csbor@comp.polyu.edu.hk).\nDigital Object Identifier 10.1109\/TMM.2005.858388\ntions and orientations within the VE. However, since the current\nCyberWalk prototype supports only static objects (i.e., objects\ncannot be changed\/modified), we focus on VEs with static ob-\njects only in this paper.\nOur goal in this project is to provide good performance\nfor virtual walkthrough with Cyber-Walk, both in terms of\nresponsiveness and image quality, under the existing constraint\nof relatively low Internet bandwidth. In [8], we have introduced\na multiresolution caching mechanism, which allows frequently\naccessed virtual objects to be cached in a client\u2019s local storage\nat appropriate resolutions. The caching mechanism is also\ncomplemented with a prefetching mechanism, which attempts\nto predict the movement of a viewer and transmits the objects\nto the client in advance. We have verified that the caching and\nprefetching mechanisms provide impressive improvement in\nsystem performance.\nIn this paper, we illustrate that when the number of viewers\nexploring the VE increases, the server will quickly become the\nbottleneck through serving queries from the clients, which in-\nclude retrieving and transmitting the requested object models\nto the clients. CyberWalk addresses this problem by employing\nparallelism using an array of object servers. We propose an\nadaptive region partitioning technique to partition the whole VE\ninto multiple regions. All objects within a region will be man-\naged by one server. Requests from viewers for any object within\na region will be served by the server managing that region. This\nreduces the number of viewers that each server needs to handle.\nWhen a viewer is crossing the boundary of two or more regions,\nall the servers of the relevant regions will be serving requests\nfrom the viewer since the viewer may be able to view objects\nwithin all these regions. However, when a server is overloaded\nby a large number of requests due to too many clients accessing\nits region, the managed region will be partitioned and part of it\nwill be transferred to a lightly loaded neighbor server. We illus-\ntrate in this paper the performance improvement of CyberWalk\nusing this parallel architecture.\nThe rest of the paper is organized as follows. Section II de-\nscribes relevant research work. Section III introduces the par-\nallel server architecture of CyberWalk. Section IV presents the\nload balancing mechanism through the adaptive region parti-\ntioning scheme. Section V discusses the performance of Cy-\nberWalk via several simulated experiments. Finally, Section VI\nbriefly concludes the paper.\nII. RELATED WORK\nIn this section, we introduce existing multiserver techniques\nfor distributed virtual environments and compare local ap-\nproach with global approach to load balancing. We then give\n1520-9210\/$20.00 \u00a9 2005 IEEE\nNG et al.: MULTISERVER SUPPORT FOR LARGE-SCALE DISTRIBUTED VEs 1055\nan overview of our CyberWalk prototype system and discuss\nthe unique features of CyberWalk to achieving load balancing.\nA. Multiservers for Distributed Virtual Environments\nAs the number of clients accessing a VE increases, the\namount of messages needed to be sent among them increases\ndramatically. The server loading in managing the VE and han-\ndling the messages also increases significantly. Traditionally, in\na VE there are two types of architecture for multiple clients to\nparticipate in: peer-to-peer and client-server.\nEarlier systems, such as NPSNET [13] and DIVE [3], are\nimplemented in a peer-to-peer architecture. This approach has\nminimal communication overheads, but may not scale well\nto handle many simultaneous clients due to the saturation of\nnetwork bandwidth in handling broadcast or multicast mes-\nsages from the clients. To improve scalability, systems such\nas BrickNet [24], Community Place [20] and MASSIVE-3\n[16], are implemented in client-server architecture. With this\napproach, each client sends messages to the server for further\npropagation to other clients and\/or servers in the same VE.\nThe advantage of this approach is that the server may perform\nmessage filtering to minimize the amount of messages needed\nto be handled by each client and to be propagated through the\nnetwork. The major limitation, however, is that as the number\nof clients accessing the VE increases, the amount of messages\nneeded to be sent among them increases dramatically. The\nserver loading in managing the VE and handling the messages\nalso increases significantly. Another problem is that the server\nmay potentially become a single point of failure.\nA distributed VE system with a multiserver architecture could\nsolve these problems. The VE may be partitioned into regions,\nand each of which is assigned to a separate server, distributing\nthe workload among them. This may also prevent the single\npoint of failure problem if clients can be connected to different\nservers dynamically. Systems of this approach include RING\n[14], NetEffect [9], and CittaTron [18].\nIn RING [14], the VE is partitioned statically and each re-\ngion is assigned to a fixed server. With this approach, some\nservers may still be overloaded if a large number of clients con-\nverge to some particular regions of the VE. A client in RING,\nhowever, may choose to connect to a server statically or dy-\nnamically based on its current position. In NetEffect [9], the\nVE is partitioned dynamically based on client density of indi-\nvidual communities (regions), and it is possible for one server\nto handle multiple communities. A master server is responsible\nfor performing the load balancing duty. It periodically checks\nthe client density of all communities and performs load bal-\nancing by transferring communities among the servers. In ad-\ndition, a client may be migrated to a different community of\nanother server when its community is overcrowded. However,\nafter the client is migrated to a new community, it needs to wait\nfor the client machine to download the scene of the new commu-\nnity. In CittaTron [18], the VE is partitioned into regions dynam-\nically based on the server loading as a result of the number of\nclients in the region. Each region is assigned to a unique server.\nThe size of each region may be adjusted during run-time and\nclients may be transferred among the servers in order to achieve\nload-balancing. However, factors such as object density and lo-\ncality are not considered for load balancing.\nExisting multiplayer online games have already implemented\nwith distributed game servers. For example, Quake III Arena\n[22] and Diablo II [11] offer a list of game servers for clients to\njoin. However, each game server maintains a unique game state,\nwhich is not shared among the servers. This is essentially a set\nof separated client-server systems running the same game and\nmay not be considered as a real multiserver system. EverQuest\n[12], in contrast, divides the entire VE into distinct zones, and\nmaintains these zones by individual game servers. EverQuest\nallows a client to travel from one zone (game server) to another\nfreely. Ultima Online [25] and Asheron\u2019s Call [1] adopt similar\napproach as EverQuest, but they divide the entire VE into visu-\nally continuous zones. The boundary of each zone is mirrored at\nneighbor server to reduce the lag problem and to improve inter-\nactivity when a user crosses from one zone to another. In addi-\ntion, Asheron\u2019s Call is technically more advanced in that it may\ndynamically transfer a portion of the zone controlled by a given\ngame server to any other lightly loaded server. Unfortunately,\nobject coherency is not considered.\nB. Local and Global Load Balancing\nThere are two major approaches to load balancing in tradi-\ntional distributed systems: global and local. Global approach\nconsiders the loading information of all servers during load\nbalancing. Usually, a single processor is responsible for col-\nlecting the loading information from all processors in the\nsystem, making the load balancing decisions, and organizing\nthe load migration processes. Local approach, on the other\nhand, considers the loading information of local servers only\nwhen making the load balancing decisions for a server. In\ngeneral, global approach can produce more accurate balancing\ndecisions as the processors responsible for handling the load\nbalancing process have the loading information of the entire\nsystem. However, it involves high overheads on network band-\nwidth consumption and network delay in collecting the large\namount of loading information from all the servers as well as\nprocessing all the collected information. These overheads are\nproportional to the scale of the system. On the other hand,\nlocal approach is generally less accurate in making the load\nbalancing decisions but it is more efficient, in particular, for\nlarge systems [26].\nC. Overview of CyberWalk\nMost existing distributed VE systems employ a complete\nreplication approach by distributing the complete geometry\ndatabase to the clients before the start of the application [2],\n[3], [21]. As the geometry database is usually large in size,\nthis approach assumes that the clients will obtain the database\nthrough a high speed network or from some other means, such\nas CDROM distribution. Another approach to distribute geom-\netry data is to send them on demand to the clients at run-time\n[13], [23]. When the VE is large, a viewer would likely only\nvisit a small section of it. It is more efficient and effective to\ntransmit only the visible region of the VE to the client to reduce\nstartup time and network traffic.\n1056 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 7, NO. 6, DECEMBER 2005\nCyberWalk [7], [8] is based on the on-demand transmission\napproach. It has many advantages over existing systems, in-\ncluding minimal transmission cost through progressive model\ntransmission and the use of viewer\/object scopes, minimal\nrendering cost through multiresolution object modeling, and\nhigh interactivity and system availability through caching and\nprefetching:\n\u2022 Viewer Scope and Object Scope: To minimize the amount\nof data needed to be handled, CyberWalk generalizes the\nArea-Of-Interest concept [13] to both viewers and objects,\nreferred to as the viewer scope and the object scope. A\nviewer scope indicates how far the viewer can see. An ob-\nject scope indicates how far an object can be seen, and\nits size is proportional to the size of the object. An ob-\nject can only be seen by a viewer when the two scopes\noverlap. Hence, objects which scopes do not overlap with\nthe viewer scope do not need to be transferred to the client\nto save transmission, processing, and memory costs. In\nCyberWalk, we define a scope as a circular region charac-\nterized by a radius. This viewer scope\/object scope may be\nconsidered as a restricted form of the Aura\/Nimbus model\n[15].\n\u2022 Multiresolution Object Modeling for Progressive Trans-\nmission and Rendering: Sending the complete models of\nonly the visible objects to the client machine on-demand\nmay still cause a possibly long pause in the walkthrough.\nInstead, CyberWalk encodes each object model in a\nformat similar to the progressive mesh [17] for progres-\nsive model transmission and reconstruction. Each object\nis modeled as an ordered list. The list begins with a\nbase mesh, which is a minimal resolution model of the\nobject, followed by a sequence of progressive records,\neach of which stores information that helps increase\nthe resolution of the model by a small amount. If we\napply the information stored in each of the progressive\nrecords to the base mesh of an object in order, the object\nmodel will gradually increase in resolution until all the\nprogressive records in the list are exhausted. On the other\nhand, we may decrease the resolution of the object model\nby reversing the operation. During run-time, the object\ndistance from the viewer determines the resolution of\nthe progressive mesh needed to be available at the client.\nSince the visibility of an object usually changes slightly\nfrom frame to frame, only a small number of progressive\nrecords need to be transmitted to the client between\nconsecutive frames.\n\u2022 Multiresolution Caching: In CyberWalk, a multires-\nolution caching technique was developed to allow\nfine-granularity of object caching and replacement. A\ncaching mechanism allows a client to utilize its memory\nand local storage to cache currently visible objects that\nare likely to be visible in the near future. A local cache\nalso supports a certain degree of disconnected operation\nwhen the Internet is temporarily unavailable.\n\u2022 Prefetching: A prefetching mechanism allows a client to\npredict objects that are likely visible in the near future and\nfetch them in advance to improve response time. In Cyber-\nWalk, an EWMA with residual adjustment scheme was\ndeveloped to predict the viewer\u2019s movement in the VE.\nConsidering the fact that most users use PCs equipped\nwith two-dimensional (2-D) mice for three-dimensional\n(3-D) navigation, we have recently developed a hybrid\nmotion prediction method for predicting the viewer\u2019s fu-\nture movement based on the motion pattern of the 2-D\nmouse [5].\nD. Unique Features of CyberWalk to Load Balancing\nAlthough there are similarities in the load balancing process\nbetween distributed VEs and traditional distributed systems,\nthere are two major differences here. First, in traditional dis-\ntributed systems, we may transfer any tasks to any servers\n[4]. This provides a greater flexibility to the load balancing\nprocess. In distributed VEs, however, virtual objects exhibit\nvisual coherency\u2014if an object is visible to the viewer, objects\nthat are geographically close to this object are likely visible to\nthe viewer too. To minimize the communication overheads, we\nneed to cluster nearby objects into the same regions as much\nas possible so that they can be served by the same servers.\nHence, in the load balancing process, an overloaded server may\ntransfer its load to its adjacent servers only.\nSecond, in traditional distributed systems, the general objec-\ntive of load balancing is to complete all the tasks within the\nshortest possible time. Hence, it is important to balance the\nloadings of all the processors as much as possible. However,\nin distributed VEs, our objective is to complete all the update\noperations within a given time (referred to as the performance\nthreshold). Hence, if we can complete all the tasks within the\ngiven time, there is no need to perform load balancing. This can\nminimize the overhead of the load balancing process.\nUnlike distributed systems, mobile communication systems\nonly provide very limited degree of load balancing. A mobile\nsystem is generally constructed by a collection of cells. Each\ncell is served by a dedicated base station (i.e., server) located at\nthe center of the cell and is allocated a fixed number of chan-\nnels. When all the channels of a cell are taken up by the mobile\nusers, it is possible for the cell to borrow additional channels\nfrom neighbor cells or from a central controller in order to serve\nadditional users. Once the cell fails to get additional channels, it\nwill have to reject the service request from new users [19]. The\nmajor difference between mobile systems and distributed VEs\nis that in mobile systems, the geographical locations of the base\nstations are basically fixed and cannot be dynamically adjusted\naccording to the density of the mobile users. On the other hand,\nin distributed VEs, we may dynamically adjust the size and lo-\ncation of the region served by a server in order to achieve load\nbalancing.\nIII. PARALLEL ARCHITECTURE OF CYBERWALK\nTo study the impact of the number of clients on the perfor-\nmance of a centralized server, we have conducted a simulated\nexperiment to measure the average latency time and response\ntime of multiple viewers on single-server CyberWalk. Latency\ntime measures the average time required to retrieve the base\nmeshes of all visible objects, i.e., from the moment the viewer\nNG et al.: MULTISERVER SUPPORT FOR LARGE-SCALE DISTRIBUTED VEs 1057\nmakes a move to the moment when there is a coarse image of\nall visible objects. Response time measures the average time that\nthe viewer needs to wait from the moment the viewer makes a\nmove to the moment when all the visible objects are available at\nthe client at their optimal resolutions. Latency time inherently\nmeasures the interactivity of the system, while response time\nmeasures the absolute performance of the system. The number\nof clients that we have experimented ranges from 2, 4, 8, 16, 32,\n64, and 128.\nThe size of the VE was set to 100 100 square units (or\ncells); 6000 virtual objects are uniformly distributed among the\ncells. The radius of each object scope is randomly generated,\nranging from 0.7% to 1% of the size of the VE, with a mean\nof 0.85%. Each object is modeled by a progressive mesh, con-\ntaining 4500 to 7000 progressive records with a mean of 5300.\nEach progressive record has a size of 40 bytes while each base\nmesh has a size of 3 kB. The database is approximately 1.2 GB.\nThe viewer\u2019s viewing angle is set to 120 degrees. The radius\nof the viewer scope is also generated in the same way as that\nof the object scope. We assume that the network bandwidths of\nthe server and of each client are 20 and 2 Mbps, respectively.\nThe client movement in the VE is determined by sampling the\nmovement of a real user who interacts with the CyberWalk pro-\ntotype using a mouse. The movement is modeled as follows.\nThe viewer moves circularly. Each movement step includes a\ntranslation of 15 units along the viewing direction, followed by\na rotation of the viewing direction of 12 degrees. At each step,\nthe viewer rotates his\/her head by 20 degrees. In addition, the\nmoving direction changes with an angle of 10 degrees, after\nevery four movement steps. This is termed changing circular\nmoving pattern (CCP).\nBoth caching and prefetching mechanisms are activated in\neach client. Since in our previous study [8], we have found that\n1% cache size is able to achieve a cache hit of 86%, the size of\nthe storage cache of each client is set to 1% of that of the data-\nbase, i.e., 12 MB. Both the server and the client programs run on\nPCs with a Pentium III 800-MHz CPU and 512-MB RAM. The\nresult of the experiment is shown in Fig. 1. Fig. 1(a) and 1(b) de-\npicts the performance of the server when the number of clients\nranges from 2 to 128. As the performance of the server cannot\nbe seen clearly in these diagrams when the number of clients is\nless than 16, we show it in a much finer scale in Fig. 1(c) and\n1(d).\nFrom the figure, both latency and response times of the server\nas observed by each client increase super-linearly as the number\nof clients increases. For example, the latency time for 128 clients\nis as high as 28 s. This is very nonfavorable for walkthrough\nas each client will start experiencing low interactivity when the\nnumber of clients accessing the VE reaches a certain value. As-\nsuming that a 0.2-s latency time is the performance threshold\nthat a client can tolerate, we observe from Fig. 1(c) that all\nclients will experience low interactivity when there are more\nthan about ten clients.\nTo address this problem, CyberWalk employs a parallel archi-\ntecture of multiple servers. A parallel architecture is fundamen-\ntally a share-nothing architecture [10]. As illustrated in Fig. 2,\neach server manages its own memory and database repository\nof virtual objects. In traditional share-nothing architecture, one\nFig. 1. Effect of number of clients on the performance of the server.\nFig. 2. Parallel architecture of CyberWalk.\nof the servers is often dedicated to be a coordinator, which has\ncomplete knowledge of the data managed in each of the other\nservers. When the coordinator receives a query from a client, it\nforward the query to the server which manages the data required\nby the query. However, the coordinator could quickly become\na bottleneck when the rate of query submission becomes high.\nAnother problem with such architecture, which is regarded as\nvery important in our application, is that any changes made by\na client may have to go through many network communication\nsteps before the client receives a reply. This extra delay can se-\nriously affect the interactivity of the system, especially if the\nservers are not physically located in the same LAN.\nIn CyberWalk, we partition the complete VE into a two-di-\nmensional array of regions. Virtual objects within a region will\nbe managed by a single server only. Each server will be serving\nonly those clients who are exploring objects within the region\nmanaged by the server. In other words, each client will be com-\nmunicating directly to the server which manages the virtual ob-\njects that the client is exploring. Under normal circumstances,\nwhen a viewer is exploring a region, the server managing the re-\ngion will be responsible for serving all requests from the viewer.\nHowever, when the viewer moves to the region boundary, it\nmay be able to see objects belonging to the neighbor regions.\nFig. 3 shows such an example. When viewer is crossing the\nboundary from region to region , i.e., when the viewer\nscope of , , touches the boundary of and , both\nservers and , in addition to , will now be serving re-\nquests from since is able to view objects within regions\n, , and .\nTo handle this situation, when first touches the boundary\nof , server will send a message to server of the format\n, where is the ID of , which is inherently\n1058 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 7, NO. 6, DECEMBER 2005\nFig. 3. Visual interaction of a client with mulitiple servers.\nthe IP address of the client machine, is the location of\nin the VE, and is the viewing direction of . Once server\nhas received such a message from server , it determines\nthe objects in that are visible to and transmits the cor-\nresponding progressive records and\/or base meshes directly to\n. will then maintain direct communication channels with\nboth and as long as overlaps with both regions. If the\nviewer scope of now touches , server will send a similar\nmessage to server and will now be communicating with\nserver , and . When eventually moves into region ,\nwill stop communicating with server and and will only\ncommunicate with server .\nAn advantage of this architecture is that each server will\neffectively be serving for a considerable less number of clients.\nWhen the number of clients inside a region has substantially\nincreased to the extent which affects the performance per-\nceived by a viewer, that region may be further partitioned and\nthe newly partitioned region may be allocated to a less busy\nneighbor server.\nAs depicted in Fig. 2, one process, named the Loading Col-\nlector (LoC) process, is dedicated to collect the loading infor-\nmation of each server. Each server will periodically inform the\nLoC process about its load, and has the flexibility of determining\nhow often it will inform the LoC process about its load. If a\nserver\u2019s loading varies more frequently, the server will inform\nthe LoC more often; otherwise, it could do it less frequently.\nThe LoC process, thus, has the most up-to-date loading infor-\nmation of each server. Approaches to determine such duration\nhas been studied in [6] and is thus beyond the scope of this\npaper. When a server is overloaded, it contacts the LoC process\nand requests for the loading information of its neighbor servers.\nThe overloaded server will then select a neighbor server with\nthe lowest load to absorb the newly partitioned region. The LoC\nprocess could also be piggybacked as a point of contact for each\nnewly-jointed client. It finds the appropriate server for the user\nto contact to explore the relevant part of the VE. Note that the\nLoC process could be hosted in any one of the servers. When\nthe server hosting the LoC process fails or is overloaded, a new\nserver could be selected to host it.\nIn order to fully realize the advantage of this parallel architec-\nture, we need to address two issues. First, we need a mechanism\nto monitor the servers\u2019 loading. Since the load of each server\nneeds to be updated frequently, this mechanism must be effi-\ncient in order not to cause extra load to each server. Second, we\nneed to properly partition the VE into regions. Since the virtual\nobjects may not be uniformly distributed among the regions, if\nthe VE is simply partitioned in a uniform manner, some servers\nwill need to manage more objects than others. This will cause\nsome servers to experience a heavier workload than others. In\naddition, some regions may contain more interesting objects and\nattract more visitors. This skewed popularity characteristic of\nvirtual objects may further affect the load distribution among\nthe servers. An efficient partitioning scheme which would con-\nsider the loading of the affected servers is needed. We term our\npartitioning scheme, adaptive region partitioning, as the parti-\ntioning of regions will be adapted to the load among the servers.\nIV. ADAPTIVE LOAD BALANCING\nIn CyberWalk, the complete VE is regularly subdivided\ninto a large number of rectangular cells. Each cell, , con-\ntains a set of objects, i.e., .\nThe VE is also partitioned into a set of regions, i.e.,\n, while each region contains an\ninteger number of cells, i.e., . Fig. 4\ndepicts a partition of nine regions, each containing nine cells\nand managed by one server.\nThere are several ways to partition the VE into regions. The\nsimplest way is to evenly partition it and we refer to it as the\neven scheme. Each region will cover the same geographical size\nof the VE and contain the same number of cells, i.e.,\n. However, since the virtual objects might\nnot be distributed uniformly within the VE, some regions may\ncontain more objects than others. This could be addressed by\npartitioning the VE based on \u201cobject density\u201d and we refer to it\nas the density scheme. In this scheme, each region will contain\napproximately the same number of objects, i.e.,\n. However, each region may cover\na different geographical size of the VE, and thus may contain\ndifferent number of cells.\nNote that when the virtual objects are distributed uniformly\nwithin the VE, the density scheme and the even scheme will\nresult in a similar partition. The density scheme attempts to\nachieve a uniform load among all servers by ensuring that each\nserver will manage the same number of objects. This is based on\nan assumption that each object has a similar degree of interest\nto the viewers and thus, a similar probability of being accessed.\nIn practice, however, viewers may show particular interests in\ncertain objects and explore certain regions more frequent than\nothers. Hence, the density scheme may not necessarily result in\na uniform load among the servers. We address this issue by our\nadaptive region partitioning scheme, in which the partitioning\nof the regions is adapted to the load among the servers.\nA. The Adaptive Region Partitioning Scheme\nWhen CyberWalk is first started, there is no information re-\ngarding to the loading of each server and the VE is partitioned\ninto regions based on the density scheme. Each server com-\nputes its own loading periodically. When server is found to\nbe overloaded, region will be partitioned. The newly parti-\ntioned region will then be allocated to a neighbor server of\nwith the lowest load. This inherently directs all future viewers\u2019\nrequests on the newly partitioned region to the neighbor server\nNG et al.: MULTISERVER SUPPORT FOR LARGE-SCALE DISTRIBUTED VEs 1059\nFig. 4. Partitioning of the VE.\nand thus reduces the load of . For example, in Fig. 4, when\nserver of region is overloaded, is partitioned into two\nsubregions, and , such that may be transferred to\na neighbor server.\nTo determine if a region needs to be partitioned, each\nserver will continuously monitor its own load by main-\ntaining two monitor windows called the short duration\nwindow, , and long duration window, , with a\nwindow size of and , respectively, where\n. (Hereafter, we will leave out when\nthe context is clear.) The short duration window monitors the\nload of within a very short duration of time. The purpose\nis to detect sudden bursts of load on the server either due to\nnetwork congestion or a sudden increase in interest on region\n. In order for a server to respond to the sudden bursts of load\nquickly, should be set as small as possible. However,\nthe smallest is limited by the frequency at which the\nserver updates its own loading. By contrast, the long duration\nwindow monitors the load of within a much longer period\nof time. The purpose is to detect a continuous high load on the\nserver. cannot be set too high or the server will not\nbe too responsive to the change in loading, and it cannot be set\nto small or it will increase the partitioning overhead. From our\nexperiments, setting to a few times higher than the\nvalue of produces reasonably good results in most\nsituations.\nWe model the server\u2019s loading by two factors: CPU loading\nfactor and network loading factor . The\nCPU loading factor captures the processing overhead required to\nidentify and retrieve the object models requested by the viewers.\nIt is modeled by the server utilization and approximated by the\nnumber of objects processed in a second\nwhere is the maximum number of objects a server can\nprocess in a second. This information inherently captures the\nCPU overhead of the server. The network loading factor cap-\ntures the transmission overhead required to transmit the progres-\nsive records to the viewers and is approximated by the amount\nof object data sent to the client per second\nwhere the effective network bandwidth is the actual network\nbandwidth allocated to the server. Hence, each server will mon-\nitor the number of bytes sent through the network and determine\nthe duration to transmit the data.\nTo determine if a region needs to be further partitioned, each\nserver maintains two sets of partitioning thresholds,\nand for the short duration window and and\nfor the long duration window. When exceeds\neither or , the region will be further partitioned.\nLikewise, when exceeds either or , the\nregion will also be partitioned. The setting of these two sets of\nthresholds determines the loading at which the server should\nactivate the partitioning process. Since each server compares\nits loading with the thresholds for the short duration window\nfrequently (once every ), the thresholds essentially\nspecify a maximum loading that the server should not exceed in\norder to maintain the system\u2019s interactivity. On the other hand,\neach server compares its average loading with the thresholds\nfor the long duration window in a less frequent manner (once\nevery ), the thresholds essentially specify an average\nloading that the server should not exceed in order to maintain\nthe system\u2019s interactivity. In other words, these two sets of\nthresholds specify the bounds on the CPU usage and Network\nusage of the server. In order to maximize the resource usage\nof the server, we may set the thresholds for the short dura-\ntion window as high as possible. The thresholds for the long\nduration window should be set lower than those of the short\nduration window. However, they should not be set too low as it\nwould waste the server\u2019s resources.\nThe following steps summarize the process of partitioning a\nregion .\n1) When server is overloaded, it contacts the LoC for the\nloading information of its neighbor servers. For instance,\nin Fig. 4, neighbor servers of are , , and .\n2) When server receives the loading information of its\nneighbor servers, it selects the neighbor server with the\nlowest load as the target server to offload some of\nits load. This load transfer is achieved by partitioning\nand allocating the newly partitioned subregion to the\ntarget server, i.e., the newly partitioned subregion will be\nmerged with managed by . Referring to Fig. 4, as-\nsuming that server is overloaded and is selected as\nthe target server, will partition into subregions\nand , and transfer to . Future requests on re-\ngion will then be handled by .\n3) Server will determine the total amount of load needed\nto be transferred to neighbor servers. We refer to such\ngoal as the target load. In our prototype, each server will\nreduce its load to 10% below the threshold value.\n4) The partitioning is performed on a per cell basis. Each\nserver will maintain a workload indicator for each cell.\nThe partitioning of a region is achieved by de-allo-\ncating one or more of its boundary cells, , from\nand transferring them to the target server, i.e.,\nand . For example, cells ,\n, and in Fig. 4 are the boundary cells of to be\nmerged with .\n1060 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 7, NO. 6, DECEMBER 2005\n5) Sometimes, it is possible for the target load to be higher\nthan the amount of load that the target server can accept.\nTo prevent overloading the target server, only transfers\na portion of the target load to the target server. It then\nselects another target server and transfers the remaining\nload to it.\nAs discussed in Section II-D that virtual objects in the VE\nexhibit visual coherency, should offload its load to only its\nneighbor servers instead of any arbitrary servers, to reduce the\ncommunication overheads of the system. In Fig. 4, if server\noffloads cells , , and at three different load balancing\noperations to arbitrary servers, these three cells may potentially\nbe offloaded to three different servers. When the viewer scope\nof a viewer overlaps with these cells, the client will have to com-\nmunicate with multiple servers in order to receive object models\nwithin these cells. On the other hand, when offloading is per-\nformed only to the neighbor servers, all these three cells may\nwell be offloaded to server and the client will only need to\ncommunicate with two servers.\nB. Identifying Target Servers and Target Cells\nWhen an overloaded server is to transfer part of its load to\na neighbor server, it needs to identify some boundary cells as\nthe target cells to be transferred and a neighbor server as the\ntarget server to accept the cells. We use Fig. 5 as an example to\nshow how we identify the target server and the target cells. We\nassume that server is overloaded and needs to transfer some\nof its load to a neighbor server.\nWe define a break point as a cell that indicates a potential\nlocation for finding boundary cells to be transferred when the\nserver is overloaded. All boundary cells that have at least two of\ntheir four sides connected to cells managed by other servers are\nidentified as break points. To minimize the cost of identifying\nbreak points during the load balancing process, each server will\nmaintain a break point list to indicate all the break points along\nits boundary. At the same time, the server will also maintain a\nboundary edge list. Each edge is either a horizontal line or a\nvertical line that joins two aligned break points. For example, in\nFig. 5, and will form an edge because they are aligned\nhorizontally; however, and will not because they are not\naligned vertically or horizontally. Hence, region has four\nedges (highlighted as thick lines) in its boundary edge list. We\nfurther maintain a neighbor server list to indicate the neighbor\nservers that are adjacent to edges found in the boundary edge\nlist. Once we have constructed the neighbor server list, we may\nselect a server with the lightest load from the neighbor server\nlist as the target server of .\nIn Fig. 5, if we assume that server is selected as the target\nserver, the boundary cells between to will be selected\nas the target cells to be transferred to , until the target load\nis reached. After transferring the boundary cells between\nand to , break points and will be removed from\nthe break point list. The cell to the left of will become a\nnew break point to be inserted to the break point list. A new\nboundary edge will then be generated which is formed by\nand the new break point. Hence, we need to remove the edge\nbetween to from the boundary edge list and add the new\nFig. 5. Break points and boundary edges of a region.\nedge to the list. If the total load of all the boundary cells between\nand is lower than the target load, we will select the cells\nalong the new edge as the target cells to be transferred to ,\nuntil the target load is reached. However, if cannot accept\nthe complete target load, will transfer the remaining load to\n, if is the next lightest loaded server in the neighbor server\nlist.\nThe basic idea of the adaptive region partitioning scheme\nis that the overloaded server always looks for a lightly loaded\nneighbor server and \u201cpeels\u201d its own boundary cells to that server.\nNote that we do not include edges that are not found in the\nboundary edge list for cell transfer, because if we do so, it will\nlikely increase the perimeters of the regions. For example, trans-\nferring the boundary cells between to will likely de-\ncrease the perimeters of regions and , while transferring\nthe boundary cells between to will likely increase the\nperimeters of the regions and hence increase the chance that a\nviewer needs to be served by more than one server.\nV. RESULTS AND DISCUSSIONS\nWe have developed a simulation model and conducted ex-\nperimental studies to quantify and evaluate the performance of\nthe parallel architecture of CyberWalk. The parameters used\nin our experiments are listed in Table I. Our experimental set-\ntings are similar to those described in Section III. In our sim-\nulation model, the VE is regularly subdivided into 100 100\ncells. The complete VE is partitioned into regions, with\n. Since each region is managed by one server,\nservers will be required.\nThere are virtual objects in the VE distributed among the\ncells. The modeling of each virtual object has been described\nin Section III. There are clients accessing the VE. Both the\nserver and the client programs run on a Pentium III PC with a\n800-MHz CPU and 512-MB RAM. The network settings are es-\nsentially the same as described earlier. We model two object dis-\ntribution patterns, , in our experiments: uniform and skew.\nIn uniform distribution, the objects are distributed uniformly\namong all regions. In skew distribution, each region contains\na random percentage of objects, ranging from 30% to 200%\nof the mean value. We study three different region partitioning\nschemes, : even, density, and adaptive. In even and density,\neach region will not be further partitioned into subregions re-\ngardless of the workload of the server. In adaptive, a region\nmight be further partitioned into subregions using the adaptive\npartitioning scheme.\nNG et al.: MULTISERVER SUPPORT FOR LARGE-SCALE DISTRIBUTED VEs 1061\nTABLE I\nPARAMETER VALUES FOR THE EXPERIMENTS\nTo monitor the load, the window size of the short duration\nwindow of each server is set to 1 sd, as we update the server\nloading once a second. The window size of the long duration\nwindow of each server is set to 5 s in our experiments. We as-\nsume the effective bandwidth ratio to be 0.8. Since the max-\nimum network bandwidth of each server is 20 Mbps, the ef-\nfective network bandwidth is thus 16 Mbps. In addition, we set\nto , which is the maximum number of objects\nthat the server can process in a second. This number is obtained\nthrough experiments.\nTo determine at what loading to initiate the partitioning\nprocess, we set both CPU and Network partitioning thresholds\nfor the short duration window and , respectively,\nto 1. As mentioned in Section IV-A, these two thresholds deter-\nmine the maximum loading of the server. We also set the two\nthresholds for the long duration window and to\n0.9, except for experiment #5, where we attempt to study how\nthe setting of the two thresholds may affect the performance of\nthe system.\nEach viewer will be residing at a random position within the\nVE when the simulation starts, and move within the VE ac-\ncording to the changing circular pattern (CCP) described in Sec-\ntion III. Our simulation program ensures that the path of each\nviewer is different. We measure latency time and response time,\nas defined in Section III, experienced by each client. In each ex-\nperiment, each metric is determined by averaging the metrics\nobtained from all movement steps of all clients. The standard\ndeviations are found to be small. Here, we present six of the\nexperiments to quantify the performance of the parallel archi-\ntecture of CyberWalk.\nA. Experiment #1\nIn the first experiment, we study the effect of number of\nservers on the performance of CyberWalk. In this experiment,\nis fixed at 6000 objects. We depict the overhead when there\nare 128 clients. The region partitioning scheme is fixed at even.\n(The effect of various region partitioning schemes are studied in\nExperiments #3 and #4.) The object distribution pattern is fixed\nat uniform. (The effect of various object distribution patterns\nwill be studied in Experiment #3.) The number of servers, ,\nranges from one, three, six, and nine. The measurements of\nthe two metrics are depicted in Fig. 6(a) and (b). Since the\nvalues of the metrics for six and nine servers are too small to\nbe noticeable, we zoom in the scale for these two data points\nin Fig. 6(c) and (d). We observe from the diagrams that both\nlatency time and response time decrease exponentially with\nFig. 6. Effect of number of servers on system performance.\nthe number of servers. Using the 0.2-s performance threshold\nfor acceptable visual perception, we can see from the figure\nthat nine (or more than six) servers are sufficient to serve for a\npopulation of 128 clients.\nB. Experiment #2\nIn the second experiment, we study the effect of number of\nobjects on the performance of CyberWalk. In this experiment,\nranges from 1000 to 6000 objects. The number of clients ranges\nfrom 16 to 128. The number of servers is fixed at nine. The re-\ngion partitioning scheme is still fixed at even, and the object\ndistribution pattern is fixed at uniform. Fig. 7 depicts the mea-\nsurements of the two metrics. The first row shows the metrics\nversus the number of objects, while the second row shows the\nmetrics versus the number of clients. We can see that both la-\ntency time and response time increase with the number of ob-\njects. It is very encouraging that all the latency times fall below\nthe 0.2-s performance threshold, meaning that users are able to\nexperience a good visual perception on the system, even with a\nlot of clients and objects.\nWhen comparing Fig. 7(b) and (d), we also observe that the\nnumber of objects does not have as big effect on the performance\nas the number of clients. This is because the amount of data, or\ndetails, transmitted depends on the distance of an object from\nthe viewer. In a typical walkthrough, only a few objects are re-\nally rendered at high detail at any time. Increasing the number of\nobjects will only moderately increase the amount of data needed\nto be sent. In contrast, increasing the number of clients will con-\ntribute to a greater amount of data transmitted from the server\nsince each client will need to request for its own copy of all the\nvisible object models.\n1062 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 7, NO. 6, DECEMBER 2005\nFig. 7. Effect of number of objects\/clients on system performance.\nFig. 8. Performance of even\/density region partitioning schemes.\nC. Experiment #3\nIn the third experiment, we compare the performance be-\ntween even and density region partitioning schemes. In this ex-\nperiment, is fixed at 6000 objects. The number of clients\nranges from 16 to 128. The number of servers is again fixed at\n9. Since two partitioning schemes differ only when the objects\nare not distributed uniformly across the VE, we will only look\nat the skew object distribution pattern here.\nIn Fig. 8, we show three sets of data in each graph, depicting\nthe lowest, the average, and the highest overheads among the\nnine servers. We observe that in even partitioning, there is a\nbig difference in performance among the servers. This indicates\nthat some servers might be overloaded, while others might be\nunder utilized. This is because in even partitioning, the VE is di-\nvided into equal-sized regions and hence each server will need\nto handle different number of objects, resulting in a nonuni-\nform workload among the servers. We also observe that when\nthere are 128 clients, the latency time experienced by the clients\nwill be higher than the 0.2-s performance threshold. In con-\ntrast, in density partitioning, the difference in overhead among\nthe servers is minimal. This indicates that density partitioning\nis able to evenly distribute the workload among the servers. In\naddition, all clients will experience a good visual perception as\nthe latency times of all clients are under the 0.2-s performance\nthreshold.\nFig. 9. Performance of density\/adaptive region partitioning scheme.\nD. Experiment #4\nIn Experiment #3, we have indicated that by partitioning the\nregions based on object density, all servers will have relatively\nuniform load and thus, similar performance. This is based on the\nassumption that each viewer has a similar degree of interest in\neach object. Therefore, if all regions have a similar number of\nobjects, they will also have a similar access probability. How-\never, in practice, viewers usually have different degrees of in-\nterest in different objects. Hence, a region may have a higher\naccess probability if the viewers have a high interest in the ob-\njects within it, disregarding of the number of objects that the\nregion contains. Consequently, a uniform object density among\nvarious regions does not necessarily result in a uniform work-\nload among the servers.\nIn the fourth experiment, we study the effect of our adap-\ntive region partitioning scheme in balancing the load among the\nservers. We fix the number of clients to 128 and the number of\nobjects to 6000. We only look at the skew object distribution\npattern in this experiment, as the even object distribution pat-\ntern has a similar behavior. For comparison purpose, we show\nresults of both density and adaptive partitioning schemes since\nthe density scheme is shown to be able to achieve certain degree\nof uniform load among the servers in Experiment #3. To model\ndifferent degrees of interest in objects, the center region is ded-\nicated to be the hottest region in which of the viewers will\nvisit.\nTo collect the loading information, each server notifies the\nLoC process of its load once a second in our experiment. We\nmeasure the performance of the hottest region by varying\nfrom 10% to 80%. Fig. 9 shows the performance of the center\nserver, i.e., the hottest server, and the average performance of its\naffected neighbor servers. We can see that both latency and re-\nsponse times increase with the number of viewers. With the den-\nsity partitioning scheme, the center server experiences a much\nhigher overhead than the neighbor servers. We can see from\nthe figure that the neighbor servers are under utilized, while the\ncenter server is overloaded. Using the 0.2-s second performance\nthreshold, we observe that the clients will start experiencing a\npoor visual perception when more than 40% of the viewers are\nvisiting the center region. By contrast, with the adaptive parti-\ntioning scheme, the overhead experienced by the center server\nis similar to that experienced by the neighbor servers. This in-\ndicates that the center server could offload its workload to its\nneighbor servers properly. Hence, each server is able to support\na much larger percentage of viewers. From Fig. 9(a), we can see\nthat even with 80% of the viewers visiting the center region, the\nlatency time of the center server (the second curve from the top)\nNG et al.: MULTISERVER SUPPORT FOR LARGE-SCALE DISTRIBUTED VEs 1063\nFig. 10. Effect of setting the partitioning thresholds on system performance.\nis only just above the 0.2-s performance threshold with our pro-\nposed method.\nE. Experiment #5\nIn this experiment, we attempt to study how the setting of\nthe partitioning thresholds would affect the performance of the\nsystem. All the other settings of this experiment are the same as\nthose in experiment #4, except that here, we vary the percentage\nof viewers visiting the hottest region from 10% to 100%. We\nwould like to see how the system would behavior when the\nloading of the hottest server reaches its thresholds. Since the\nthresholds for the short duration window specify the maximum\nloading of the server, we keep them at 1 throughout the exper-\niment. We vary the thresholds for the long duration window to\nsee how they affect the load distribution.\nFig. 10 shows results of this experiment. The two numbers\nassociated with each curve indicate the thresholds for the\nshort\/long duration windows. At low viewer percentages, we\ncan see from Fig. 10(a) that the setting of the thresholds has very\nlittle effect on the load distribution. The difference between\nthe loading of the hottest server and the average loading of the\naffected neighbor servers is roughly the same for all threshold\nsettings. However, as the percentage of viewers visiting the\nhottest region increases, the difference in loading decreases\nwhen the thresholds are set lower (e.g., 0.5). This is because\nat lower threshold values, the hottest server will become over-\nloaded at lower loading and activate the partitioning process\nmore often. As the number of viewers increases, all curves\nwill approach to their maximum latency times. We can see\nthat with low threshold setting, difference in loading between\nthe hottest server and the affected neighbor servers becomes\nsmall. This is because at higher loading, the hottest server will\ninitiate the partitioning process more frequently to keep its\nown load to within the threshold. Concurrently, some of the\naffected neighbor servers may become overloaded themselves\nand transfer some of their loads to their own neighbor servers,\nwhich in effect increases the number of affected neighbor\nservers to share the load. We may note that there is a wider\ngap between the loading of the hottest server and the average\nloading of the affected server when the thresholds are set at\n0.9. This indicates that the loading of the servers are not fully\nsaturated yet.\nJudging from the results, setting lower threshold values for\nthe long duration window allows the system to complete the task\nwell before the 0.2-s performance threshold. However, since the\nservers would now initiate the partitioning process at a lower\nthreshold, it can only serve fewer clients. On the other hand, by\nsetting higher threshold values for the long duration window,\nFig. 11. Partitioning of the VE when there is a large number of clients.\nthe server will initiate the partitioning process at higher loading.\nThis will increase the number of clients that the system can serve\nbut at the same time increase the latency time. From Fig. 10(a),\nthe hottest server has exceeded the 0.2-s performance threshold\nwhen the thresholds of the long duration window are set at 0.9.\nF. Experiment #6\nFig. 11 demonstrates how the shape of the regions changes\nas we introduce viewers into the VE. Fig. 11(a) shows the\ninitial partitioning of the VE, based on object density, into nine\nregions managed by nine servers. (Grey dots represent objects.)\nHence, regions with high object density have smaller size.\nFig. 11(b) shows the new partitioning after we have introduced\na large number of clients (shown as black dots) and let them\nwalk around for a while. We can see that regions with a lot\nof clients are significantly smaller in size now. We can also\nsee that the size of some regions becomes larger even though\nthey also have some clients inside, for example, the left middle\nregion. This is because they have taken up some of the loading\nfrom their neighbor regions. As they are not overloaded yet,\nthey do not distribute any of their own loads to other regions.\nOverall, our adaptive region partitioning scheme can effectively\ndistribute the load among all the servers while minimizing the\ncommunication costs.\nG. Evaluations: Local Approach Versus Global Approach\nAlthough the global approach to load balancing can achieve\na more accurate load balancing in general, there are two reasons\nfor us to adopt the local approach here. First, the local approach,\nin particular our proposed method, in general has a much lower\ncomputation overhead as it only needs to process loading infor-\nmation of the neighbor servers. This is important as the load bal-\nancing process runs on the overloaded processor. Second, unlike\nthe global approach that generally aims at achieving even load\ndistribution among the processor, in our application, we only\nneed to solve the overloading problem as it occurs. This can re-\nduce overall network consumption and computation overhead.\nAs long as the system can complete all the tasks within the per-\nformance threshold, the user will be able to enjoy a satisfactory\nperformance even if some servers may have a high loading than\nthe others.\nAlthough it is difficult to compare the complexity of the\nlocal approach with the global approach under all possible\nsituations, a general comparison can be found in [26], which\n1064 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 7, NO. 6, DECEMBER 2005\ndescribes a detailed comparison among various load balancing\nmethods in terms of information dependency, load balancing\noverheads and aging of information. The conclusion is that\ndiffusive methods in general perform better than other methods\nincluding the global methods. Since our method is also a dif-\nfusive method, which propagates the excessive load out from\nthe overloaded server, our method also performs better than the\nglobal methods in general. Further, due to visual coherency of\nvirtual objects, global methods cannot simply pass any cells\nfrom an overloaded server to any lightly loaded server. The\nload still has to propagate from the overloaded server through\nall the intermediate servers to the lightly loaded server. Hence,\nthe performance of the global methods is expected to be even\nworst when applied here.\nTo explain the above point, we may consider a situation where\nall the neighbor servers of an overloaded server are nearly over-\nloaded. With our method, the overloaded server, , will first\nindicate to a neighbor server, , that will need to transfer\ncertain amount of its load to . If determines that it will\nbe overloaded after receiving the load, it will initiate a parti-\ntioning process to transfer some of its load to its own neighbor\nserver, , before accepting the load from . With a global\nmethod, even though if we can identify that is the nearest\nlightly loaded server of , we cannot simply pass the boundary\ncells of to . We need to pass the boundary cells of to\n, which in turn passes its own boundary cells to .\nVI. CONCLUSION\nIn this paper, we have described a parallel architecture to sup-\nport virtual walkthrough and illustrated its implementation in\nour CyberWalk prototype. We have pointed out that in tradi-\ntional client\/server architecture, a single server has limitation in\nserving multiple clients. As one alternative to improving the per-\nformance, we propose a parallel architecture and employ mul-\ntiple servers in serving the clients. We have discussed several\nways of partitioning the virtual environment into multiple re-\ngions and studied their behaviors under various object distribu-\ntion patterns. We have also introduced the adaptive region par-\ntitioning scheme to address the problem of nonuniform access\namong the regions. This scheme, in effect, partitions the virtual\nenvironment based on the viewers\u2019 degrees of interest on the re-\ngions.\nWe have studied our parallel architecture via simulation and\nexperiments. Our studies show that the adaptive region parti-\ntioning scheme is very effective in maintaining a uniform load\namong the servers. Although we have not conducted experi-\nments on other moving patterns, we believe the CCP moving\npattern studied in this paper is able to present an overall perfor-\nmance behavior of our approach.\nREFERENCES\n[1] Asheron\u2019s Call. [Online] Available: http:\/\/www.microsoft.com\/\ngames\/zone\/asheronscall\/\n[2] J. Calvin, A. Dicken, and B. Gaines et al., \u201cThe SIMNET virtual world\narchitecture,\u201d in Proc. IEEE VRAIS, 1993, pp. 450\u2013455.\n[3] C. Carlsson and O. Hagsand, \u201cDIVEL\u2014a multi-user virtual reality\nsystem,\u201d in Proc. IEEE VRAIS, 1993, pp. 394\u2013400.\n[4] T. Casavant and J. Kuhl, \u201cA taxonomy of scheduling in general-purpose\ndistributed computing systems,\u201d IEEE Trans. Softw. Eng., vol. 14, no. 2,\npp. 141\u2013154, Feb. 1988.\n[5] A. Chan, R. W. H. Lau, and B. Ng, \u201cMotion prediction for caching\nand prefetching in mouse-driven DVE navigation,\u201d ACM Trans. Internet\nTechnol., vol. 5, no. 1, pp. 70\u201391, Feb. 2005, to be published.\n[6] B. Chan, A. Si, and H. V. Leong, \u201cA framework for cache manage-\nment for mobile databases: design and evaluation,\u201d J. Distrib. Parallel\nDatabases, vol. 10, no. 1, pp. 23\u201357, 2001.\n[7] J. Chim, M. Green, R. W. H. Lau, H. V. Leong, and A. Si, \u201cOn caching\nand prefetching of virtual objects in distributed virtual environments,\u201d\nin Proc. ACM Multimedia, Sep. 1998, pp. 171\u2013180.\n[8] J. Chim, R. W. H. Lau, H. V. Leong, and A. Si, \u201cCyberWalk: A web-\nbased distributed virtual walkthrough environment,\u201d IEEE Trans. Mul-\ntimedia, vol. 5, no. 4, pp. 503\u2013515, Dec. 2003.\n[9] T. Das and G. Singh et al., \u201cNetEffect: a network architecture for large-\nscale multi-user virtual world,\u201d in Proc. ACM VRST, 1997, pp. 157\u2013163.\n[10] D. DeWitt, S. Ghandeharizadeh, and D. Schneider, \u201cA performance\nevaluation of the gamma database machine,\u201d in Proc. ACM SIGMOD,\nChicago, IL, Jun. 1988.\n[11] Diablo II, Starcraft.. [Online] Available: http:\/\/www.blizzard.com\/\n[12] EverQuest.. [Online] Available: http:\/\/everquest.station.sony.com\/\n[13] J. Falby, M. Zyda, D. Pratt, and R. Mackey, \u201cNPSNET: hierarchical data\nstructures for real-time three-dimensional visual simulation,\u201d Comput.\n& Graph., vol. 17, no. 1, pp. 65\u201369, 1993.\n[14] T. Funkhouser, \u201cRING: a client-server system for multi-user virtual en-\nvironments,\u201d in Proc. Symp. Interactive 3D Graphics, Monterey, CA,\nApr. 1995.\n[15] C. Greenhalgh and S. Benford, \u201cMASSIVE: a distributed virtual reality\nsystem incorporating spatial trading,\u201d in Proc. Int. Conf. Distributed\nComputing Systems, 1995, pp. 27\u201334.\n[16] C. Greenhalgh, J. Purbrick, and D. Snowdon, \u201cInside MASSIVE-3:\nFlexible support for data consistency and world structuring,\u201d in Proc.\nInt. Conf. Collaborative Virtual Environments, 2000, pp. 119\u2013127.\n[17] H. Hoppe, \u201cProgressive meshes,\u201d in Proc. ACM SIGGRAPH, New Or-\nleans, LA, Aug. 1996.\n[18] M. Hori, T. Iseri, and K. Fujikawa et al., \u201cScalability issues of dynamic\nspace management for multiple-server networked virtual environ-\nments,\u201d in Proc. IEEE Pacific Rim Conf. Communications, Computers\nand Signal Processing, 2001, pp. 200\u2013203.\n[19] I. Katzela and M. Naghshineh, \u201cChannel assignment schemes for\ncellular mobile telecommunication systems: a comprehensive survey,\u201d\nIEEE Pers. Commun., vol. 3, no. 3, pp. 10\u201331, Jun. 1996.\n[20] R. Lea, Y. Honda, K. Matsuda, and S. Matsuda, \u201cCommunity place: ar-\nchitecture and performance,\u201d in Proc. VRML\u201997, Feb. 1997, pp. 41\u201350.\n[21] I. Pandzic, T. Capin, E. Lee, N. Thalmann, and D. Thalmann, \u201cA flex-\nible architecture for virtual humans in networked collaborative virtual\nenvironments,\u201d in Proc. Eurographics, 1997, pp. 177\u2013188.\n[22] Quake.. [Online] Available: http:\/\/www.idsoftware.com\/\n[23] G. Singh, L. Serra, W. Png, and H. Ng, \u201cBrickNet: a software toolkit for\nnetwork-based virtual worlds,\u201d Presence, vol. 3, no. 1, pp. 19\u201334, 1994.\n[24] G. Singh, L. Serra, W. Png, A. Wong, and H. Ng, \u201cBrickNet: sharing\nobject behaviors on the net,\u201d in Proc. IEEE VRAIS, 1995, pp. 19\u201327.\n[25] Ultima Online.. [Online] Available: http:\/\/www.uo.com\/\n[26] M. Willebeek-LeMair and A. Reeves, \u201cStrategies for dynamic load\nbalancing on highly parallel computers,\u201d IEEE Trans. Parallel Distrib.\nSyst., vol. 4, no. 9, pp. 979\u2013993, Sep. 1993.\nBeatrice Ng received the B.Sc. degree in computer\nstudies from the City University of Hong Kong in\n2000.\nShe is currently a research student in the De-\npartment of Computer Science, City University of\nHong Kong. Her research interests include computer\ngraphics and distributed virtual environments.\nNG et al.: MULTISERVER SUPPORT FOR LARGE-SCALE DISTRIBUTED VEs 1065\nRynson W. H. Lau received the (top) first-class\nhonors degree in computer systems engineering in\n1988 from the University of Kent, Canterbury, U.K.,\nand the Ph.D. degree in computer graphics in 1992\nfrom the University of Cambridge, Cambridge, U.K.\nHe is currently an Associate Professor at the City\nUniversity of Hong Kong. He previously taught at\nthe Hong Kong Polytechnic University. From 1992 to\n1993, he was with the University of York, York, U.K.,\nworking on a defense project on image processing.\nHis research interests include computer graphics, vir-\ntual reality, and multimedia systems.\nAntonio Si received the Ph.D. degree from the Uni-\nversity of Southern California.\nHe is currently with Oracle Corporation, Red-\nwood Shores, CA. Previously, he was an Assistant\nProfessor at the Hong Kong Polytechnic University\nand a Software Engineer at Sun Microsystems Inc.\nHis research interests are in mobile computing,\ninternet computing, and digital libraries.\nDr. Si has served on the program committees for\nseveral international conferences and as external\nreviewers for a number of international conferences\nand journals. He is a member of the ACM and the IEEE Computer Society.\nFrederick W. B. Li received the B.A. (Hons.) degree\nin computing studies and the M.Phil. degree in com-\nputer graphics from the Hong Kong Polytechnic Uni-\nversity in 1994 and 1998, respectively, and the Ph.D.\ndegree in computer graphics in 2001 from the City\nUniversity of Hong Kong.\nHe is currently an Assistant Professor at the\nHong Kong Polytechnic University. He was the\nResearch Manger of an R&D project funded by the\nHong Kong Government Innovation and Technology\nCommission, PCCW, and Sun Microsystems from\n2001 to 2003. His research interests include surface modeling, virtual reality,\ncomputer animation and computer networking.\n"}