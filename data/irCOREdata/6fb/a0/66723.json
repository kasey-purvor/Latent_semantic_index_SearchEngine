{"doi":"10.1016\/S1474-0346(02)00002-2","coreId":"66723","oai":"oai:dro.dur.ac.uk.OAI2:311","identifiers":["oai:dro.dur.ac.uk.OAI2:311","10.1016\/S1474-0346(02)00002-2"],"title":"The introduction of a design heuristics extraction method.","authors":["Matthews,  P. C.","Blessing,  L. T. M.","Wallace,  K. M."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2002-01","abstract":"This paper introduces a novel method for analyzing conceptual design data.  Given a database of previous designs, this method identifies relationships between design components within this database. Further, the method transforms these relationships into explicit design knowledge that can be used to generate a 'heuristic-based' model of the design domain for use at the conceptual stage.  This can be viewed as a knowledge extracting method for the conceptual design stage.  Such a method is particularly interesting, as the conceptual stage typically lacks explicit models to describe the trade-offs that must be made when designing.  The method uses either Principal Components Analysis or Self Organizing Maps to identify the relationships, and this paper describes all the elements required by the method to successfully extract and verify design knowledge from design databases. \\u","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66723.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/311\/1\/311.pdf","pdfHashValue":"3311ecdf51b1e97f2f4a0b35f6fa21d5a49e8c57","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:311<\/identifier><datestamp>\n      2011-05-31T09:23:50Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        The introduction of a design heuristics extraction method.<\/dc:title><dc:creator>\n        Matthews,  P. C.<\/dc:creator><dc:creator>\n        Blessing,  L. T. M.<\/dc:creator><dc:creator>\n        Wallace,  K. M.<\/dc:creator><dc:description>\n        This paper introduces a novel method for analyzing conceptual design data.  Given a database of previous designs, this method identifies relationships between design components within this database. Further, the method transforms these relationships into explicit design knowledge that can be used to generate a 'heuristic-based' model of the design domain for use at the conceptual stage.  This can be viewed as a knowledge extracting method for the conceptual design stage.  Such a method is particularly interesting, as the conceptual stage typically lacks explicit models to describe the trade-offs that must be made when designing.  The method uses either Principal Components Analysis or Self Organizing Maps to identify the relationships, and this paper describes all the elements required by the method to successfully extract and verify design knowledge from design databases. \\ud\n<\/dc:description><dc:subject>\n        Conceptual design<\/dc:subject><dc:subject>\n         Self-organizing maps<\/dc:subject><dc:subject>\n         Knowledge extraction<\/dc:subject><dc:subject>\n         Data-mining<\/dc:subject><dc:subject>\n         Design heuristics.<\/dc:subject><dc:publisher>\n        Elsevier<\/dc:publisher><dc:source>\n        Advanced engineering informatics, 2002, Vol.16(1), pp.3-19 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2002-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:311<\/dc:identifier><dc:identifier>\n        issn:1474-0346<\/dc:identifier><dc:identifier>\n        doi:10.1016\/S1474-0346(02)00002-2<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/311\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1016\/S1474-0346(02)00002-2<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/311\/1\/311.pdf<\/dc:identifier><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["1474-0346","issn:1474-0346"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2002,"topics":["Conceptual design","Self-organizing maps","Knowledge extraction","Data-mining","Design heuristics."],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n30 July 2008\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nMatthews, P. C. and Blessing, L. T. M. and Wallace, K. M. (2002) \u2019The introduction of a design heuristics\nextraction method.\u2019, Advanced engineering informatics., 16 (1). pp. 3-19.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1016\/S1474-0346(02)00002-2\nPublisher\u2019s copyright statement:\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\nThe Introduction of a Design Heuristics\nExtraction Method\nPC Matthews a,\u2217 LTM Blessing b KM Wallace a\naEngineering Design Centre, Cambridge University Engineering Department,\nTrumpington Street, Cambridge CB2 1PZ, United Kingdom\nbKonstruktiontechnik und Entwicklungsmethodik, TU-Berlin, Stra\u00dfe des 17 juni\n135, Berlin D-10623, Germany\nAbstract\nThis paper introduces a novel method for analyzing conceptual design data. Given\na database of previous designs, this method identifies relationships between design\ncomponents within this database. Further, the method transforms these relation-\nships into explicit design knowledge that can be used to generate a \u2018heuristic-based\u2019\nmodel of the design domain for use at the conceptual stage. This can be viewed\nas a knowledge extracting method for the conceptual design stage. Such a method\nis particularly interesting, as the conceptual stage typically lacks explicit models\nto describe the trade-offs that must be made when designing. The method uses\neither Principal Components Analysis or Self Organizing Maps to identify the re-\nlationships, and this paper describes all the elements required by the method to\nsuccessfully extract and verify design knowledge from design databases.\nKey words: Conceptual Design, Self Organizing Maps, Knowledge Extraction,\nData Mining, Design Heuristics.\n1 Introduction\nIn early stages of engineering design, designers have the greatest freedom to\nexplore the solution space for a given problem. At the same time, designers\nwish to know if the lines of thought they are pursuing are worthwhile, or\n\u2217 Corresponding author.\nEmail addresses: pm131@eng.cam.ac.uk (PC Matthews),\nblessing@kt10.kf.tu-berlin.de (LTM Blessing), kmw@eng.cam.ac.uk (KM\nWallace).\nPreprint submitted to Elsevier Science 7th December 2001\nif their efforts would be better spent following a different set of solutions.\nThis requires an understanding of the complex relationships within the given\ndesign domain. At the conceptual design stage, this understanding can be\nin the form of underlying trends, existing solutions, trade-offs, or awareness\nof possible technological alternatives. These are used to explore the solution\nspace for potentially feasible designs. Only the best solutions from this search\nshould be taken forward for more detailed consideration.\nThe problem is that this understanding of the design domain is frequently the\nresult of many years of experience within the domain. As a result, this knowl-\nedge tends not to be explicitly expressed, but rather resides tacitly in the form\nof a designer\u2019s experience. By assuming this knowledge is implicitly manifest\nwithin existing designs, this paper presents a novel method for extracting this\nknowledge and thereby making it explicit. However, this knowledge is likely to\nbe quite coarse and hence the method is referred to as the Heuristics Extrac-\ntion Method (HEM), thereby implying that this knowledge does not guarantee\ngood results, but improves the chances of doing so.\nThe HEM is based on recognizing patterns in existing solutions. This pattern\nrecognition triggers a first coarse model of the design space, providing a link\nfrom the design parameter space, the space describing the design concepts for\na particular family of designs, and the evaluation space, the space describing\npossible evaluations for the design family. A neural network could be trained\nto learn these relationships and thus provide such a mapping. The initial aim\nwas to use this mapping to support evaluation of new concepts. A new concept,\ndefined in parameter space, could then be mapped into the evaluation space,\nthus providing a possible evaluation. However, designers in industry expressed\nthat they could not trust such as system as they did not know the rules that\nhad been applied and hence that would justify the evaluation results.\nThe aim then became to extract relationships that are implicit within trained\nneural networks (i.e. in the design concepts) to explicitly provide designers\nwith domain knowledge that is useful for evaluation and generation of con-\ncepts. These relationships are expressed either in prose, e.g. \u2018As efficiency\ntends to 100%, there is a sudden large rise in EINOx, hence, low efficiency\nimplies low EINOx\u2019, or as an algebraic expression, relating a number of design\ncomponents. A component is defined in this paper as an element of the design\ndescription, e.g. the diameter of a hole, number of wheels, or the measurement\nof an evaluation criterion. 1 For example, a hollow cylinder design space can\nbe described using the length (\u2113) and radius (r) and these can be combined\nto represent the mass, m = C1\u2113r+C2r\n2. Such an algebraic relationship is also\nfrequently referred to as a feature.\n1 the use of the term component follows the terminology used in the neural network\ncommunity, as opposed to the mechanical engineering community\n2\nThe remainder of the paper is structured as follows: Section 2 reviews related\nwork that is used by the analysis method; Section 4 describes the properties\nof design domains that are particularly suited to this method; Sections 5\u2013\n9 describe the various elements of the heuristics extraction element of the\nmethod. Section 10 provides some empirical evidence on the success of the\nHEM. Finally, Sections 11 and 12 provide a discussion of the potential of the\nHEM and conclusions to the paper. The future work that is being undertaken\nis given in Section 13.\n2 Data Analysis Methods\nThe nature of the design tasks carried out during the early design stages are\nunstructured; varied in type of solution; and with few, coarse details. During\nthis part of the design process the designer should not be constrained in any\nmanner that unduly reduces the solution space, e.g. by limiting creativity.\nThis creates a challenge for producing a computational tool for this activity.\nA computational tool can be considered as the encoding of a method. For\nthis tool to be useful, it must also have some rules or algorithms to help the\nusers with their tasks. However, using such rules is in contradiction with the\nrequirement that there be few constraints on designers. As a result, the aim is\nto explore the possibilities of automatically generating a non-restrictive model\nfor the early design stages. As the relationships between elements of different\ndesign domains differ, these models will be different for each domain, so the\nmethod of generating them needs to be portable across domains. Finally, as\nthe model needs to be non-restrictive, it should be viewed more as a method\nof guiding designers in the direction of better solutions when help is needed.\nThe approach taken in this research for extracting such a coarse model of the\ndesign domain has its origins in rule extraction and rule induction methods.\nThese methods frequently are based on identifying class boundaries within a\nsample distribution and then expressing this boundary explicitly. There are a\nnumber of algorithms designed to partition a domain based on decision trees.\nThese decision trees form (symbolic) rules about how to classify elements in a\ndomain. An example of such a rule based induction method is ID3 (Rich and\nKnight, 1991; Luger, 1994). This aims to minimize the complexity of the tree\nit generates, thereby maximizing the information contained in each decision\nnode. However, when the case space is large, the decision trees tend to get\nquite complicated and difficult for human interpretation.\nA further example of using symbolic machine learning techniques for extract-\ning rules is given by Reich and Travitzky (1995). This example generated a\nset of rules describing the corrosion properties for a set of materials. However,\nthese were typically if. . . then type rules, which can only separate the domain\n3\nparallel to the component axes, that is, the domain is separated according to a\nsingle component at a time. More complex rules could produce general linear\nseparations of the domain. These can only be successful in the event that the\ndomain is known, or is likely to be, separable in such a manner. This approach\nwill be of be little use where the separation rules are non-linear.\nAnother approach attempts to generate rules describing the distribution based\non a neural network that has been trained on (and verified on) the distribu-\ntion. Tickle et al. (1998) have devised a taxonomy for classifying rule extrac-\ntion methods from artificial neural networks. This taxonomy has been applied\nto a cross section of neural network architectures. The aim of their work was\nnot only rule extraction, but also rule initialization (where specific rules are\ninserted into a neural network) and rule refinement (where rule networks are\n\u2018tweaked\u2019 so that particular rules are slightly modified). The complexity of\nrules extracted from neural networks tends to provide a challenge when they\nare to be used by a human. Efforts have been made to optimize the extracted\nrules for parsimony so that they constitute a \u2018better\u2019 explanation of the dis-\ntribution learnt by the neural network for a human reader (Corbett-Clark\nand Tarassenko, 1997; Corbett-Clark, 1998). Ultsch (1993) makes an initial\nattempt at extracting rules from Self Organizing Maps (SOM), by examining\nwhere the feature map needs to \u2018stretch\u2019 to cover the distribution. This suffers\nfrom only examining the overall structure of the SOM, rather than looking at\nindividual components. This prevents the identification of relationships that\noccur between specific pairs of components that are too small to affect the\noverall SOM structure.\nThe above methods typically require large sample distributions to provide\ngood results. For this research, two computational methods have been used\nto analyze design databases: Principal Components Analysis (PCA), a \u2018tradi-\ntional\u2019 statistical procedure (Diamantaras and Kung, 1996); and Self Organiz-\ning Maps (SOM), a \u2018neural\u2019 computational method inspired by the structure\nof the biological brain (Kohonen, 1997). Both of these methods provide sta-\nble results with considerably smaller databases than required for traditional\ndata-mining techniques. On overview of these will be given in Section 7.\n3 Representation Issues\nThe aim of this research is to learn about relationships in a given design do-\nmain. The assumption is that designs and their evaluations can be parametri-\ncally represented, that is, as a set of attribute-value pairs. Due to the nature\nof conceptual design, these attributes are unlikely to be determined with great\naccuracy and might have to represent broad decisions which will require fur-\nther detailing later in the design process. This also implies that the design and\n4\nevaluation parameters\/attributes are not necessarily of a quantitative nature.\n3.1 Design space representation\nBased on the above assumption, in the proposed HEM designs are represented\nin a vector format. This vector contains both the physical description and the\nevaluation parameters for a concept. The evaluations are either result from\nbuilding the object and taking measurements, or they are estimations based\non a model developed previously. There are 5 types of vector components:\n(1) continuous valued (e.g. length measurement);\n(2) ordinal (e.g. number of wheels);\n(3) boolean (i.e. True\/False);\n(4) fuzzy valued; and\n(5) 1-of-k (e.g. encoding a vehicle\u2019s driving wheels as one of \u2018front-wheel\ndrive\u2019, \u2018rear-wheel drive\u2019, or \u2018all-wheel drive\u2019)\nThis representation permits \u2018coarser\u2019 data to be captured, which is useful as\nthe design is only at the concept stage. Capturing designs using this format\nhas the advantage of being readily usable by various computational methods.\nThe vector components must be determined prior to using the HEM. In first\ninstance, a \u2018superset\u2019 of components should be captured. These are then pro-\ncessed to determine if any of the vector components are redundant. In this\nway, the set of most relevant components can be obtained (Matthews et al.,\n2000). For example, in some specific design domain, the color of an object\nmay have no effect on its cost (and the remainder of the design). Initially\ncolor would be included in the component super-set, but would then be found\nto be redundant and therefore removed from further processing.\nThe following section describes an alternative means of describing the design\nspace. This description is based on algebraic relationships between design and\nevaluation components, and hence these relationships are continuous. The\npurpose of such a representation is that these relationships explicitly express\nproperties of the design space.\n3.2 Feature Space\nAs seen in the previous sections, a simple way of describing multivariate ob-\njects parametrically is to express each measurable component by its value.\nHowever, for each distribution of objects (e.g. a particular design domain),\nthere will be combinations of these components that represent relationships\n5\nwithin the design. This representation has two advantages: firstly it reveals\nstructure within the design space, and secondly it can provide a means for\nreducing the dimensionality of the design space which in turn will result in\na faster and more accurate processing later. For example, consider the do-\nmain of similarly proportioned boxes: these can be described parametrically\nby (w, d, h,m, \u03c1); where w is width, d is depth, h is height, m is mass, and \u03c1\nis the planar density of the material used to construct the box. As all boxes in\nthis domain are similarly proportioned, the three dimension components are\nlinearly related:\nw\n\u03b1\n=\nd\n\u03b2\n=\nh\n\u03b3\n(1)\nThese three components are combined linearly to form a feature, which in this\ncase will represent the \u2018size\u2019 of the box:\nf1 = a1w + a2d+ a3h (2)\nwhere (a1, a2, a3) are chosen to \u2018maximize\u2019 the variation of f1 with the varia-\ntion of the box dimension components (which in this case will be the direction\ncosines of the line described by the box dimensions, given by (\u03b1, \u03b2, \u03b3) in Equa-\ntion 1). Similarly, the mass, m, of such a box is proportional to the surface\narea of the box:\nm = 2(wd+ dh+ hw)\u03c1 (3)\nCombining Equations 1, 2, and 3 a second (non-linear) feature (\u2018weight\u2019) is\nproduced, expressed in terms of \u03c1 and the first feature:\nf2 = a4\u03c1f\n2\n1 (4)\nwhere a4 is some constant of proportionality. These two features (f1, f2) form a\ntwo-dimensional description of the boxes that is equivalent to the 5-dimensional\nparametric description. In addition, the feature description represents the ob-\njects in what could be considered a more natural manner: namely a size com-\nponent and weight component. This illustration has demonstrated both linear\n(Equation 2) and non-linear (Equation 4) features.\n4 Relevant Design Domains\nThis research has developed methods for extracting knowledge to provide sup-\nport within the early stages of the design process.\n6\nThe HEM identifies relationships between design components. Therefore, this\nmethod is particularly useful for design domains where there are few explicitly\nknown relationships. These domains will not have explicit models that can\nbe used during the conceptual design stage or that are difficult to analyze\nanalytically (e.g. designs requiring extensive computational fluid dynamics for\nevaluations). However, there must be a number of prior designs, complete\nwith evaluation data, available to train the system with. It is assumed that\nthis data is costly, either in terms of material or computational resources,\nand therefore will not necessarily be abundant. Also, the method can handle\nmissing observations from within the data, which is advantageous as occasional\nmeasurement can be missing from designs.\nFor the purpose of validating the method, relatively mature domains were cho-\nsen and hence domain experts had a degree of understanding of these domains.\nThese domains relied on empirical studies to build up their explicit knowledge\nbases. The aim was to determine if the HEM could automatically extract\nrelevant relationships from a database of previous designs. It was therefore\nnecessary for the domain to be known.\nThree design domains were identified as case studies: one department based\nstudent design project and two aerospace projects. The department based case\nstudy was used to develop the representation used to encode the design space.\nThe first aerospace project was the analysis of the preliminary design of gas\nturbine combustors. This case study aimed to examine the use of the HEM\nin a domain where measurements were missing from some of the examples\nused to train the system and where the relationships were unclear. The second\naerospace case study examined preliminary wing design. This case study aimed\nto extract more complex relationships within the domain by incorporating\nnovel data preprocessing techniques into the HEM.\nThe case studies provide illustrations for properties of design domains that\ndetermine how successful this HEM will be. The next sections describe the\nheuristics extraction method, and also provide further details as to what design\ndomains are appropriate and inappropriate for the application of this method.\n5 HEM Overview\nThe Heuristics Extraction Method consists of the full process from the repre-\nsentation of the conceptual design space through to the generation and verifi-\ncation of the relationships extracted from the sample design distribution. This\nrepresents not only the required computational elements, but also interfacing\nwith domain experts.\nInsert\nFig-\nure 1\nhere\n7\nThe overall structure of this method consists of five steps:\n(1) Design concept space parameterization;\n(2) Data collection, and if necessary preprocessing;\n(3) Training;\n(4) Post-processing (heuristic generation); and\n(5) Verification of heuristics by domain experts.\nThe interaction with domain experts is an important part of this process.\nWithout this, the extraction method might identify relationships that are of\nlittle or no use to designers. Domain experts are very important during the\ninitial design space parameterization stage, as they are likely to be aware of\nwhich design parameters do play important roles in the design process. They\ncan also help direct the method\u2019s searching by highlighting parameters they\nfeel are particularly important.\nThe individual steps taken by the HEM are expanded below.\n6 Data Preparation\nThe HEM is a data driven method, and therefore relies on the data collection\nstage prior to processing. This requires an initial design space analysis stage,\nwhere the conceptual design space is described parametrically. Once the design\nspace has been parameterized, it must then be populated with examples. These\nare typically taken from collections of previous designs. Once such a database\nis available, a preprocessing algorithm is used to help the HEM identify more\ncomplex relationships, and a pruning algorithm to reduce the computational\ncomplexity incurred due to including irrelevant components. An overview of\nthe computation element of the HEM is given in Figure 2.\nInsert\nFig-\nure 2\nhere\n6.1 Design concept parameterization\nParameterizing design concepts appears initially to be contrary to the aims of\nconceptual design where the designer should not be restricted in any manner.\nHowever, when designing within a family of designs, there are a series of\ndecisions that are prescribed (Matthews, 1998; Matthews et al., 2000). The\noutcome of these decisions can be parameterized, even if the outcomes are\nnot numerical values (Ball et al., 1998). The aim of the parameterization is to\ncapture a \u2018super-set\u2019 of design decisions, which will be pruned down at a later\nstage.\nThe parameterization is determined by examining all previous designs. These\n8\ndesigns are stripped of details that can or need only be determined at a much\nmore mature state of the design (e.g. wiring or pipe routing details, stress or\nother finite element details) and parameters that are known to have little or no\neffect on the evaluation (e.g. the color of the body work). From the remaining\ndesign details, parameters are grouped into classes: those that are constant\nthroughout the family (e.g. external dimensions of a series of gas turbine com-\nbustors designed for a particular aero engine), and those that change within\nthe family (and hence distinguish different designs). The class of constant pa-\nrameters is then also discarded, as these parameters offer no information for\ndistinguishing designs. The remaining parameters form the \u2018conceptual de-\nscription\u2019 for the designs within this given family. This conceptual description\nis then augmented with the technical and any other relevant, e.g. aesthetic,\nevaluations for that design family.\n6.2 Data collection\nOnce a conceptual description, including evaluation criteria, has been gener-\nated, all previous examples need to be encoded according to it. This forms a\nset of vectors that will be used to train the system (the \u2018training distribution\u2019).\nIn the event of a design lacking the measurement of a particular parameter,\nthis parameter is left marked as unknown. It will then be up to the particular\ntraining algorithm to deal with this situation.\nThis is a relatively straightforward step. Provided that designs do not undergo\nlarge changes between final design concepts and final product, any changes\nthat do occur can be considered as a small noise component. Such noise typi-\ncally does not have a large overall effect on the final outcome.\n6.3 Data preprocessing\nThe data collection stage covered the steps required to transform a set of pre-\nvious designs into a numerical representation that can be processed by either\nthe PCA algorithm or the SOM algorithm. To make this a useful represen-\ntation, there must be intrinsic relationships that can be identified by either\nPCA or SOM. There are design representations where this is not the case.\nFor example, consider the design of a rod: a possible parameterization for this\nwould be the start and end points in space (a, b), and the mass of the rod\n(m = \u03c1|a \u2212 b|, where \u03c1 is the linear density). The set of previous designs\nconsists of a collection of random (a, b) values and appropriate m values. The\ndifficulty here lies in that the methods investigated (PCA and SOM) can only\nlearn relationships between pairs of components. In this case, there are no\n9\nrelationships between any pair of components and the training will not stabi-\nlize. However, if the training set used the parameterization (|a\u2212b|, m), a clear\ncorrelation emerges.\nThis example highlights the importance of generating a useful description of\nthe design family. However, there are cases when it might not be possible to\nform a useful description in the first instance. In some data representations\nthe relationships between components do not occur in pairs but rather in\nlarger groupings, e.g. triples. The need for preprocessing is identified after an\ninitial (exploratory) training iteration. Three ways of identifying the need for\npreprocessing are:\n(1) components that are expected to play an important role (e.g. by a domain\nexpert) are not being included in the final set of heuristics;\n(2) there are far fewer heuristics than expected;\n(3) training on different subsets of the original data yields little or no stability\nin the heuristics generated, or in intermediate steps.\nOf these criteria, the first two require a domain expert to review the initial set\nof heuristics generated. The third criterion tests how stable the algorithm\u2019s\nresults are, if different portions of the training data are used (e.g. \u2018k-fold\ncross verification\u2019). This can be seen by examining the difference in the linear\nfeatures generated by PCA or the component maps generated by the SOM. It\neffectively identifies \u2018noisy\u2019 components within the data. Assuming that if a\nset of components is believed to be useful, but appears only as random noise\nto either PCA or SOM, then these components might need to be combined in\nsome manner to generate useful information about the design (although this\ninformation might not be directly useful to a designer) such that the PCA and\nSOM can stabilize.\nIn the first instance, components should be combined algebraically using what-\never prior domain knowledge there is (for example, a set of related measure-\nments might be able to be combined usefully by adding or subtracting them,\nas illustrated earlier). If this does not produce satisfactory results, compo-\nnents can be combined with a \u2018brute-force\u2019 method, that is testing all possible\ncombinations. The basic algebraic operations (addition, subtraction, multipli-\ncation, and division) are applied to each pair of components resulting in 4\nmore components for each original component-pair (Matthews, 2001). Finally,\nin the event that this option is exhausted, it is possible that the design space is\ntoo chaotic within its defined boundary, and the initial assumption about the\ncontinuity of the mapping between design components is insufficiently true\nfor this method to generate heuristics successfully. In this event, either im-\nportant design components are being omitted in the representation or there\nare no continuous relationships between the components. In both cases, this\nheuristics extraction method cannot be applied.\n10\n7 Core Technology\nThe basic analysis algorithm, or \u2018core technology\u2019, is some form of numerical\ncorrelation. This core technology takes as input the (possibly) preprocessed\ndesign data, and performs some analysis or re-representation of this data.\nFor comparison purposes, two data analysis algorithms were used: Principal\nComponents Analysis (PCA) and Self Organizing Maps (SOM).\nThis section will first state the necessary properties a design domain must\nhave for this to be successfully analyzed by this method. Following this, a brief\noverview will given to the backgrounds of the two algorithms used. Subsequent\nsections will then describe how the HEM implements these technologies as part\nof the overall analysis method.\n7.1 Space assumptions\nThere are some basic assumptions that need to be made about the properties\nof the design space. As described in Section 6.3, designs are represented as a\nreal-valued vector. If there are a total of N components to this design vector,\nthe design space, X, can be embedded into RN . Not all elements of RN will be\nlegal designs (for example, a negative length component could be represented\nin RN , but would not be a member ofX). This can be algebraically represented\nas X \u2282 RN . It shall be assumed that X forms a piecewise connected set, that\nis for any design x \u2208 X, there exists a small region centered around x that\nlies fully within X. This assumption means that any design can be perturbed\n(modified) by a small amount, and still be a legal design (see Figure 3). Note\nthat if the design space is the union of disjointed sets, i.e. X = \u222aXi, each Xi\nrepresents a cluster of similar designs.\nInsert\nFig-\nure 3\nhere\nFor practical matters, it will be necessary that any design sample distribution,\ngenerated from the database of previous designs, has sufficiently large \u2018chunks\u2019\nin each cluster. This is because a region needs to be sufficiently large to be\nidentified, and ideally, there needs to be several examples or elements (designs)\nfrom this region to be able to learn about designs in this area of the domain.\nA key assumption needed is that the mapping between design parameter sub-\nspace and evaluation sub-space is at least piecewise continuous. That is, for any\npoint within the design parameter sub-space, the mapping to the evaluation\nsub-space will be continuous for at least a small region around the point in\nparameter space (so a small change in the design parameters will result in no\nmore than a small change in the evaluation sub-space).\n11\n7.2 PCA overview\nPrincipal Components Analysis (PCA) is a factor analysis type of statistical\nmethod for analysing multivariate data (Diamantaras and Kung, 1996). PCA\nidentifies linear correlations within the data, and the variation of these cor-\nrelations. The first component is the linear correlation that can describe the\ngreatest variance within the dataset. These components are then ordered in\ndescending magnitude of variation in a manner such that the new co-ordinate\nsystem remains orthogonal (see Figure 4). The new coordinate system, (f1, f2)\nin the figure, represents a set of features in decreasing order of importance (as\ndetermined by the component\u2019s variance within the dataset) as each new co-\nordinate is a linear combination of the original variables, (x1, x2) in the figure.\nHence the PCA based algorithm is a feature extraction method, albeit a sim-\nple one. This is usually used to re-structure the data representation in such a\nmanner that the \u2018less\u2019 significant components can be identified and discarded,\nthereby representing the data in a more compact form. For example, the data\nin Figure 4 could be represented in one dimension by approximating it to a\nline (discarding the deviation component from this line, which could possibly\nbe due to noise). With design data, this also provides a method of identifying\nwhich of the design parameters have a lesser effect on the design during the\nconceptual stage, and hence need not be precisely determined early on in the\ndesign process.\nInsert\nFig-\nure 4\nhere\nFor higher dimension spaces, let X represent the matrix of observations 2 (i.e.\neach column xi represents observations for a particular parameter and the rows\nrepresent individual observations). First, the covariance matrix, R = (rij), is\nconstructed:\nrij = E[(xi \u2212 x\u00afi)(xj \u2212 x\u00afj)] (5)\nwhere E[ \u00b7 ] is the expectation operator and x\u00afi is the mean of the i\nth parameter.\nNote that by the construction of R, it is a real symmetric matrix. A property\nof real symmetric matrices is that they have real eigenvalues \u03bbi with associated\neigenvectors ei where e\nT\ni ej = \u03b4ij , i.e. the eigenvectors for distinct eigenvalues\nare orthogonal. These eigenvectors represent the principal components of the\ndata. Further, the size of eigenvalues gives an indication of how important the\ncomponent is. It is therefore sensible to order the eigenvalues in descending\norder (\u03bb1 \u2265 \u03bb2 \u2265 . . . \u2265 \u03bbN ).\nThe variable space can then be orthogonally transformed into feature space\nusing the matrix spanned by the (normalized) eigenvectors:\n2 an observation is the numerical value for a parameter for a given design instance\n12\nQ=(e1 e2 \u00b7 \u00b7 \u00b7 eN)\nT (6)\nf =Qx (7)\nwhere x represents a vector in variable space, f represents a vector in feature\nspace, and Q is the transformation matrix. This transformation matrix is a\nlinear re-representation of the design domain. Section 8.1 describes how this\nmatrix is analyzed to extract the design heuristics.\n7.3 SOM overview\nIn its most simple interpretation, the Self Organizing Maps are an \u2018elastic\nnetwork\u2019 of points fitted to some given distribution (Kohonen, 1997). This\nnetwork has the interesting property that the network nodes (index vectors)\nmaintain the global ordering with respect to the original distribution space,\nthat is, there is no \u2018knotting\u2019 of the network. A knotted network would be\ntroublesome as traveling in one direction along the network would not guar-\nantee that the same point in design space would not be revisited. The topology\npreserving nature of this network results in the map produced forming a non-\nparametric model for the design space.\nThe SOM is an unsupervised learning algorithm: that is, it must learn the\nstructure of the space without any prompts. The SOM is \u2018trained\u2019 by present-\ning it the sample data several times, with the SOM re-adjusting itself with\neach iteration. The SOM is made up of a set of M nodes each containing a\ncodebook vector wi that represents a point in the design space. Initially, these\nare set randomly. The data is then presented to the network in a random or-\nder. For each data sample (x), a winning node c is the node whose codebook\nvector has the greatest similarity to the data vector, in this case determined\nby the Euclidean distance:\nd(wc,x) \u2264 d(wi,x) \u2200i \u2208 {1, . . . ,M} (8)\nThe codebook vector of this winning node is then adjusted so that its distance\nwith this data vector would be decreased (where \u03b1 is the \u2018learning rate\u2019 with\n0 \u2264 \u03b1 \u2264 1):\nw\u2032c = wc \u2212 \u03b1(wc \u2212 x) (9)\nThe neighboring nodes of c are also updated in a similar manner, but with\na smaller value of \u03b1. This ensures that the neighborhood remains topologi-\ncally equivalent to the neighborhood of x in variable space. This procedure\nis repeated for all data points, with \u03b1 decreasing, until the codebook vectors\nstabilize. Once this is achieved, the SOM is said to be ordered (or trained).\n13\nOnce the network has been ordered, the nodes can be considered to be \u2018clas-\nsifiers\u2019 of their regions within the design space. When the network is given a\ndesign, the node that represents the area of domain that is most similar to\nthis given design is activated. Due to the topological ordering of the network,\nmoving to neighboring nodes represents moving to nearby regions within the\ndesign space. The network can be considered to represent features locally, that\nis, the network can be used to approximate the relationship between param-\neters for small regions represented by neighboring lattice points. In a similar\nmanner to identifying and interpreting the linear features discovered by PCA,\nthe SOM features will also need to be identified and interpreted by a domain\nexpert.\n8 Post-processing\nThe post-processing algorithms that identify potential relationships from the\ntraining data represent a novel method for knowledge elicitation and are a\nsignificant contribution arising from this research. Two significantly differ-\nent post-processing algorithms are used to generate heuristics. This is due to\nthe difference in the output of the PCA and SOM algorithms. The overall\npost-processing method involves the following steps: (1) analyze the output\nfor relationships between components; (2) simplify these relationships by de-\nscribing them in a concise manner; (3) check these relationships with the full\nsample distribution. The third step is required in the event the second step\noversimplified a relationship to the extent that it is incorrect.\nThe next sections describe the methods for extracting heuristics from the PCA\nand SOM algorithms, and how these heuristics are checked against the sample\ndistribution.\n8.1 Principal Components Analysis\nAs described in Section 7.2, PCA is a statistical technique that identifies the\nlinear dependence structure of a multivariate stochastic observation (Diaman-\ntaras and Kung, 1996). This dependence structure can be analyzed to extract\na set of implicit linear equations that describe the relationships between design\nand evaluation parameters. These linear features are described as follows:\nfi =\nN\u2211\nj=1\neijxj (10)\n14\nThese features represent an orthogonal transformation of the axis (as described\nby Equation 7), and are too complex for extracting useful heuristics: so any\nterms with |eij| < \u0398, are removed for some threshold \u0398. Hence, the simplified\nfeatures are given by:\nf \u2032i =\n\u2211\n|eij |\u2265\u0398\neijxj (11)\nwhere \u0398 is chosen such that most features can be described using about 10 de-\nsign parameters. Most of these simplified features can be given some physical\ninterpretation. For example, consider the design of a small autonomous vehi-\ncle (see Section 10.1). Vehicles are characterized by five components: distance\nbetween sensor array and drive axle (d); drive wheel diameter (w); number\nof microswitches (n); construction quality, or \u2018duty\u2019 (D); and line following\nability (L). Of these five components, the first three are design parameters\n(components that the designer can set) and the last two are evaluations (de-\ntermined as a result of the design parameters). Using data from the case study,\nthe first linear feature is given by:\nf1 = 0.9996d+ 0.0016w \u2212 0.0018n+ 0.0139D + 0.0225L\nSimplifying this by setting \u0398 = 0.01 results in:\nf \u20321 = 0.9996d+ 0.0139D + 0.0225L\nThis feature indicates that the largest variation occurs with d, the distance\nbetween the IR sensor array and the drive axle; and that increasing d increases\nthe duty score (D) and improves line following (L). The remaining two compo-\nnents (wheel size and number of microswitches) appear to have little variation\nwith respect to variation in d, and hence are removed.\nThese features need to be examined individually to extract heuristics. Recall\nthat the factors, eij , are the eigenvector components of the covariance matrix\ngenerated from the training distribution. Hence, in the simplified features, f \u2032i ,\nthese will represent components that are either strongly correlated or anti-\ncorrelated (depending on the sign). This linear scaling permits us to predict\nhow changing one component in the feature is likely to change the others and\nby what amount.\nThe features can be grouped into three categories: features containing only de-\nsign parameters; features containing only evaluation parameters; and features\ncontaining both design and evaluation parameters. The features containing\nonly design parameters indicate how various design parameters are related,\nand potentially provide an indication of how novel combinations could be made\n15\nby breaking such relationships. The features containing only evaluations indi-\ncates how various evaluations are related (and therefore suggest trade-offs that\nmight need to be made). Finally, the features containing both design compo-\nnents and evaluation components indicate how those designs and evaluations\nare directly related.\nThe limitation of this method is that PCA generates a set of orthogonal linear\nfeatures. However, the actual design features are not necessarily either orthog-\nonal or linear. Therefore it is desirable to use a non-linear method to produce\nfurther results.\n8.2 Self Organizing Maps\nSOMs provide a simpler (low dimension) representation of a distribution in\na high dimensional space in a manner that ensures that the topology of the\ndistribution is maintained. From this simplified representation, the aim is to\nextract knowledge about the distribution (i.e. heuristics about the particu-\nlar design domain). The local non-linear correlation algorithm described in\nSection 7.3 provides the basis for the knowledge extraction method.\nThe SOM generates a two dimensional map of the distribution using a regular\nlattice of nodes. These nodes are representative points within the distribution.\nThe advantage of using a two dimensional map is that the distribution can be\neasily visualized. This is accomplished by plotting the values of each compo-\nnent of the distribution on its own 2D map (the value being represented by\nshading). Components are correlated when their respective maps visually have\n\u2018similar appearances\u2019 (Figure 5(a)). Further, components can be locally cor-\nrelated if they both have an area in which they appear similar, when globally\nthey are dissimilar (Figure 5(b)).\nAnti-correlations (Figure 5(c)) are identified using an edge detection tech-\nnique, based on the gradients of the component maps. Due to the small size\nof the component maps, this is a coarse approximation to the true gradient\n(or the edges are difficult to identify, see Figure 5(d)), and so results from\nthis method tend to be weaker. Both correlations and anti-correlations are\nidentified here.\nInsert\nFig-\nure 5\nhere\nBecause of the large number of pairwise comparisons that have to be made,\nclustering methods are used to identify smaller groups of component maps\nthat are worth more detailed comparison. Clustering is performed based on\nthe \u2018visual\u2019 similarity between maps, measured using a Tanimoto metric and\nstored in a matrix (Matthews, 2001). The (i, j)th element of this matrix repre-\nsents the similarity between component maps i and j. Originally, this matrix\nwas rearranged so that similar components were placed together in blocks,\n16\nwhich is a similar clustering procedure to that of Ling (1973). Figure 6 is a\ngraphical representation of the re-arranged similarity matrix. The two axes\nrepresent the component indices, with the shading of each cell representing\nthe value of the corresponding matrix entry where black is no similarity rang-\ning through to white which represents complete similarity. Elements that are\nbelow a given threshold are reset to zero. The blocks are similar to the features\ngenerated by the PCA method, except that the similarity measure is quali-\ntative rather than quantitative as is the case with the factors generated with\nPCA. However, the main purpose is to generate small groups of components\nthat demonstrate similar characteristics. It is worth noting that this method\nonly discovers positive correlations, as it identifies similarity in matching val-\nues. As mentioned earlier, anti-correlations can be detected using the same\nmethod on the second order method (edge detection) matrices.\nInsert\nFig-\nure 6\nhere\nA shortcoming of using the re-arranged similarity matrix is that it is difficult\nto generate accurate groups of similar components. This is due to not being\nable to generate mutually exclusive sets of components. From Figure 6, it\ncan be seen that the group structure is more complex than a set of mutually\nexclusive sets, namely that there are overlapping regions.\nThere are various clustering techniques available, most of these generating hi-\nerarchical partitions (Kaufman and Rousseeuw, 1990; Matthews et al., 2001).\nOne non-hierarchical method was mentioned (Jardine and Sibson, 1968), and\nthis was implemented to generate \u2018clumps\u2019 of components. These clumps rep-\nresent a more natural grouping of components as each component can be in a\nnumber of clumps, which can represent the (partial) similarity a component\nhas to each of these clumps. This is illustrated in Figure 7. On the left of this\nfigure, is an example of the shape a typical re-ordered similarity matrix will\nhave. Next to this, is a possible hierarchical representation of how the com-\nponents could be structured. It should be noted that for the elements from\nthe overlapping region (A \u2229 B), a decision must be made as to which group\nthey shall belong to. A non-hierarchical clustering method does not have this\nproblem, as it is possible for groups to overlap when necessary (as shown with\nthe rightmost tree structure). However, it should be noted that the algorithm\nimplemented is computationally inefficient (memory-wise), and there will be\ndifficulties with analyzing high-dimensional distributions.\nInsert\nFig-\nure 7\nhere\n8.3 Component pruning\nAs discussed in Section 6.1, when initially parameterizing the design space,\nthe experimenter is encouraged to include more parameters than are believed\nto be necessary. This is good practice, as it is more difficult to identify miss-\ning parameters than to identify redundant ones. Also, the preprocessing al-\n17\ngorithm introduces a large number of new combined components. However,\nover-parameterization of the design space can present a problem when train-\ning neural networks if coupled with relatively sparsely populated data training\npoints, e.g. when there are only a couple of data points per dimension. It is\ntherefore desirable to reduce the dimensionality of the input data. The re-\ndundancy identification (pruning) method can be summarized as identifying\ncomponents with little effect on the remainder of the design space.\nA simple method to determine the number of components to be pruned is\nby using the eigenvalues from PCA. As the eigenvalues decrease, the impor-\ntance of the associated feature (and hence dimension in the feature space)\ndecreases. Therefore, a threshold is set and the linear features with associated\neigenvalues below this threshold are discarded. This determines the intrinsic\ndimensionality of the design space, ni, and this number of original components\nis selected.\nThe SOM is used to identify component pairs that behave similarly, based on\nthe component maps. Component maps that have little similarity to any other\ncan be assumed to represent redundant components, as these are not involved\nin any relationships and therefore do not reveal any structure of the domain\n(Matthews, 2001). This redundancy becomes clearer when the components are\ngrouped using a non-hierarchical clustering method, described in Section 8.2.\nThese redundant components will only occur in singleton groups.\nIt must be noted when pruning components that these components might\nnot be totally redundant: they might be components that need to be com-\nbined with others in a preprocessing phase to identify meaningful relationships\nwithin the design space. Therefore, it is recommended that before any compo-\nnents are discarded, a domain expert should confirm that these components\nare not likely to play an important role.\n9 Generating and Evaluating Heuristics\nThe previous two sections involved the numerical processing of design data and\nidentifying sets of components that have some form of relationship between\nthem. This section describes how this information is interpreted to generate\nlinguistic (or algebraic, where appropriate) heuristics that can be readily used\nby designers.\n18\n9.1 Principal Components Analysis\nThe final result of the PCA algorithm was a set of simplified linear rela-\ntionships between components (recall Equation 11). It was argued that these\nfeatures represent design characteristics. Hence, these features can be trans-\nformed into linguistic heuristics by describing the trends or correlations en-\ncoded by the linear relationship. Attention must be paid to the orthogonality\nof the linear features, namely, if two features appear to contradict each other\nthe more important (according to the associated eigenvalue) should be used.\n9.2 Self Organizing Maps\nAnalyzing the component similarity matrix results in a set of clumps of com-\nponents that share similar behaviors. Each of these clumps is taken in turn.\nFor each clump, all the member component maps are plotted side-by-side (see\nFigure 5). These plots are visually inspected to determine the nature of the\nsimilar behavior, namely, is the behavior shared globally or does it only occur\nlocally? If it occurs locally, what are the conditions for the shared behavior?\nThese relationships are expressed in a simple manner that reflects the com-\nponent maps (e.g. Figure 5(b) would be High EICO implies High EINOx).\nUnder certain conditions, the Tanimoto metric can falsely identify two maps\nas similar. This occurs when one map is mainly covered by average values and\nthe other map has a significant number of its nodes at extreme values, thereby\ncausing the averages to be similar. An example of such a pair of maps is given\nin Figure 8. This highlights the need to visually compare the component maps\nto filter out such examples.\nInsert\nFig-\nure 8\nhere\n9.3 Checking against training data\nThe interpretation of the PCA and SOM outputs involves a \u2018smoothing\u2019 or\nsimplification process. This process can result in the over-simplification of\nsome relationships to the degree that they are no longer correct. Hence, it\nis necessary to check the relationships generated against the original training\ndata.\nThe checking of the heuristics is achieved by plotting the components of the\nrelationship against each other. These plots fall into three categories:\n(1) Continuous valued against continuous valued: this can be verified by a\nscatter-plot (see Figures 9(a) and 9(b));\n19\n(2) Boolean against continuous valued: this can be verified using two fre-\nquency plots (see Figures 9(c) and 9(d));\n(3) Boolean against boolean: this can be verified using two frequency plots\n(see Figures 9(e) and 9(f)).\nInsert\nFig-\nure 9\nhere\nThe first of these verification methods is a traditional scatter-plot. In the\nevent that the points appear to lie on a curve, an exact relationship can\noften be found by investigating the curve that best fits through the points\n(see Figure 9(a)). If the scattering does not converge to any form of curve,\nit can be inferred that there is no direct relationship between the two given\ncomponents (see Figure 9(b)).\nBoolean components can arise from two types of component: either an origi-\nnally boolean component (e.g. a member of a 1-of-k choice) or from a continu-\nous component that is split into cases, e.g. component values below and above\na certain threshold. A Boolean-continuous relationship will take the form of:\nif component x lies in a given range, component y is more likely to take the\nvalue y0 otherwise y is more likely to take the value y1. Note that this is a\nprobabilistic relationship rather than a definite one. This is a characteristic of\ndesign heuristics: they do not hold in all cases, just a significant majority. The\ninferred relationship can be verified by plotting (1) the frequency of elements\nbeing classified as True according to the continuous component, and (2) the\nfrequency of elements being classified as False according to the continuous\ncomponent. If these two plots have most of their \u2018mass\u2019 at opposite ends of\nthe continuous component\u2019s range, it can be inferred that this heuristic is cor-\nrect (see Figure 9(c)); if the mass of both plots lie in the same place, it can\nbe inferred that the heuristic is incorrect (see Figure 9(d)).\nThe Boolean-Boolean relationship can be considered as a restricted version\nof the Boolean-continuous relationship, namely the range has now been re-\nplaced with a True\/False outcome space. In a similar manner to the Boolean-\ncontinuous, such relationships can be verified by being able to infer (with rel-\natively high confidence) class membership (i.e. the one component) based on\nthe condition of the second component (see Figures 9(e) and 9(f), for positive\nand negative examples respectively).\nHeuristics that fail this self-verification stage are removed. The remaining\nheuristics are now known to be accurate as far as the supplied training distri-\nbution is concerned. The process of checking pairs of components might appear\nto make the use of PCA and SOM redundant, however, this checking process\nis considerably more expensive than generating the groups. The clustering\ngreatly reduces the number of component pairs that will need checking.\nIt is not known how useful or novel these heuristics are, and therefore it is\nnecessary to have a second verification round with a group of domain experts.\n20\n9.4 Expert verification of the HEM results\nThe final verification is done by a group of domain experts. This verification\nof the heuristics serves three purposes: (1) how accurate are the heuristics\naccording to an expert\u2019s opinion; (2) how novel are the heuristics; and (3) how\nimportant are the heuristics. The first purpose aims to widen the context\nbeyond the training distribution by using the expert\u2019s tacit domain knowledge.\nThis is also important to prove the heuristic generating method is capable of\nidentifying relationships accurately, and serves to validate the method. The\nsecond purpose aims to measure how \u2018interesting\u2019 the heuristics are. This is\nuseful for targeting the heuristics to a particular expertise level, and ultimately\ncan be used to identify novel relationships that have been discovered from the\ndistribution. The third purpose is to gauge how important the heuristics are\nfor the domain.\nThe heuristics are evaluated using a paper method: each heuristic is reported\nwith an evaluation scoring table that is to be completed by the expert. The\nexperts are also encouraged to comment on the heuristics. Once this has been\ncompleted, the heuristics that have scored well are kept. The heuristics that\nhave scored poorly according to the domain experts require further investi-\ngation, as these relationships were verified as accurate with respect to the\ntraining distribution. After further consideration, they can either be rejected\nor included, in a modified form, in the final set.\nFinally, this verification process provides an accuracy measure subjective to\nthe experts\u2019 opinions. Therefore, it is possible that even where an expert be-\nlieves there is no relationship, such a relationship does indeed exist.\nOther methods exist for the purpose of evaluating machine learning, for exam-\nple Arciszewski (1997) and Reich and Barai (1999). However, for the purposes\nof this research, such methods are not necessary as it is sufficient to demon-\nstrate that valid heuristics have been identified and extracted from the training\ndistribution.\n10 Application of HEM to Case Studies\nThe scope of this research was to generate accurate relationships from a given\ndesign domain. Hence, the method was able to be validated through the verifi-\ncation of the accuracy of the outputs of the method applied to three case stud-\nies. The novelty and importance of the heuristics were also measured to gain\nan indication of how applicable the HEM would be for industrial uses. These\ncase studies are reviewed, highlighting the conclusions that can be drawn from\n21\neach.\n10.1 Integrated Design Project\nThe Integrated Design Project (IDP) involves teams of six students (2 will\nbecome mechanical specialists, 2 electronic, and 2 software). These teams are\nrequired to design, build and test a semi-autonomous vehicle that is able to\nnavigate a course (marked out by a white line) and perform various pallet\nhandling tasks, usually of the format fetch-classify-place. Each vehicle is con-\nstructed from a fixed kit of parts (with no restrictions on bulk materials such\nas steel or wood) and must be completed in four weeks.\nThe IDP case study was based within the Department which permitted easy\naccess to prior design examples, their evaluations, and domain experts. This\ncase study represented a product that involved the interaction of different\nmodules (electrical, software, and mechanical). Only the mechanical module\nwas represented, as it was not possible to acquire previous design data, and\nhence model, the electrical and software elements. While these are important\nfactors, the benefits of having both data and domain experts locally available\nmade this a valuable case study.\nThe PCA method produced a set of orthogonal linear features which proved to\nbe difficult to interpret into useful heuristics. The results generated from the\nSOM method were considerably simpler to transform into heuristics. Further,\nit was shown that 17 out of the 22 heuristics identified using the PCA method\nwere also identified using the SOM.\nA total of 42 heuristics was generated. The heuristic accuracy verification re-\nsulted in a total of 12 (28.6%) scoring as highly accurate, 13 (31.0%) scoring\nas moderately accurate, and 15 (35.7%) scoring as not very accurate (leav-\ning 2 (4.8%) undecided). The novelty scores for these heuristics were roughly\nopposite to the accuracy scores and the importance scores roughly followed\nthe accuracy scores. One of the shortcomings of this case study was the lack\nof inclusion of the electrical and software modules. These interact with the\nmechanical module, and can have either a positive or negative influence on\nthe design\u2019s final performance.\n10.2 Combustor\nThe combustor provides a core function of an aircraft gas turbine engine. Each\ncombustor design represents a very costly development, and hence there is the\ndesire to maximize the amount of information extracted from combustor rig\n22\ntests. Due to this expense, there were only a few examples to analyze. Further,\nsome of the examples had measurements missing. It is also known that the\ncombustor domain is non-linear, and hence that linear methods will not be\nvery successful. These properties hindered the use of the PCA method, as it\nrequires all measurements to be present. The SOM on the other hand, can be\ntrained with incomplete datasets.\nNevertheless, both PCA and SOM were used to process the data. To use the\nPCA, the data elements with missing measurements were removed leaving 79\ndata points for 37 dimensions: i.e. about 2 data points per dimension. The lin-\near features were compared to the heuristics generated from the SOM. A total\nof 24 (51.1%) of the heuristics generated using the SOM could not have be\nidentified in the PCA results. The remaining heuristics that were potentially\nidentifiable from the PCA processing would require greater effort to be identi-\nfied. This is because the SOM post-processing reports potential relationships\nin a manner that is considerably easier to check. A rapid initial checking is\npossible by comparing the relevant SOM component maps side-by-side prior\nto further testing. There is no parallel method for rapidly checking small sets\nof components for potential relationships with the PCA. No heuristics were\nidentified from the PCA processing that had not been identified using the\nSOM.\nA total of 47 heuristics was generated. The heuristic verification resulted in 27\n(57.4%) scoring as highly accurate, 6 (12.8%) scoring as moderately accurate,\nand 11 (23.4%) scoring as not very accurate (leaving 3 (6.4%) undecided).\nAlthough a large portion of these heuristics did not score highly on novelty,\nthis case study demonstrated the potential of using the heuristics extraction\nmethod as a means of generating a coarse design model for a given domain.\n10.3 Wing\nThe wing case study represented the second industrial design project. This\ndiffered from the combustor design domain in two ways. Firstly, the wing\ndesigns used were a result of a genetic algorithm search of the design space\nrather than actual wing designs. Secondly, the design space was represented\nby a small number (13) of components in comparison to both the IDP (84)\nand the combustor (37). A direct application of both PCA and SOM on this\ndataset revealed no potential relationships. Hence, it was necessary to perform\nthe additional preprocessing step in the method.\nA total of 25 heuristics was generated. These were represented as trade-offs\nthat would be expected in the design space and were phrased as algebraic\nproportionalities, i.e. omitting all constants and coefficients, for example one\n23\nof the heuristics takes the form of A\u2212B \u221d C\u00d7D where A,B,C,D represent\nwing design components. However, the domain experts were unable to verify\nthe heuristics for a two reasons. Firstly, the representation of the relationships\nwas difficult to follow and most of the relationships were not considered to be\nrelevant. This can be addressed by improving the formatting of the heuristics\nto be less algebraic and to flag design components that the heuristics extraction\nmethod should focus on. The second difficulty was that the wing design space\nwas not fully encoded. This resulted in the identification of overly general\nrelationships that were thought to be of little use to designers. More specific\nrelationships that take design context into account are needed.\nA self verification method was developed to provide some measure of the\naccuracy of the heuristics by measuring the \u2018goodness\u2019 of each heuristic. This\nwas achieved by measuring the variance of the data about the relation and the\noverlap of these variances at the extreme ranges of the relationship. The aim\nwas to provide some means of scoring the heuristics that would be comparable\nto the other two case studies. This scoring was possible as all the heuristics\nwere algebraic type relationships. However, no comment or measurement could\nbe provided on either the novelty or importance of these heuristics as these can\nonly be supplied by domain experts. This self-verification resulted in 8 (32%)\nheuristics being scored as highly accurate, 7 (28%) scoring as moderately\naccurate, and 8 (32%) scoring as not very accurate (leaving 2 (8%) undecided).\n11 Discussion\nThis paper has described the research and development of a novel design data\nanalysis method aimed at the generation of heuristics. This heuristics extrac-\ntion method used PCA and SOM at its core to analyze design data. These\ntechniques required the assumption that there was some form of piecewise con-\ntinuous mapping between the design components (as discussed in Section 7.1).\nThis was wrapped by a methodology for collecting and preparing the design\ndata for the two numerical techniques. A method was developed to interpret\nthe results of these numerical techniques into relevant design relationships.\nThese relationships were then re-represented in the form of \u2018design heuristics\u2019,\nwith the aim of verifying the relationships using domain experts. There were\nalso developments for interfacing between the major elements of the HEM.\nThese were the preprocessing and filtering (\u2018component pruning\u2019) algorithms\nprior to the core processing, and the clustering method used to rapidly identify\ngroups of components sharing similar behavior (recall the overall structure as\nshown in Figure 2).\nIn a more abstract sense, the HEM generates a set of \u2018interesting questions\u2019\nto ask a domain expert. The full process of analyzing the design data, gener-\n24\nating the relationships that form the basis of the questions, and using the ex-\nperts\u2019 answers forms the heuristic extraction method. The main challenge this\nmethod faces is ensuring that the design space is sufficiently well described. It\nwas demonstrated that over-described design spaces could be reduced to the\nrelevant set of design components, but design spaces where important com-\nponents were missing were unlikely to provide useful results. The case studies\ninvestigated in this paper illustrated these issues.\n11.1 Exploitation\nOnce the heuristics have been extracted, there are several uses for them. The\nmost immediate is to inform designers of implicit relationships that exist be-\ntween components in the design domain. Although there has been expert in-\ntervention, this method is useful for four reasons: (1) the expert is given a set\nof relationships (without explanation) rather than having to generate these;\n(2) the expert might not be consciously aware of some of the relationships\nand would not expressly write these down if asked; (3) some of the relation-\nships might be totally novel, and therefore unknown; and (4) these explicit\nrelationships can be used by novices to learn about the design domain. This\nrepresents a relatively efficient use of an expert\u2019s time for documenting and\nacquiring knowledge about a design domain, as it only requires a single \u2018inter-\nview\u2019 with the expert. Other methods typically require several interviews (for\nexample, the Spede method described in Ahmed (2001) requires between 5\nand 12).\nThe heuristics can also be used to identify regions in the design space that have\nnot been explored. Such regions can be identified where there appear to be\nrelationships between components with no good reason, resulting in an extra\n(implicit) constraint on the design space. Once these have been identified,\ndesigners can either experiment in this region or rationalize the constraints,\nwhich again leads to greater understanding of the design domain.\nOne final use of the heuristics is trend identification. This is where the relation-\nships are extended beyond the training distribution to generate estimates of\nhow a design family would behave if their design parameters were set beyond\nthe range of what has currently been tried. It should be noted that this can be\nrisky, as the relationships identified have only been verified within the training\ndistribution, and there might be some physical phenomena or constraints that\nprevent the design distribution extending in this manner.\nThe development of the Heuristics Extraction Method has provided a novel\nmethodology for extracting design knowledge from databases of previous de-\nsigns. This methodology includes a method for presenting the computational\n25\nresults to design experts, and using this process to focus their attention on\npotentially unknown relationships that occur within the given design domain.\nThis analysis method potentially impacts the manner which empirical knowl-\nedge is gathered, by automatically generating hypotheses (i.e. the relationships\nidentified). Researchers will then be able to focus on the interesting relation-\nships and further test them, either empirically or analytically.\n11.2 HEM Requirements and Limitations\nThe HEM has a number of requirements which in turn imposes certain lim-\nitations. These were given in Section 3.1 as a set of abstract mathematical\nproperties that a design space must have for the HEM to successfully identify\nrelationships. In design terms, these requirements state that:\n(1) the designs used to train the HEM must all come from the same family,\ni.e. a single representation can be used to describe all designs of this\nfamily;\n(2) there must be some continuity in this space, i.e. designs can be modified\nby a small amount with no more than a small effect on the overall design;\nand\n(3) the design space must have been partially explored in some manner to\nprovide the training data for the HEM.\nThe three case studies satisfied these requirements. Each case study had a\nrepresentation that covered all design examples. From these representations,\ncontinuous relationships existed between various components, even though\nthese were contained in more complex relationships in the wing case study.\nFinally, each case study was based on a database of previous examples.\nOf the above requirements, the continuity requirement provides the greatest\nlimitation. The HEM is not able to generate heuristics relating to components\nthat do not have continuous relationships with other design components. For\nexample, designs where the placement of some resonating element is used\nwould not have the continuity property: e.g. the element needs to be located\nexactly for resonance to occur. This breaks the continuity requirement as a\nsmall change in the position results in a large change in the overall design.\nThis is, for example, a property of designs where the reduction of audible noise\nis an issue. Hence, it is not expected to be possible to analyze such domains\nwith the HEM.\n26\n12 Conclusions\nThe primary aim was to develop a method for extracting design heuristics\nfrom a given set of previous design examples. This was achieved by identifying\nrelationships between various design components, which could be either design\nparameters or evaluations. The overall heuristics extraction method used two\ndifferent data analysis algorithms at its core: Principal Components Analysis\nand Self Organizing Maps. The post-processing of the results of these two\nmethods provided a novel means for identifying these heuristics. Two methods\nwere used to provide a comparison between the mature PCA method and the\nmore recent SOM. This demonstrated how the SOM\u2019s flexibility could identify\nmore general relationships than PCA linear analysis.\nThis research aimed to extract accurate relationships from a given design do-\nmain by analyzing prior design examples. The three case studies demonstrated\nthis accuracy, and through this it can be inferred that the method developed\nhas reasonable validity for this purpose.\nTwo main contributions were provided for the mechanical design domain. The\nfirst was a novel means of analyzing design domains using databases of previ-\nous examples from the domain. This analysis method provides designers with a\nset of explicit heuristic-based relationships that provide greater understanding\nof the domain. The second contribution arises from the first: the relationships\ngenerated reflect any implicit rules or patterns designers follow, even if they\nare not aware of doing so. In following such patterns, designers restrict their\nsearch space. By highlighting such behavior, designers can ensure they escape\nfrom such patterns and perform a more complete search of the design space.\nOverall, the extracted heuristics provide an explicit description of the behavior\nof the design space. This can be used by industry to help better understand\nthe design relationships of a current product line.\n13 Future Work\nAs a result of this work, various aspects of the algorithm and technique have\nbeen identified as requiring greater attention. These aspects can be divided\ninto the three main computational elements of the HEM: the preprocessing,\nthe numerical processing method, and the post-processing.\nThe preprocessing method could benefit from investigation into more sophisti-\ncated methods to explore the relationship space, as opposed to a \u2018brute-force\u2019\nrecombination. A potential method being investigated is the use of genetic\nprogramming techniques for searching the relationship space.\n27\nThis paper describes two specific numerical processing methods (PCA and\nSOM). It would be beneficial to investigate other such methods. Work is\nplanned on experimenting with using larger lattice dimensions for the SOM.\nWhile this has the advantage of being able to more closely represent the de-\nsign space, it will no longer be possible to generate two dimensional projection\nmaps, and therefore visualize the design space. This will require a more accu-\nrate similarity metric to replace the Tanimoto metric currently being used.\nFinally, the post-processing methods need to be improved. The non-hierarchical\nclustering method implemented is too inefficient for very high dimensional\nspaces. This clustering method is being reviewed to reduce the overheads it\nincurs. Further, the method with which the final heuristics are generated and\npresented to domain experts is also being reviewed, based on the results of\nthe wing case study.\nAcknowledgments\nThis work was fully funded by the University Technology Partnership, a collab-\norative research project between the universities of Cambridge, Sheffield and\nSouthampton; and with industrial partners bae systems and Rolls-Royce.\nReferences\nAhmed, S. (2001). Understanding the Use and Reuse of Experience in Engi-\nneering Design, PhD thesis, Cambridge University Engineering Department.\nArciszewski, T. (1997). Engineering semantic evaluation of decision rules,\nJournal of Intelligent and Fuzzy Systems 5: 285\u2013295.\nBall, N. R., Matthews, P. C. and Wallace, K. M. (1998). Managing conceptual\ndesign objects: An alternative to geometry, in J. S. Gero and F. Sudweeks\n(eds), Artificial Intelligence in Design \u201998, Kluwer, Dordrecht, Lisbon, Por-\ntugal, pp. 67\u201386.\nCorbett-Clark, T. A. (1998). Explanation from Neural Networks, DPhil thesis,\nDepartment of Engineering Science, Oxford University.\nCorbett-Clark, T. A. and Tarassenko, L. (1997). A principled framework and\ntechnique for rule extraction from multi-layer perceptrons, Proceedings of\nthe 5th International Conference on Artificial Neural Networks, IEE Con-\nference Publication No 440, pp. 233\u2013238.\nDiamantaras, K. I. and Kung, S. Y. (1996). Principal Component Neural\nNetworks: Theory and Applications, Adaptive and Learning Systems for\nSignal Processing, Communication, and Control, John Wiley, New York,\nNY.\n28\nJardine, N. and Sibson, R. (1968). The construction of hierarchic and non-\nhierarchic classifications, The Computer Journal 11(2): 177\u2013184.\nKaufman, L. and Rousseeuw, P. J. (1990). Finding Groups in Data: An Intro-\nduction to Cluster Analysis, Probability and Mathematical Statistics, John\nWiley, New York, NY.\nKohonen, T. (1997). Self-Organizing Maps, number 30 in Springer Series in\nInformation Sciences, second edn, Springer-Verlag, Berlin.\nLing, R. F. (1973). A computer generated aid for cluster analysis, Communi-\ncations of the ACM 16(6): 355\u2013361.\nLuger, G. F. (1994). Cognitive Science: The Science of Intelligent Systems,\nAcadmic Press, San Diego, CA.\nMatthews, P. C. (1998). Preliminary evaluation of conceptual mechanical de-\nsigns, First year report, Cambridge University Engineering Department.\nMatthews, P. C. (2001). The Application of Self Organizing Maps in Concep-\ntual Design, PhD thesis, Cambridge University Engineering Department.\nMatthews, P. C., Langdon, P. M. and Wallace, K. M. (2001). New techniques\nfor design knowledge exploration: A comparison of three data grouping ap-\nproaches, in S. J. Culley, A. H. B. Duffy, C. McMahon and K. M. Wallace\n(eds), Proceedings of the 13th International Conference on Engineering De-\nsign, Vol. 2, IMechE, Professional Engineering Publishing, London, Glas-\ngow, pp. 107\u2013113.\nMatthews, P. C., Wallace, K. M. and Blessing, L. T. M. (2000). Design heuris-\ntics extraction: Acquiring engineering knowledge from previous designs, in\nJ. S. Gero (ed.), Artificial Intelligence in Design 2000, Kluwer, Dordrecht,\npp. 435\u2013453.\nReich, Y. and Barai, S. V. (1999). Evaluating machine learning models for\nengineering problems, Artificial Intelligence in Engineering 13(2): 257\u2013272.\nReich, Y. and Travitzky, N. (1995). Machine learning of material behaviour\nknowledge from empirical data, Materials & Design 16(5): 251\u2013259.\nRich, E. A. and Knight, K. (1991). Artificial Intelligence, second edn, McGraw-\nHill, Inc, New York.\nTickle, A. B., Andrews, R., Golea, M. and Diedrich, J. (1998). The truth\nwill come to light: Direction and challenges in extracting the knowledge\nembedded within trained artificial neural networks, IEEE Transactions on\nNeural Networks 9(6): 1057\u20131068.\nUltsch, A. (1993). Self organized feature maps for monitoring and knowl-\nedge aquisition of a chemical process, in S. Gielen and B. Kappen (eds),\nProceedings of the International Conference of Artificial Neural Networks,\nSpringer-Verlag, London, pp. 864\u2013867.\n29\nReport to Experts\n(Verification)\nDisseminate Knowledge\n(Heuristics Output)\n(Computational Element)\nProcess with PCA\/SOM\nAnalyse and Parametrically\nRepresent Design Domain\nif results\nunsatisfactory\nData Collection & Preprocessing\nFigure 1. Overview of the HEM process\nTrainingAlgorithm\nReport\nSimilarity\nTraining\nAlgorithm\n(PCA\/SOM)\nVe\nrify\nTra\ninin\ng\nData\nTraining\nData\nVerification\nData\nDesign Data\nPreprocessing\n(optional)\nHeuristics\nDesign\nComponent Similarity Metrics\nDiscussions with Domain Experts\nif not satisfactory, preprocess\nFigure 2. Overview of the heuristic extraction algorithm\n30\nxX1\nX2\nX3\nR\nN\nX = X1 \u222aX2 \u222aX3\nFigure 3. Illustration of a piecewise connected set\nx2\nf1\nf2\nx1\nFigure 4. The Principal Components of some two-dimensional data\n31\n  \n550\n600\n650\n700\n750\n800\n850\nActual T30 (K)\n  \n10\n15\n20\n25\n30\nMap: SOM 04\u2212Jun\u22122000, Data: sCOMB, Size: 8  6\nActual W31 (lb\/s) Mcc R1\n(a) Basic (global) correlation\n  \n5\n10\n15\n20\n25\n30\nEICO\n  \n1\n2\n3\n4\n5\n6\n7\nMap: SOM 04\u2212Jun\u22122000, Data: sCOMB, Size: 8  6\nEIHC\n(b) Local correlation\n  \n10\n20\n30\n40\n50\n60\n70\nEINOxC\n  \n65\n70\n75\n80\n85\n90\n95\nMap: SOM 04\u2212Jun\u22122000, Data: sCOMB, Size: 8  6\nEffcy\n(c) Anti correlated maps\n  \n10\n20\n30\n40\n50\n60\nEINOxC\n  \n5\n10\n15\n20\n25\n30\n35\n40\n45\nMap: Edges of: SOM 04\u2212Jun\u22122000, Data: sCOMB, Size: 8  6\nEffcy\n(d) Edge detection on anti-correlated\nmaps\nFigure 5. Interpreting the Self Organizing Maps for a variety of cases\n32\n5 10 15 20 25 30 35\n5\n10\n15\n20\n25\n30\n35\nComponent Index\nCo\nm\npo\nne\nnt\n In\nde\nx\nFigure 6. The sorted Tanimoto similarity matrix: black = no similarity, white =\nsimilarity (taken from the Combustor dataset)\nHierarchical Non-hierarchical\nAA\nA\nBB\nB\nA \u2229B\nFigure 7. Illustration of hierarchical versus non-hierarchical clustering\n  \n\u22121.65\n\u22121.6\n\u22121.55\n\u22121.5\n\u22121.45\n\u22121.4\n\u22121.35\n\u22121.3\nVar28\n  \n1\n2\n3\n4\n5\n6\n7\n8\n9\nMap: SOM 30\u2212May\u22122001, Data: GA Wing data (extended), Size: 20  15\nVar29\nFigure 8. Example of where the Tanimoto metric has incorrectly identified two maps\nas similar\n33\n(a) Continuous\u2013continuous\ndata: example of positive ev-\nidence for a relationship\n(b) Continuous\u2013continuous\ndata: example of negative\nevidence for a relationship\nClass A\nClass B\n(c) Boolean\u2013continuous\ndata: example of positive\nevidence for a relationship\n(d) Boolean\u2013continuous\ndata: example of negative\nevidence for a relationship\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0000\n\u0000\n\u0000\n\u0001\n\u0001\n\u0001\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001A\nB\nA\nB\nFalse True\n(e) Boolean\u2013Boolean: exam-\nple of positive evidence for a\nrelationship\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0000\n\u0000\n\u0000\n\u0001\n\u0001\n\u0001\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\n\u0001\nB\nB\nFalse True\nA\nA\n(f) Boolean\u2013Boolean: exam-\nple of negative evidence for a\nrelationship\nFigure 9. Plots used to check heuristics against data: contrasting when a relationship\nis present between components (left hand side) and when there is no relationship\n(right hand side)\n34\n"}