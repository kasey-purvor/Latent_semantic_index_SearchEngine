{"doi":"10.1016\/j.robot.2010.02.004","coreId":"55029","oai":"oai:eprints.lincoln.ac.uk:2277","identifiers":["oai:eprints.lincoln.ac.uk:2277","10.1016\/j.robot.2010.02.004"],"title":"Data association and occlusion handling for vision-based people tracking by mobile robots","authors":["Cielniak, Grzegorz","Duckett, Tom","Lilienthal, Achim J."],"enrichments":{"references":[{"id":18444262,"title":"A boosted particle  Multitarget detection and tracking, in:","authors":[],"date":"2004","doi":"10.1007\/978-3-540-24670-1_3","raw":"K. Okuma, A. Taleghani, N. De Freitas, J. J. Little, D. G. Lowe, A boosted particle lter: Multitarget detection and tracking, in: Proc. ECCV, Vol. 1, 2004, pp. 28{39.","cites":null},{"id":18444242,"title":"A decision-theoretic generalization of on-line learning and an application to boosting, in:","authors":[],"date":"1995","doi":"10.1006\/jcss.1997.1504","raw":"Y. Freund, R. E. Schapire, A decision-theoretic generalization of on-line learning and an application to boosting, in: Computational Learning Theory: Eurocolt, Springer-Verlag, 1995, pp. 23{37. 22[2] G. Cielniak, T. Duckett, A. Lilienthal, Improved data association and occlusion handling for vision-based people tracking by mobile robots, in: Proc. of the IEEE\/RSJ International Conference on Intelligent Robots and Systems (IROS), San Diego, CA, USA, 2007, pp. 3436{3441.","cites":null},{"id":18444247,"title":"A mobile vision system for robust multi-person tracking, in:","authors":[],"date":"2008","doi":"10.1109\/cvpr.2008.4587581","raw":"A. Ess, B. Leibe, K. Schindler, L. Van Gool, A mobile vision system for robust multi-person tracking, in: Proc. of the IEEE Computer Vision and Pattern Recognition, 2008.","cites":null},{"id":18444258,"title":"An algorithm for tracking multiple targets, in: Proc.","authors":[],"date":"1979","doi":"10.1109\/tac.1979.1102177","raw":"D. B. Reid, An algorithm for tracking multiple targets, in: Proc. IEEE Trans. Autom. Control, Vol. 6, 1979, pp. 843{854.","cites":null},{"id":18444293,"title":"An MCMC-based particle  for tracking multiple interacting targets, in:","authors":[],"date":"2004","doi":"10.1007\/978-3-540-24673-2_23","raw":"Z. Khan, T. Balch, F. Dellaert, An MCMC-based particle lter for tracking multiple interacting targets, in: Proc. ECCV, 2004.","cites":null},{"id":18444285,"title":"Appearance models for occlusion handling, in:","authors":[],"date":"2001","doi":"10.1016\/j.imavis.2005.06.007","raw":"A. Senior, A. Hampapur, Y.-L. Tian, L. Brown, S. Pankanti, R. Bolle, Appearance models for occlusion handling, in: Proceedings of the 2nd IEEE Workshop on Performance Evaluation of Tracking and Surveillance, Kauai, Hawaii, USA, 2001.","cites":null},{"id":18444264,"title":"Bayesian Multiple Target Tracking, Artech House,","authors":[],"date":"1999","doi":null,"raw":"L. D. Stone, T. L. Corwin, C. A. Barlow, Bayesian Multiple Target Tracking, Artech House, 1999.","cites":null},{"id":18444292,"title":"Beyond the Kalman Filter -Particle Filters for Tracking Applications, Artech House,","authors":[],"date":"2004","doi":null,"raw":"B. Ristic, S. Arulampalam, N. Gordon, Beyond the Kalman Filter -Particle Filters for Tracking Applications, Artech House, Boston, 2004.","cites":null},{"id":18444281,"title":"Bramble: A Bayesian multiple-blob tracker., in:","authors":[],"date":"2001","doi":"10.1109\/iccv.2001.937594","raw":"M. Isard, J. MacCormick, Bramble: A Bayesian multiple-blob tracker., in: Proc. of the International Conference on Computer Vision, Vol. 2, Vancouver, British Columbia, Canada, 2001, pp. 34{41. 24[23] A. Argyros, M. Lourakis, Real-time tracking of multiple skin-colored objects with a possibly moving camera, in: Proc. of the 8th European Conference on Computer Vision, Prague, Czech Republic, 2004, pp. Vol III: 368{379.","cites":null},{"id":18444296,"title":"Condensation { conditional density propagation for visual tracking","authors":[],"date":"1998","doi":"10.1007\/bfb0015549","raw":"M. Isard, A. Blake, Condensation { conditional density propagation for visual tracking 29 (1) (1998) 5{28. 25[34] M. A. Stricker, M. Orengo, Similarity of color images, in: Storage and Retrieval for Image and Video Databases, 1995, pp. 381{392.","cites":null},{"id":18444250,"title":"Development of ALACRANE: A mobile robotic assistance for exploration and rescue missions, in:","authors":[],"date":"2007","doi":"10.1109\/ssrr.2007.4381269","raw":"A. Garcia-Cerezo, A. Mandow, J. Martinez, J. Gomez-de Gabriel, J. Morales, A. Cruz, A. Reina, J. Seron, Development of ALACRANE: A mobile robotic assistance for exploration and rescue missions, in: Proc. of the IEEE International Workshop on Safety, Security and Rescue Robotics, Rome, Italy, 2007.","cites":null},{"id":18444299,"title":"Evaluating multi-object tracking, in: Workshop on Empirical Evaluation Methods in Computer Vision,","authors":[],"date":"2005","doi":"10.1109\/cvpr.2005.453","raw":"K. Smith, D. Gatica-Perez, J. M. Odobez, S. Ba, Evaluating multi-object tracking, in: Workshop on Empirical Evaluation Methods in Computer Vision, San Diego, CA, USA, 2005. Grzegorz Cielniak is a lecturer in Computer Science at the University of Lincoln, UK. He obtained his Ph.D. in Computer Science from  Orebro University, Sweden in 2007 and M.Sc. from Wroclaw University of Technology, Poland in 2000. The Ph.D thesis addresses a problem of real-time people tracking for mobile robots. His research interests include mobile robotics, vision systems, people tracking, AI and ying robots. 26Tom Duckett is a Reader in Computer Science at the University of Lincoln, where he is also Director of the Centre for Vision and Robotics Research. He was formerly a docent (Associate Professor) at  Orebro University, where he was leader of the Learning Systems Laboratory within the Centre for Applied Autonomous Sensor Systems. He obtained his Ph.D. in Computer Science from Manchester University in 2001, M.Sc. with distinction in Knowledge Based Systems from Heriot-Watt University in 1995 and B.Sc. (Hons.) in Computer and Management Science from Warwick University in 1991, and has also studied at Karlsruhe and Bremen Universities. His research interests include mobile robotics, navigation, machine learning, AI, computer vision, and sensor fusion for perception-based control of autonomous systems. Achim Lilienthal is a docent (associate professor) at the AASS Research Center in  Orebro, Sweden, where he is leading the Learning Systems Lab. His main research interests are mobile robot olfaction, robot vision, robotic map learning and safe navigation systems. Achim Lilienthal obtained his Ph.D. in computer science from T ubingen University, Germany and his M.Sc. and B.Sc. in Physics from the University of Konstanz, Germany. The Ph.D. 27thesis addresses gas distribution mapping and gas source localisation with a mobile robot. The M.Sc. thesis is concerned with an investigation of the structure of (C60)+ n clusters using gas phase ion chromatography.","cites":null},{"id":18444245,"title":"Face tracking and hand gesture recognition for human-robot interaction, in:","authors":[],"date":"2004","doi":"10.1109\/robot.2004.1308101","raw":"L. Br ethes, P. Menezes, F. Lerasle, J. Hayet, Face tracking and hand gesture recognition for human-robot interaction, in: Proc. IEEE ICRA, New Orleans, LA, USA, 2004, pp. 1901{1906.","cites":null},{"id":18444246,"title":"Garc a-Silvente, People detection and tracking using stereo vision and color,","authors":[],"date":"2007","doi":"10.1016\/j.imavis.2006.07.012","raw":"R. Mu~ noz Salinas, E. Aguirre, M. Garc a-Silvente, People detection and tracking using stereo vision and color, Image and Vision Computing 25 (6) (2007) 995{1007.","cites":null},{"id":18444290,"title":"Kr ose, Keeping track of humans: Have i seen this person before?, in:","authors":[],"date":"2005","doi":"10.1109\/robot.2005.1570420","raw":"W. Zajdel, Z. Zivkovic, B. J. A. Kr ose, Keeping track of humans: Have i seen this person before?, in: Proc. IEEE ICRA, Barcelona, Spain, 2005.","cites":null},{"id":18444268,"title":"M2tracker: A multi-view approach to segmenting and tracking people in a cluttered scene using region-based stereo, in:","authors":[],"date":"2002","doi":"10.1007\/3-540-47969-4_2","raw":"A. Mittal, L. S. Davis, M2tracker: A multi-view approach to segmenting and tracking people in a cluttered scene using region-based stereo, in: Proc. IEEE CVPR, 2002, pp. 18{36.","cites":null},{"id":18444254,"title":"Mobile Robot Navigation with Intelligent Infrared Image Interpretation,","authors":[],"date":"2009","doi":"10.1007\/978-1-84882-509-3","raw":"W. Fehlman, M. Hinders, Mobile Robot Navigation with Intelligent Infrared Image Interpretation, Springer London, 2009.","cites":null},{"id":18444295,"title":"Monte Carlo data association for multiple target tracking, in:","authors":[],"date":"2001","doi":"10.1049\/ic:20010239","raw":"R. Karlsson, F. Gustafsson, Monte Carlo data association for multiple target tracking, in: In IEEE Target tracking: Algorithms and applications, The Netherlands, 2001.","cites":null},{"id":18444291,"title":"Novel approach to nonlinear\/non-Gaussian Bayesian state estimation,","authors":[],"date":"1993","doi":"10.1049\/ip-f-2.1993.0015","raw":"N. J. Gordon, D. J. Salmond, A. F. M. Smith, Novel approach to nonlinear\/non-Gaussian Bayesian state estimation, Proc. Inst. Elect. Eng. F 140 (2) (1993) 107{113.","cites":null},{"id":18444271,"title":"P Realtime tracking of the human body,","authors":[],"date":"1997","doi":"10.1109\/34.598236","raw":"C. R. Wren, A. Azarbayejani, T. Darrell, A. P. Pentland, Pnder: Realtime tracking of the human body, IEEE Transactions on Pattern Analysis and Machine Intelligence 19 (7) (1997) 780{785.","cites":null},{"id":18444248,"title":"Pedestrian detection in infrared images, in:","authors":[],"date":"2003","doi":"10.1109\/ivs.2003.1212991","raw":"M. Bertozzi, A. Broggi, P. Grisleri, T. Graf, M. Meinecke, Pedestrian detection in infrared images, in: Proc. of the IEEE Intelligent Vehicles Symposium, Columbus, USA, 2003, pp. 662{667.","cites":null},{"id":18444243,"title":"People tracking by mobile robots using thermal and colour vision,","authors":[],"date":"2007","doi":"10.1016\/j.robot.2006.04.013","raw":"G. Cielniak, People tracking by mobile robots using thermal and colour vision, Ph.D. thesis,  Orebro University (April 2007).","cites":null},{"id":18444283,"title":"Probabilistic framework for segmenting people under occlusion, in:","authors":[],"date":"2001","doi":"10.1109\/iccv.2001.937617","raw":"A. Elgammal, L. S. Davis, Probabilistic framework for segmenting people under occlusion, in: Proc. of the International Conference on Computer Vision, Vancouver, Canada, 2001.","cites":null},{"id":18444294,"title":"Probabilistic object tracking based on machine learning and importance sampling, in:","authors":[],"date":"2005","doi":"10.1007\/11492429_20","raw":"P. Li, H. Wang, Probabilistic object tracking based on machine learning and importance sampling, in: Proc. of the Iberian Conference on Pattern Recognition and Image Analysis, Vol. 1, 2005, pp. 161{167.","cites":null},{"id":18444249,"title":"Probabilistic template based pedestrian detection in infrared videos, in:","authors":[],"date":"2002","doi":"10.1109\/ivs.2002.1187921","raw":"H. Nanda, L. Davis, Probabilistic template based pedestrian detection in infrared videos, in: IEEE Intelligent Vehicle Symposium, Versailles, France, 2002.","cites":null},{"id":18444297,"title":"Rapid object detection using a boosted cascade of simple features, in:","authors":[],"date":"2001","doi":"10.1109\/cvpr.2001.990517","raw":"P. Viola, M. Jones, Rapid object detection using a boosted cascade of simple features, in: Proc. IEEE CVPR, 2001.","cites":null},{"id":18444266,"title":"Real-time closed-world tracking, in:","authors":[],"date":"1997","doi":"10.1109\/cvpr.1997.609402","raw":"S. S. Intille, J. Davis, A. Bobick, Real-time closed-world tracking, in: Proc. IEEE CVPR, 1997, pp. 697{703.","cites":null},{"id":18444252,"title":"robot league team: Darmstadt rescue robot team (germany),","authors":[],"date":null,"doi":null,"raw":"M. Andriluka, M. Friedmann, S. Kohlbrecher, J. Meyer, K. Petersen, C. Reinl, P. Schau, P. Schnitzspan, A. Armin Strobel, D. Thomas, 23O. von Stryk, Robocuprescue 2009 - robot league team: Darmstadt rescue robot team (germany), Tech. rep., Technische Universitt Darmstadt (2009).","cites":null},{"id":18444244,"title":"Sensor fusion for vision and sonar based people tracking on a mobile service robot, in:","authors":[],"date":"2002","doi":null,"raw":"T. Wilhelm, H. J. B ohme, H. M. Gross, Sensor fusion for vision and sonar based people tracking on a mobile service robot, in: Int. Workshop on Dynamic Perception, Bohum, Germany, 2002, pp. 315{320.","cites":null},{"id":18444298,"title":"Tools and techniques for video performance evaluation, in:","authors":[],"date":"2000","doi":"10.1109\/icpr.2000.902888","raw":"D. S. Doermann, D. Mihalcik, Tools and techniques for video performance evaluation, in: Proc. ICPR, Vol. 4, Barcelona, Spain, 2000, pp. 4167{4170.","cites":null},{"id":18444260,"title":"Tracking and Data Association,","authors":[],"date":"1988","doi":"10.1109\/joe.1983.1145560","raw":"Y. Bar-Shalom, T. Fortmann, Tracking and Data Association, Academic Press, 1988.","cites":null},{"id":18444287,"title":"Tracking groups of people 1 (80)","authors":[],"date":"2000","doi":"10.1006\/cviu.2000.0870","raw":"S. Mckenna, S. Jabri, Z. Duric, A. Rosenfeld, Tracking groups of people 1 (80) (2000) 42{56.","cites":null},{"id":18444256,"title":"Tracking multiple moving objects with a mobile robot, in:","authors":[],"date":"2001","doi":"10.1109\/cvpr.2001.990499","raw":"D. Schulz, W. Burgard, D. Fox, A. B. Cremers, Tracking multiple moving objects with a mobile robot, in: Proc. IEEE CVPR, 2001.","cites":null},{"id":18444279,"title":"Tracking people in presence of occlusion, in:","authors":[],"date":"2000","doi":null,"raw":"S. Khan, M. Shah, Tracking people in presence of occlusion, in: Asian Conference on Computer Vision, Taipei, Taiwan, 2000.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-05-31","abstract":"This paper presents an approach for tracking multiple persons on a mobile robot with a combination of colour and thermal vision sensors, using several new techniques. First, an adaptive colour model is incorporated into the measurement model of the tracker. Second, a new approach for detecting occlusions is introduced, using a machine learning classifier for pairwise comparison of persons (classifying which one is in front of the other). Third, explicit occlusion handling is incorporated into the tracker. The paper presents a comprehensive, quantitative evaluation of the whole system and its different components using several real world data sets","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/55029.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/2277\/1\/cielniak10data.pdf","pdfHashValue":"b0687bfd030df362cbf7d1655e264403da71962b","publisher":"Elsevier B.V.","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:2277<\/identifier><datestamp>\n      2013-11-18T16:09:43Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F48:6A6163735F48363730<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47343030<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F48:6A6163735F48363731<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373430<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/2277\/<\/dc:relation><dc:title>\n        Data association and occlusion handling for vision-based people tracking by mobile robots<\/dc:title><dc:creator>\n        Cielniak, Grzegorz<\/dc:creator><dc:creator>\n        Duckett, Tom<\/dc:creator><dc:creator>\n        Lilienthal, Achim J.<\/dc:creator><dc:subject>\n        H670 Robotics and Cybernetics<\/dc:subject><dc:subject>\n        G400 Computer Science<\/dc:subject><dc:subject>\n        H671 Robotics<\/dc:subject><dc:subject>\n        G740 Computer Vision<\/dc:subject><dc:description>\n        This paper presents an approach for tracking multiple persons on a mobile robot with a combination of colour and thermal vision sensors, using several new techniques. First, an adaptive colour model is incorporated into the measurement model of the tracker. Second, a new approach for detecting occlusions is introduced, using a machine learning classifier for pairwise comparison of persons (classifying which one is in front of the other). Third, explicit occlusion handling is incorporated into the tracker. The paper presents a comprehensive, quantitative evaluation of the whole system and its different components using several real world data sets.<\/dc:description><dc:publisher>\n        Elsevier B.V.<\/dc:publisher><dc:date>\n        2010-05-31<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/2277\/1\/cielniak10data.pdf<\/dc:identifier><dc:identifier>\n          Cielniak, Grzegorz and Duckett, Tom and Lilienthal, Achim J.  (2010) Data association and occlusion handling for vision-based people tracking by mobile robots.  Robotics and Autonomous Systems, 58  (5).   pp. 435-443.  ISSN 0921-8890  <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1016\/j.robot.2010.02.004<\/dc:relation><dc:relation>\n        10.1016\/j.robot.2010.02.004<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/2277\/","http:\/\/dx.doi.org\/10.1016\/j.robot.2010.02.004","10.1016\/j.robot.2010.02.004"],"year":2010,"topics":["H670 Robotics and Cybernetics","G400 Computer Science","H671 Robotics","G740 Computer Vision"],"subject":["Article","PeerReviewed"],"fullText":"Data Association and Occlusion Handling for\nVision-Based People Tracking by Mobile Robots\nGrzegorz Cielniak\u2217,a, Tom Ducketta, Achim J. Lilienthalb\naSchool of Computer Science, University of Lincoln,\nLN6 7TS Lincoln, United Kingdom\nbCentre for Applied Autonomous Sensor Systems, O\u00a8rebro University,\nSE-701 82 O\u00a8rebro, Sweden\nAbstract\nThis paper presents an approach for tracking multiple persons on a mobile\nrobot with a combination of colour and thermal vision sensors, using several\nnew techniques. First, an adaptive colour model is incorporated into the\nmeasurement model of the tracker. Second, a new approach for detecting\nocclusions is introduced, using a machine learning classifier for pairwise com-\nparison of persons (classifying which one is in front of the other). Third, ex-\nplicit occlusion handling is incorporated into the tracker. The paper presents\na comprehensive, quantitative evaluation of the whole system and its different\ncomponents using several real world data sets.\nKey words: AdaBoost, Occlusion Detection, Thermal Vision, Colour\nVision, Bayesian estimation\n1. Introduction\nThis paper addresses the problem of people detection and tracking by mo-\nbile robots in indoor environments. A system that can detect and recognise\npeople is an essential part of any mobile robot that is designed to operate\nin populated environments. Information about the presence and location of\n\u2217Corresponding author. Tel.: +44 1522 837398; fax: +44 1522 886974.\nEmail addresses: gcielniak@lincoln.ac.uk (Grzegorz Cielniak),\ntduckett@lincoln.ac.uk (Tom Duckett), achim.lilienthal@tech.oru.se (Achim J.\nLilienthal)\nPreprint submitted to Robotics and Autonomous Systems February 4, 2010\npersons in the robot\u2019s surroundings is necessary to enable interaction with\nthe human operator, and also for ensuring the safety of people near the robot.\nThe presented people tracking system uses a combination of thermal and\ncolour information to robustly track persons. The thermal camera simplifies\nthe detection problem, which is especially difficult on a mobile platform. The\nsystem is based on a fast and efficient sample-based tracking method that en-\nables tracking of people in real-time. The measurement model using gradient\ninformation from the thermal image is fast to calculate and allows detection\nand tracking of persons under different views. An explicit model of the hu-\nman silhouette effectively distinguishes persons from other objects in the\nscene. Moreover the process of detection and localisation is performed simul-\ntaneously so that measurements are incorporated directly into the tracking\nframework without thresholding of observations. With this approach persons\ncan be detected independently from current light conditions and in situations\nwhere other popular detection methods based on skin colour would fail.\nA very challenging situation for a tracking system occurs when multiple\npersons are present on the scene. The tracking system has to estimate the\nnumber and position of all persons in the vicinity of the robot. Tracking of\nmultiple persons in the presented system is realised by an efficient algorithm\nthat mitigates the problems of combinatorial explosion common to other\nknown algorithms. A sequential detector initialises an independent tracking\nfilter for each new person appearing in the image, using thermal information.\nA single filter is automatically deleted when it stops tracking a person.\nWhile thermal vision is good for detecting people, it can be very difficult\nto maintain the correct association between different observations and per-\nsons, especially where they occlude one another, due to the unpredictable\nappearance and social behaviour of humans. To address these problems\nthe presented tracking system uses additional information from the colour\ncamera, introducing several techniques for improving data association and\nocclusion handling.\nFirst, an adaptive colour model is incorporated into the measurement\nmodel of the tracker to improve data association. For this purpose an efficient\nintegral image based method is used to maintain the real-time performance\nof the tracker.\nSecond, to deal with occlusions the system uses an explicit method that\nfirst detects situations where people occlude each other. This is realised by a\nnew approach based on a machine learning classifier for pairwise comparison\nof persons that uses both thermal and colour features provided by the tracker.\n2\nOur approach uses the AdaBoost algorithm [1] to build the classifier from\nthe available thermal and colour features.\nThird, the information from the occlusion detector is incorporated into\nthe tracker for occlusion handling and to resolve situations where persons\nreappear in a scene.\nFurther to our previously published results [2], this paper presents a com-\nprehensive, quantitative evaluation of the whole system and its different com-\nponents using several real world data sets recorded in an office environment\n(see also [3] for further details). We analyse the relative influence of dif-\nferent visual features for occlusion handling, and further demonstrate the\nrobustness and efficiency of the approach.\n1.1. Related Work\nMany approaches for people tracking on mobile platforms are based on\nskin colour and face recognition (e.g., [4, 5]). However these methods require\npersons to be close to and facing the robot so that their hands or faces\nare visible. Stereo vision provides extra range information that makes the\nsegmentation of persons easier, allowing for detection and tracking of both\nstanding and moving people regardless their orientation [6, 7]. In both\nthese systems, the coarse depth information provided by the stereo-camera\nhas proven sufficient to resolve the majority of short-term occlusions.\nOur system makes use of thermal vision that takes advantage of the fact\nthat humans have a distinctive thermal profile compared to nonliving objects.\nMoreover thermal information is not influenced by changing lighting condi-\ntions and allows detection of people even in darkness. Infrared sensors have\nbeen applied to detect pedestrians in driving assistance systems (e.g. [8],[9])\nbut their use in robotic applications is limited, probably due to the high price\nof the sensors. So far, thermal cameras were deployed mostly on mobile plat-\nforms designed for search and rescue missions [10, 11]. The recent work of\n[12] describes the use of a thermal sensor for detection and classification of\nnon-heat generating objects used for mobile robot navigation.\nOther people tracking systems are based on range-finder sensors such as\nlaser scanner and sonar that are very popular sensors in mobile robotics for\nnavigation and localisation tasks. The system in [13] uses a laser scanner sen-\nsor to track multiple persons. It is based on a particle filter and JPDAF data\nassociation, uses a global representation of the environment, requires thresh-\nolded sensor data and deals with occlusions of non-interacting persons only.\n3\nIn contrast, our system uses sensor coordinates, incorporates unthresholded\ndata and can reason about occlusions of interacting persons.\nClassical tracking algorithms usually handle the detection and tracking\ntasks separately in order to simplify the whole problem [14, 15]. However,\nsuch an architecture can cause loss of information between these steps, in ad-\ndition to the computational cost of detection by exhaustive search of all possi-\nble object states [16]. The alternative approach considers these two problems\nsimultaneously (track-before-detect, also called unified tracking [17]). The\npresented system is designed in this latter spirit, using a track-before-detect\ntechnique.\nTo deal with problems of occlusions several authors proposed solutions\nthat use special sensors or their special arrangement. One example system\nuses a camera placed above the observed scene [18]. Persons observed from\nsuch a view-point cannot occlude each other. Another example is a multi-\ncamera system [19] where ambiguities caused by occlusion are resolved by\ncombining information from different cameras placed in different places. All\nthese solutions can be used only in a few, controlled scenarios and their use\nin mobile applications would be especially troublesome if not impossible.\nIn the majority of people tracking systems the problem of occlusion is\nsolved within the tracking framework. Possible approaches handle occlu-\nsions either implicitly without reasoning, or model them explicitly. Implicit\nsolutions use kinematic information as well as dedicated measurement models\n[20, 21, 22, 23]. However the behaviour of people tends to be highly unpre-\ndictable in general, and they may or may not interact. Therefore implicit\napproaches can deal only with specific cases, i.e., short-term occlusions. The\nproposed system uses an explicit approach to deal with occlusions. This rea-\nsoning requires domain specific knowledge, i.e., detection of situations when\npersons appear to merge and split, and making decisions about their be-\nhaviour during occlusion (see for example [24, 25, 26, 27]). We use colour\nas additional information that helps to detect occluded persons and resolve\nocclusions when occluded persons appear again on the scene.\nIn the next section we introduce the experimental platform. Section 3\npresents the basic tracker using gradient information from the thermal cam-\nera. The next sections describe the techniques developed to maintain the\ncorrect associations between observations and persons, by exploiting the com-\nbination of thermal and colour vision: incorporation of colour information\ninto the measurement model (Section 4), an occlusion detector based on the\nmachine learning algorithm AdaBoost (Section 5) and the occlusion han-\n4\ncolour camera\nthermal camera\nFigure 1: ActivMedia PeopleBot robot equipped with a thermal camera and a standard\ncamera (left). Example of an image from the colour camera (right-top) and thermal camera\n(right-bottom).\ndling procedure (Section 6). Experimental results are presented in Section\n7, followed by conclusions and suggestions for future work.\n2. Experimental Set-up\nWe used an ActivMedia PeopleBot robot (Fig. 1) equipped with different\nsensors, including a colour pan-tilt-zoom camera (VC-C4R, Canon) and, a\nthermal camera (Thermal Tracer TS7302, NEC), and an Intel Pentium III\nprocessor (850 MHz). The colour and thermal camera are mounted close\nto each other, which simplifies the calibration procedure between the two\ncameras (see Section 4.1).\nThe robot was operated in an indoor environment (a corridor and labo-\nratory room). Persons taking part in the experiments were asked to walk in\nfront of the robot while it performed a corridor following behaviour or while\nthe robot was stationary. At the same time, image data were collected with\na frequency of 15Hz. The resolution of both thermal and colour images was\n320 \u00d7 240 pixels. In our set-up the visible range on the grey-scale thermal\nimage was equivalent to the temperature range from 24 to 36 \u25e6C.\n5\n3. Basic Tracker Using Thermal Vision\n3.1. Particle-based Tracking of a Single Person\nTo reliably estimate the location and movement of persons it is neces-\nsary to apply a tracking procedure. Our system uses a particle filter to\nprovide an efficient solution to this problem despite the high dimensionality\nof the state space. The particle filter performs both detection and tracking\nsimultaneously without exhaustive search of the state space. Moreover the\nmeasurements are incorporated directly into the tracking framework without\nany preprocessing such as thresholding that could cause loss of information.\nThe posterior probability p(xt|z1:t) of the system being in state xt given a\nhistory of measurements z1:t is approximated by a set of N weighted samples\nsuch that\np(xt|z1:t) \u2248\nN\u2211\ni=1\nwi\u03b4(xt \u2212 xit). (1)\nEach xit describes a possible state together with a weight w\ni\nt which is propor-\ntional to the likelihood that the system is in this state. We use a standard\nSampling Importance Resampling (SIR) filter [28] starting with a uniform\ninitial distribution. The dynamic model used in the particle filter is random\nwalk with drift. The measurement model used to calculate new weights for\nparticles is presented later in Section 3.3. The resampling step was imple-\nmented using the systematic resampling algorithm [29].\n3.2. Tracking Multiple Persons\nThe above method is extended to the multi-person case by detecting new\npersons incrementally as they appear while maintaining existing tracks of\npersons. This system uses a set of independent particle filters to track dif-\nferent persons. To assign new filters to new persons we use a sequential\ndetector consisting of a set of N randomly initialised particles. These parti-\ncles are used to \u201ccatch\u201d a new person entering the scene. To avoid multiple\ndetections in the same or similar regions, the weight of detection particles is\npenalised by a factor \u03c8d < 1 in cases where particles cross already detected\nareas. The weight update equation for the ith detection particle is modified to\nwit \u221d p(zt|xt = xit)\u03c8, where \u03c8 = \u03c8d if particle i overlaps with other detected\nregions and \u03c8 = 1 otherwise. Thus already existing filters naturally limit the\nsearch space for the detector. Detection occurs when the average fitness of\nthe particles exceeds a certain threshold for a few consecutive frames (3 in\n6\nwh\nh\/2\nd\n(x,y)\n\u0394\n6\n\u0394\n1\n\u0394\n2\n\u0394\n3\n\u0394\n4\n\u0394\n5\n\u0394\n7\nFigure 2: The elliptic measurement model for thermal images. Model parameters are\nshown on the left. Division of ellipses into 7 regions is shown on the right.\nour experiments). Then the particles from the detector are used to initialise\na new tracker before being re-initialised for detection of the next new person.\nA solution based on independent tracking filters is computationally inex-\npensive and appropriate for on-line applications, but suffers in cases when\ntracked persons are too close to each other. To reduce these problems we\nexplicitly model interactions between persons by penalising the weights of\nparticles that intersect with other detected regions. The weight update equa-\ntion for established tracking filters is changed to wit \u221d p(zt|xt = xit)\u03c8, where\n\u03c8 = e(\u2212\u03c1gij) and gij expresses the amount of overlap between particle i and\nalready detected region tracked by filter j, which is multiplied by a factor \u03c1 in\nthe exponent of the penalty term. The penalty factor \u03c1 allows for specifying\nthe \u201cstrength\u201d of interactions between persons and the amount of handled\npartial occlusions. This solution is similar to the interaction model proposed\nby [30], where the authors propose a Random Markov Field using a joint\nstate space representation. The treatment of interactions in both approaches\nhas the drawback that in the case of occlusions weaker filters disappear. Mo-\ntion information could help here only in specific situations where persons are\njust passing by each other at sufficient speeds. However this is not the case\nin situations where people stop to talk, shake hands, walk in groups, etc.\n3.3. Elliptic Contour Model\nThe measurement model used by our thermal tracker is a contour model\nconsisting of two ellipses: one describes the position of the body part and the\n7\nother measures the position of the head part (Fig. 2). Thus we obtain a 9-\ndimensional state vector: xt = (x, y, w, h, d, vx, vy, vw, vh) where (x, y) is the\nmid-point of the body ellipse with width w and height h. The height of the\nhead is calculated by dividing h by a constant factor. The displacement of\nthe middle of the head part from the middle of the body ellipse is described\nby d. We also model velocities of the body part as (vx, vy, vw, vh). The\nvelocity of the d component has very noisy characteristics and is therefore\nnot considered in the state vector. To calculate the importance weight wit of\na sample i with state xit we divide the ellipses into m = 7 different regions\n(see Fig. 2) and for each region j the image gradient \u2206ij between pixels in the\ninner and outer parts of the ellipse is calculated. The gradient is maximal\nif the ellipses fit the contour of a person in the image data. A fitness value\nf i for each sample i is then calculated as the sum of all gradients multiplied\nwith individual weights \u03b1j for each region: f\ni =\n\u2211m\nj=1 \u03b1j\u2206\ni\nj. The weights \u03b1j\nsum to one and are chosen such that the shoulder parts have lower weight to\nminimise the measurement error that occurs due to different arm positions.\nThe fitness value is finally scaled to values in [0, 1] in order to represent a\nlikelihood:\npg(zt|xit) =\nexp(\u03ba \u00b7 (f i \u2212 \u03b8))\nexp(\u03ba \u00b7 (f i \u2212 \u03b8)) + exp(\u03ba \u00b7 (\u03b8 \u2212 f i)) , (2)\nwhere \u03b8 denotes a fitness threshold and the value of \u03ba defines the slope of\nthe likelihood function. This kind of likelihood function was also used in [31]\nfor visual tracking of objects.\nWhen the mean gradient value from Eq. 2 is greater than 0.5 then a person\nis considered to be detected. We also check the uncertainty of the estimate\n[32] to avoid detections in wrong regions when the posterior is multi-modal\n(e.g. for multiple persons).\nThis approach is similar to the work by Isard and Blake [33] for tracking\npeople in a greyscale image. However, they use a spline model of the head\nand shoulder contour which cannot be applied in situations where the person\nis far away or visible in a side view, because there will be no recognisable\nhead-shoulder contour. The elliptic contour model used here is able to cope\nwith these situations.\n8\na) b)\nFigure 3: Rectangular features: a) thermal image b) colour image with regions corre-\nsponding to different body parts from which colour information is extracted.\n4. ADAPTIVE COLOUR MODEL\n4.1. Colour representation\nSince the baseline between the cameras is small compared to the distance\nto persons, it is possible to align the thermal and colour images by affine\ntransformation. We then use an efficient colour representation proposed in\n[34] based on the first three moments (mean, variance and skewness) of the\ncolour distribution. This representation was shown to be more effective than\nhistogram methods (e.g., [35]) in the domain of image indexing. To include\ninformation about the spatial layout of the colour we divided the region\ncorresponding to a person\u2019s body into rectangular sub-areas from which we\ncalculate the colour statistics (see Fig. 3b). The position and size of these\nregions are determined from the information provided by the elliptic contour\nmodel.\n4.2. Colour likelihood\nThe appearance model based on colour moments is created every time\na new detection occurs, i.e. a new track is initialised in the thermal image.\nBy using the affine transformation we are able to determine the region corre-\nsponding to a person on the colour image (see Fig. 3). From three rectangular\nregions corresponding to the person\u2019s head, torso and legs we collect colour\nstatistics ct of the first three moments (m1,m2,m3) for three colour chan-\nnels (R,G,B). Finally we obtain a feature vector ct of size 3 \u00d7 3 \u00d7 3 = 27.\nTo make the model more robust to changing light conditions we adapt it\nwhile a person is tracked. In our implementation we store colour statistics\n9\nfrom the last nk frames and calculate their mean value. The parameter nk\ninfluences the robustness and adaptivity of the colour model. In our experi-\nments nk = 10 corresponding to 0.7 s. We use Euclidean distance to measure\nthe similarity between the model c?t and region of interest ct. Finally, the\nlikelihood model for colour information is\npc(zt|xt) = exp\n(\u2212\u03bbd2t ) , (3)\nwhere \u03bb is a parameter that determines the shape of the colour likelihood.\nSince \u03bb scales the distance, higher values of \u03bb mean that the colour-based\nlikelihood model is more peaked, thus having more importance when com-\nbined with the gradient information from the ellipse model.\n4.3. Rapid rectangular features\nThe colour moments can be rapidly calculated using an integral image\nrepresentation [36]. The estimators for the first three moments of the colour\ndistribution can be obtained by means of k-statistics calculated using sums\nof the rth powers of the colour data:\nSr =\nx+w\u2211\ni=x\ny+h\u2211\nj=y\nIr(i, j), (4)\nwhere I(i, j) is a pixel value of the colour image selected from the rectangular\nregion specified by coordinates {x, y, x + w, y + h}. Each Sr can be quickly\ncalculated using the integral image representation. The first three k-statistics\nare obtained as\nk1 = S1\/n, (5)\nk2 =\nnS2 \u2212 S21\nn(n\u2212 1) , (6)\nk3 =\n2S31 \u2212 3nS1S2 + n2S3\nn(n\u2212 1)(n\u2212 2) , (7)\nwhere n = w \u00d7 h. Finally the normalised values of estimators for mean m1,\nvariance m2 and skewness m3 can be obtained as m1 = k1, m2 = k2\/k1 and\nm3 = k3\/k\n3\n2\n2 . The normalisation is performed to balance the influence of each\nmoment on the final score.\n10\n4.4. Combining thermal and colour information\nIf we assume that the likelihoods for the gradient model pg(zt|xt) (Eq. 2)\nand colour model pc(zt|xt) (Eq. 3) are independent then the data fusion can\nbe realised by taking a product of these two likelihoods\np(zt|xt) = pg(zt|xt)pc(zt|xt). (8)\nThe parameters \u03ba, \u03b8 (gradient model) and \u03bb (colour model) specify the shape\nof the gradient and colour likelihood functions, thus specifying the impor-\ntance of the respective features. The influence of possible correlations be-\ntween colour and thermal distributions should be investigated more thor-\noughly in future work.\nWhen a person is not detected, a colour model cannot be built and only\ngradient information can be used to update the weight of the particles of\na single tracking filter as wit = pg(zt|xit)\u03c8. However as soon as a person is\ndetected the colour model can be created and the weight update equation\nchanges to:\nwit = pg(zt|xit)pc(zt|xit)\u03c8, i = 1, . . . , N. (9)\nNote that the sequential detector relies only on gradient information from\nthe thermal image.\n5. OCCLUSION DETECTION WITH ADABOOST\nTo detect occlusions we propose an approach that sorts the order of all\npersons in the image according to pairwise comparisons. The proposed oc-\nclusion classifier specifies which one of two overlapping persons is in front of\nthe other. The order of the persons from front-to-back is then determined\nby a sort procedure requiring MO \u00b7 log(MO) comparisons where MO specifies\nthe number of overlapping persons.\nThere are several features that could indicate the correct order of two\noverlapping persons in the image, from which we have chosen a set of three\nthermal and three colour features:\n\u2022 The strength (i.e., mean gradient value) of a tracking filter, since a\nperson for which the corresponding tracker indicates a higher confi-\ndence is more likely to be in the front. This feature is, however, very\nnoisy and is affected by many factors such as movement of the camera,\ntemperature of the environment, etc.\n11\nFigure 4: Relationship of the different thermal features to the apparent distance of a\nperson taken from the ground truth data.\n\u2022 The top and bottom of the elliptic model can also indicate the depth\nof a person since closer persons appear taller and closer to the upper\nand bottom border of the image. However the bottom part can be cut\nwhen persons stand too close to the camera. The top of a person\u2019s head\nis a more reliable feature, though it is affected by the different height\nof persons.\n\u2022 Another set of features is the colour similarity of the region corre-\nsponding to a person. We have chosen three such regions including the\noverlapping, non-overlapping and whole areas of a person. Occluded\npersons should have lower similarity values.\nSince a single feature cannot easily determine the right order of the per-\nsons we use a boosting algorithm [1] to weight and combine a number of\n\u201cweak classifiers\u201d built from these features, resulting in a strong classifier\nwith much improved occlusion detection accuracy.\nTo give an impression of the discriminative power of the thermal features\nused, we present a graphical representation of their relationship to the ap-\nparent distance of a person taken from the ground truth data (see Fig. 4).\nThis distance, calculated as the difference between the bottom part of the\nperson\u2019s bounding box taken from ground truth and the bottom of the image,\nuniquely determines the order of the persons. Note that range information\nfrom a laser scanner could also be used to simplify this problem. However\nin this work we consider an exclusively vision-based system. (It would not\n12\nbe meaningful to provide a similar visualisation for the colour features, since\nthese features are based on comparisons of two tracked persons rather than\na single tracked person as in the thermal case.)\nWe use the AdaBoost (Adaptive Boosting) classification algorithm [1] for\nselecting the best combination of features to detect occlusions. AdaBoost\ncombines results from so-called \u201cweak\u201d classifiers ht(x) into one \u201cstrong\u201d\nclassifier H(x) = sign(\n\u2211T\nt=1 \u03b1tht(x)), where T is the number of weak clas-\nsifiers and \u03b1t is an importance weight given to each \u201cweak\u201d classifier ht(x)\naccording to the performance during the iterative learning process (see [36]\nfor details). During learning focus is put on the training examples which were\nmost difficult to classify (this process is called \u201cboosting\u201d). As a result we\nobtain a final classifier that performs better than any of the weak classifiers\nalone.\nFollowing [36] we use simple weak classifiers based on a single-valued\nfeature fj(x)\nhj(x) =\n{\n1 : pjfj(x) < pj\u03b8j\n0 : otherwise,\n(10)\nwhere \u03b8j is a threshold and pj = {\u22121, 1} is a parity indicator determining\nthe direction of the inequality sign. During the training procedure optimal\nvalues of \u03b8j and pj are determined by minimising the number of misclassified\ntraining examples.\nIn addition, we use weak classifiers based on a weighted combination of\nfeatures fj(x) =\n\u2211G\ni=1 \u03b1ifi(x), where \u03b1i specifies the weight for an input\nfeature fi(x) (G = 2 in our experiments). We discretise possible weight\nvalues \u03b1i from the range {\u22121, 1} into Nf fractions. As a result we obtain\na sufficient number of different weak classifiers for selection by the boosting\nalgorithm.\n6. OCCLUSION HANDLING\nThe learned occlusion classifier can be used to improve tracking perfor-\nmance during occlusion. It is used in two different ways: first, to alter the\npenalising policy between the trackers (as described in Section 3), and sec-\nond, to re-identify occluded persons when they reappear.\nOur interaction model for tracking multiple persons allows tracking of\npeople that overlap to a certain degree. This is achieved by modifying the\ninteraction factor \u03c1 to prevent target fetching (i.e., to prevent two filters in\n13\nclose proximity from collapsing around the same tracked object). The pro-\nposed pairwise occlusion classifier is used to determine which of the tracking\nfilters is occluded. We consider two possible situations: partial occlusion and\ntotal occlusion. During partial occlusion, some part of a person is still visi-\nble. However, the gradient along the contour is disturbed, which can cause\na quick disappearance of the tracker. To avoid this we change the penalty\nequation to \u03c8 = e(\u2212\u03c1ogij), where the penalty term \u03c1o < \u03c1 is used to model\ninteractions between the partially occluded tracking filters. Interaction with\nother filters (non-overlapping with this pair) remains unchanged.\nAn update procedure for the tracker with improved occlusion handling\nis presented in Algorithm 1. The steps of the algorithm are explained as\nfollows: first thermal and colour likelihoods are measured for all particles in\neach tracking filter. Then the occlusion handling procedure is applied that\nfirst determines all overlaps between filters, then orders the overlapping filters\nusing the proposed AdaBoost algorithm. After ordering it is possible to de-\ntermine occluded and occluding filters, but also partial and total occlusions.\nThe penalty terms of all partially occluded filters are modified (\u03c1 = \u03c10). In\nthe last step of the algorithm the weights of all particles are calculated tak-\ning into account the respective penalty terms and the final estimates for each\ntracking filters are calculated.\nAlgorithm 1 Update procedure for tracking filters that are not totally oc-\ncluded.\nfor each person:\n- measure gradient likelihood pg and colour likelihood pc for each particle\nocclusion handling:\n- detect overlaps between persons\n- analyse occlusions between overlapping persons:\n- classify occlusions using AdaBoost\n- sort occluding\/occluded persons\n- assign partial\/total occlusions\n- adjust the penalty term \u03c1 for each person\nfor each person:\n- calculate weights wit for corresponding particle filter including any penalty\n- update particle filter\nWhen the head contour of a person becomes occluded the corresponding\n14\ntracker is considered to be totally occluded. This means that we can only\nguess the true position of this person. We assume that the state of the oc-\ncluded person is the same as the state of the occluding person. No penalty is\nconsidered for the occluded tracker. We keep particles of the totally occluded\ntracker for a short time (we use a value of 8 frames here) in situations when\nquick occlusions occur and the velocity of particles may allow resolution of\nthis occlusion. However after this time has elapsed the particles of the tracker\nare removed and the only information kept is the colour model. When a new\nperson is detected this information is used to match the colour model to all\noccluded trackers. If the colour model is most similar to the closest occluded\ntracker then the detected person is considered to be an occluded one. Other-\nwise the person is considered to be a new person. To avoid situations where\nthe occluded tracker stays forever behind the occluding one, we also specify\na maximum duration of occlusion (in our case 10 s). This minimises errors in\nthe case where an occluded person disappears from the scene in some other\nway (e.g., through a door or a corridor behind an occluding person) or in\ncases of missed assignments to newly detected persons.\n7. Experiments\n7.1. Evaluation\nOur system was tested on the data collected by the robot during several\nruns. We collected 11 tracks using a corridor following behaviour and 42\ntracks with a stationary robot resulting in 53 different tracks including 12\ndifferent persons (5607 images containing at least one person and 6769 images\nin total). The total count of marked-up persons was 10256 with 1289 cases\nof occlusions, which is around 13% of all cases. To obtain the ground truth\ndata we used a flood-fill segmentation algorithm corrected afterwards by hand\nusing the ViPER-GT tool [37]. We considered only a bounding box around a\nperson. The top and bottom edges were determined from the contours of the\nhead and feet while the sides were specified by the maximum width of the\ntorso (without arms). The cases when persons appeared too close (< 3m)\nto or too far (> 10m) from the robot were not taken into account. The\nsize of the bounding box was specified as 2 \u00b7 width and 3.5 \u00b7 height of the\nelliptic contour model, an approximation to the proportions of the human\nbody. Bounding boxes from the ground truth data are referred to as targets\nand those from the tracker as candidates.\n15\ndetection localisation\nrecall NR\nNT\n|AT\u2229AR|\n|AT |\nprecision NR\nNC\n|AT\u2229AR|\n|AC |\naccuracy 2\u00b7NR\nNT+NC\n2\u00b7|AT\u2229AR|\n|AT |+|AC |\nTable 1: Detection and localisation metrics.\nWe use two kinds of metrics that indicate the quality of the tracking pro-\ncedure: detection metrics (counting persons) and localisation metrics (area\nmatching). Each type of metric is further divided into three statistics: re-\ncall, precision and accuracy. Recall indicates true positives (\u201chits\u201d), precision\nindicates false alarms, and accuracy is a combination of both recall and pre-\ncision (see Table 1). These metrics allow thorough testing of the properties\nand performance of the tracker as in [37] and [38].\nA candidate is considered to be correctly detected if the overlap ratio\nbetween the candidate and target bounding boxes is greater than 50%. The\ndetection metrics take into account the number of correctly detected candi-\ndates NR in one frame and compare it with the number of targets NT and\nnumber of all candidates NC . The final result is a weighted average of all\nframes. This weighting corresponds to the number of persons at each frame.\nOtherwise frames with a single person would have the same influence on the\nfinal results as frames with more persons. The localisation metrics express\nrelations between areas corresponding to correctly detected candidates AR,\nall candidates AC and targets AT . The final result is a weighted average of\nall frames. All of the metrics are normalised to give percentages.\nThanks to the evaluation metrics, we could optimise all system parame-\nters based on the test data. As the performance criterion we chose an area\naccuracy metric that reflects the overall performance of the tracker. The\ninfluence of each parameter on the performance of the tracker was checked\nindependently. Experiments for each parameter value were repeated 10 times\nwith different random variations in the particle filter run with N = 1000 for\neach trial. The optimal values obtained for the most important system pa-\nrameters were as follows: \u03ba = 0.125, \u03b8 = 22, \u03bb = 50, \u03c1 = 2 and \u03c10 = 0.5.\n16\nrecall precision accuracy\n0\n20\n40\n60\n80\n100\ndetection metrics\nr a\nt e\n \n[ %\n]\nrecall precision accuracy\n0\n20\n40\n60\n80\nlocalisation metrics\nr a\nt e\n \n[ %\n]\ngradient\n+ colour\n+ occ. detector\ngradient\n+ colour\n+ occ. detector\nFigure 5: Detection and localisation metrics for tracking multiple persons without and\nwith colour information and with occlusion handling procedure.\n7.2. Training of the AdaBoost Classifier\nTo train the AdaBoost classifier the described thermal and colour features\nwere extracted from the collected data. The only cases considered were\nsituations when two or more people were overlapping. Moreover since the\nbehaviour of the tracker without proper occlusion handling is unpredictable\nafter a total occlusion occurs, only those examples that preceded the moment\nof the total occlusion were selected. During the occlusions, the colour models\nof the respective persons were not updated. In this way we obtained 121\npositive and 121 negative examples giving a total of 242 examples.\nWe created additional weak classifiers based on weighted sums of pairs\nof features with 20 fractions giving, in the case of all six thermal and colour\nfeatures used, 1200 new weak classifiers. We used 60% of randomly selected\ninput examples as a training set and the remaining part as a test set. Each\ntraining procedure was repeated 10 times.\n7.3. Results\n7.3.1. Tracking Results\nFig. 5 shows the tracking performance using only thermal gradient infor-\nmation, with additional colour information, and with both colour information\n17\nFigure 6: Selected thermal images from the sequence showing the output from the tracker\nbefore, during and after the occlusion of three simultaneously tracked persons. The bound-\ning boxes corresponding to occluded persons are marked by a dotted line. An exam-\nple video sequence os available at http:\/\/robots.lincoln.ac.uk\/users\/gcielniak\/\npapers\/multiperson results.html.\nand explicit occlusion handling. Each experiment was repeated 10 times with\ndifferent random variations in the particle filter for each trial using N = 1000\nparticles per filter. Both the detection and localisation metrics indicate a sig-\nnificant improvement when using additional colour information (p < 0.01).\nThis leads to more precise estimates and decreases the number of cases where\nthe tracker loses track of a person. However the overall accuracy (84.2% in\ndetection and 68.7% in localisation) is affected by low recall values. Adding\nthe occlusion detector gives an increase of 6.8% in area recall metrics and\n3.1% in area accuracy metrics. It is important to notice that the presented\nresults are calculated from all frames in the data set. The proportion of\nframes featuring occlusion situations to the total number of frames (in our\ncase being 13%) affects the absolute result values. The differences in the\nperformance should be more pronounced with the higher values of this ratio.\nExamples of the output from the tracker can be seen in Fig. 6.\n7.3.2. Occlusion Classification\nThe strong classifier learned from the combination of thermal and colour\nfeatures was able to predict occlusions correctly in around 89% of all cases\n18\nFeature type Results [%]\nthermal 76.4\u00b1 4.5\ncolour 69.0\u00b1 1.9\nboth 89.4\u00b1 2.5\nTable 2: Classification results for different feature types.\nCombination of features Results [%] T total\nsingle features 74.9\u00b1 4.9 6\nweighted pairs 89.4\u00b1 2.5 1206\nweighted triplets 89.4\u00b1 1.8 129206\nTable 3: Classification results for different combination of features to create weak classifiers\n(resulting in T total weak classifiers).\n(see Table 2). This gives a significant advantage over the results obtained\nwhen thermal and colour features were used separately (p < 0.01). Thermal\nfeatures provided significantly better results than colour features alone.\nTable 3 shows results for different methods of combining features into\nweak classifiers. The comparatively bad results when using single features\nare caused by the low number of weak classifiers. The proposed method of\nusing a weighted combination of pairs of features increased the performance of\nthe final classifier by around 15%. We also made tests with weighted triplets\nof features for comparison. Despite the much higher number of possible\nweak classifiers the difference in performance compared to weighted pairs\nwas not found to be significant (based on a paired t-test with confidence\nlevel p = 0.01).\nFrom the results presented in Table 4 we can get an impression about\nhow much information is provided by a single feature. The most reliable\nfeatures are the top of a person\u2019s head, colour similarity of the whole region\nand of the non-overlapping area. Weak classifiers based on combinations\nof these features had the highest importance (see Table 5). Other features\nalso contributed to the final classifier (e.g., the position of the bottom of\nthe elliptic model) even though their individual performance was relatively\npoor. The influence of the 4 best weak classifiers is considerable and gives a\n19\nSingle feature Results [%]\nstrength 50.1\u00b1 4.9\ntop 73.0\u00b1 3.9\nbottom 56.5\u00b1 4.3\ncolour 67.6\u00b1 3.0\ncolour o 45.6\u00b1 2.7\ncolour no 67.4\u00b1 2.9\nTable 4: Classification results for single features (colour o and colour no stand for the\ncolour similarity of the overlapping and non-overlapping areas respectively).\nperformance of around 85% compared to 89% when using the 10 best weak\nclassifiers.\n7.3.3. Processing Time\nTable 6 presents the average processing time needed for calculation of\n1000 samples when using different colour representations. It takes about\ntwo times longer to calculate one step of the tracking procedure when using\nall three moments compared to the tracker based on thermal information\nonly (around 30Hz on a 2.00 GHz processor when using 1000 samples). A\ngood trade-off between time requirements and performance of the tracker\nfor our set-up is a representation using just the first moment of the colour\ndistribution (46% more time compared to the gradient based tracker). The\noverall performance of the tracker based on this representation is about 2%\nlower than the variant using the three colour moments. When tracking mul-\ntiple persons, additional processing time is required for calculation of penalty\nterms for the detector and individual tracking filters. In our case tracking\none person required around 8% extra time for the detector and in the case\nof four persons around 36% extra time is needed for calculation of penalty\nterms between the trackers.\n8. Conclusions and Future Work\nWe presented a people tracking system that uses a combination of ther-\nmal and colour information to robustly track persons. While thermal vision\nis good for detecting people, it can be very difficult to keep track of which ob-\nservation corresponds to which person, due to the unpredictable appearance\n20\nPlace Weight of a feature Results\nstrength top bottom colour colour o colour no [%]\n1 -0.05 - - - - 1.00 79.0\u00b1 4.7\n2 - -0.05 - 1.00 - - 80.1\u00b1 4.9\n3 - -1.00 0.45 - - - 83.4\u00b1 3.7\n4 - -0.75 1.00 - - - 85.1\u00b1 2.9\n5 - - 0.05 - - 1.00 84.9\u00b1 3.0\n6 - -0.80 1.00 - - - 87.0\u00b1 3.4\n7 - - 0.10 - - 1.00 86.7\u00b1 2.5\n8 -0.55 1.00 - - - - 88.6\u00b1 2.4\n9 -1.00 0.05 - - - - 89.8\u00b1 2.3\n10 - - -0.05 1.00 - - 89.7\u00b1 2.6\nTable 5: 10 best weak classifiers with their respective weights (colour o and colour no\nstand for the colour similarity of the overlapping and non-overlapping areas respectively).\nThe classification results indicate the performance improvement after adding each weak\nclassifier.\nPlatform Model\ngradient colour I colour III\n[ms] [ms] [ms]\nrobot int. image - 5.1 16.1\n0.85 GHz 1000 samples 33.4 50.2 68.8\nmodern PC int. image - 2.1 4.9\n2.00 GHz 1000 samples 13.5 17.7 25.9\nTable 6: Average processing time needed to calculate 1000 samples using different mea-\nsurement models. \u201cColour I\u201d and \u201ccolour III\u201d correspond to a colour representation using\nthe first moment and the first three moments respectively.\n21\nand social behaviour of humans. To address these problems the presented\ntracking system uses additional information from the colour camera. An\nadaptive colour model is incorporated into the measurement model of the\ntracker to improve data association. For this purpose an efficient integral\nimage based method is used to maintain the real-time performance of the\ntracker.\nTo deal with occlusions, the system uses an explicit method that first\ndetects situations where people occlude each other. This is realised by a new\napproach based on a machine learning classifier for pairwise comparison of\npersons using both thermal and colour features provided by the tracker. This\ninformation is then incorporated into the tracker for occlusion handling and\nto resolve situations when persons reappear in a scene.\nWhile the thermal camera has its own strengths (the system can work in\nbadly lit or completely dark environments), we believe the ultimate system\nfor people tracking should combine different modalities and their strengths to\nachieve better performance. The proposed approach can be easily extended\nto include other features (e.g. depth from a stereo-camera) that could further\nimprove the tracking performance.\nWe believe that the question of how to handle occlusions is impossible to\nanswer in a general way, i.e. independently of a particular application. How-\never our solution demonstrates that it is plausible to deal with occlusions\nto some extent and through experiments we showed that this increases the\noverall performance of the tracker. Such a solution has obvious pitfalls that\nshould be considered in future work, such as proper handling of classification\nerrors, wrong assignments after occlusions, uniformly dressed people, etc. A\nfurther approach to improve detection of occlusions would be to incorporate\ntracking of individual body parts [23], though this would increase the com-\nplexity and computational cost of the solution. The mobile robot itself could\nbe used to check if the occluded person is really behind another person by\ntaking appropriate actions. Recognition of human behaviour could also help\nto solve this kind of problem.\nReferences\n[1] Y. Freund, R. E. Schapire, A decision-theoretic generalization of on-line\nlearning and an application to boosting, in: Computational Learning\nTheory: Eurocolt, Springer-Verlag, 1995, pp. 23\u201337.\n22\n[2] G. Cielniak, T. Duckett, A. Lilienthal, Improved data association and\nocclusion handling for vision-based people tracking by mobile robots, in:\nProc. of the IEEE\/RSJ International Conference on Intelligent Robots\nand Systems (IROS), San Diego, CA, USA, 2007, pp. 3436\u20133441.\n[3] G. Cielniak, People tracking by mobile robots using thermal and colour\nvision, Ph.D. thesis, O\u00a8rebro University (April 2007).\n[4] T. Wilhelm, H. J. Bo\u00a8hme, H. M. Gross, Sensor fusion for vision and sonar\nbased people tracking on a mobile service robot, in: Int. Workshop on\nDynamic Perception, Bohum, Germany, 2002, pp. 315\u2013320.\n[5] L. Bre`thes, P. Menezes, F. Lerasle, J. Hayet, Face tracking and hand\ngesture recognition for human-robot interaction, in: Proc. IEEE ICRA,\nNew Orleans, LA, USA, 2004, pp. 1901\u20131906.\n[6] R. Mun\u02dcoz Salinas, E. Aguirre, M. Garc\u00b4\u0131a-Silvente, People detection and\ntracking using stereo vision and color, Image and Vision Computing\n25 (6) (2007) 995\u20131007.\n[7] A. Ess, B. Leibe, K. Schindler, L. Van Gool, A mobile vision system for\nrobust multi-person tracking, in: Proc. of the IEEE Computer Vision\nand Pattern Recognition, 2008.\n[8] M. Bertozzi, A. Broggi, P. Grisleri, T. Graf, M. Meinecke, Pedestrian\ndetection in infrared images, in: Proc. of the IEEE Intelligent Vehicles\nSymposium, Columbus, USA, 2003, pp. 662\u2013667.\n[9] H. Nanda, L. Davis, Probabilistic template based pedestrian detection\nin infrared videos, in: IEEE Intelligent Vehicle Symposium, Versailles,\nFrance, 2002.\n[10] A. Garcia-Cerezo, A. Mandow, J. Martinez, J. Gomez-de Gabriel,\nJ. Morales, A. Cruz, A. Reina, J. Seron, Development of ALACRANE:\nA mobile robotic assistance for exploration and rescue missions, in:\nProc. of the IEEE International Workshop on Safety, Security and Res-\ncue Robotics, Rome, Italy, 2007.\n[11] M. Andriluka, M. Friedmann, S. Kohlbrecher, J. Meyer, K. Petersen,\nC. Reinl, P. Schau\u00df, P. Schnitzspan, A. Armin Strobel, D. Thomas,\n23\nO. von Stryk, Robocuprescue 2009 - robot league team: Darmstadt res-\ncue robot team (germany), Tech. rep., Technische Universitt Darmstadt\n(2009).\n[12] W. Fehlman, M. Hinders, Mobile Robot Navigation with Intelligent In-\nfrared Image Interpretation, Springer London, 2009.\n[13] D. Schulz, W. Burgard, D. Fox, A. B. Cremers, Tracking multiple mov-\ning objects with a mobile robot, in: Proc. IEEE CVPR, 2001.\n[14] D. B. Reid, An algorithm for tracking multiple targets, in: Proc. IEEE\nTrans. Autom. Control, Vol. 6, 1979, pp. 843\u2013854.\n[15] Y. Bar-Shalom, T. Fortmann, Tracking and Data Association, Academic\nPress, 1988.\n[16] K. Okuma, A. Taleghani, N. De Freitas, J. J. Little, D. G. Lowe,\nA boosted particle filter: Multitarget detection and tracking, in:\nProc. ECCV, Vol. 1, 2004, pp. 28\u201339.\n[17] L. D. Stone, T. L. Corwin, C. A. Barlow, Bayesian Multiple Target\nTracking, Artech House, 1999.\n[18] S. S. Intille, J. Davis, A. Bobick, Real-time closed-world tracking, in:\nProc. IEEE CVPR, 1997, pp. 697\u2013703.\n[19] A. Mittal, L. S. Davis, M2tracker: A multi-view approach to segmenting\nand tracking people in a cluttered scene using region-based stereo, in:\nProc. IEEE CVPR, 2002, pp. 18\u201336.\n[20] C. R. Wren, A. Azarbayejani, T. Darrell, A. P. Pentland, Pfinder: Real-\ntime tracking of the human body, IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence 19 (7) (1997) 780\u2013785.\n[21] S. Khan, M. Shah, Tracking people in presence of occlusion, in: Asian\nConference on Computer Vision, Taipei, Taiwan, 2000.\n[22] M. Isard, J. MacCormick, Bramble: A Bayesian multiple-blob tracker.,\nin: Proc. of the International Conference on Computer Vision, Vol. 2,\nVancouver, British Columbia, Canada, 2001, pp. 34\u201341.\n24\n[23] A. Argyros, M. Lourakis, Real-time tracking of multiple skin-colored\nobjects with a possibly moving camera, in: Proc. of the 8th European\nConference on Computer Vision, Prague, Czech Republic, 2004, pp. Vol\nIII: 368\u2013379.\n[24] A. Elgammal, L. S. Davis, Probabilistic framework for segmenting peo-\nple under occlusion, in: Proc. of the International Conference on Com-\nputer Vision, Vancouver, Canada, 2001.\n[25] A. Senior, A. Hampapur, Y.-L. Tian, L. Brown, S. Pankanti, R. Bolle,\nAppearance models for occlusion handling, in: Proceedings of the 2nd\nIEEE Workshop on Performance Evaluation of Tracking and Surveil-\nlance, Kauai, Hawaii, USA, 2001.\n[26] S. Mckenna, S. Jabri, Z. Duric, A. Rosenfeld, Tracking groups of people\n1 (80) (2000) 42\u201356.\n[27] W. Zajdel, Z. Zivkovic, B. J. A. Kro\u00a8se, Keeping track of humans: Have i\nseen this person before?, in: Proc. IEEE ICRA, Barcelona, Spain, 2005.\n[28] N. J. Gordon, D. J. Salmond, A. F. M. Smith, Novel approach to\nnonlinear\/non-Gaussian Bayesian state estimation, Proc. Inst. Elect.\nEng. F 140 (2) (1993) 107\u2013113.\n[29] B. Ristic, S. Arulampalam, N. Gordon, Beyond the Kalman Filter -\nParticle Filters for Tracking Applications, Artech House, Boston, 2004.\n[30] Z. Khan, T. Balch, F. Dellaert, An MCMC-based particle filter for track-\ning multiple interacting targets, in: Proc. ECCV, 2004.\n[31] P. Li, H. Wang, Probabilistic object tracking based on machine learning\nand importance sampling, in: Proc. of the Iberian Conference on Pattern\nRecognition and Image Analysis, Vol. 1, 2005, pp. 161\u2013167.\n[32] R. Karlsson, F. Gustafsson, Monte Carlo data association for multiple\ntarget tracking, in: In IEEE Target tracking: Algorithms and applica-\ntions, The Netherlands, 2001.\n[33] M. Isard, A. Blake, Condensation \u2013 conditional density propagation for\nvisual tracking 29 (1) (1998) 5\u201328.\n25\n[34] M. A. Stricker, M. Orengo, Similarity of color images, in: Storage and\nRetrieval for Image and Video Databases, 1995, pp. 381\u2013392.\n[35] M. Swain, D. Ballard, Color indexing 7 (1991) 11\u201332.\n[36] P. Viola, M. Jones, Rapid object detection using a boosted cascade of\nsimple features, in: Proc. IEEE CVPR, 2001.\n[37] D. S. Doermann, D. Mihalcik, Tools and techniques for video perfor-\nmance evaluation, in: Proc. ICPR, Vol. 4, Barcelona, Spain, 2000, pp.\n4167\u20134170.\n[38] K. Smith, D. Gatica-Perez, J. M. Odobez, S. Ba, Evaluating multi-object\ntracking, in: Workshop on Empirical Evaluation Methods in Computer\nVision, San Diego, CA, USA, 2005.\nGrzegorz Cielniak is a lecturer in Computer Science at the University of\nLincoln, UK. He obtained his Ph.D. in Computer Science from O\u00a8rebro Uni-\nversity, Sweden in 2007 and M.Sc. from Wroclaw University of Technology,\nPoland in 2000. The Ph.D thesis addresses a problem of real-time people\ntracking for mobile robots. His research interests include mobile robotics,\nvision systems, people tracking, AI and flying robots.\n26\nTom Duckett is a Reader in Computer Science at the University of Lincoln,\nwhere he is also Director of the Centre for Vision and Robotics Research. He\nwas formerly a docent (Associate Professor) at O\u00a8rebro University, where he\nwas leader of the Learning Systems Laboratory within the Centre for Applied\nAutonomous Sensor Systems. He obtained his Ph.D. in Computer Science\nfrom Manchester University in 2001, M.Sc. with distinction in Knowledge\nBased Systems from Heriot-Watt University in 1995 and B.Sc. (Hons.) in\nComputer and Management Science from Warwick University in 1991, and\nhas also studied at Karlsruhe and Bremen Universities. His research interests\ninclude mobile robotics, navigation, machine learning, AI, computer vision,\nand sensor fusion for perception-based control of autonomous systems.\nAchim Lilienthal is a docent (associate professor) at the AASS Research\nCenter in O\u00a8rebro, Sweden, where he is leading the Learning Systems Lab. His\nmain research interests are mobile robot olfaction, robot vision, robotic map\nlearning and safe navigation systems. Achim Lilienthal obtained his Ph.D.\nin computer science from Tu\u00a8bingen University, Germany and his M.Sc. and\nB.Sc. in Physics from the University of Konstanz, Germany. The Ph.D.\n27\nthesis addresses gas distribution mapping and gas source localisation with\na mobile robot. The M.Sc. thesis is concerned with an investigation of the\nstructure of (C60)\n+\nn clusters using gas phase ion chromatography.\n28\n"}