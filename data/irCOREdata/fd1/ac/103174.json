{"doi":"10.1109\/IJCNN.2008.4634184","coreId":"103174","oai":"oai:epubs.surrey.ac.uk:3032","identifiers":["oai:epubs.surrey.ac.uk:3032","10.1109\/IJCNN.2008.4634184"],"title":"A Behavioral Model of Sensory Alignment in the Superficial and Deep Layers of the Superior Colliculus","authors":["Casey, MC","Pavlou, A"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2008","abstract":"The ability to combine sensory information Is an important attribute of the brain. Multisensory integration in natural systems suggests that a similar approach in artificial systems may be important. Multisensory integration is exemplified in mammals by the superior colliculus (SC), which combines visual, auditory and somatosensory stimuli to shift gaze. However, although we have a good understanding of the overall architecture of the SC, as yet we do not fully understand the process of integration. While a number of computational models of the SC have been developed, there has not been a larger scale implementation that can help determine how the senses are aligned and integrated across the superficial and deep layers of the SC. In this paper we describe a prototype implementation of the mammalian SC consisting of self-organizing maps linked by Hebbian connections, modeling visual and auditory processing in the superficial and deep layers. The model is trained on artificial auditory and visual stimuli, with testing demonstrating the formation of appropriate spatial representations, which compare well with biological data. Subsequently, we train the model on multisensory stimuli, testing to see if the unisensory maps can be combined. The results show the successful alignment of sensory maps to form a multisensory representation. We conclude that, while simple, the model lends itself to further exploration of integration, which may give insight into whether such modeling is of benefit computationally","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:3032<\/identifier><datestamp>\n      2017-10-31T14:07:36Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:436F6D707574696E67<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/3032\/<\/dc:relation><dc:title>\n        A Behavioral Model of Sensory Alignment in the Superficial and Deep Layers of the Superior Colliculus<\/dc:title><dc:creator>\n        Casey, MC<\/dc:creator><dc:creator>\n        Pavlou, A<\/dc:creator><dc:description>\n        The ability to combine sensory information Is an important attribute of the brain. Multisensory integration in natural systems suggests that a similar approach in artificial systems may be important. Multisensory integration is exemplified in mammals by the superior colliculus (SC), which combines visual, auditory and somatosensory stimuli to shift gaze. However, although we have a good understanding of the overall architecture of the SC, as yet we do not fully understand the process of integration. While a number of computational models of the SC have been developed, there has not been a larger scale implementation that can help determine how the senses are aligned and integrated across the superficial and deep layers of the SC. In this paper we describe a prototype implementation of the mammalian SC consisting of self-organizing maps linked by Hebbian connections, modeling visual and auditory processing in the superficial and deep layers. The model is trained on artificial auditory and visual stimuli, with testing demonstrating the formation of appropriate spatial representations, which compare well with biological data. Subsequently, we train the model on multisensory stimuli, testing to see if the unisensory maps can be combined. The results show the successful alignment of sensory maps to form a multisensory representation. We conclude that, while simple, the model lends itself to further exploration of integration, which may give insight into whether such modeling is of benefit computationally.<\/dc:description><dc:date>\n        2008<\/dc:date><dc:type>\n        Conference or Workshop Item<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        attached<\/dc:rights><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/3032\/2\/Casey_Pavlou_IJCNN2008.pdf<\/dc:identifier><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/3032\/4\/licence.txt<\/dc:identifier><dc:identifier>\n          Casey, MC and Pavlou, A  (2008) A Behavioral Model of Sensory Alignment in the Superficial and Deep Layers of the Superior Colliculus  In: International Joint Conference on Neural Networks, 2008-06-01 - 2008-06-08, Hong Kong, PEOPLES R CHINA.     <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/IJCNN.2008.4634184<\/dc:relation><dc:relation>\n        10.1109\/IJCNN.2008.4634184<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/3032\/","http:\/\/dx.doi.org\/10.1109\/IJCNN.2008.4634184","10.1109\/IJCNN.2008.4634184"],"year":2008,"topics":[],"subject":["Conference or Workshop Item","PeerReviewed"],"fullText":"  \n  \nAbstract\u2014The ability to combine sensory information is an \nimportant attribute of the brain.  Multisensory integration in \nnatural systems suggests that a similar approach in artificial \nsystems may be important.  Multisensory integration is \nexemplified in mammals by the superior colliculus (SC), which \ncombines visual, auditory and somatosensory stimuli to shift \ngaze.  However, although we have a good understanding of the \noverall architecture of the SC, as yet we do not fully \nunderstand the process of integration.  While a number of \ncomputational models of the SC have been developed, there has \nnot been a larger scale implementation that can help determine \nhow the senses are aligned and integrated across the superficial \nand deep layers of the SC.  In this paper we describe a \nprototype implementation of the mammalian SC consisting of \nself-organizing maps linked by Hebbian connections, modeling \nvisual and auditory processing in the superficial and deep \nlayers.  The model is trained on artificial auditory and visual \nstimuli, with testing demonstrating the formation of \nappropriate spatial representations, which compare well with \nbiological data.  Subsequently, we train the model on \nmultisensory stimuli, testing to see if the unisensory maps can \nbe combined.  The results show the successful alignment of \nsensory maps to form a multisensory representation.  We \nconclude that, while simple, the model lends itself to further \nexploration of integration, which may give insight into whether \nsuch modeling is of benefit computationally. \nI. INTRODUCTION \nHE ability to fuse, process and act upon sensory \ninformation is an important attribute of humans and \nanimals.  Traditionally, low level processing of sensory \ninformation, such as for vision, audition and touch, were \nthought to occur in isolation from other senses.  However, \nthere is now a wealth of evidence suggesting that even low \nlevel processing is multisensory [1,2], with perhaps \nunisensory processing being the rarity [3].  This change in \nour understanding of natural cognitive systems has \nimplications for artificial systems.  First, since \ncomputational techniques are an established tool for \nexploring models of cognition, these must also be able to \nintegrate sensory stimuli in order to prove effective.  \nSecond, such an integrative approach may help us overcome \nthe limitations of existing computational paradigms (cf. \nspeaker independent speech recognition [4]). \n \nManuscript received December 14, 2007.  \nM. C. Casey is with the Department of Computing, University of Surrey, \nGuildford, Surrey, GU2 7XH, UK (corresponding author phone: +44 (0) \n1483 689635; fax: +44 (0) 1483 686051; e-mail: m.casey@ surrey.ac.uk). \nA. Pavlou is with the Department of Computing, University of Surrey, \nGuildford, Surrey, GU2 7XH, UK (e-mail: a.pavlou@ surrey.ac.uk). \nMultisensory integration in mammalian brains is \nexemplified by the superior colliculus (SC) [5].  While the \nSC was originally thought to process only visual stimuli, \nstudies have demonstrated that it combines visual, auditory \nand somatosensory topographic maps into a multisensory \nrepresentation [1].  The integration of these stimuli then \ncauses gaze shifts (the combination of head movements and \neye saccades) [5].  In the SC, stronger reactions are obtained \nto multisensory stimuli compared to just unisensory stimuli.  \nThis multisensory enhancement (and indeed suppression) \noccurs in deep layers of the SC, and is controlled by \ndescending afferents from the cortex (anterior ectosylvian \nsulcus and the lateral suprasylvian sulcus) [2].  However, \nalthough the general architecture of the SC is well \nestablished, as yet we do not fully understand the process of \nintegration and the role of cortical feedback [6]. \nWhile a multisensory model of the SC may help answer \nthis question of how integration is influenced in natural \nsystems, such a model may also help to establish new ways \nin which artificial sensory inputs can be combined in a \ncomputational system.  Previous computational models of \nthe SC have focused on understanding the outputs of the SC \nfor saccades (for example [7]).  Such models have been used \nto improve our understanding of multisensory enhancement, \nsuppression and saccades, but these models necessarily \nignore the wider behavioral aspects of the SC, such as \nsensory alignment in both the superficial and deep layers. \nIn this paper, we describe a computational model of the \nSC that integrates representations of visual and auditory \nstimuli, simulating both the superficial and deep layers, and \naligns these unisensory representations.  While this is a \nsimple behavioral model constructed using a number of \nassumptions about the inputs, it demonstrates how different \nsensory maps may be integrated through a process of \nlearning in order to translate sensory coordinate spaces into \na single multisensory representation.  Here, in order to \nexplore whether integration can be learnt, we make the \nsimplifying assumption that integration is affected through \nassociative learning without cortical feedback.  While this \nhypothesis is perhaps a limitation of the model, we \ndemonstrate however that an emergent property of this \nintegration is a simplified form of multisensory \nenhancement, albeit not as capable as some previous \ndynamic models.  This early prototype provides insight into \nhow larger behavioral models of brain function can be \ndeveloped, and at the same time successfully simulates \nbehavior in the SC that compares well to biology. \nA Behavioral Model of Sensory Alignment in the Superficial and \nDeep Layers of the Superior Colliculus \nMatthew C. Casey and Athanasios Pavlou \nT \n  \nIn section II of this paper, we review past computational \nmodels of the SC to motivate our method.  In section III we \ndescribe the model. In section IV we evaluate the model\u2019s \noutputs compared with the biological evidence.  In section V \nwe conclude and summarize the insights gained. \nII. MODELING THE SUPERIOR COLLICULUS \nThe SC has been characterized as a laminated structure \nwith superficial and deep layers [5].  The superficial layers \nreceive visual stimuli directly from the retina and visual \ncortex.  The deep layers receive auditory and somatosensory \nstimuli, but respond to multisensory signals to produce a \nmotor output via so called burst and buildup neurons.  Of \nparticular interest is that the different layers of the SC form \nmutually aligned spatial sensory maps for the different \nmodalities, which appears crucial for multisensory \nintegration in that the alignment allows the maps to be \ncombined and matched with appropriate motor outputs for \neye, head and body orienting.  The process of alignment of \nsensory and motor maps occurs during development of the \nSC to form a coordinate transformation, with in particular, \nvision being used to calibrate auditory responses (cf. [8] for \na model of the inferior colliculus). \nThere have been several models of the SC developed that \nfocus on replicating the functional neuronal saccadic output \nof the SC.  For example, Grossberg et al [7] used Adaptive \nResonance Theory (ART) to model the burst and buildup \nresponses of the deep SC.  Of note is that their model learnt \nthe transformation between auditory, visual and motor \nresponses using associative learning, demonstrating both \nmultisensory enhancement and suppression.  Similar \ndynamic models of the physiology of the deep SC have been \ndeveloped to help determine how the SC controls \nmovement.  Here, models have been used to simulate \nparallel pathways between the SC and cerebellum [9], the \ncompetitive combination of exogenous (sensory) and \nendogenous (voluntary) information to simulate response \ntimes for saccade initiation [10] and antisaccades [11], and \nensemble-coding that adds intended saccade trajectory \ninformation to each output spike of the SC [12]. \nIn contrast to these physiologically motivated models, \ncomputational paradigms have also been used to explore \nhow neural information is encoded for multisensory \nenhancement and suppression, using Bayesian and \nperceptron models [13].  However, despite being of interest \nbecause of the more abstract, computational approach, these \nmodels have still focused on the deep SC.  Unlike such past \nmodels, in this paper we simulate the behavior of both the \nsuperficial and deep layers of the SC to explore sensory \nalignment and integration.  We take motivation from [7], in \nwhich the associations between representations were learnt, \nbut add the formation of  sensory topographic maps.  For \nease of development, we use simple and well understood \nrate-coded algorithms [14,15], rather than the more complex \nneural dynamic models.  We hope to gain insight into \nwhether such techniques will prove beneficial for larger-\nscale implementations of brain function. \nIII. A BEHAVIORAL MODEL OF SENSORY ALIGNMENT \nOur model consists of sensory topographic maps that are \naligned and linked together to form a multisensory space.  \nWe simplify our approach in two ways.  First, we consider \nonly visual and auditory stimuli within an approximate \nazimuth and elevation range for humans, but do not consider \ntouch.  Here, we use an auditory space that surrounds the \nhead completely (elevation [-180, 180], azimuth [-90, 90]), \nat a fixed depth.  The visual space is a subset of this \n(elevation [-65, 65], azimuth [-90, 90]), allowing us to \nevaluate both single modality stimuli locations (auditory \nonly), multimodal locations (auditory and visual) as well as \ncoincident (same location multisensory) and non-coincident \n(different location multisensory) inputs. \nSecond, our model of the biology is simplified by using \nrate-coded neural network models.  While rate-coded \nmodels are highly abstract, they provide a useful prototyping \ntool for computational modeling with some degree of \nplausibility.  For example, Hebbian learning [15] is an \nestablished method that is both biologically supported and \nimplemented in a range of rate-coded algorithms.  In \nparticular, Kohonen\u2019s Self-organizing Map (SOM) [14] has \nbecome an established method for forming topographic \nmaps.  In our model we use two SOMs to form topographic \nmaps of the auditory and visual spaces.  We then link these \ntogether using Hebbian connections to align the visual and \nauditory representations (Fig. 1). \nA. Input Representation \nAs input to our model we use a simplified representation \nof visual and auditory space that corresponds to a spatial \nstimulus in each modality.  Although there is limited \nunderstanding of the actual input to the SC, it is clear that \nthe SC translates inputs into spatial topographic maps in \norder to determine the co-ordinates for any gaze shift.  For \nour model we use Gaussian activity patterns (see for \nexample [16]) to achieve this for both the auditory and \nvisual modalities (Fig. 2).  Each pattern is centered at \nregular discrete intervals to give the value of an input x at \nelevation i and azimuth j as: \nHebbian Linkage\nVisual\nSOM\nMultisensory\nOutput\nAuditory\nSOM\n \nFig. 1. Block diagram of the SC model. The model consists of a SOM for \nboth the visual and auditory senses, connected together via Hebbian links. \nThe linkage acts as a coordinate translator of the visual SOM to the auditory \nSOM. These are then combined to form the multisensory map. \n  \n\u239f\u239f\u23a0\n\u239e\n\u239c\u239c\u239d\n\u239b \u2212\u2212\n= 2\n22\n\u03c3\u03bb\nji\nij ex  (1) \nwhere \u03bb is the amplitude, and \u03c3 is the bandwidth. \nThe use of this representation offers a flexible and simple \nway of testing feature detection [17]. At the same time it \nallows us to define receptive field refinement by adjusting \nthe amplitude and bandwidth of the Gaussian pattern. For \nboth auditory and visual inputs the amplitude and bandwidth \nvary between two pairs of values corresponding to a dense \n(fovea) and a non-dense region (peripheral).  For the denser \nfiring regions, greater amplitude with smaller width is used \nto capture the refined representation of the stimuli occurring \nwithin these spatial windows (Fig. 2). \nB. Sensory Maps \nThe topographic organization of stimuli is a property met \nin brain structures across a number of species [1]. \nKohonen\u2019s SOM [14] is a biologically motivated algorithm \nthat builds upon earlier studies that examined spatial \nordering in sets of feature-sensitive cells [18].  We use SOM \nto form our auditory and visual topographic maps, using the \nstandard weight update rule.  The output of a map is \ncalculated as the normalized inverse distance between an \ninput vector xaud and each of the weight vectors audnw , in this \ncase for the auditory map, for each neuron n, with \naudNn \u2264\u22641 , so that: \naud\nn\naud\naud\nn\nwx\nu \u2212=\n1  (2) \naudaud\naudaud\nnaud\nn\nuy \u03b1\u03b2\n\u03b1\n\u2212\n\u2212=  (3) \nIn this way we highlight similarly responding areas of the \nmap for the Hebbian links.  The normalization parameters \nare chosen to be the minimum \u03b1aud and maximum \u03b2aud values \nof audnu obtained from the map\u2019s unisensory training for \ncalibration.  The same equations hold for the visual map \nwith the appropriate change of superscripts. \nC. Multisensory Translation \nOur output multisensory space has a spatial range that \ncorresponds to the superset of elevation and azimuth values \nfor the sensory inputs, which in this case corresponds to the \nauditory space.  Therefore, to form a multisensory \nrepresentation, we combine the outputs of the auditory SOM \nwith that of the translated outputs of the visual SOM.  The \ntranslation is achieved by learning the association between \nthe output of the auditory and visual maps for coincident \nstimuli at the same spatial location (a multisensory \nstimulus).  To form the association between the map \nresponses we use Hebbian learning [15].  Here each neuron \nin the visual map is connected to each neuron in the auditory \nmap. \nThe output from the links are the normalized weighted \nsummations of the outputs from the visual map visy  \nmultiplied by the corresponding connection weight linkniw : \n\u2211\n=\n=\nvisN\ni\nvis\nini\nlink\nn ywu\n1\n (4) \nDuring training, the link weights are updated using the \nactivity product rule: \naud\nn\nvis\ninini yyww \u03b7+=\u2032  (5) \nusing learning rate \u03b7.  These are then normalized to have \nunit magnitude to prevent exponential growth: \nni\nni\nni w\nww \u2032\n\u2032=  (6) \nOnce trained, the output from the links are normalized in \nthe same way for the sensory map outputs, using the \nminimum \u03b1link and maximum \u03b2link values of linknu obtained \nfrom the multisensory training data. \nlinklink\nlinklink\nnlink\nn\nuy \u03b1\u03b2\n\u03b1\n\u2212\n\u2212=  (7) \nFinally, the multisensory output is formed from the \nsummation of the normalized auditory map and visual link \noutputs.  Normalization of the values is used here to equally \nbias the contribution of each modality: \nlinkaudms yyy +=  (8) \nIV. EXPERIMENTS AND EVALUATION \nTo evaluate the model, we explore the ability of the \nSOMs to represent distinct sensory inputs spatially, and then \nhow each of these representations can be combined using \nHebbian learning to give a multisensory space. All \nexperiments were carried out using Matlab (version \n7.3.0.298) and the SOM Toolbox [19] 1. \n Unisensory training took place using two independently \ngenerated data sets for each of the SOMs.  The data sets \nused Gaussian pattern activations centered in random \nlocations (uniformly distributed) within the auditory and \nvisual spaces. The selection of training data used varied \ndepending upon the input space, with additional data \n \n1 Matlab source and experimental data files for this work can be found at \nhttp:\/\/www.cs.surrey.ac.uk\/BIMA\/People\/M.Casey\/software.html. \nAzimuthElevation\nA\nm\nplitude\n \nFig. 2. Sample auditory input with 7 individual patterns shown. Each \nGaussian pattern represents a single input for the auditory SOM. The central \n3 peaks are inside the dense region, and hence are less wide and with a \nhigher peak than the remaining patterns outside this region. \n  \nselected explicitly for each of the dense regions to ensure \ngood coverage of stimuli.  For a given input, discrete values \nfor the Gaussian were calculated using a uniform elevation \nand azimuth step size. \nFor multisensory training of the links, datasets were \ngenerated for both the auditory and visual space using a \nsingle set of randomly selected centers (uniformly \ndistributed). The centers were selected from the range of \ncoordinates from the visual space to ensure that coincident \nstimuli were presented to both the auditory and visual maps \nto allow an association to form between the outputs. \n For both unisensory and multisensory training phases, \nrandomly selected testing data were generated using the \nsame specifications as for the training data.  Details of the \ntraining and testing data sets are presented in Table I. \nA. Unisensory maps \n The sizes of the auditory and visual SOMs were selected \nto be computationally efficient, although being \napproximately in proportion with the corresponding input \nspace (Table II).  After training each SOM individually on \nthe training data for 1000 epochs, both maps were tested on \nthe testing data.  The results show that both SOMs organized \nthe inputs based upon spatial location, and in particular, \nlarger regions of the maps correspond to the dense areas. \nFig. 3(a) shows the auditory SOM U-matrix output \n(grayscale background), overlaid with the best matching \nunits for different azimuth strips. On the lower part of the \nmap, clusters of all inputs centered within an azimuth of [30, \n64.9] and [65, 90] (dark blue and blue) were observed. At \nthe center of the map, only clusters of azimuth strips [-0.9, \n29] and [-29.9, -1] were found. These clusters (green, dark \ngreen) comprised approximately 55% of the auditory SOM \nand included all inputs centered within the dense area, which \nis only 33% of the total input space, hence showing an \nincreased representation within the map corresponding with \nincreased acuity. Finally, the last two clusters corresponded \nto azimuth strips [-64.9, -30] and [-90, 65]. \nFig. 3(b) shows the same visualization for the visual \nSOM, which shows the same pattern of organization. Note \nhere that the dense area represents over 26% of the visual \nSOM, compared to just 1% of the visual space. \nThe first step of our model\u2019s training has successfully \norganized the visual and auditory maps based on the inputs\u2019 \nspatial relationships.  In particular, the map has yielded \nexpanded coverage of inputs centered within the dense \nareas. These results are in accordance with biological \nfindings showing a magnification of the central areas on \nvisual and auditory processing in the SC [1].  Furthermore, \nthe preservation of spatial similarity in each modality \nimplies that they can be aligned through a simple linear \ntranslation, such as a Hebbian association.  \nB. Multisensory Integration \nHaving developed the two sensory maps to process \nunisensory stimuli, we then combined them in the SC model \nto train the Hebbian links on multisensory stimuli. Prior to \ntraining, the outputs from both maps were normalized using \nthe unisensory training minimum and maximum responses \n(\u03b1aud=0.43, \u03b2aud=1.54, \u03b1vis=0.37, \u03b2vis=1.97).  The links were \nthen trained for 100 epochs with a learning rate 1.0=\u03b7 on \nthe multisensory training data.  The links were then \nnormalized (\u03b1link=0.42, \u03b2link=2.98). \nTo determine if the links had established a \nTABLE I \nTRAINING AND TESTING DATA \n Auditory Visual \nInput Space \nNon-dense area Elevation [-90, -30); (30, 90] [-65, -15); (15, 65] \n Azimuth [-180, 180] [-90, -15); (15, 90] \n Gaussian \u03bb = 0.5; \u03c3 = 10 \u03bb = 0.5; \u03c3 = 10 \nDense area Area 33% 1% \n Elevation [-30, 30] [-15, 15] \n Azimuth [-180, 180] [-15, 15] \n Gaussian \u03bb = 1; \u03c3 = 5 \u03bb = 1; \u03c3 = 5 \nDiscrete step size 5 5 \nInput dimension 2701 (37 by 73) \n999 \n(27 by 37) \nUnisensory training and testing \nWhole area examples 1675 810 \nAdditional dense examples 825 (33%) 90 (10%) \nTotal examples 2500 900 \nMultisensory training and testing \nWhole area examples 1000 1000 \nAdditional dense examples 125 (11%) 125 (11%) \nTotal examples 1125 1125 \nTABLE II \nSOM PARAMETERS \n Auditory Visual \nSize 20 by 15 Naud = 300 10 by 10 Nvis = 100 \nLattice Rectangular Rectangular \nTraining epochs 1000 1000 \nLearning rate Type Inverse Inverse \n Initial rate 0.5 0.5 \nNeighborhood Type Gaussian Gaussian \n Initial radius 20 10 \n Final radius 1 1 \n(a) Auditory Map (b) Visual Map \n  \n \n[-90, 90],[-65, -30]\n[-90, 90],[-29.9, -15]\n[-90, 90],[-14.9, 0]\n[-90, 90],[0.1, 15.9]\n[-90, 90],[16, 30.9]\n[-90, 90], [31, 65]  \n[-180, 180],[-90, -65]\n[-180, 180],[-64.9, -30]\n[-180, 180],[-29.9, -1]\n[-180, 180],[-0.9, 29]\n[-180, 180],[30, 64.9]\n[-180, 180],[65, 90]  \nFig. 3. U-matrix visualization of the (a) auditory and (b) visual SOMs\noverlaid with best matching units. Testing data were visualized by selecting \nazimuth strips that gradually cover the entire input space. The best matching \nunits are colored depending on their corresponding inputs\u2019 centers. Both \nmaps have learnt to preserve the spatial relationships of their inputs. Note\nthat the representation of the dense areas comprise a significant portion of \nthe maps: 2500 hits for the auditory SOM cover 55% of the map, whereas \n900 hits for the visual SOM cover 26% of the map. \n  \ncorrespondence between visual and auditory map units, we \nexamined the coincidence of auditory unit hits via the \nHebbian links for each input. The effectiveness of the \ntraining was examined by recording the number of \ncoincident hits between the maximum visual link output and \nthe best matching unit of the auditory SOM. A direct hit is \nrecorded when the link output matches exactly the auditory \noutput.  We also explored how near each of the hits were by \nrecording the coincidence between the maximum link output \nand units within a defined radius of the auditory best \nmatching unit. \nFig. 4 shows the auditory best matching units overlaid \nwith the Hebbian link outputs. We can see from these testing \nresults that the links learnt to approximately associate \nauditory and visual stimuli map locations.  Here, the \nHebbian link outputs covered approximately 35% of the \nauditory SOM for a radius of 0 units, rising to 46% for a \nradius of 4 units.  As expected, for spatial locations outside \nof the visual space (essentially a zero input to the visual \nmap), the linkage could not translate this to an auditory unit, \neven though this would be within the auditory space.  When \nusing an input centered within the visual space then the \nHebbian linkage activation area approximated that of the \nauditory activation. Direct unit to unit translation was \nachieved with 27.28% of the testing data.  Although this \npercentage of direct hits appears low, if we look at the \ncoincidence of link outputs to units within a defined radius \nof the auditory best matching unit, the percentage \nsignificantly increases.  For a radius of 1 unit, the \ncoincidence rises to 61.51% and for 2 units this rises to \n70.66%.  Overall therefore, the links learn to associate \nbetween units with the most hits, as seen by the high \ncoincidence compared to the low coverage of the map. \nThe Hebbian links therefore successfully translate the \nvisual spatial representation into an auditory representation, \naligning the different sensory spatial coordinate systems.  \nWith this translation in place, we can now form the \nmultisensory space as the addition of the auditory output and \nthe visual link output.  Fig. 5 shows the outputs from the \nmodel for (a, b, c) a non-coincident auditory and visual \ninput, and (d, e, f) a coincident auditory and visual input. \nWhen selecting an input at the edge of the auditory space \n(outside the visual field), the link output is correspondingly \nlow and is the same as the average link output (0.28).  The \nresulting multisensory response (0.57) was therefore driven \nmost by the auditory response.  When using a multisensory \ninput centered within the visual space, the link activation \narea approximated that of the auditory activation resulting in \nan enhanced response (0.74). Maximum multisensory \nenhancement was achieved only in the case of direct hits. \nThe increase in output seen with coincident multisensory \nstimuli is commensurate with multisensory enhancement, in \nwhich a higher activation is achieved [20].  However, while \nour model shows that we have successfully integrated \nsensory responses, and in particular translated the visual \nrepresentation into an auditory representation, it uses a too \nsimplistic combination method to show multisensory \nsuppression when only one sensory stimulus is presented \n(seen by the 0.57 response), especially since current research \nhas started to show that the enhancement response is \nlogarithmic in nature [6].  While our model does not show \nthis type of response, it does successfully align the sensory \nmaps and combine them to show enhancement.  Further \nwork can be used to refine the combination, including \nvarying unimodal stimuli intensity and combination method \n(for example multiplicative or logarithmic). \nMap units  \nFig. 4. Auditory best matching units overlaid with visual Hebbian link \noutputs. Blue circles signify that the maximal link output corresponds to the \nrequired auditory unit (0 radius).  Red circles denote that the maximal \noutput (for a different input) is located within a radius of 1 unit in the \nauditory map, green a radius of 2 units, black a radius of 3 units and an X a \nradius of 4 units \u2013 for example, unit (12, 8). \nMap units\nA\nctivation\nMap units\nA\nctivation\nMap units\nA\nctivation\nMap units\nA\nctivation\nMap units\nA\nctivation\nMap units\nA\nctivation\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFig. 5. Multisensory responses to non-coincident (a, b, c) and coincident \ninputs (d, e, f).  Each plot shows the value of the outputs from the auditory \nSOM (a, d), visual links (b, e) and multisensory outputs (c, f). \n(a, b, c) Auditory and visual stimulus with centre (elevation -9.6, azimuth \n161.1) and multisensory output (0.57) at coordinate (20, 8). The link (b) and \nauditory response (a) at that location is 0.28 and 0.28, respectively. The \nlocation of the maximum link response (0.29) is at (14, 15).  \n (d, e, f) Auditory and visual stimuli with centre (elevation 18.3, azimuth -\n7.6) and multisensory output (0.74) at coordinate (10, 11). The link (e) and \nauditory response (d) on that location is 0.45 and 0.29, respectively. The \nlocation of the maximum link response (0.45) is at (8, 13). \n  \nV. CONCLUSION \nIn this paper we have presented a simple behavioral \nmodel of the superficial and deep layers of the mammalian \nSC.  Our model comprises a visual (superficial) and auditory \n(deep) topographic map, each behaving in a way that is \ncomparable to the biology, with the preservation of spatial \nsimilarity and the increased representation of denser regions \n(fovea).  We then combined these unisensory representations \nusing associative learning to test whether this is a viable \nmethod for modeling integration.  Through the presentation \nof coincident multisensory stimuli, the visual map is aligned \nwith the auditory map.  Once aligned, the representation \nshows a simple form of multisensory enhancement (deep), \nwhich is a consequence of the additive combination of \n(translated) sensory signals. \nWith this work we set out to answer two questions.  First, \ncan we use a computational model to understand better the \nprocess of sensory alignment and multisensory integration \nwithin the SC?  Using simple techniques we have \nsuccessfully shown that Hebbian learning can be used to \nalign two spatial representations through a process of \nmultisensory training (although not three senses).  Not only \ndoes this show that this is possible with a simple model, it \nalso demonstrates how cortical feedback may play a role in \nthe integration.  Although we have not explicitly included \ncortical control, implicitly we have selected to associate \nmultisensory responses together during training.  In this way \nour training regimen has simulated cortical selectivity, \nalthough our approach is not sufficiently sophisticated to \nprovide further insight.  Future work will address this issue \nby explicitly modeling cortical feedback and enhancing the \nintegration. \nSecond, we chose to implement a fuller model of the SC \nin order to gain insight into whether such modeling is of \nbenefit computationally.  While this has obviously not \nprovided a step change in computational capabilities, what \nthis has shown is that we can learn to associate sensory \nstimuli.  Although our choice of input representation is \nsimple, they can be replaced by simple real-world artificial \nsensory inputs, which can be input to the model at discrete \ntime intervals.  While this excludes more complex inputs, \nsuch as images, videos or sound without further \npreprocessing, it does mean that this model can be \nembedded into a prototype robot in order to get it to react to \nmultisensory stimuli, much like the cats used in the \nbiological experiments [1].  The insight is therefore that we \nshould incrementally increase the size and complexity of \nsuch models, and embed them into the real-world where \npossible to find the step change.  \nACKNOWLEDGEMENT \nWe would like to thank Jim Austin, Jim Bednar, Alan \nMurray, Leslie Smith, Barry Stein and Stefan Wermter for \nearly discussions on this work, as well as the three \nanonymous reviewers for their constructive comments. \nREFERENCES \n[1] B.E.Stein and M.A.Meredith, The Merging of the Senses. Cambridge, \nMA.: A Bradford Book, MIT Press, 1993. \n[2] B.E.Stein, W.Jiang and T.R.Stanford, \"Multisensory Integration in \nSingle Neurons of the Midbrain,\" G.A.Calvert, C.Spence, and \nB.E.Stein, Ed. Cambridge, MA.: A Bradford Book, MIT Press, 2004, \npp. 243-264. \n[3] A.A.Ghazanfar and C.E.Schroeder, \"Is Neocortex Essentially \nMultisensory,\" Trends in Cognitive Sciences, vol. 10, pp. 278-285, \n2006. \n[4] M.Denham and T.Tarassenko, \"Sensory Processing,\" Foresight \nCognitive Systems Project, London, 2003. \n[5] A.J.King, \"The Superior Colliculus,\" Current Biology, vol. 14, pp. \nR335-R338, 2004. \n[6] B.A.Rowland, T.R.Stanford and B.E.Stein, \"A Model of the Neural \nMechanisms Underlying Multisensory Integration in the Superior \nColliculus,\" Perception, vol. 36, pp. 1431-1443, 2007. \n[7] S.Grossberg, K.Roberts, M.Aguilar and D.Bullock, \"A Neuronal \nModel of Multimodal Adaptive Saccadic Eye Movement Control by \nSuperior Colliculus,\" Journal of Neuroscience, vol. 17, pp. 9706-\n9725, 1997. \n[8] A.Haessly, J.Sirosh and R.Miikkulainen, \"A Model of Visually \nGuided Plasticity of the Auditory Spatial Map in the Barn Owl,\" in \nProceedings of the 17th Annual Conference of the Cognitive Science \nSociety, 1995, pp. 154-158. \n[9] C.Quaia, P.Lef\u00e8vre and L.M.Optican, \"Model of the Control of \nSaccades by Superior Colliculus,\" Journal of Neurophysiology, vol. \n82, pp. 999-1018, 1999. \n[10] T.P.Trappenberg, M.C.Dorris, D.P.Munoz and R.M.Klein, \"A Model \nof Saccade Initiation Based on the Competitive Integration of \nExogenous and Endogenous Signals in the Superior Colliculus,\" \nJournal of Cognitive Neuroscience, vol. 13, pp. 256-271, 2001. \n[11] V.Cutsuridis, N.Smyrnis, I.Evdokimidis and S.Perantonis, \"A Neural \nModel of Decision-making by the Superior Colliculus in an \nAntisaccade Task,\" Neural Networks, vol. 20, pp. 690-704, 2007. \n[12] H.H.L.M.Goossens and A.J.Van Opstal, \"Dynamic Ensemble Coding \nof Saccades in the Monkey Superior Colliculus,\" Journal of \nNeurophysiology, vol. 95, pp. 2326-2341, 2006. \n[13] P.E.Patton and T.J.Anastasio, \"Modeling Cross-Modal Enhancement \nand Modality-Specific Suppression in Multisensory Neurons,\" Neural \nComputation, vol. 15, pp. 783-810, 2003. \n[14] T.Kohonen, \"Self-Organized Formation of Topologically Correct \nFeature Maps,\" Biological Cybernetics, vol. 43, pp. 59-69, 1982. \n[15] D.O.Hebb, The Organization of Behavior: A Neuropsychological \nTheory. New York: John Wiley & Sons, 1949. \n[16] R.Miikkulainen, J.A.Bednar, Y.Choe and J.Sirosh, Computational \nMaps in the Visual Cortex. New York: Springer Science+Business \nMedia, 2005. \n[17] J.L.McClelland, A.Thomas, B.D.McCandliss and J.A.Fiez, \n\"Understanding Failures of Learning: Hebbian Learning, Competition \nfor Representational Space and Some Preliminary Experimental Data,\" \nJ.Reggia, E.Ruppin, and D.Glanzman, Ed. Oxford: Elsevier, 1999, pp. \n75-80. \n[18] C.von der Malsburg, \"Self-organization of Orientation Sensitive Cells \nin the Striate Cortex,\" Kybernetik, vol. 14, pp. 85-100, 1973. \n[19] J.Vesanto, J.Himberg, E.Alhoniemi and J.Parhankangas, \"Self-\nOrganizing Map in Matlab: the SOM Toolbox,\" \nhttp:\/\/www.cis.hut.fi\/projects\/somtoolbox\/package\/papers\/toolbox2pa\nper.pdf, 2000. \n[20] M.T.Wallace, M.A.Meredith and B.E.Stein, \"Multisensory Integration \nin the Superior Colliculus of the Alert Cat,\" Journal of \nNeurophysiology, vol. 80, pp. 1006-1010, 1998. \n \n \n"}