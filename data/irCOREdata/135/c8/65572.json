{"doi":"10.1016\/j.compgeo.2008.09.005","coreId":"65572","oai":"oai:dro.dur.ac.uk.OAI2:5452","identifiers":["oai:dro.dur.ac.uk.OAI2:5452","10.1016\/j.compgeo.2008.09.005"],"title":"Simulation-based calibration of geotechnical parameters using parallel hybrid moving boundary particle swarm optimization.","authors":["Zhang,  Y.","Gallipoli,  D.","Augarde,  C. E."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2009-05-01","abstract":"Simulation-based optimization methods have been recently proposed for calibrating geotechnical models from laboratory and field tests. In these methods, geotechnical parameters are identified by matching model predictions to experimental data, i.e. by minimizing an objective function that measures the difference between the two. Expensive computational models, such as finite difference or finite element models are often required to simulate laboratory or field geotechnical tests. In such cases, simulation-based optimization might prove demanding since every evaluation of the objective function requires a new model simulation until the optimum set of parameter values is achieved. This paper introduces a novel simulation-based \u201chybrid moving boundary particle swarm optimization\u201d (hmPSO) algorithm that enables calibration of geotechnical models from laboratory or field data. The hmPSO has proven effective in searching for model parameter values and, unlike other optimization methods, does not require information about the gradient of the objective function. Serial and parallel implementations of hmPSO have been validated in this work against a number of benchmarks, including numerical tests, and a challenging geotechnical problem consisting of the calibration of a water infiltration model for unsaturated soils. The latter application demonstrates the potential of hmPSO for interpreting laboratory and field tests as well as a tool for general back-analysis of geotechnical case studies","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/65572.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/5452\/1\/5452.pdf","pdfHashValue":"f579dda369e365fc15b078dbde2fdbfbb3e9bce2","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:5452<\/identifier><datestamp>\n      2011-11-03T12:07:34Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Simulation-based calibration of geotechnical parameters using parallel hybrid moving boundary particle swarm optimization.<\/dc:title><dc:creator>\n        Zhang,  Y.<\/dc:creator><dc:creator>\n        Gallipoli,  D.<\/dc:creator><dc:creator>\n        Augarde,  C. E.<\/dc:creator><dc:description>\n        Simulation-based optimization methods have been recently proposed for calibrating geotechnical models from laboratory and field tests. In these methods, geotechnical parameters are identified by matching model predictions to experimental data, i.e. by minimizing an objective function that measures the difference between the two. Expensive computational models, such as finite difference or finite element models are often required to simulate laboratory or field geotechnical tests. In such cases, simulation-based optimization might prove demanding since every evaluation of the objective function requires a new model simulation until the optimum set of parameter values is achieved. This paper introduces a novel simulation-based \u201chybrid moving boundary particle swarm optimization\u201d (hmPSO) algorithm that enables calibration of geotechnical models from laboratory or field data. The hmPSO has proven effective in searching for model parameter values and, unlike other optimization methods, does not require information about the gradient of the objective function. Serial and parallel implementations of hmPSO have been validated in this work against a number of benchmarks, including numerical tests, and a challenging geotechnical problem consisting of the calibration of a water infiltration model for unsaturated soils. The latter application demonstrates the potential of hmPSO for interpreting laboratory and field tests as well as a tool for general back-analysis of geotechnical case studies.<\/dc:description><dc:subject>\n        Parameter identification<\/dc:subject><dc:subject>\n         Unsaturated soils<\/dc:subject><dc:subject>\n         Particle swarm optimization<\/dc:subject><dc:subject>\n         Parallel computing.<\/dc:subject><dc:publisher>\n        Elsevier<\/dc:publisher><dc:source>\n        Computers and geotechnics, 2009, Vol.36(4), pp.604-615 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2009-05-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:5452<\/dc:identifier><dc:identifier>\n        issn:0266-352X<\/dc:identifier><dc:identifier>\n        doi:10.1016\/j.compgeo.2008.09.005<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/5452\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1016\/j.compgeo.2008.09.005<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/5452\/1\/5452.pdf<\/dc:identifier><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["0266-352x","issn:0266-352X"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2009,"topics":["Parameter identification","Unsaturated soils","Particle swarm optimization","Parallel computing."],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n08 April 2009\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nZhang, Y. and Gallipoli, D. and Augarde, C. E. (2009) \u2019Simulation-based calibration of geotechnical\nparameters using parallel hybrid moving boundary particle swarm optimization.\u2019, Computers and geotechnics.,\n36 (4). pp. 604-615.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1016\/j.compgeo.2008.09.005\nPublisher\u2019s copyright statement:\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\n Use policy \n \nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without \nprior permission or charge, for personal research or study, educational, or not-for-profit purposes \nprovided that : \n \n\u0083 a full bibliographic reference is made to the original source \n\u0083 a link is made to the metadata record in DRO \n\u0083 the full-text is not changed in any way \n \nThe full-text must not be sold in any format or medium without the formal permission of the copyright \nholders.  \n \nPlease consult the full DRO policy for further details. \n \nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom \nTel : +44 (0)191 334 2975 | Fax : +44 (0)191 334 2971 \nhttp:\/\/dro.dur.ac.uk \nDurham Research Online \n Deposited in DRO:\n17 March 2009\nVersion of attached file:\nAccepted\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nZhang, Y. and Gallipoli, D. and Augarde, C. E. (2009) 'Simulation-based calibration of geotechnical parameters\nusing parallel hybrid moving boundary particle swarm optimization.', Computers and geotechnics., 36 (4),\npp.\u0000604-615.\nFurther information on publishers website:\nhttp:\/\/dx.doi.org\/10.1016\/j.compgeo.2008.09.005\nSimulation-based calibration of geotechnical parameters using parallel hybrid \nmoving boundary particle swarm optimisation \nY. Zhang1, D. Gallipoli2 and C.E. Augarde1 \n1School of Engineering, Durham University, Durham, UK \n2Department of Civil Engineering, University of Glasgow, Glasgow, UK \n \nAbstract \nSimulation-based optimization methods have been recently proposed for calibrating \ngeotechnical models from laboratory and field tests. In these methods, geotechnical \nparameters are identified by matching model predictions to experimental data, i.e. by \nminimizing an objective function that measures the difference between the two. \nExpensive computational models, such as finite difference or finite element models, are \noften required to simulate laboratory or field geotechnical tests. In such cases, \nsimulation-based optimization might prove demanding since every evaluation of the \nobjective function requires a new model simulation until the optimum set of parameter \nvalues is achieved. This paper introduces a novel simulation-based \u201chybrid moving \nboundary particle swarm optimization\u201d (hmPSO) algorithm that enables calibration of \ngeotechnical models from laboratory or field data. The hmPSO has proven effective in \nsearching for model parameter values and, unlike other optimization methods, does not \nrequire information about the gradient of the objective function. Serial and parallel \nimplementations of hmPSO have been validated in this work against a number of \nbenchmarks, including numerical tests, and a challenging geotechnical problem \nconsisting of the calibration of a water infiltration model for unsaturated soils. The latter \napplication demonstrates the potential of hmPSO for interpreting laboratory and field \ntests as well as a tool for general back-analysis of geotechnical case studies.  \n \nKeywords: parameter identification, unsaturated soils, particle swarm optimization, \nparallel computing  \n   \n 2\nINTRODUCTION \nDuring the last decade, research has been carried out on the use of optimization \nmethods for the identification of geotechnical parameters from both laboratory tests [1] \nand field tests (e.g. pressuremeter tests) [2]. Optimization methods require the use of \nrelevant engineering models [3] to simulate a particular test for which measurements \nare available. For laboratory tests, it is possible to use relatively simple numerical or \nanalytical models to reproduce soil behaviour at stress-point level when a uniform \ndistribution of stresses and strains can be reasonably assumed. On the other hand, \nmore complex models (such as finite element or finite difference models) are \nnecessary for field tests because of the non-uniform stress and strain fields generated \nwithin the soil domain. In the latter case, the use of optimization methods might \ntherefore prove computationally demanding because a new model simulation must be \nperformed for every single evaluation of the objective function until the optimum set of \nparameter values is achieved. \nIn this paper, global optimization methods are described for minimizing nonlinear \nobjective functions over a bounded search space. Mathematically, the problem is \nexpressed as \nminimize  ( )xf  \n           (1) \nsubject to  nD\u2208x  and ul xxx \u2264\u2264  \nIn Eqn 1 x is the vector [x1, x2, . . . , xn]T located within the range [xl , xu] of the n-\ndimensional space Dn , xl and xu are the lower and upper search bounds respectively \nand f(x) is the objective function, which is calculated either from a closed-form \nmathematical solution or (as in this case) from a computer simulation of a real \nengineering problem for which measurements are available.  \nFor geotechnical optimization problems, the aim is to obtain a set of model parameter \nvalues that provide the best match between model simulations and measurements. \nInterest does not lie in finding the absolute global minimum of the objective function, as \nthis merely confirms a close match between simulation and reference measurements, \nbut rather in finding the global minimum within a restricted search space corresponding \n  \n 3\nto the range of realistic parameter values for engineering design.   \nMoreover, the complexity of current engineering models can often lead to the existence \nof several local minima within the search space, each with very similar values of the \nobjective function despite large differences in some of the associated parameter values. \nThe number of local minima and the corresponding values of the objective function also \ndepend on the particular computational model adopted for the simulations as well as on \nthe accuracy of the calculations (which can sometimes lead to \u201cnoise\u201d in the objective \nfunction).  Simulation-based calibration of engineering parameters therefore requires \nthe combination of a global search strategy capable of coping with several local minima \ntogether with accurate robust computational models.  \nEvolutionary Algorithms (EAs) are population-based iterative stochastic algorithms that \nhave proved effective in many areas of engineering for solving complex optimization \nproblems where the objective function has a large number of local minima. EAs are \nbased on Darwinian theories simulating the evolutionary process of a population of \nindividuals towards the best position (i.e. the final solution of the optimization problem) \nby competition and natural selection. One of the best known EAs is the Genetic \nAlgorithm (GA), where the optimization process consists in finding the best position \niteratively through operations, such as crossover and mutation, on a population of \nindividuals and by application of rules for the selection of individuals into subsequent \ngenerations. Particle Swarm Optimization (PSO) is a newer evolutionary technique \ninspired by the observed social behaviour of bird flocks and fish schools and was first \nintroduced by Kennedy and Eberhart [4]. More recently, PSO has been shown to have \nincreased effectiveness and better computational efficiency over other EAs while \nmaintaining simplicity [5-8]. In contrast to the GA, PSO has no evolution operators such \nas crossover and mutation which makes it ideal for asynchronous parallel \nimplementation [16], an important issue for simulation-based optimization where \nparallel implementation is necessary as discussed later. An additional advantage of \nPSO over other EAs is the ease with which it can be combined with other search \nalgorithms. \nAlthough PSO is effective in finding a global optimum, it suffers from what is termed \n\u201cpremature convergence\u201d when the optimization process prematurely converges to a \nlocal optimum because it is no longer able to explore other areas of the search space \nwhere better positions or the global best position may lie [9]. Another weakness is that \n   \n 4\nPSO searching ability reduces significantly in the later stages of optimisation [10] and \nthis sometimes leads to slow convergence rates. \nTo overcome such known weaknesses of PSO, a hybrid optimisation approach can be \nadopted. A local search method can be incorporated into PSO to carry out quick and \nefficient explorations around potential optima at a much lower computational cost [30]. \nLocal search methods tend to find the best solution around the neighbourhood of the \nstarting point with a better rate of convergence than PSO in the same situation. \nHowever, local search methods are rarely the best approach to explore the whole \nspace of potential solutions, hence the need of combining them with an effective global \ntechnique like PSO.  \nHybridization of a global search method with a local search method has proved very \neffective in solving optimization problems by making good use of the strengths of both \nglobal and local methods [5, 13-15]. For example, performance of GAs has often been \nimproved through hybridization with a local search method [29] following two \nmethodologies. The first is to use the local search technique during the evolutionary \nloop, i.e. local searches are regularly carried out in parallel with the evolutionary \nalgorithm to accelerate convergence rate.  The second is to use the local search \ntechnique at the end of the evolutionary loop, i.e. one local search is carried out using \nresults from the GA as initial guesses to improve the solution. Similar ideas have also \nbeen applied to PSO [30, 31] \nIn this paper we describe the development and use of a hybrid global-local optimization \nmethod particularly suited to the calibration of geotechnical models by using finite \nelement simulations of real engineering problems. The proposed algorithm differs from \nother proposals in two main aspects: firstly a moving-boundary PSO method is \nemployed for the global search and secondly an approximate local search algorithm is \nused instead of a complete local search. We present details of the proposed hybrid \noptimization method together with its implementation in both serial and parallel \nenvironments. The performance of the algorithm is also demonstrated with specific \nreference to the determination of geotechnical parameters  \nPARTICLE SWARM OPTIMIZATION \nThe original PSO algorithm was introduced as a population-based stochastic global \noptimization method by Kennedy and Eberhart [4, 17]. It is inspired by the observed \n  \n 5\nsocial behaviour of \u201cswarms\u201d in which a group of independent \u201cindividuals\u201d or \u201cparticles\u201d \ntry to achieve a goal by acting in a complex and coordinated way. As for the GA, PSO \nstarts by randomly initializing a population of individuals in the search space. The \nexploration for the best solution then continues with particles changing through \nsuccessive \u201cgenerations\u201d (a generation is an iteration of the algorithm corresponding to \none update of the whole population) according to rules until a termination criterion is \nmet. The basic PSO used in this paper is denoted as \u201cbPSO\u201d and was introduced by \nShi and Eberhart [18] as a variant of the original PSO. \nBefore presenting the detailed algorithm, some notation is defined. The size of the \nswarm population is Np. The current position of the ith particle is represented by the n-\ndimensional vector xi in the search space and by the corresponding fitness f(xi) (this is \nthe \u201cquality\u201d of the particle\u2019s position measured by the value of the objective function). \nEach particle also has a \u201cvelocity\u201d vi, which measures the change of the particle\u2019s \nposition in each generation.  The best position achieved so far by the ith particle pbest \nis identified by the point Pi and the corresponding fitness f(Pi). The best position \nachieved so far by the whole swarm gbest is identified by the point Pg and the \ncorresponding fitness f(Pg) . Each particle has therefore a \u201cmemory\u201d of its own best \nposition as well as a \u201csocial\u201d knowledge of the best position achieved by neighbouring \nparticles.  With specific reference to geotechnical applications, each vector xi contains \na different set of parameter values for an individual model simulation and the search \nspace is the space of all model parameters. \nThe PSO starts by randomly initializing each particle\u2019s velocity vi and position xi and by \ncalculating the corresponding fitness f(xi). After initialization, particles \u201cfly\u201d around in \nthe search space for many generations seeking for the optimum fitness, i.e. the \nminimum of the objective function (in each generation the algorithm updates both \nvelocity and position of all particles within the swarm). The rule for updating velocity is:   \n( ) ( )ig22ii11i1i xPxPvv \u2212+\u2212+=+ rcrcw kkk       (2) \nwhere the superscripts  k,  k+1 etc. indicate the generation number, r1 and  r2 are two \nrandom factors in the interval [0,1], wk is the inertia weight and c1 and c2 are the \n\"cognitive\" and \"social\" weights respectively (default values for most problems are c1 = \nc2 = 2.0 [20]). To avoid the algorithm becoming divergent, the particles' velocities are \n   \n 6\nconfined to a range -vmax < vi  < +vmax where the limit velocity vmax  determines the \nmaximum change of position for a given particle. The parameter vmax is usually set \nequal to half the width of the search domain to cover the span of the parameter space. \nUpdated particles\u2019 positions at generation k+1 are obtained by incrementing the \nposition at generation k with the velocity at k+1. \nThe inertia weight wk [19] controls the \u201cmomentum\u201d of the particle. A large value of the \ninertia weight favours global exploration by searching new areas, while a small value \nfavours local exploration. A linear decrease of inertia weight with iterations is \nintroduced to focus in on a global minimum [19]: \n( ) ( ) minminmax * wMaxIterkMaxIterwwwk +\u2212\u2212=      (3) \nwhere wk is the inertia weight for the kth  iteration,  MaxIter is the maximum iteration \nnumber and wmax  and wmin are the maximum and minimum inertia weights set by the \nuser.  \nLOCAL SEARCH METHOD \nAmong the variety of local search methods available in the literature, the Nelder-Mead \n(NM) method [12] has been chosen in this research. Similarly to the bPSO used for the \nglobal search, the NM method is a \u201cdirect\u201d algorithm, i.e. it requires evaluation of the \nobjective function at different points within the search space but does not require \ninformation about the gradient of the objective function. This feature is particularly \nuseful for simulation-based optimisation problems, such as the geotechnical \napplications considered here. For these problems, gradient measures are difficult to \nobtain from computational models, e.g. a finite difference or finite element simulation of \nan engineering boundary value problem will give no information about how results \nchange when model parameters are varied.  \nThe NM method requires the definition of  n+1 starting points in the search space, \nwhere n is the dimension of the search space. These points form the initial vertices of a \nworking simplex, which is then subjected to a series of \u201ctransformations\u201d aimed at \ndecreasing the values of the objective function at its vertices. Such transformations are \ngoverned by four operations controlling reflection, contraction, expansion and \nshrinkage of the simplex. The simplex must also remain non-degenerate throughout \n  \n 7\nthe series of transformations. This means that, if any of the n+1 vertices is taken as the \norigin, the n vectors connecting the origin with the remaining vertices must span the n-\ndimensional space or, in other words, the vertices of the simplex must not lie in the \nsame hyperplane. The sequence of transformations is terminated when the size of the \nworking simplex or the difference between the values of the objective function at its \nvertices become smaller than a given tolerance. Full details of the NM method can be \nfound in [12].  \nA HYBRID MOVING-BOUNDARY PSO WITH APPROXIMATE LOCAL SEARCH  \nSince the introduction of the original PSO, various improvements have been proposed \nto reduce difficulties associated to parameter selection [21, 22], to avoid premature \nconvergence [23-27] and to increase computational speed [28]. The increase of \ncomputational speed is achieved in this work while keeping swarm diversity to avoid \npremature convergence, by proposing a new hybrid moving-bounds PSO algorithm \n(hmPSO) that combines the effectiveness of the bPSO and NM methods for global and \nlocal searches respectively.  \nThe new algorithm operates in a normalized search space. Normalization is particularly \nimportant for geotechnical applications where model parameter values can vary over \nranges of very different magnitude. For example, permeability can range from 10-3 to \n10-9 ms-1 while elastic bulk modulus can range from 104 to 107 Pa.  Normalization \ntherefore facilitates comparison between all parameter scales and enables calculation \nof normalized \u201cdistances\u201d in the search space. Such calculation is frequently performed \nduring the optimization process and, if the dimensions of the search space are of \ndisparate sizes, the algorithm will result in errors.   \nMoreover, for those parameters whose values evenly span several orders of magnitude, \nlogarithmic normalisation of the search range will allow an efficient random initialization \nof particle positions by obtaining a uniform spread across the different orders of \nmagnitude. Given that PSO has a stochastic basis, the quality of the initial random \ndistribution of particle positions over the search space is crucial to the performance of \nthe algorithm. \n   \n 8\nLinear normalization of a given parameter range involves linear mapping between the \noriginal search range and a scaled domain. In particular, the jth component of the of \nthe ith particle position xi j  is normalized by the following relationship: \n( ) NminNminNmax\njj\njj\nj +\u2212\n\u2212\n\u2212\n=\nlu\nli\ni xx\nxx\nx       (4) \nwhere jix  is the jth normalized component the ith particle position, xu j  and xl j  are the \njth components of the upper and lower bounds of the search space while  Nmax and \nNmin are the normalized upper and lower bounds (assumed to be the same for all \ncomponents). \nLogarithmic normalization of a given parameter range involves linear mapping between \nthe logarithmic representation of the original search range and a scaled domain. By \nusing similar definitions as in Eqn 4, the jth component of the ith particle position xi j  is \nlogarithmically normalized by using the following relationship: \n( ) NminNminNmax\njj\njj\nj +\u2212\n\u2032\u2212\u2032\n\u2032\u2212\u2032\n=\nlu\nli\ni xx\nxx\nx       (5) \nwhere this time the logarithms of the jth components are used in the normalization, i.e.  \njj ii xlogx =\u2032  , jj ll xlogx =\u2032  , jj uu xlogx =\u2032     (6) \nIn the following it is assumed that all dimensions of the search space are normalized  \n(either using linear or logarithmic scaling as appropriate for each parameter) between \nthe same lower and upper bounds Nmin=1 and Nmax=2 respectively. \nThe sequential hmPSO algorithm \nThe hybrid moving-bounds hmPSO algorithm consists of three main components: a \nglobal bPSO, a local bPSO and a direct local search performed by the NM method.  \nThe global bPSO operates over the whole search domain and employs a \u201cglobal \nswarm\u201d that is initialized only once at the start of the optimization process.  \n  \n 9\nThe local bPSO operates over a smaller search sub-domain centred on the current \nbest particle position and employs a \u201clocal swarm\u201d, whose particle positions are \ninitialized every time the boundaries of the search sub-domain are updated. By using a \ncontracted search range, the efficiency of the bPSO algorithm improves significantly \n[32].  \nThe use of global and local swarms allows simultaneous exploration of the search \nspace and exploitation of the most promising search regions. The global swarm \ninvestigates the original search space to maintain the diversity of the population while \nthe local swarm focuses on a smaller search space to increase efficiency. Particles fly \nwithin their respective search spaces as during normal bPSO except that particles in \nthe local swarm fly over a smaller search sub-domain. Exchange of \u201csocial\u201d knowledge \nbetween the two swarms is also ensured by sharing the same best particle position \ngbest.  \nThe Simplex routine is called every NL bPSO generations (note that each iteration \nincludes updates of all particles in the global and local swarms) to undertake a fast \nexploration of the sub-region containing the best n+1 particle positions, where n is the \ndimension of the search space. If the best n+1 particle positions form a non-\ndegenerated simplex, they are used as initial vertices. Otherwise, a regular simplex is \ncreated starting from the centroid of a sub-set of best particle positions (in this work the \ncentroid of the best Np\/4 particle positions is used).  \nThe solution xL corresponding to the smallest value of the objective function found by \nthe NM method is taken as the centre of the updated search sub-domain [xL-\u03b4I ,  xL+\u03b4I] \nexplored by the local bPSO. The size of the search sub-domain is given by the scalar \nradius \u03b4 multiplied by the n-dimensional unit vector I.   \nThe convergence rate of the NM method is very sensitive to the quality of the starting \npoints and is significantly improved by using the best n+1 particle positions provided by \nthe bPSO as the initial vertices of the simplex. Similarly, premature convergence and \nlow efficiency of bPSO are overcome by alternating sequences of bPSO generations \nwith local exploration of the most promising areas of the search space.  \n   \n 10\nA schematic representation of the algorithm is given in Figure 1 where k is the iteration \nnumber, particleIDs are the labels of particles allocated to the local bPSO search \n(these are chosen beforehand) and IL is the counter of local searches. \nIn comparison with the original bPSO, the hmPSO requires the following sequence of \nadditional computations: \n- sorting particles in accordance with their best position Pi; \n- performing a NM local search; \n- initializing the local swarm over the updated search sub-domain [xL-\u03b4I ,  xL+\u03b4I].  \nThese three additional computations are carried out every NL generations of the bPSO.  \nThe focus of the local bPSO is progressively restricted during the optimization process \nby decreasing the radius \u03b4 of the search sub-domain as the number of local searches \nIL increases. The radius \u03b4 starts from a relatively high value of 0.4 and then decreases \nlinearly with the number of local searches until reaching a limit value of 0.2, \ncorresponding to the maximum number of local searches set by the user. It is useful \nrecalling here that the radius of the entire normalized search space is equal to 1.0.  \nIn hmPSO, the NM local search is halted after a maximum number of transformations \neven if the termination criterion is not met. This tends to happen especially for the first \nfew local searches as the initial guesses of the simplex vertices are poor and probably \nmisleading. Such earlier termination of the algorithm can save significant computational \ntime and avoids getting \u201ctrapped\u201d into minima. \nThe parallel hmPSO algorithm \nEAs are renowned for being computationally expensive and the bPSO algorithm \ntypically requires thousands of generations to converge. Each generation involves Np \nevaluations of the objective function and hence, for the simulation-based optimization \nprocess considered here, Np model simulations.  \nParallel implementation is an appealing means of increasing algorithmic efficiency by \nassigning different particles to different processors working simultaneously.  Parallel \nimplementation of bPSO can either be \u201csynchronous\u201d or \u201casynchronous\u201d. In the \n  \n 11\n\u201csynchronous\u201d implementation, generations are carried out sequentially and a new \ngeneration is started only after the previous generation is completed. This means that \neach particle must wait for all other particles to update their positions before moving to \nthe next generation. In the \u201casynchronous\u201d implementation the idea of sequential \ngenerations is abandoned as each particle position is continuously updated (regardless \nof the status of neighbouring particles) while an uninterrupted sequence of local \nsearches is also simultaneously executed. \nIn a simulation-based optimization process, each simulation may take a different \namount of time depending on parameter values and model non-linearity. This will result \nin load unbalances among processors and, in a synchronous parallel implementation, \nthe slowest processor will determine the speed of the algorithm.   \nUnlike GAs, for which the synchronous parallel implementation is the only option (due \nto synchronized communication between individuals during crossover and mutation), \nbPSO lends itself to asynchronous parallel implementation. In bPSO, particles only \nshare the \u201csocial\u201d knowledge of the best position gbest and this single piece of \ninformation is easily distributed by using a client-server model, where each particle \nqueries a central store of shared data. For distributed computing, clients and servers \nare independently placed on network nodes, which may also use different hardware \nand operating systems.  \nThe parallel server-client implementation of the hybrid moving-bounds hmPSO \nalgorithm is schematically explained by Figure 2. The server-client model consists of \none server and a number of clients divided in two distinct groups, i.e. the particle client \ngroup and the local search client group. Nodes in the particle client group request \ninformation directly from the server and they do not communicate among themselves. \nOn the other hand, nodes in the local search client group interact according to a \nmaster-slave model. The master is responsible for managing and coordinating all \nslaves in the group while the server only communicates with the master of the local \nsearch client group. In this implementation, a single processor has been dedicated to \nNM local searches so the total number of processors Np+2, i.e. one server processor, \nNp processors for Np particles and one processor for the local searches. \nThe server is responsible for storing and managing shared data, listening to queries by \nall clients and returning the relevant responses. The clients are responsible for \n   \n 12\nevaluating the objective function at each particle position (particle client group) or \nperforming a local search (local search client group).  \nThe flowchart of the whole algorithm is illustrated in Figure 3 where dotted lines \nindicate the flow of data. The parallel programming library MPI [35] is used for the data \ncommunications among processors. The algorithm is started by initializing particle \nclients whose position is then communicated to the server. Subsequently, the algorithm \nadvances as the server continues receiving and parsing different types messages from \nboth particle and local search clients.  \nWhen the server receives the best position pbest of a given client particle, a check is \nperformed whether the swarm best position gbest has changed and an updated gbest is \nsent back to particle clients for calculating the next position. When the server receives \na local search solution xL, a check is again performed whether the swarm best position \ngbest has changed. Subsequently, as particles best positions pbest are continuously \nupdated by bPSO, the server asks the client to perform the next local search based on \nfresh improved initial guesses.  \nLocal searches are used to guide the local swarm towards global optimum and, when a \nnew local search solution xL becomes available, the focus of the bPSO search sub-\ndomain is shifted to the region around such solution. At the same time, a request is \nsent by the server to the client particles of the local swarm to reinitialize their positions \nand velocities over the new search sub-domain. If the server receives a request to stop \nfrom a client (either a particle or a local search client) due to the attainment of a \nminimum of the objective function within the set tolerance, it sends an order to \nterminate the algorithm to all clients.  \nNote that, in order to ensure that reasonable initial guesses are used for the very first \nlocal search, a preset number of bPSO evaluations must be carried out by particle \nclients before local search clients become active.  \nASSESSMENT OF ALGORITHM PERFORMANCE \nIn this section, the serial and parallel implementations of the hmPSO algorithm are \nassessed for a number of benchmark tests. The benchmarks include five numerical \ntests based on artificial objective functions, with well defined mathematical forms, as \nwell as one simulation-based geotechnical optimization test consisting in the calibration \n  \n 13\nof an unsaturated soil infiltration model. The five numerical tests are used to assess \nrobustness and efficiency of the (serial) hmPSO algorithm with respect to the standard \nbPSO algorithm while the simulation-based optimization compares the performances of \nthe parallel and serial implementations hmPSO.  \nThe termination criterion should be set to end computations when the global minimum \nof the objective function is found within a given tolerance. In real optimization problems, \nhowever, this is not easy task since the global minimum of the objective function is not \nknown in advance. For example, in the engineering application to the identification of \nmodel parameters, the objective function is given by the difference between model \nsimulations and measurements from laboratory or field tests; the smaller such \ndifference, the more accurate the evaluation of model parameters.  \nIn the benchmark tests presented in this work the global minimum of the objective \nfunction is known a priori (and is equal to zero as shown later) so the definition of a \nsuitable termination tolerance is easier than for the general optimization problem. Other \ntermination criteria used in PSO include ending computations when the smallest value \nof the objective function remains unchanged over a certain number of generations and \nwhen the number of function evaluations exceeds a maximum value set by the user. \nThis maximum value usually depends on type and dimension of the particular \noptimization problem and should be set to avoid termination when convergence might \nstill be achieved. \nNumerical tests \nNumerical tests are used to assess the robustness and efficiency of hmPSO in finding \nthe global minimum of five mathematical functions. The characteristics of such \nfunctions, which have been previously employed in the literature to test optimisation \nalgorithms [34], are summarized in Table 1. Inspection of Table 1 indicates that four of \nthe five test functions are multimodal, for which the search of the global minimum can \nprove particularly challenging because the large number of local minima increases the \nlikelihood of premature convergence. To ensure that the algorithm is tested under the \nmost general conditions, four out of five functions are shifted so that their respective \nglobal optima do not lie at the centre of the search space. In addition, for each function, \nthe search is performed in both five-dimensional and ten-dimensional spaces, where \nthe range of variation for each dimension is kept the same (see Table 1). \n   \n 14\nThese numerical tests follow the procedures set by the 2005 IEEE Congress on \nEvolutionary Computation [34], which defined methods for evaluating algorithmic \nefficiency and robustness. To measure the performance of the algorithms accurately, \nthe search for the global minimum of each function is repeated 25 times by using \nexactly the same set of parameters as given in Table 2. Indices measuring algorithm \nperformance are therefore defined based on such 25 runs in a statistical sense so as to \ntake randomness into account.  \nFor each run, the objective function ( )xf  is defined as: \n( ) ( ) ( )[ ]*xxx ggabsf \u2212=         (7) \nwhere g(x) is the chosen test function, g(x*) is the global minimum of such test \nfunction and abs indicates the absolute value. A run is considered successful if the \nvalue of the objective function drops below a termination tolerance of 1.0e-5 within a \nmaximum number of function evaluations equal to 10000 times the test dimension. \nRobustness is measured by the \u201csuccess rate\u201d over 25 runs, which is compared in \nTable 3 for both algorithms and all numerical tests. It can be seen that hmPSO \nsignificantly outperforms bPSO by a notably higher success rate in all tests with the \nonly exception of the Rasgrigin function. In several tests, bPSO also failed to find the \nglobal minimum while hmPSO succeeded to find it. The better performance of hmPSO \nwith respect to bPSO is also confirmed by Figure 4, which provides histograms of the \nmean values (in a logarithmic scale) of the objective function calculated by the two \nalgorithms over 25 runs for each numerical test. In Figure 4 the largest positive and \nnegative deviations from such mean values are also given as error bars. \nSimilarly, Figure 5 shows histograms of the mean number of function evaluations (in a \nlogarithmic scale) performed by both algorithms over a total of 25 runs for each \nnumerical test. Again, the largest positive and negative deviations from such means \nare given in Figure 5 as error bars. It is worth noting that, for Shifted Rosenbrock (n=5), \nShifted Rosenbrock (n=10) and Shifted Griewank (n=10), the number of bPSO function \nevaluations is constant for all 25 runs. This is because, for these three numerical tests, \nthe bPSO algorithm failed to converge in all runs and the number of function \nevaluations was always equal to the maximum limit (i.e. 10000 times the test \ndimension). Inspection of Figure 5 indicates that hmPSO shows significantly greater \n  \n 15\nefficiency than bPSO by using a smaller number of function evaluations in all numerical \ntests.  \nFigure 6 illustrates the convergence rate of both algorithms for the Shifted Rosenbrock \n(n=10) numerical test and provides graphical evidence of the dramatically better \nperformance of hmPSO in comparison with bPSO. The sequence of NM local searches \nimproves the solution in a step-wise fashion as evident from the jumps in the hmPSO \ncurve of Figure 6, which correspond to the availability of new solutions from local \nsearches. It is also worth pointing out that, if the NM method is used on its own without \ncombination with bPSO, all numerical tests fail to converge.  \nSimulation-based optimization test \nThe above numerical tests demonstrate that the serial version of the hmPSO algorithm \nis both robust and efficient. This section investigates the application of the hmPSO \nalgorithm to the identification of parameter values for the specific geotechnical model of \none-dimensional water infiltration in an unsaturated soil column with a rigid soil \nskeleton. One of the challenges of such application is the lack of a priori knowledge \nabout the nature of the search space and, in particular, whether the objective function \npresents a large or small number of local minima.  \nThis application is also used to compare the performances of the serial and parallel \nimplementations of hmPSO where, in the latter, each processor is allocated to a \ndifferent particle as previously described. The hardware for the parallel implementation \nof hmPSO is a Linux cluster \u201cHamilton\u201d at Durham University. The cluster consists of \n96 dual-processor dual-core Opteron 2.2 GHz nodes with 8 GBytes of memory and a \nMyrinet fast interconnect for running MPI code as well as 135 dual-processor Opteron \n2 GHz nodes with 4 GBytes of memory and a Gigabit interconnect. The operating \nsystem is SuSE Linux 10.0 (64-bit) and disk storage capacity is 3.5 Terabytes. \nWater flow is modelled according to the \u201c\u03b8 -based\u201d form of Richards\u2019 equation [36]: \nz\nK\nz\nD\nzt \u2202\n\u2202\n+\u23a5\u23a6\n\u23a4\u23a2\u23a3\n\u23a1\n\u2202\n\u2202\n\u2202\n\u2202\n=\n\u2202\n\u2202 )()( \u03b8\u03b8\u03b8\u03b8        (8) \nwhere K(\u03b8) is the unsaturated hydraulic conductivity (m\/s), \u03b8 is the volumetric water \ncontent, D(\u03b8) is the unsaturated hydraulic diffusivity (m2\/s), t is time (s) and z is the \n   \n 16\nvertical distance (m) measured positive upwards. The initial and boundary conditions \nare expressed as: \n( ) ohzh =0,       Lz <<0      (9) \n( ) bhth =,0        0>t      (10) \n( ) thtLh =,   or  ( ) tqtLq =,     0>t      (11) \nwhere L is the height of the soil column (m), h and q are the pressure head and flux (m \nand m\/s) respectively, ho is the initial pressure head distribution across the column, hb \nand ht are the constant pressure heads at the bottom and top of the soil column \nrespectively and qt is the constant flux imposed at the top of the soil column. \nThe Mualem-van Genuchten model [36, 37] is employed to describe the soil water \nretention properties in which the relationship between the effective degree of saturation \nSe and pressure head h is given by: \n[ ] me hS \u2212+= \u03b2\u03b11      (12) \nwhere \u03b1 (>0, m-1) is a parameter related to the inverse of the air-entry pressure, \u03b2 (>1) \nis a dimensionless parameter related to the pore size and m is a dimensionless \nparameter depending on \u03b2 through the relationship m=1\u20131\/\u03b2. \nThe effective saturation Se is given by: \n( ) ( )rsreS \u03b8\u03b8\u03b8\u03b8 \u2212\u2212= .     (13) \nwhere \u03b8s is the saturated volumetric water content and \u03b8r is the residual volumetric \nwater content;  \nThe unsaturated hydraulic conductivity K is given as the product of the saturated \nhydraulic conductivity Ks  by the dimensionless relative permeability kr . The relative \npermeability, which is smaller than unity, accounts for partial saturation through the \nfollowing dependency on the effective degree of saturation: \n  \n 17\n( ) 2121 11 \u23a5\u23a6\u23a4\u23a2\u23a3\u23a1 \u2212\u2212== mmeesrs SSKkKK      (14) \nThe unsaturated diffusivity D(\u03b8) can be derived as: \n( )\n( ) [ ]21 1121 \u2212+\u2212\u2212== \u2212\u2212 AASm mKddhKD merss \u03b8\u03b8\u03b1\u03b8      (15) \nwhere A=(1-Se1\/m)m . \nThe determination of the five parameters \u03b2, \u03b1, \u03b8s, \u03b8r and Ks through simulation-based \noptimization of an infiltration model has been shown to be a challenging problem [38]. \nThis problem is here used as a benchmark test for the hmPSO algorithm where \nexperimental measurements are replaced by the results computed from a finite \ndifference model with known parameter values (the \u201cforward\u201d analysis). In the forward \nanalysis, the height of the unsaturated soil column is assumed as L = 1.0 m and the \nparameter values are \uf020 chosen as \u03b1 = 3.35m-1, \u03b2 = 2, Ks = 9.22E-5 m\/s, \u03b8s = 0.368 and \n\u03b8r = 0.102. These are realistic parameter values corresponding to a field site in New \nMexico (see Celia et al. [39]). The chosen initial and boundary conditions are: \n( ) m100, \u2212=zh            Lz <<0      (16) \n( ) m10,0 \u2212=th              0>t      (17) \n( ) m75.0,m0.1 \u2212=th     0>t      (18) \nIn the finite difference simulation, the duration of the infiltration process is 6 hours and \nthis has been divided in equal time steps of 36 seconds while the soil column was \ndiscretised using 100 elements along the vertical direction. The computed profiles of \npressure head and water content at different times are illustrated in Figures 7 and 8.   \nThe test consists in searching through a five-dimensional space for the vector x=[\u03b1, \u03b2, \nKs, \u03b8s, \u03b8r]T whose components are the model parameter values providing an optimum \nmatch to the data in Figures 7 and 8 (as if these data were experimental \nmeasurements). Given that these data can be perfectly matched by the model, the aim \nis to assess the ability and efficiency of the hmPSO algorithm in returning the same set \n   \n 18\nparameter values of the forward analysis. Table 4 shows the search ranges used in this \noptimization test for each of the five model parameters.  \nTable 5 provides the algorithmic settings for the hmPSO, where the maximum number \nof evaluations of the objective function was set at a relatively high value of 500000 \nconsidering the difficulties of this particular benchmark and to allow a reasonable \ncomparison of the serial and parallel implementations of hmPSO.  \nThe following objective function (similar to that used in [40]) is minimized in this \noptimization test:  \n( ) ( ) ( )[ ] ( ) ( )[ ]\u2211 \u2211\n= =\n\u239f\u239f\u23a0\n\u239e\n\u239c\u239c\u239d\n\u239b\n\u0394\u2212\u0394+\u2212=\np\nj\njjQ\nl\ni\njijih tQtQwththwf\n1\n2*\n1\n2*x     (19) \nwhere \u2206Q*(tj) and \u2206Q(tj) are the cumulative water content changes for the whole soil \ncolumn at time tj (as computed from the forward analysis and the hmPSO algorithm \nrespectively), hi*( tj) and hi(tj) are the pressure heads at a depth of zi and time tj (as \ncomputed from the forward analysis and the hmPSO algorithm respectively), l is the \nnumber of depths at which the values of pressure head are computed, p is the number \nof times when the values of pressure head and water content are computed and wh \nand wQ are weighting factors accounting for the difference in units of the two additive \nterms of pressure head and water content respectively.  \nIn the following example, the pressure heads and cumulative water content changes \nare computed at the six different times of 1, 2, 3, 4, 5 and 6 hours (i.e. p=6 in Eqn 19). \nAt each of these times, pressure heads are computed at the six different depths of 0.05 \n0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.8, and 1.0 metres (i.e. l=11 in Eqn 19). The two \nweighting factors wh and wQ  are set to 1.0 and 10.0 respectively. \nTable 6 shows the results from the last five NM local searches performed during a \ntypical parallel run of the hmPSO algorithm (using 20 particles and 22 processors). The \nfinal solution corresponds to a value of objective function below the termination \ntolerance of 1.0E-5 with an estimated set of parameter values x = [3.35, 2.00, 9.22E-5, \n0.368, 0.102]T. The perfect match with the parameter values used for the forward \nanalysis confirms the ability of the hmPSO algorithm to find the global minimum of the \nobjective function with remarkable accuracy. The benefits of the combination of the \n  \n 19\nbPSO with the NM method are also evident from inspection of Table 6. The quality of \nthe initial guesses by the bPSO at the simplex vertices progressively improves with the \nnumber of local searches while the increased accuracy of the NM solution focuses the \nsearch sub-domain of the local bPSO.  \nTable 7 shows the number of objective function evaluations performed by each of the \n48 processors allocated to individual particles of the swarm. Inspection of Table 7 \nindicates that large differences exist in the number of objective function evaluations by \ndifferent particles leading to significant imbalance between processors. This also \nconfirms the importance of asynchronous, as opposed to synchronous, parallel \nimplementation in order to limit the impact of variable computational speed among \nprocessors. \nTable 8 shows that, as for the numerical tests, the hmPSO achieves high success rates \nover 25 runs (in both serial and parallel implementations) while bPSO fails to converge \nin any of the 25 runs. The robustness of hmPSO is also confirmed by the fact that the \nhigh success rate remains practically constant when different numbers of \nprocessors\/particles are used.  \nIt has also been noted that success rate starts to deteriorate when the number of \nprocessors exceeds 37 as the maximum allowed number of objective function \nevaluations is attained but the minimum of the objective function is still greater than the \ntermination tolerance (note that the maximum number of function evaluations is fixed in \nTable 5 regardless of the number of processors). This happens because, as a larger \nnumber of processors is used, the overall number of function evaluations per unit time \nrises accordingly so, even if the run tends to last shorter, the number of function \nevaluations performed during the run increases. On the other hand, the number of local \nsearches per unit time remains largely unchanged (see Figure 9) and hence a shorter \ncomputational time means a lower number of local searches. It can therefore happen \nthat, when using more than 37 processors, a relatively low number of local searches is \nperformed before the maximum number of function evaluations is attained. Given the \nkey role played by local searches in the definition of the bPSO sub-domain, a reduced \nnumber of local searches might impact on the efficiency of the particle swarm and \nmight lead to failure of the algorithm. \nThe convergence characteristics of the hmPSO for different runs using single and \nmultiple processors are illustrated in Figure 10. As previously mentioned, the overall \n   \n 20\nnumber of function evaluations becomes significantly larger when moving from a serial \nsingle-processor implementation to a parallel implementation with 47 processors. On \nthe other hand, Table 8 shows that computational time becomes significantly smaller \nwhen moving from a serial single-processor implementation (around 1.5 hours) to a \nparallel implementation with 47 processors (12 minutes). The variation of \ncomputational time and parallel speedup with number of processors is also illustrated \nin Figure 11. \nCONCLUSIONS \nThe paper presents a novel \u201chybrid moving boundary particle swarm optimization\u201d \nalgorithm (hmPSO) that enables calibration of geotechnical models from laboratory or \nfield measurements. A simulation-based optimization process is devised to match \nexperimental data to model predictions by minimizing an objective function that \nmeasures the difference between them.  \nThe hmPSO algorithm is the result of hybridization of a \u201cbasic particle swarm \noptimization\u201d (bPSO) algorithm with a NM local search algorithm. The bPSO includes \ntwo distinct particle swarms flying over the \u201cglobal\u201d domain (i.e. exploring the entire \nsearch space) and the \u201clocal\u201d sub-domain (i.e. exploiting the most promising search \nregion) respectively.  \nSerial and parallel implementations of hmPSO are described and validated on a \nnumber of benchmark tests. These include purely numerical tests, where the minimum \nof multimodal mathematical functions is sought, as well as a challenging geotechnical \nparameter determination based on the analysis of water infiltration in an unsaturated \nsoil column. Such application clearly demonstrates the potential of hmPSO for back-\nanalysis of geotechnical case studies as well as for routine interpretation of laboratory \nand field tests.  \nA number of important features of hmPSO have emerged during validation of the \nalgorithm. The combination of bPSO with the NM local search greatly improves \nefficiency and avoids premature convergence to a local minimum. NM local searches \nare also key in moving and progressively narrowing the search sub-domain exploited \nby the local bPSO swarm. An efficient combination of bPSO and NM local searches is \nimplemented in this work by using a client-server model  \n  \n 21\nSimulation-based hmPSO applications to geotechnical problems can be \ncomputationally very demanding and parallel implementation has been shown very \neffective in reducing computation time. A markedly non-linear parallel speedup has \nbeen observed during the application of hmPSO to the unsaturated flow problem \nconsidered in this work. This can be explained by considering that, as more processors \nare used, the number of particle function evaluations increases but this is not matched \nby a similar increase in the number of local searches, which would be required to \nmaintain algorithm scalability. \nLoad unbalance between different processors is detrimental to the performance of \nparallel hmPSO but negative impact can be reduced by adopting asynchronous \nimplementation, where each particle in the swarm is continuously updated regardless \nof the status of neighbouring particles.  \nReferences \n \n1. Mattsson H, Klisinski M and Axelsson K. Optimization routine for identification of \nmodel parameters in soil plasticity. International Journal for Numerical and \nAnalytical Methods in Geomechanics, 2001, 25: 435-472. \n2. Zentar R, Hicher PY, and Moulin G. Identification of soil parameters by inverse \nanalysis. Computers and Geotechnics, 1999, 28: 129-144. \n3. Mattsson H, Axelsson K and Klisinski M. On a constitutive driver as a useful tool in \nsoil plasticity. Advances in Engineering Software, 1997, 30: 661-668. \n4. Kennedy J and Eberhart R. Particle swarm optimization. In IEEE, Neural Networks \nCouncil Staff, IEEE Neural Networks Council editor Proc. IEEE International \nConference on Neural Networks, IEEE, 1995, p.1942-1948.  \n5. Perez R and Behdinan K. Particle Swarm Approach for Structural Design \nOptimization. Computers and Structures, 2007, 85:1579-1588.  \n6. Zhang W, Liu M and Clerc Y. An adaptive pso algorithm for reactive power \noptimization. In Sixth international conference on advances in power system \ncontrol, operation and management (APSCOM), Hong Kong, China, 2003, p. 302-\n307.   \n7. Fourie P and Groenwold A. The particle swarm optimization algorithm in size and \nshape optimization. Structural and Multidisciplinary Optimization, 2002, 23(4):259-\n267.  \n8. Venter G and Sobieszczanski-Sobieski J. Multidisciplinary optimization of a \ntransport aircraft wing using particle swarm optimization. Structural and \nMultidisciplinary Optimization, 2004, 26(1):121-131. \n9. Weise T. Global Optimization Algorithms - Theory and Application. Free ebook at \nwww.it-weise.de\/projects\/book.pdf. \n   \n 22\n10. Xie X, Zhang W and Yang Z. A dissipative particle swarm optimization. In \nProceedings of the 2002 Congress on Evolutionary Computation (CEC\u201902), Hawaii, \nUSA, 2002, p. 1456\u20131461. \n11. Dos Santos Coelho L and Mariani V. Particle swarm optimization with quasi-\nNewton local search for solving economic dispatch problem. In IEEE International \nConference on Systems, Man and Cybernetics, Taipei, Taiwan, Piscataway, NJ, \nIEEE Press, 2006, p. 3109-3113. \n12. Nelder J and Mead R. A simplex method for function minimization. The Computer \nJournal, 1965, 7:308\u2013313. \n13. Renders J and Flasse S. Hybrid methods using genetic algorithms for global \noptimization. IEEE Trans Syst Man Cybern B Cybern,1996, 26(2):243\u2013258.  \n14. Yen R, Liao J, Lee B and Randolph D. A hybrid approach to modeling metabolic \nsystems using a genetic algorithm and Simplex method. IEEE Transactions on \nSystems, Man and Cybernetics Part-B, 1998, 28(2):173\u2013191.  \n15. Fan S, Liang Y and Zahara E. Hybrid simplex search and particle swarm \noptimization for the global optimization of multimodal functions. Engineering \nOptimization, 2004, 36:401\u2013418. \n16. Schutte J, Reinbolt J, Fregly B, Haftka R and George A. Parallel global optimization \nwith the particle swarm algorithm. International Journal of Numerical Methods in \nEngineering, 2004,  61(13):2296-2315.  \n17. Eberhart R and Kennedy J. A new optimizer using particle swarm theory. In \nProceedings of the Sixth International Symposium on Micro Machine and Human \nScience, Nagoya, Japan, 1995, p. 39\u201343. \n18. Shi Y and Eberhart R. Parameter selection in particle swarm optimization. In \nProceedings of the 1998 Annual Conference on Evolutionary Computation, \nSpringer-Verlag, New York, 1988, p. 591-600. \n19. Shi Y and Eberhart R. A modified particle swarm optimizer. In IEEE International \nConference on Evolutionary Computation, Anchorage, Alaska, 1998, p. 69-73. \n20. Shi Y and Eberhart R. Parameter selection in particle swarm optimization. In \nProceedings of the Seventh Annual Conference on Evolutionary, New York, \nSpringer,1998, p. 591\u2013600. \n21. Hu X and Eberhart R. Adaptive particle swarm optimization: detection and response \nto dynamic systems. In Proceedings of the IEEE Congress on Evolutionary \nComputation (CEC 2002), Honolulu, Hawaii, USA, 2002, p.1666-1670.  \n22. Clerc M. Tribes - a parameter free particle swarm optimizer. \nhttp:\/\/www.mauriceclerc.net. \n23. L\u00f8bjerg M, Rasmussen T and Krink K. Hybrid particle swarm optimizer with \nbreeding and subpopulations. In Proc. of the Third Genetic and Evolutionary \nComputation Conference, 2001, 1:469-476. \n24. L\u00f8vbjerg M and Krink T. Extending particle swarm opimisers with self-organized \ncriticality. In Proceedings of Fourth Congress on Evolutionary Conference, 2002, p. \n1588-1593. \n25. van den Bergh F. An analysis of particle swarm optimizers. PhD Thesis, \nDepartment of Computer Science, University of Pretoria, Pretoria, South Africa, \n2002. \n  \n 23\n26. Bird S and Li X. Adaptively choosing niching parameters in a PSO. In Proceedings \nof the 8th Conference on Genetic and Evolutionary Computation, Seattle, \nWashington, USA, 2006, p.3-10. \n27. Brits R, Engelbrecht A and Bergh B. A niching particle swarm optimizer. In \nProceedings of the 4th Asia-Pacific Conference on Simulated Evolution and \nLearning (SEAL'02), Orchid Country Club, Singapore, 2002, p.692--696. \n28. Vesterstrom J, Riget J and Krink T. Division of labor in particle swarm optimisation. \nIn IEEE 2002  Proceedings of the Congress on Evolutionary Computation, 2002, p. \n1570-1575 \n29. Yun Y. Hybrid genetic algorithm with adaptive local search scheme. Computers and \nIndustrial Engineering, 2006; 51(1): 128-141. \n30. Das S, Koduru P, Gui M, Cochran M, Wareing A, Welch S and Babin B. Adding \nlocal search to particle swarm optimization. In Proc. IEEE 2006 Congress on \nEvolutionary Computation (CEC 2006), Vancouver, Canada, 2006, p. 428-433.  \n31. Fan S and Zahara E. A hybrid simplex search and particle swarm optimization for \nunconstrained optimization. European Journal of Operational Research, 2007, \n181(2):527-548. \n32. Genovesi S, Monorchio A, Mittra R and Manara G. A sub-boundary approach for \nenhanced particle swarm optimization and its application to the design of artificial \nmagnetic conductors. IEEE Transactions on Antennas and Propagation, 2007; \n55(3):766-770. \n33. Cantu-Paz E. Markov chain models of parallel genetic algorithms. IEEE \nTransactions on Evolutionary Computation, 2000, 4(3):216-226. \n34. Suganthan P, Hansen N, Liang J, Deb K, Chen Y, Auger A and Tiwari S. Problem \ndefinitions and evaluation criteria for the CEC 2005 special session on real \nparameter optimization. National Chiao Tung University, Natural Computing \nLaboratory, Hsinchu, Taiwan, NC Lab Report No. NCL-TR-2005001, 2005. \n35. Snir M, Otto S, Huss-Lederman S, Walker D and Dongarra J. MPI: The Complete \nReference. MIT Press, 1996. \n36. Van Genuchten M. A closed form equation for predicting the hydraulic conductivity \nof unsaturated soils. Soil Sci. Soc. Am. J., 1980, 44:892-898.  \n37. Mualem Y. A new model predicting the hydraulic conductivity of unsaturated porous \nmedia. Water Resour. Res., 1976, 12:513\u2013522. \n38. Zhang Y, Augarde C, Gallipoli D. Identification of hydraulic parameters for \nunsaturated soils using particle swarm optimization. In Proceedings 1st European \nConference on Unsaturated Soils, Durham, United Kingdom, 2008, p.765-771 \n39. Celia M, Bouloutas E and Zarba R. A general mass-conservative numerical solution \nfor the unsaturated flow problem. Water Resources Research, 1990, 26(7):1483-\n1496. \n40. Simunek J, van Genuchten M, Gribb M and Hopmans J. Parameter estimation of \nunsaturated soil hydraulic properties from transient flow processes. Soil & Tillage \nResearch, 1998, 47(1):27-36. \n \nTABLES \n \nTable 1: Mathematical functions for numerical tests \nFunction  f(x) Features Dimension Range Global \nminimum\nRasgrigin Multimodal n=5, n=10 [-5.12, 5.12]n g (x*) =0 \nGriewank Multimodal, shifted n=5, n=10 [-600, 600] n g (x*) =0 \nAckley Multimodal, shifted n=5, n=10 [-32, 32] n g (x*) =-140 \nSphere Unimodal, shifted n=5, n=10 [-100, 100] n g (x*) =-450 \nRosenbrock Multimodal, shifted  n=5, n=10 [-100, 100] n g (x*) =390 \n \n \nTable 2:  Settings for hmPSO in numerical tests \nParameter Value \nSwarm size 20 particles for n=5 40 particles for n=10 \nRange of inertia weight, [wmin , wmax] [0.4, 0.75] \nCognitive weight,  c1 2.0 \nSocial weight, c2 2.0 \nMaximum velocity, vmax 0.3 (xu - xl)  \nTermination tolerance  1.0 e-5 \nMaximum number of function evaluations 10000 n \nRange of sub-domain radius,  [\u03b4min , \u03b4max] [0.2, 0.4] \nMaximum iteration number of Nelder-\nMead method 300 \n \n  \n 25\nTable 3: Success rates during numerical tests \nFunction Algorithm Success rate \nShifted Sphere \nn=5 \nhmPSO 100% \nbPSO 28% \nShifted Sphere \nn=10 \nhmPSO 100% \nbPSO 8% \nShifted Rosenbrock \nn=5 \nhmPSO 100% \nbPSO 0% \nShifted Rosenbrock \nn=10 \nhmPSO 72% \nbPSO 0% \nRasgrigin \nn=5 \nhmPSO 96% \nbPSO 96% \nRasgrigin \nn=10 \nhmPSO 12% \nbPSO 16% \nShifted Griewank \nn=5 \nhmPSO 8% \nbPSO 4% \nShifted Griewank \nn=10 \nhmPSO 96% \nbPSO 0% \nShifted Ackley \nn=5 \nhmPSO 100% \nbPSO 64% \nShifted Ackley \nn=10 \nhmPSO 100% \nbPSO 12% \n \n   \n 26\n Table 4.  Parameter ranges for the Mualem-van Genuchten model [33] \nParameter Minimum value Maximum value \n\u03b2 1.001 3.5 \n\u03b1 0.1 m-1 9.6 m-1 \n\u03b8s 0.21 0.7 \n\u03b8r 0.001 0.2 \nKs 5.0E-8 m\/s 5.0E-4 m\/s \n \n  \n 27\nTable 5:  Settings for hmPSO in simulation-based optimization test \nParameter Value \nSwarm size 30 particles per processor \nRange of inertia weight, [wmin , wmax] [0.4, 0.75] \nCognitive weight,  c1 2.0 \nSocial weight, c2 2.0 \nMaximum velocity, vmax 0.3 (xu - xl)  \nTermination tolerance  1.0 e-5 \nMaximum number of evaluations of the \nobjective function 500000 \nRange of sub-domain radius,  [\u03b4min , \u03b4max] [0.2, 0.4] \nMaximum number of NM transformations 300 \n \n   \n 28\nTable 6: Sequence of NM local searches in parallel hmPSO for a run using 20 particles \nLocal \nsearch \ncounter \n \nObjective \nfunction \nvalue \nParameters \n  \u03b2            \u03b1         \u03b8r           \u03b8s             Ks \n1 \ninitial \nguesses at \nvertices of  \nsimplex \n1.59E+00 2.945 1.927 0.081 0.529  9.39E-05 \n1.86E+00 4.096 1.994 0.210 0.347  9.54E-05 \n2.51E+00 2.483 2.389 0.171 0.612  8.99E-05 \n3.19E+00 4.511 2.457 0.210 0.355  3.61E-04 \n3.94E+00 3.537 1.807 0.136 0.578  1.22E-04 \n4.02E+00 1.684 2.634 0.051 0.554  3.34E-05 \nsolution 6.08E-02 2.924 2.016 0.083 0.526  1.00E-04 \n2 \ninitial \nguesses at \nvertices of  \nsimplex \n6.08E-02 2.924 2.016 0.083 0.526  1.00E-04 \n2.61E-01 3.561 1.961 0.087 0.395  1.23E-04 \n4.79E-01 3.274 1.956 0.064 0.604  1.62E-04 \n5.06E-01 4.689 1.910 0.095 0.476  3.41E-04 \n5.23E-01 3.414 1.943 0.104 0.408  1.03E-04 \n5.27E-01 3.948 1.911 0.050 0.851  4.07E-04 \nsolution 3.60E-02 3.014 2.012 0.092 0.450  8.91E-05 \n3 \ninitial \nguesses at \nvertices of  \nsimplex \n3.60E-02 3.014 2.012 0.092 0.450  8.91E-05 \n1.44E-01 3.414 1.976 0.101 0.382  9.95E-05 \n2.22E-01 3.617 1.961 0.072 0.646  2.40E-04 \n2.61E-01 3.561 1.961 0.087 0.395  1.23E-04 \n4.79E-01 3.274 1.956 0.064 0.604  1.62E-04 \n5.06E-01 4.689 1.910 0.095 0.476  3.41E-04 \nsolution 1.54E-02 3.178 2.005 0.098 0.401  8.90E-05 \n4 \ninitial \nguesses at \nvertices of  \nsimplex \n1.54E-02 3.178 2.005 0.098 0.401  8.90E-05 \n1.42E-01 3.160 2.001 0.090 0.498  1.16E-04 \n1.44E-01 3.414 1.976 0.101 0.382  9.95E-05 \n1.63E-01 3.166 2.001 0.071 0.497  1.23E-04 \n1.67E-01 3.450 2.007 0.106 0.381  1.06E-04 \n2.22E-01 3.617 1.961 0.072 0.646  2.40E-04 \nsolution 7.32E-04 3.345 2.000 0.102 0.370  9.26E-05 \n5 \ninitial \nguesses at \nvertices of  \nsimplex \n7.42E-04 3.345 2.000 0.102 0.370  9.26E-05 \n6.90E-02 3.602 1.993 0.079 0.657  2.53E-04 \n8.91E-02 3.629 1.988 0.109 0.343  1.04E-04 \n9.65E-02 3.883 1.988 0.079 0.590  2.85E-04 \n1.42E-01 3.160 2.001 0.090 0.498  1.16E-04 \n1.44E-01 3.414 1.976 0.101 0.382  9.95E-05 \nsolution 2.73E-06 3.350 2.000 0.102 0.368  9.22E-05 \n \n \n \n \n \n \n  \n 29\nTable 7:  Number of evaluations of the objective function for different particles allocated \nto different processors (for a specific run with 48 particles and 50 processors) \nParticle no.  Function evaluations Particle no. \nFunction \nevaluations Particle no.  \nFunction \nevaluations \n1 7211  17 8383  33 8016  \n2 9268  18 8049  34 6850  \n3 8274  19 7164  35 7028  \n4 6989  20 6994  36 6706  \n5 6417  21 6555  37 7177  \n6 7196  22 6845  38 6733  \n7 6754  23 6976  39 6495  \n8 5757  24 7140  40 7339  \n9 8926  25 7505  41 6546  \n10 9052  26 8139  42 7290  \n11 8095  27 6988  43 6017  \n12 7235  28 7117  44 7248  \n13 7287  29 7172  45 6229  \n14 6330  30 7300  46 6813  \n15 6286  31 7284  47 7327  \n16 6982  32 6065  48 7825  \n   \n 30\nTable 8: Efficiency and success rate over 25 runs for bPSO, hmPSO (serial) and \nhmPSO (parallel)  \n No. of particles \nNo. of \nprocessors \nComputational time (s) Parallel \nspeedup \nSuccess \nrate  (%) Lowest Largest Mean \nbPSO 30 1 47921 77638 69087  0 \nhmPSO (serial) 30 1 1352 46666 5595  100 \nhmPSO (parallel) \n15 17 530 2278 1147 4.87 96 \n20 22 349 4624 1176 4.75 100 \n30 32 454 1510 903 6.19 100 \n35 37 412 1292 823 6.79 100 \n45 47 349 1093 719 7.77 96 \n  \n 31\nFIGURES \n \nFig. 1: Serial hmPSO algorithm \n  \nforeach particle i = 1, . . . , Np do \nxi   \u2190 generate random xi \u2208[xl , xu]; \nvi   \u2190 generate random vi \u2208[-vmax , +vmax]; \nf(xi)    \u2190 evaluate objective function ; \nif (f(xi) < f(Pi)) Pi\u2190 xi \nif (f(xi) < f(Pg)) Pg\u2190 xi \nend foreach \n \nk\u21900 \nIL\u21900 \nrepeat \n foreach particle i = 1, . . . , Np do \n  update velocity vi using Eq. 2 \n  apply bounds constraints on vi \n  update position xi \napply bounds constraints on xi \n f(xi)    \u2190 evaluate objective function; \nif (f(xi) < f(Pi)) Pi\u2190 xi \nif (f(xi) < f(Pg)) Pg\u2190 xi \n    end foreach particle \n    \nif (mod (k, NL) =0) \n sortParticles( )  \n  xL = doLocalSearch(best n+1 particles) ; \n  restartSubSwarm (particleIDs), [xL-\u03b4I ,  xL+\u03b4I] ; \nIL\u2190 IL + 1  \n    endif \n k\u2190 k+1 \nwhile (stop criteria is not met)  \n   \n 32\n \n \nFig. 2: Client-server model for parallel hmPSO \n \n \n \n  \n 33\n \n \nFig. 3. Flowchart of the parallel hmPSO \n \n \n \ninitialize particle\nwith x \n \n, v \n \nand f \n  \nsend x , f  to server\ndata received ?\n(1) receive \ndata\nsend stop\nupdate gbest\nsend pbest \/ stop \ninformation to server\nstop criterion? \nsort particle\ndo local search\nsend results \/ stop \ninformation to server\nsend initial \nguess\nc alculate fitness\nu pdate particle best\nupdate v , x\nservice type\nserver processparticle client process \nlocal search \nclient process \ndata received ?\ngbest\ns top info\ns top \ninitial guess\nstop info\nstop\nstop\nstop\nupdate \ngbest\nsend gbest\nparticle local search\nsend restart\ns top criterion?\nr eceive data\nr eceive data \n   \n 34\n \nFig. 4. Objective function values during numerical tests \n \n1.0E-10 1.0E-07 1.0E-04 1.0E-01 1.0E+02 1.0E+05 1.0E+08\nShifted Sphere (n=5)\nShifted Sphere (n=10)\nShifted Rosenbrock (n=5)\nShifted Rosenbrock (n=10)\nRasgrigin (n=5)\nRasgrigin  (n=10)\nShifted Griewank (n=5)\nShifted Griewank  (n=10)\nShifted Ackley (n=5)\nShifted Ackley  (n=10)\nObjective function value\nbPSO\nhmPSO\n  \n 35\n \n \nFig. 5. Number of objective function evaluations during numerical tests \n \n \n  \n1.0E+02 1.0E+03 1.0E+04 1.0E+05 1.0E+06\nShifted Sphere (n=5)\nShifted Sphere (n=10)\nShifted Rosenbrock (n=5)\nShifted Rosenbrock (n=10)\nRasgrigin (n=5)\nRasgrigin  (n=10)\nShifted Griewank (n=5)\nShifted Griewank  (n=10)\nShifted Ackley (n=5)\nShifted Ackley  (n=10)\nNumber of objective function evaluations\nbPSO\nhmPSO\n   \n 36\n \nFig. 6: Convergence characteristics of bPSO and hmPSO for Shifted Rosenbrock \n(n=10) function \n  \n1.E-08\n1.E-06\n1.E-04\n1.E-02\n1.E+00\n1.E+02\n1.E+04\n1.E+06\n1.E+08\n1.E+10\n0 20000 40000 60000 80000 100000\nObjective function evaluations\nO\nbj\nec\ntiv\ne \nfu\nnc\ntio\nn \nm\nin\nim\num\n \nbPSO \nhmPSO \n  \n 37\n \n \nFigure 7. Water pressure head profile. \n \n \n \n \n \nFigure 8. Water content profile \n0.7\n0.8\n0.9\n1.0\n-11.0 -9.5 -8.0 -6.5 -5.0 -3.5 -2.0 -0.5\nPressure head (m)\nEl\nev\nat\nio\nn \n(m\n)\n0.7\n0.8\n0.9\n1.0\n0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60\nWater content\nEl\nev\nat\nio\nn \n(m\n)\n t = 1 hr \n t = 2 hr \n t = 3 hr \n t = 4 hr \n t = 5 hr \n t = 6 hr \n t = 1 hr \n t = 2 hr \n t = 3 hr \n t = 4 hr  t = 5 hr \n t = 6 hr \n Figure 9. Number of local searches versus computational time for parallel hmPSO. \nEach point represents the average over successful runs out of a total of 25 runs when \nusing 17, 22, 32, 37 and 47 processors. \n600\n700\n800\n900\n1000\n1100\n1200\n10 12 14 16 18 20 22 24\nNumber of local serches\nC\nom\npu\nta\ntio\nna\nl t\nim\ne \n(s\n)\n  \n 39\n1.E-06\n1.E-05\n1.E-04\n1.E-03\n1.E-02\n1.E-01\n1.E+00\n1.E+01\n1.E+02\n0 50000 100000 150000 200000 250000 300000 350000 400000 450000\nObjective function evaluations\nO\nbj\nec\ntiv\ne \nfu\nnc\ntio\nn \nm\nin\nim\num\n \nFig. 10: Convergence characteristics for serial and parallel hmPSO  \n47 processors\n17 processors1 processor\n   \n 40\n \nFigure 11. Computational time and parallel speedup versus number of processors for \nhmPSO. Each point represents the average over successful runs out of a total of 25 \nruns when using 1, 17, 22, 32, 37 and 47 processors. \n \n \n \n0\n1000\n2000\n3000\n4000\n5000\n6000\n0 10 20 30 40 50\nNumber of processors\nC\nom\npu\nta\ntio\nna\nl t\nim\ne \n(s\n)\n1\n2\n3\n4\n5\n6\n7\n8\nP\nar\nal\nle\nl s\npe\ned\nup\nComputational time\nParallel speedup\n1\n"}