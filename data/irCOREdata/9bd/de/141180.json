{"doi":"10.1080\/00207540500543265","coreId":"141180","oai":"oai:dspace.lib.cranfield.ac.uk:1826\/6882","identifiers":["oai:dspace.lib.cranfield.ac.uk:1826\/6882","10.1080\/00207540500543265"],"title":"Robust kernel distance multivariate control chart using support vector\nprinciples","authors":["Camci, Fatih","Chinnam, R. B.","Ellis, R. D."],"enrichments":{"references":[{"id":37929007,"title":"A class of distribution-free control charts.","authors":[],"date":"2004","doi":"10.1111\/j.1467-9876.2004.0d489.x","raw":"Chakraborti, S., Van der Laan, P. and Van de Wiel, M., A class of distribution-free control charts. Journal of the Royal Statistical Society, Series C, 2004, 55(3), 443-462 Chen, Q., Kruger, U., Meronk, M. and Leung, A. Y. T., Synthetic of t2 and q statistics for process monitoring, control engineering practice. Control Engineering Practice, 2004, 12, 745-755.","cites":null},{"id":37929030,"title":"A comparison of neural networks to spc charts. Computers and Industrial Engineering,","authors":[],"date":"1991","doi":"10.1016\/0360-8352(91)90097-p","raw":"Pugh, G. A., A comparison of neural networks to spc charts. Computers and Industrial Engineering, 1991, 21, 253-25528 Raich, A. and Cinar, A., Statistical process monitoring and disturbance diagnosis in multivariate continuous processes. Journal of the American Institute of Chemical Engineers, 1996, 42, 995-1009.","cites":null},{"id":37929037,"title":"A kernel-distance-based multivariate control chart using support vector methods.","authors":[],"date":"2003","doi":"10.1080\/1352816031000075224","raw":"Sun, R. and Tsung, F., A kernel-distance-based multivariate control chart using support vector methods. International Journal of Production Research, 2003, 41, 2975-2989.","cites":null},{"id":37929032,"title":"A markov chain model for the multivariate exponentially weighted moving averages control chart.","authors":[],"date":"1996","doi":"10.2307\/2291599","raw":"Runger, G. C. and Prabhu, S. S., A markov chain model for the multivariate exponentially weighted moving averages control chart. Journal of the American Statistical Association (JASA), 1996, 91, 1701-1706.","cites":null},{"id":37929020,"title":"A multivariate exponentially weighted moving average control chart.","authors":[],"date":"1992","doi":"10.2307\/1269551","raw":"Lowry, C. A., Woodall, W. H., Champ, C. W. and Rigdon, S. E., A multivariate exponentially weighted moving average control chart. Technometrics, 1992, 34, 46-53.","cites":null},{"id":37929025,"title":"A nonparametric multivariate control chart based on data depth.","authors":[],"date":"2004","doi":null,"raw":"Messaoud, A., Weihs, C. and Hering, F. (2004). A nonparametric multivariate control chart based on data depth. Dortmund, Germany, Department of Statistics, University of Dortmund. Montgomery, D. C., Introduction to statistical quality control. (New York: Wiley).","cites":null},{"id":37929019,"title":"A quality index based on data depth and multivariate rank tests.","authors":[],"date":"1993","doi":"10.2307\/2290720","raw":"Liu, R. Y. and Singh, K., A quality index based on data depth and multivariate rank tests. Journal of the American Statistical Association (JASA), 1993, 88, 252-260.27 Lowry, C. A. and Montgomery, D. C., A review of multivariate control charts. IIE Transactions, 1995, 27, 800-810.","cites":null},{"id":37929029,"title":"A smooth nonparametric approach to multivariate process capability.","authors":[],"date":"2001","doi":"10.1198\/004017001750386314","raw":"Polansky, A. M., A smooth nonparametric approach to multivariate process capability. Technometrics, 2001, 43, 199-211.","cites":null},{"id":37929028,"title":"Adequately address abnormal situation operations. Chemical Engineering Progress,","authors":[],"date":"1995","doi":null,"raw":"Nimmo, I., Adequately address abnormal situation operations. Chemical Engineering Progress, 1995, 91, 36-45.","cites":null},{"id":37929026,"title":"An introduction to kernel-based learning algorithms.","authors":[],"date":"2001","doi":"10.1109\/72.914517","raw":"M\u00fcller, K. R., Mika, S., R\u00e4tsch, G., Tsuda, K. and Sch\u00f6lkopf, B., An introduction to kernel-based learning algorithms. IEEE Neural Networks, 2001, 12, 181-201.","cites":null},{"id":37929009,"title":"Automation and the total quality paradigm,","authors":[],"date":"1992","doi":null,"raw":"Chinnam, R. B. and Kolarik, W. J., Automation and the total quality paradigm, in Proceedings of the 1st IERC, 1992, Chicago, IL, IIE.","cites":null},{"id":37929042,"title":"Design strategies for the multivariate exponentially weighted moving average control chart.","authors":[],"date":"2004","doi":"10.1002\/qre.568","raw":"Testik, M. C. and Borror, C. M., Design strategies for the multivariate exponentially weighted moving average control chart. Quality and Reliability Engineering International, 2004, 20, 571-577.29 Vapnik, V., Statistical learning theory. (New York: Wiley).","cites":null},{"id":37929024,"title":"Functions of positive and negative type and their connection with the theory of integral equations.","authors":[],"date":"1909","doi":"10.1098\/rspa.1909.0075","raw":"Mercer, J., Functions of positive and negative type and their connection with the theory of integral equations. Philosophical Transactions of the Royal Society of London, Series A, 1909, 209, 415-446.","cites":null},{"id":37929031,"title":"Mathematics of success and failure. Circuits and Devices,","authors":[],"date":"1991","doi":"10.1109\/101.101753","raw":"Rose, K., Mathematics of success and failure. Circuits and Devices, IEEE, 1991, 7, 26-30.","cites":null},{"id":37929005,"title":"Multiscale PCA with application to multivariate statistical process monitoring.","authors":[],"date":"1998","doi":"10.1002\/aic.690440712","raw":"Bakshi, B., Multiscale PCA with application to multivariate statistical process monitoring. Journal of the American Institute of Chemical Engineers, 1998, 44, 1596-1610.","cites":null},{"id":37929004,"title":"Multiscale statistical process control using wavelets: Theoretical analysis and properties.","authors":[],"date":"2001","doi":"10.1002\/aic.690490412","raw":"Aradhye, H. B., Bakshi, B. R., Strauss, R. A. and Davis, J. F. (2001). Multiscale statistical process control using wavelets: Theoretical analysis and properties. Columbus, OH, Ohio State University.","cites":null},{"id":37929027,"title":"Multivariate cumulative sum control charts based on projection pursuit. Statistica Sinica,","authors":[],"date":"2001","doi":null,"raw":"Ngai, H. and Zhang, J., Multivariate cumulative sum control charts based on projection pursuit. Statistica Sinica, 2001, 11, 747-766.","cites":null},{"id":37929033,"title":"Multivariate extensions to cumulative sum control charts. Quality and Reliability Engineering International,","authors":[],"date":"2004","doi":"10.1002\/qre.571","raw":"Runger, G. C. and Testik, M. C., Multivariate extensions to cumulative sum control charts. Quality and Reliability Engineering International, 2004, 20, 587 - 606.","cites":null},{"id":37929015,"title":"Multivariate quality control-illustrated by the air testing of sample bombsights. Techniques of statistical analysis.","authors":[],"date":"1947","doi":null,"raw":"Hotelling, H. (1947). Multivariate quality control-illustrated by the air testing of sample bombsights. Techniques of statistical analysis. C. Eisenhart, M. W. Hastay and W. A. Wallis. New York, McGraw-Hill: 111-184.","cites":null},{"id":37929017,"title":"Nonlinear programming,","authors":[],"date":"1951","doi":"10.1007\/978-3-0348-0439-4_11","raw":"Kuhn, H. and Tucker, A., Nonlinear programming, in Proceedings of 2nd Berkeley Symposium on Mathematical Statistics and Probabilistics, 1951, Berkely, CA, University of California Press, 481-492.","cites":null},{"id":37929022,"title":"Nonlinear programming.","authors":[],"date":"2004","doi":"10.1016\/j.compchemeng.2004.07.027","raw":"Computers & Chemical Engineering, 2004, 28, 1157-1166. Mangasarian, O. L., Nonlinear programming. (Philadelphia, PA: Society for Industrial and Applied Mathematics).","cites":null},{"id":37929006,"title":"Nonparametric control charts: An overview and some results.","authors":[],"date":"2001","doi":null,"raw":"Chakraborti, S., Van der Laan, P. and Bakir, S. T., Nonparametric control charts: An overview and some results. Journal of Quality Technology, 2001, 33, 304-315.","cites":null},{"id":37929036,"title":"On shewhart-type nonparametric multivariate control charts based on data depth. Frontiers in Statistical Quality Control,","authors":[],"date":"2001","doi":"10.1007\/978-3-642-57590-7_13","raw":"Stoumbos, Z. G. and Reynolds, M. R., On shewhart-type nonparametric multivariate control charts based on data depth. Frontiers in Statistical Quality Control, 2001, 6, 207-227.","cites":null},{"id":37929038,"title":"One class classification.","authors":[],"date":"2001","doi":"10.1109\/icpr.2004.1334542","raw":"Tax, D. M. (2001). One class classification. Delft, The Netherlands, Delft Technical University.","cites":null},{"id":37929044,"title":"Principal-component analysis of multiscale data for process monitoring and fault diagnosis.","authors":[],"date":"2004","doi":"10.1002\/aic.10260","raw":"Yoon, S. and MacGregor, J. F., Principal-component analysis of multiscale data for process monitoring and fault diagnosis. Journal of the American Institute of Chemical Engineers, 2004, 50, 2891-2903.","cites":null},{"id":37929023,"title":"Process performance monitoring using multivariate statistical process control.","authors":[],"date":"1996","doi":"10.1049\/ip-cta:19960321","raw":"Martin, E. B., Morris, A. J. and Zhang, J., Process performance monitoring using multivariate statistical process control. IEE Proceedings, 1996, 143, 132-144.","cites":null},{"id":37929035,"title":"Quality control charts. Bell System Technical Journal,","authors":[],"date":"1926","doi":"10.1002\/j.1538-7305.1926.tb00125.x","raw":"Shewhart, W. A., Quality control charts. Bell System Technical Journal, 1926, 22, 593-603. Smith, A. E., X-bar and r control chart interpretation using neural computing.","cites":null},{"id":37929043,"title":"Research issues and ideas in statistical process control.","authors":[],"date":"1999","doi":null,"raw":"Woodall, W. H. and Montgomery, D. C., Research issues and ideas in statistical process control. Journal of Quality Technology, 1999, 31, 376-386.","cites":null},{"id":37929021,"title":"Statistical process control of multivariate processes. Control Engineering Practice,","authors":[],"date":"1995","doi":"10.1016\/0967-0661(95)00014-l","raw":"MacGregor, J. F. and Kourti, T., Statistical process control of multivariate processes. Control Engineering Practice, 1995, 3, 403-414. Manabu, K., Shinji, H., Iori, H. and Hiromu, O., Evolution of multivariate statistical process control: Application of independent component analysis and external analysis.","cites":null},{"id":37929013,"title":"Statistical process control: What you don't measure can hurt you!","authors":[],"date":"2003","doi":"10.1109\/ms.2003.1184166","raw":"Eickelmann, N. and Anant, A., Statistical process control: What you don't measure can hurt you! Software, IEEE, 2003, 20, 49-51.","cites":null},{"id":37929040,"title":"Support vector data description.","authors":[],"date":"2004","doi":"10.1023\/b:mach.0000008084.60811.49","raw":"Tax, D. M. J. and Duin, R. P. W., Support vector data description. Machine Learning, 2004, 54, 45-66.","cites":null},{"id":37929039,"title":"Support vector domain description. Pattern Recognition Letters,","authors":[],"date":"1999","doi":"10.1016\/s0167-8655(99)00087-2","raw":"Tax, D. M. J. and Duin, R. P. W., Support vector domain description. Pattern Recognition Letters, 1999, 20, 1191-1199.","cites":null},{"id":37929046,"title":"Support vector machines for class representation and discrimination,","authors":[],"date":"2003","doi":"10.1109\/ijcnn.2003.1223940","raw":"Yuan, C. and Casasent, D., Support vector machines for class representation and discrimination, in International Joint Conference on Neural Networks, 2003, Portland, OR, 1610-1615.","cites":null},{"id":37929008,"title":"Support vector machines for recognizing shifts in correlated and other manufacturing processes.","authors":[],"date":"2002","doi":"10.1080\/00207540210152920","raw":"Chinnam, R. B., Support vector machines for recognizing shifts in correlated and other manufacturing processes. International Journal of Production Research, 2002, 40, 4449-4466.","cites":null},{"id":37929034,"title":"The effect of non-normality on the control limits of X charts.","authors":[],"date":"1976","doi":null,"raw":"Schilling, E. G. and Nelson, P.R., The effect of non-normality on the control limits of X charts. Journal of Quality Technology, 1976, 8, 183-187.","cites":null},{"id":37929011,"title":"Using radial basis function neural networks to recognize shifts in correlated manufacturing process parameters. IIE Transactions,","authors":[],"date":"1998","doi":"10.1080\/07408179808966453","raw":"Cook, D. F. and Chiu, C., Using radial basis function neural networks to recognize shifts in correlated manufacturing process parameters. IIE Transactions, 1998, 30, 227-234. Cristianini, N. and Taylor, J. S., An introduction to support vector machines and other kernel-based learning methods. (Cambridge: Cambridge University Press).","cites":null},{"id":37929041,"title":"Wavelet-pls regression models for both exploratory data analysis and process monitoring.","authors":[],"date":"2000","doi":"10.1002\/1099-128x(200009\/12)14:5\/6<383::aid-cem616>3.0.co;2-5","raw":"Teppola, P. and Minkkinen, P., Wavelet-pls regression models for both exploratory data analysis and process monitoring. Journal of Chemometrics, 2000, 14, 383-399.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2012-01-24","abstract":"It is important to monitor manufacturing processes in order to improve product\nquality and reduce production cost. Statistical Process Control (SPC) is the\nmost commonly used method for process monitoring, in particular making\ndistinctions between variations attributed to normal process variability to\nthose caused by \u2018special causes\u2019. Most SPC and multivariate SPC (MSPC) methods\nare parametric in that they make assumptions about the distributional properties\nand autocorrelation structure of in-control process parameters, and, if\nsatisfied, are effective in managing false alarms\/-positives and false-\nnegatives. However, when processes do not satisfy these assumptions, the\neffectiveness of SPC methods is compromised. Several non-parametric control\ncharts based on sequential ranks of data depth measures have been proposed in\nthe literature, but their development and implementation have been rather slow\nin industrial process control. Several non-parametric control charts based on\nmachine learning principles have also been proposed in the literature to\novercome some of these limitations. However, unlike conventional SPC methods,\nthese non-parametric methods require event data from each out-of-control process\nstate for effective model building. The paper presents a new non-parametric\nmultivariate control chart based on kernel distance that overcomes these\nlimitations by employing the notion of one-class classification based on support\nvector principles. The chart is non-parametric in that it makes no assumptions\nregarding the data probability density and only requires \u2018normal\u2019 or in-control\ndata for effective representation of an in-control process. It does, however,\nmake an explicit provision to incorporate any available data from out-of-control\nprocess states. Experimental evaluation on a variety of benchmarking datasets\nsuggests that the proposed chart is effective for process mo","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/141180.pdf","fullTextIdentifier":"http:\/\/dx.doi.org\/10.1080\/00207540500543265","pdfHashValue":"a0b491190e004d9aa2c636591dd5d0d9514f410c","publisher":"Taylor & Francis","rawRecordXml":"<record><header><identifier>\noai:dspace.lib.cranfield.ac.uk:1826\/6882<\/identifier><datestamp>2012-02-13T11:20:18Z<\/datestamp><setSpec>hdl_1826_24<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>Robust kernel distance multivariate control chart using support vector\nprinciples<\/dc:title><dc:creator>Camci, Fatih<\/dc:creator><dc:creator>Chinnam, R. B.<\/dc:creator><dc:creator>Ellis, R. D.<\/dc:creator><dc:subject>Control chart, Support vector machines, Kernel distance<\/dc:subject><dc:description>It is important to monitor manufacturing processes in order to improve product\nquality and reduce production cost. Statistical Process Control (SPC) is the\nmost commonly used method for process monitoring, in particular making\ndistinctions between variations attributed to normal process variability to\nthose caused by \u2018special causes\u2019. Most SPC and multivariate SPC (MSPC) methods\nare parametric in that they make assumptions about the distributional properties\nand autocorrelation structure of in-control process parameters, and, if\nsatisfied, are effective in managing false alarms\/-positives and false-\nnegatives. However, when processes do not satisfy these assumptions, the\neffectiveness of SPC methods is compromised. Several non-parametric control\ncharts based on sequential ranks of data depth measures have been proposed in\nthe literature, but their development and implementation have been rather slow\nin industrial process control. Several non-parametric control charts based on\nmachine learning principles have also been proposed in the literature to\novercome some of these limitations. However, unlike conventional SPC methods,\nthese non-parametric methods require event data from each out-of-control process\nstate for effective model building. The paper presents a new non-parametric\nmultivariate control chart based on kernel distance that overcomes these\nlimitations by employing the notion of one-class classification based on support\nvector principles. The chart is non-parametric in that it makes no assumptions\nregarding the data probability density and only requires \u2018normal\u2019 or in-control\ndata for effective representation of an in-control process. It does, however,\nmake an explicit provision to incorporate any available data from out-of-control\nprocess states. Experimental evaluation on a variety of benchmarking datasets\nsuggests that the proposed chart is effective for process mon<\/dc:description><dc:publisher>Taylor & Francis<\/dc:publisher><dc:date>2012-01-24T23:02:24Z<\/dc:date><dc:date>2012-01-24T23:02:24Z<\/dc:date><dc:date>2012-01-24<\/dc:date><dc:type>Article<\/dc:type><dc:identifier>0020-7543<\/dc:identifier><dc:identifier>http:\/\/dx.doi.org\/10.1080\/00207540500543265<\/dc:identifier><dc:identifier>http:\/\/dspace.lib.cranfield.ac.uk\/handle\/1826\/6882<\/dc:identifier><dc:language>en_UK<\/dc:language><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0020-7543","0020-7543"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2012,"topics":["Control chart, Support vector machines, Kernel distance"],"subject":["Article"],"fullText":"International Journal of Production Research,\nVolume 46, Issue 18, 2008, pp 5075-5095\nA robust kernel-distance multivariate control chart using support\nvector principles\nF. Camci\u2020, R. B. Chinnam*\u2021, and R. D. Ellis\u2021\n\u2020Impact Technologies, LLC, 200 Canal View Boulevard, Rochester, NY 14623\n\u2021Department of Industrial & Manufacturing Engineering, Wayne State University,\nDetroit, MI 48201, USA\n* Corresponding Author: Tel: +313-577-4846; Fax: +313-577-8833; E-mail: r_chinnam@wayne.edu\n2Abstract\nIt is important to monitor manufacturing processes in order to improve product quality and\nreduce production cost. Statistical Process Control (SPC) is the most commonly used method for\nprocess monitoring, in particular, making distinctions between variations attributed to normal\nprocess variability to those caused by \u2018special causes\u2019. Most SPC and multivariate SPC (MSPC)\nmethods are parametric in that they make assumptions about the distributional properties and\nauto-correlation structure of in-control process parameters, and if satisfied, are effective in\nmanaging false alarms\/positives and false negatives. However, when processes do not satisfy\nthese assumptions, the effectiveness of SPC methods is compromised. Several non-parametric\ncontrol charts based on sequential ranks of data depth measures have been proposed in the\nliterature, but their development and implementation have been rather slow in industrial process\ncontrol. Several non-parametric control charts based on machine learning principles have also\nbeen proposed in the literature to overcome some of these limitations. However, unlike\nconventional SPC methods, these non-parametric methods require event data from each out-of-\ncontrol process state for effective model building. This paper presents a new non-parametric\nmultivariate control chart based on kernel-distance that overcomes these limitations by\nemploying the notion of one-class classification based on support vector principles. The chart is\nnon-parametric in that it makes no assumptions regarding the data probability density and only\nrequires \u2018normal\u2019 or in-control data for effective representation of an in-control process. It does\nhowever make an explicit provision to incorporate any available data from out-of-control process\nstates. Experimental evaluation on a variety of benchmarking datasets suggests that the proposed\nchart is effective for process monitoring.\nKeywords: Control chart; Support vector machines; Kernel-distance\n3I. Introduction\nIn order to improve product quality and reduce production cost, it is necessary to detect\nequipment malfunctions, failures, or other special events as early as possible. For example,\naccording to the survey conducted by (Nimmo 1995; Chen, Kruger et al. 2004), the US-based\npetrochemical industry could save up to $10 billion annually if abnormal process behavior could\nbe detected, diagnosed and appropriately dealt with. By monitoring the performance of a process\nover time, statistical process control (SPC) attempts to distinguish process variation attributed to\ncommon causes from variation attributed to special causes, and hence, forms a basis for process\nmonitoring and equipment malfunction detection (Martin, Morris et al. 1996). It is also the most\ncommonly used tool to analyze and monitor processes (Eickelmann and Anant 2003).\nMost SPC methods are parametric in that they make assumptions about the distributional\nproperties and auto-correlation structure of in-control process parameters, and if satisfied, are\nvery effective in managing false alarms\/positives and false negatives. Among others, they offer\nthe following statistical and practical advantages: 1) only data from an in-control state is\nnecessary to initialize the control chart, 2) data from a relatively limited number of sub-groups\n(say 20 to 30) is adequate for accurate initialization, and 3) tradeoffs between false\nalarms\/positives (Type-I error) and false negatives (Type-II errors) can be managed (by changing\nthe width of the control limits and the sub-group sample size, respectively). While conventional\nSPC charts (such as the Shewhart type control charts (Shewhart 1926), the cumulative sum\n(CUSUM) control charts, and the exponentially weighted family control charts) are developed for\nunivariate processes (Manabu, Shinji et al. 2004), multivariate statistical process control (MSPC)\nis employed to monitor processes that have correlated multi-variables (Montgomery 2001). One\ntype of MSPC is multivariate charts extended from univariate SPC methods, including\nHotelling\u2019s 2T chart (Hotelling 1947), multivariate EWMA (Lowry, Woodall et al. 1992);\n(Runger and Prabhu 1996), (Testik and Borror 2004) and multivariate CUSUM charts (Ngai and\n4Zhang 2001); (Runger and Testik 2004). Another type of MSPC is based on latent variable\nprojection, such as Principal Component Analysis (PCA) and Partial Least Squares (PLS)\n(MacGregor and Kourti 1995) (Raich and Cinar 1996) (Yoon and MacGregor 2004). An\nalternative approach to account for the dynamic aspects of the data in MSPC is to use MRA\n(multi-resolution analysis) (Bakshi 1998; Teppola and Minkkinen 2000). For a good review of\nMSPC charts, see (Lowry and Montgomery 1995). Among these, Hotelling\u2019s 2T chart is widely\nused in practice (Sun and Tsung 2003).\nThe standard assumption behind majority of SPC and MSPC methods mentioned above is\nthat the process variables follow a Gaussian distribution (Rose 1991), a questionable assumption\nin several industrial processes (Polansky 2001), and in particular, highly automated processes\n(Chinnam and Kolarik 1992). (Schilling and Nelson 1976) and many other researchers have\ninvestigated the effects of non-normality on the control limits and charting performance. To\nalleviate such effects, some distribution-free or non-parametric control charts have been proposed\nbased on sequential ranks of data depth measures (Liu and Singh 1993; Aradhye, Bakshi et al.\n2001; Stoumbos and Reynolds 2001; Chakraborti, Van der Laan et al. 2003; Messaoud, Weihs et\nal. 2004), but their development and implementation have been rather slow in industrial process\ncontrol (Chakraborti, Van der Laan et al. 2001).\nSeveral non-parametric control charts based on machine learning and pattern recognition\nprinciples have also been proposed in the literature. For example, (Cook and Chiu 1998) proposed\nradial basis function (RBF) networks to recognize shifts in correlated manufacturing processes,\n(Chinnam 2002) proposed support vector machines (SVMs) for recognizing shifts in correlated\nand other manufacturing processes, and (Smith 1994) and (Pugh 1991) considered multi-layer\nperceptron (MLP) networks for implementing Shewhart type control charts, all relaxing the\nGaussian assumption. While these methods have shown success in relaxing the Gaussian\nassumption, the fundamental limitation with these and most other machine learning methods\nproposed in the literature for process control is that they cast the problem as that of\n5\u2018classification\u2019 or \u2018pattern recognition\u2019, and hence, strictly require example data from all out-of-\ncontrol states of interest. This is a critical limitation for obtaining example cases from all such\nstates might be difficult, expensive, or even impossible. The second limitation is that they make\nno explicit provision to make tradeoffs between Type-I errors (false alarms) and Type-II errors\n(inability to detect shifts in process condition). Most of these machine learning methods also\nnecessitate modeling and training for each specific failure type. A model that is developed for a\nspecific type abnormal event (out-of-control state) cannot necessarily give good classification\naccuracy for another type of abnormal event.\nThis paper presents a new non-parametric kernel-distance control chart that employs the\nnotion of one-class classification or novelty detection to overcome these limitations and adopts\nsupport vector machine (SVM) principles for doing so. There are several reasons for basing the\nproposed control chart on SVM principles: 1) They are a system for efficiently training linear\nlearning machines in the kernel-induced feature space, 2) they successfully control the flexibility\nof kernel-induced feature space through generalization theory, and 3) they exploit existing\noptimization theory in doing so. An important feature of SVM systems is that, while enforcing the\nlearning biases suggested by generalization theory, they also produce \u2018sparse\u2019 dual\nrepresentations of the hypothesis, resulting in extremely efficient algorithms (Cristianini and\nTaylor 2000). This is due to the Karush-Kuhn-Tucker conditions (Kuhn and Tucker 1951)\n(Mangasarian 1994), which hold for the solution and play a crucial role in the practical\nimplementation and analysis of these machines. Another important feature of the support vector\napproach is that due to Mercer\u2019s conditions on the kernels (Mercer 1909), the corresponding\noptimization problems are convex and hence have no local minima. This fact, and the reduced\nnumber of non-zero parameters, marks a clear distinction between these systems and other\nmachine learning algorithms, such as neural networks (Cristianini and Taylor 2000). The end\nresult is that the proposed kernel-distance control chart is non-parametric, only requires data from\nan in-control process state, makes provision to utilize any available data from out-of-control\n6states, and allows some tradeoff between Type-I and Type-II errors. The proposed kernel-distance\ncontrol chart supports both univariate and multivariate processes and can monitor both process\nlocation and dispersion aspects through a single control chart.\nA notable exception in the literature that also offers several similar features is another kernel-\ndistance control chart (called k-chart) independently proposed by (Sun and Tsung 2003), also\nbased on a one-class classifier, Support Vector Data Description (SVDD), developed by David\nTax (Tax 2001; Tax and Duin 2004). However, a significant limitation with SVDD, and hence the\nk-chart by Sun and Tsung, is that it lacks any ability to make a good distinction between outliers\nand normal data within the training set. In addition, they make no provision to utilize any\navailable data from out-of-control states, and lastly, they offer no structured method for making\ntradeoffs between Type-I and Type-II errors. This can also result in poor representation of the in-\ncontrol process state if the data available for initializing the control chart is not pre-processed for\nelimination of outliers. Our proposed kernel-distance control chart overcomes these limitations by\nintegrating the SVDD method with principles borrowed from Support Vector Representation and\nDiscrimination Machine (SVRDM) by Yuan and Casasent (2003). Many of these positive\nattributes of our proposed kernel distance control chart are illustrated in Figure 1. Here, we\nparticularly work with the same multivariate process control dataset employed by (Sun and Tsung\n2003). Figure 1(a) illustrates a plot of \u2018normal\u2019 process data projected onto the space of the two\nmost dominant principal components (identical to Figures 11-12 of (Sun and Tsung 2003)), along\nwith a known \u2018outlier\u2019 point (from Figure 13 of (Sun and Tsung 2003)). If one were to use the\ndata from Figure 1(a) to initialize the k-chart, there is no guarantee that the outlier point will be\nrecognized to be an outlier (see for example Figure 9 of (Sun and Tsung 2003)). On the contrary,\nthe kernel distance control chart proposed here gives us several options. Supposing that the\noutlier data point is not so labeled and presented for chart initialization, the proposed method\nrecognizes the point to be an outlier (as shown in Figure 1(a)). If the outlier point is labeled prior\nto chart initialization as an \u2018outlier\u2019, the proposed method recognizes this label and puts it outside\n7the normal boundary (as shown in Figure 1(b)). If for some reason, one chooses to treat this\noutlier point as a \u2018normal\u2019 data point, the proposed method will accept this constraint and treats\nthe point as normal and defines normal process boundary with the point as a boundary point (as is\nevident from Figure 1(c)). Thus, the proposed kernel distance control chart is very robust and can\ntake advantage of any available data\/knowledge regarding process faults. In addition, our\nproposed method also offers an effective heuristic for optimizing the kernel parameters,\nsomething missing from SVDD and the k-Chart from Sun and Tsung. For all these reasons, we\nlabel the proposed method the robust kernel-distance control chart or rk-Chart in short.\n(a) (b)\n(c)\nFigure 1. Flexibility of rk-Chart. In Panel a) the point well outside the normal data is \u201cunlabeled\u201d but is\nrecognized and declared to be an outlier by rk-Chart. In panel (b) the point is \u201clabeled\u201d as an outlier, and\nthe rk-Chart again declares it an outlier making use of the labeling. The results are identical to those\nwithout labeling. Finally, in panel c) the point is \u201clabeled\u201d as normal, and rk-Chart accepts this constraint\nand treats the point as normal and defines normal process boundary with the point as a boundary point. In\nall these cases, the rk-Chart was tuned with typical parameters (specified later).\n8The rest of this paper is organized as follows: Section II provides background information on\nsupport vector machines, the theory behind rk-Chart is presented in Section III, experimental\nresults in Section IV, and concluding remarks in Section V.\nII. Support Vector Machines\nThis section provides a brief background on support vector machines (SVMs) rooted in\nStatistical Learning Theory, a notion first introduced by (Vapnik 1998). We first explain the\nbasics of SVMs for binary classification and then discuss how the technique can be extended to\ndeal with the problem of one-class classification for developing rk-Chart.\nII.1. Binary classification \u2013 Linear case\nSVMs belong to the class of maximum margin classifiers. They perform pattern recognition\nbetween two classes by finding a decision surface that has maximum distance to the closest points\nin the training set, which are termed support vectors. We start with a training set of points\nd\ni \uf0cex \uf03d , 1,2...,i N\uf03d where each point ix belongs to one of two classes identified by the label\n\uf07b \uf07d1, 1iy \uf0ce \uf02d and d is the dimensionality of the points. Let the training set be denoted\nby 1{( , )}\nN\ni i iy \uf03d\uf0c1\uf03d x . Assuming linearly separable data, the goal of maximum margin classification\nis to separate the two classes by a hyperplane such that the distance to the support vectors is\nmaximized. This is achieved by minimizing\n2w subject to the constraint ( ) 1 0i iy b\uf0d7 \uf02b \uf02d \uf0b3x w ,\nwhere w is normal to the hyper-plane. Figure 2 illustrates these concepts for the separable case.\nIn order to provide for non-separable cases, the formulation is modified as follows:\nMinimize\n2\niw C \uf078\uf02b \uf0e5 (1)\nSubject to: \uf028 \uf029 1 0i i iy b \uf078\uf0d7 \uf02b \uf02d \uf02b \uf0b3x w (2)\nThis quadratic optimization problem can be solved efficiently using the following Lagrangian\ndual formulation:\n9Maximize\n,\n1\n2i i j i j i ji i j\ny y\uf061 \uf061 \uf061\uf02d \uf0d7\uf0e5 \uf0e5 x x (3)\nSubject to: 0 i C\uf061\uf0a3 \uf0a3 and . 0i i\ni\ny\uf061 \uf03d\uf0e5 (4)\nwhere i\uf061 denote the Lagrange multipliers. The Lagrangian formulation of the problem offers the\nadvantage of having Lagrange multipliers in the constraints and training data in the form of inner\nproducts between data vectors (M\u00fcller, Mika et al. 2001). In the solution, non-zero i\uf061 values\nrepresent support vectors that are on the separating hyper-plane (satisfying the equation\n0b\uf0d7 \uf02b \uf03dw x ).\n-b\n||w||\nw\nMargin\n2\/||w||\nSupport\nVectors\nOrigin\nSeperating\nHyperplane\nFigure 2: Separation of classes by hyper-plane.\nII.2. Binary classification \u2013 Nonlinear case\nIn many cases, classes are not linearly separable. In order to learn non-linear relations with a\nlinear machine, we need to select a set of non-linear features and to rewrite the data in the new\nrepresentation. This is equivalent to applying a fixed non-linear mapping of the data to a feature\nspace, in which the linear machine can be used. Thus, the non-linear separable case could be\nhandled in two steps: first a fixed non-linear mapping transforms the data into a feature space F ,\nand then a linear machine is used to classify them in the feature space. One important property of\nlinear learning machine is that it can be expressed in a dual representation. That is, the hypothesis\ncan be expressed as a linear combination of the training points so that the decision rule can be\nevaluated using just inner products between the test point and the training points. If one could\n10\ncompute the inner product ( ) ( )i\uf066 \uf066\uf0d7x x in feature space directly as a function of the original\ninput points, it becomes possible to merge the two steps needed to build a non-linear learning\nmachine. We call such a direct computation method a kernel function. A kernel is a function K ,\nsuch that for all , X\uf0cex z\n( , ) ( ) ( )K \uf066 \uf066\uf03d \uf0d7x z x z (5)\nwhere \uf066 is a mapping from X to an inner product feature space F .\nBy replacing the inner product with an appropriately chosen \u2018kernel\u2019 function, one can\nimplicitly perform a non-linear mapping to a high dimensional feature space without increasing\nthe number of tunable parameters, provided the kernel computes the inner product of the feature\nvectors corresponding to the two inputs. Thus, the use of kernels makes it possible to map the\ndata implicitly into a feature space and to train a linear machine in such a space, potentially side-\nstepping the computational problems inherent in evaluating the feature map.\nIn practice, the approach taken is to define a kernel function directly, hence implicitly\ndefining the feature space. In this way, we avoid the feature space not only in the computation of\ninner products, but also in the design of the learning machine itself. Mercer\u2019s theorem (Mercer\n1909) provides a characterization of when a function ( , )K x z is a kernel (i. e. what properties are\nnecessary to ensure that it is a kernel for some feature space). An important family of kernel\nfunctions is the polynomial kernel:\n( , ) (1 )dK \uf03d \uf02b \uf0d7x z x z (6)\nwhere d is the degree of the polynomial. In this case the components of the mapping are all the\npossible monomials of input components up to the degree d . An even more popular kernel in the\nliterature is the Gaussian (Tax and Duin 1999; Yuan and Casasent 2003):\n2 2\n( , )K e \uf073\uf02d \uf02d\uf03d x zx z (7)\n11\nFor a good discussion on making kernels, see (Cristianini and Taylor 2000). For more\ndetailed information on the broader topic of support vector machines see (M\u00fcller, Mika et al.\n2001).\nIII. Robust Kernel-Distance Control Chart (rk-Chart)\nIII.1. rk-Chart as a one-class classifier\nAs stated earlier, the proposed robust kernel-distance control chart (rk-Chart) employs the notion\nof one-class classification for process monitoring, and in doing so, models the boundary of\nprocess data from an in-control state and declares the process to be in control or out of control\ndepending on where the new observation lies with respect to the boundary that exists in the\nfeature space.\nIn the statistical process control literature, the two most important measures that are of\nparticular interest in the context of process monitoring are measures of process central tendency\nand process dispersion. As originally proposed by W. Shewhart (Shewhart 1926), the sample\narithmetic mean is the most employed measure for central tendency (among other measures such\nas mode and median). While the simplest statistical parameter that describes variability in\nobserved data is the sample range, the sample standard deviation is a better estimate of\nvariability for it considers every observation. Not unlike many multivariate statistical process\ncontrol (MSPC) methods, the proposed rk-Chart jointly models these measures of central\ntendency as well as dispersion in a multi-dimensional space. While we recommend the sample\nmean and standard deviation as statistical measures for process monitoring (resulting in a 2-\ndimensional space), the proposed rk-Chart method is a general method and can incorporate any\nother type of location and dispersion measures, promising application specific measures, as well\nas other measures such as higher-order statistical moments (e. g., skewness and kurtosis). The\nonly requirement for rk-Chart is that the vector of sample statistical measures is real, di \uf0cex \uf03d ,\nwhere d denotes the dimensionality of the vector. However, note that the number of support\n12\nvectors (as well as the size of the training set) necessary for adequate representation of normal or\nin-control process state will increase as a function of d . In the case of multivariate processes, the\nfeature space could simply be the process variable space or a transformation thereof (such as the\nspace of dominant principal components). While it is also possible to introduce features from the\nco-variance matrix of the subgroup sample to monitor process dispersion, care should be\nexercised to manage d . Given that rk-Chart models the process data in two or higher\ndimensional spaces, in the case of univariate processes, it is necessary that the subgroup sample\nsize be greater than one to facilitate extraction of at least two features (such as mean and standard\ndeviation). While rk-Chart can in theory deal with both univariate and multivariate processes, and\njointly monitors both location and dispersion measures, it does not necessarily have the ability to\nidentify the type of process fault (such as a process location shift or process variance shift).\nThus, in developing the proposed control chart, we deviate from classical SVM designed for\nbinary classification to representation of boundary from a single class (i. e. di \uf0cex \uf03d ). rk-Chart is\nparticularly inspired from Support Vector Data Description (SVDD) (Tax and Duin 1999) and\nSupport Vector Representation Machine (SVRM) (Yuan and Casasent 2003) and gives the\nminimum volume closed spherical boundary around the in-control process data, represented by\ncenter c and radius r. Minimization of the volume is achieved by minimizing r2, which\nrepresents structural error (M\u00fcller, Mika et al. 2001):\nMin 2r (8)\nSubject to: 2 2i r i\uf02d \uf0a3 \uf022x c , ix : thi data point (9)\nThe formulation above does not allow any data to fall outside of the sphere. In order to make\nprovision within the model for potential outliers within the training set, a penalty cost function is\nintroduced as follows (for data that lie outside of the sphere):\nMin 2 i\ni\nr C \uf078\uf02b \uf0e5 (10)\n13\nSubject to: 2 2i ir \uf078\uf02d \uf0a3 \uf02bx c , 0i\uf078 \uf0b3 i\uf022 (11)\nwhere C is the coefficient of penalty for each outlier (also referred to as the regularization\nparameter) and i\uf078 is the distance between the thi data point and the hyper-sphere. Once again,\nthis is a quadratic optimization problem and can be solved efficiently by introducing Lagrange\nmultipliers for constraints (Vapnik 1998):\n\uf07b \uf07d\n2\n2\n( , , , , )\n( 2 )\ni\ni\ni i i i i i i\ni i\nL r r C\nr\n\uf078\n\uf061 \uf078 \uf067 \uf078\n\uf03d \uf02b \uf02d\n\uf02b \uf02d \uf0d7 \uf02d \uf0d7 \uf02b \uf0d7 \uf02d\n\uf0e5\n\uf0e5 \uf0e5\nc \u03be \u03b1 \u03b3\nx x c x c c\n(12)\nwhere i\uf067 and i\uf061 are Lagrange multipliers, 0i\uf067 \uf0b3 , 0i\uf061 \uf0b3 , and i i\uf0d7x x is inner product of ix and ix .\nNote that for each training data point ix , a corresponding i\uf061 and i\uf067 are defined. L is minimized\nwith respect to r , c , and \u03be , and maximized with respect to \u03b1 and \u03b3 . Taking the derivatives of\n(12) with respect to r , c , \u03be , and equating them to zero, we obtain the following constraints:\ni i\ni\n\uf061\uf03d\uf0e5c x (13)\n0i iC \uf061 \uf067\uf02d \uf02d \uf03d i\uf022 (14)\n1i\ni\n\uf061 \uf03d\uf0e5 (15)\nGiven that 0i\uf067 \uf0b3 , 0i\uf061 \uf0b3 , constraint (14) can be rewritten as:\n0 i C\uf061\uf0a3 \uf0a3 i\uf022 (16)\nThe following quadratic programming equations can be obtained by substituting (13), (14), (15),\nand (16) in (12).\nMax\n,\n( ) ( )i i i i j i j\ni i j\n\uf061 \uf061 \uf061\uf0d7 \uf02d \uf0d7\uf0e5 \uf0e5x x x x (17)\nSubject to: 0 i C\uf061\uf0a3 \uf0a3 i\uf022 , 1i\ni\n\uf061 \uf03d\uf0e5 (18)\nStandard algorithms exist for solving this problem (Tax 2001). The above Lagrange\nformulation also allows further interpretation of the values of \u03b1 . If necessary, the Lagrange\n14\nmultipliers ( i\uf061 , i\uf067 ) will take a value of zero in order to make the corresponding constraint term\nzero in (12). Thus, the rk-Chart formulation satisfies the Karush-Kuhn-Tucker (KKT) conditions\nfor achieving a global optimal solution. Noting that i iC \uf061 \uf067\uf03d \uf02b , if one of the multipliers becomes\nzero, the other takes on a value of C . When a data point ix is inside the sphere, the\ncorresponding i\uf061 will be equal to zero. If it is outside of the sphere, i.e. 0i\uf078 \uf03e , i\uf067 will be zero\nresulting in i\uf061 to be C . When the data point is at the boundary, i\uf061 and i\uf067 will be between zero\nand C to satisfy (15). The quadratic programming solution often yields a 'few' data points with a\nnon-zero i\uf061 value, or support vectors. What is of particular interest is that support vectors can\neffectively represent the data while remaining sparse. Let { : 0}SV i ix \uf061\uf03d \uf0b9S denote the set of\nsupport vectors.\nIn general, it is highly unlikely that a hyper-sphere can offer a good representation for the\nboundary of in-control process data in the \u2018original input space\u2019. Hence, data ought to be\ntransformed to a \u2018higher dimensional feature space\u2019 where it can be effectively represented using\na hyper-sphere. Not unlike SVMs, rk-Chart also employs kernels to achieve this transformation\nwithout compromising computational complexity. Thus, the dot product in (17) is replaced by a\nKernel function, leading us once again to the following quadratic programming problem:\nMax\n,\n( , ) ( , )i i i i j i j\ni i j\nK K\uf061 \uf061 \uf061\uf02d\uf0e5 \uf0e5x x x x (19)\nSubject to 0 i C\uf061\uf0a3 \uf0a3 i\uf022 , 1i\ni\n\uf061 \uf03d\uf0e5 (20)\nIII.2. Gaussian kernel optimization within rk-Chart\nThe proposed robust kernel-distance control chart employs the one-class classification\nformulation from above along with a Gaussian kernel. The Gaussian kernel has been shown to\nparticularly offer better performance over other kernels for one-class classification problems (see\n(Tax 2001) for more discussion on this) and hence the motivation for using it. The issue is\n15\noptimization of the scale parameter \uf073 of (7). While \uf073 could be specified by the user, rk-Chart\nemploys the procedure outlined in Table 1 for choosing \uf073 .\nTable 1: Heuristic procedure for choosing \uf073 for the Gaussian kernel.\nStep 1: Calculate the average 'nearest neighbor distance', denoted dn , between all the data points in the\ndataset (i. e. ( )nndd in E x\uf0c1\uf03d where || || || || ,nndi j i k ix i k j i\uf03d \uf02d \uf0a3 \uf02d \uf022 \uf0b9 \uf0b9x x x x E\uf0c1 denotes\naverage operator over the training set\uf0c1 ).\nStep 2: For each data point ix :\n2.1: Construct a local robust kernel-distance \u2018boundary\u2019, denoted - Lirk B , utilizing the set\n2 2{ :|| || (2 ) }i j j i dn\uf03d \uf02d \uf0a3S x x x (i. e. data within a sphere of radius 2 dn ).\u00a7 In building\n- Lirk B , set || ( ) ||i i\nL\ni i iE E\uf073 \uf03d \uf02dS Sx x for the Gaussian kernel (i. e. the average distance of\ndata within iS to the mean of iS ). Let\n*\nir denote the optimal radius of -\nL\nirk B , i. e. the\nquadratic programming solution.\n2.2: If *- ( )Li i irk B r\uf062\uf0cfx , i. e. the data point is rejected by a scaled or inner - Lirk B of radius *ir\uf062\nwhere 0 1\uf062 \uf03c\uf03d (a parameter pre-specified by the user, typically around 0.95), it is added\ninto the boundary list, denoted BLS . It is also necessary that for any data point to be part of\nBLS , it cannot be accepted by any other inner - Ljrk B . In addition, if i i\uf0baS x , ix is\nexcluded from BLS for it might be an outlier. Fig. 3 illustrates this procedure for\ndetermination of BLS .\nStep 3: The optimal global - Grk B (i. e. the rk-Chart boundary that represents the complete training set\n\uf0c1 ) is then constructed using the following Gaussian kernel parameter:\n*\nmin( ) max( )\narg max { ( )}\nnnd nnd\ni i\nBL SV\nx x\uf073\n\uf073 \uf073\n\uf0a3 \uf0a3\n\uf03d \uf0bbS S .\n\u00a7 Tax and Duin (Tax and Duin 1999) show that this setting of the local sphere radius to 2 dn results in good\nperformance.\nFigure 3: Determination of class boundary list: Data points on the boundary will be rejected by inner local\nrk-Boundaries.\nBoundary Point\nInterior Point\nLocal rk-Boundary\nInner Local rk-Boundary\nLocal Sphere\nof radius 2x dn\n16\nIn the context of a global rk-Chart, smaller \uf073 values yield more representing points (i. e. support\nvectors) and a tighter hyper-sphere, whereas larger values give fewer support vectors and result in\na bigger hyper-sphere. The goal is to identify a value for \uf073 that results in good agreement\nbetween the 'support vector list' SVS of the global rk-Chart boundary and the 'boundary list' BLS\nresulting from local rk-Boundaries (see Table 1 for precise definitions of these terms, and Figure\n3 for a depiction of the class boundary list):\n*\nmin( ) max( )\narg max { ( )}\nnnd nnd\ni i\nBL SV\nx x\uf073\n\uf073 \uf073\n\uf0a3 \uf0a3\n\uf03d \uf0bbS S (21)\nIn general, smaller \uf073 values result in a global support vector list that is a superset of the\nboundary list, with some points that are not part of the boundary list. On the contrary, larger \uf073\nvalues result in a global support vector list that is a subset of the boundary list. In assessing this\nagreement, rk-Chart computes the fitness of a \uf073 value by employing a two-part strategy:\neffective representation and compactness. Effective representation is achieved by ensuring that\nthe global support vector list 'best matches' the boundary list. Compactness on the contrary\nemphasizes a smaller support vector list, which improves generalization. Compactness is\nmanaged through a user-defined parameter 0 1\uf056\uf0a3 \uf0a3 . The higher the value of \uf056 the more\ncompact the support vector list and the higher the Type II error (i. e. inability to detect novel\nconditions), resulting in a larger hyper-sphere.\nThere is typically a \uf073 value, denoted by c\uf073 , that results in near perfect agreement between\nthe support vector list and the boundary list. As \uf073 exceeds c\uf073 , the support vector list gets\nsmaller. The actual value of \uf073 employed in constructing the proposed global rk-Chart is:\n( )c max c\uf073 \uf073 \uf056 \uf073 \uf073\uf03d \uf02b \uf02d (22)\nFigure 4 illustrates the influence of different compactness levels on the quality of representation,\nusing an example dataset. The innermost rk-Chart boundary provides effective representation but\nwith 20 support vectors all of which are in BLS ( 0)\uf056 \uf03d , whereas the outermost rk-Chart boundary\n17\nachieves compactness with just 2 support vectors ( 1)\uf056 \uf03d . While the parameters\nmin min( )\nnnd\nix\uf073 \uf03d , max max( )nndix\uf073 \uf03d , c\uf073 are calculated empirically, the compactness parameter \uf056\nand the rk-Chart boundary scaling parameter \uf062 (employed for constructing the scaled inner\n- Lirk B ) need to be pre-specified by the user or require repeated trials (typically set around 0.95).\nFigure 4: Influence of \uf056 on rk-Chart representation.\nOnce the optimal \uf073 value is calculated based on the desired degree of compactness, one can\nconstruct \u2018inner\u2019 and \u2018outer\u2019 boundary representations by correspondingly changing the radius of\nthe rk-Chart hyper-sphere. Figure 5 illustrates this procedure for the same dataset from Figure 4.\nIt is clear that as the radius is changed the overall geometric shape is maintained while the scale\nchanges.\nFigure 5: Influence of scaling rk-Chart hyper-sphere radius (using parameter \uf062) on boundary\nrepresentation.\n18\nIII.3. Learning from abnormal data\nThere are occasions when observation samples from abnormal or out-of-control process states are\navailable in the training set. Not exploiting this information can have a major detrimental effect\non the performance of the control chart (in terms of Type-I and Type-II errors). The earlier rk-\nChart formulation can be modified as follows to exploit any available examples from out-of-\ncontrol states:\nMin 2 i o j\ni j\nr C C\uf078 \uf078\uf02b \uf02b\uf0e5 \uf0e5 (23)\nSubject to: 2i ir \uf078\uf02d \uf0a3 \uf02bx c and 2j jr \uf078\uf02d \uf0b3 \uf02dx c (24)\nwhere oC is the penalty value for an out-of-control state data point falling inside the\nrepresentation boundary.\nThe Lagrange formulation of this new problem is as follows:\n\uf07b \uf07d\n2\n2\n( , , , , )\n( 2 )\ni o j i i\ni j i\nj j i i i i i\nj i\nL r r C C\nr\n\uf078 \uf078 \uf067 \uf078\n\uf067 \uf078 \uf061 \uf078\n\uf03d \uf02b \uf02b \uf02d\n\uf02d \uf02d \uf02b \uf02d \uf0d7 \uf02d \uf0d7 \uf02b \uf0d7\n\uf0e5 \uf0e5 \uf0e5\n\uf0e5 \uf0e5\nc \u03be \u03b1 \u03b3\nx x c x c c\n(25)\nTo satisfy KKT condition, we once again take derivates of the cost function w.r.t. the parameters\nand equate them to zero, leading to the following equations:\n1i i\ni i\n\uf061 \uf061\uf02d \uf03d\uf0e5 \uf0e5 (26)\ni i j j\ni j\n\uf061 \uf061\uf03d \uf02d\uf0e5 \uf0e5c x x (27)\nSubstituting (26) and (27) in (25), leads us to following Lagrange formulation:\n,\n, ,\n,\n( , , , , ) ( ) ( ) ( )\n( ) ( )\n( )\ni i i i i i i j i j\ni I i J i j I\ni j i j i j i j\ni I j J i J j I\ni j i j\ni j J\nL r \uf061 \uf061 \uf061 \uf061\n\uf061 \uf061 \uf061 \uf061\n\uf061 \uf061\n\uf0ce \uf0ce \uf0ce\n\uf0ce \uf0ce \uf0ce \uf0ce\n\uf0ce\n\uf03d \uf0d7 \uf02d \uf0d7 \uf02d \uf0d7\n\uf02b \uf0d7 \uf02b \uf0d7\n\uf02d \uf0d7\n\uf0e5 \uf0e5 \uf0e5\n\uf0e5 \uf0e5\n\uf0e5\nc \u03be \u03b1 \u03b3 x x x x x x\nx x x x\nx x\n(28)\n19\nwhere, I is the class of in-control state examples and J is the class of out-of-control state\nexamples. This formulation can be simplified by labeling out-of-state classes as '-1' and in-\ncontrol class as '+1'.\n1\n1\ni\ni\ni\nx I\ny\nx J\n\uf0ce\uf0ec\uf03d \uf0ed\uf02d \uf0ce\uf0ee\n(29)\n'\ni i iy\uf061 \uf061\uf03d (30)\nSubstituting (29) and (30) in (28) results in the following:\nMax ' ' '\n{ , } , { , }\n( ) ( )i i i i j i j\ni I J i j I J\n\uf061 \uf061\uf061\n\uf0ce \uf0ce\n\uf0d7 \uf02d \uf0d7\uf0e5 \uf0e5x x x x (31)\n0 i C\uf061\uf0a3 \uf0a3 (32)\n0 j oC\uf061\uf0a3 \uf0a3 (33)\n1i i\ni i\n\uf061 \uf061\uf02d \uf03d\uf0e5 \uf0e5 (34)\nAs can be seen from equations (31-34), the formulation essentially remains the same with an\nadditional constraint for the corresponding Lagrange multiplier value of the out-of-control state\nexample.\nIV. Experimental results\nThe application of rk-Chart will be discussed in two subsections: First, rk-Chart is evaluated\nusing datasets that follow common probability distributions (i. e. normal, lognormal and\nexponential). We then evaluate rk-Chart using a benchmarking dataset (i. e. the Smith dataset)\n(Smith 1994) that has been used extensively for methodological development and evaluation in\nthe process control literature (Chinnam 2002).\nFirst, normal, lognormal and exponential distributions are employed to generate data.\nParameters for the distributions and a sample of dataset are given in Table 2 and Figure 6,\nrespectively. For a typical X chart, it is recommended to have at least 20-25 patterns (Woodall\n20\nand Montgomery 1999). In our experiment, 250 samples are generated for each distribution.\nSamples are grouped with size of 10 and two features, the mean and the standard deviation, are\ncalculated for each sample group, resulting in only 25 two-dimensional data points.\nTable 2: Parameters of distributions for evaluation experiments.\nNormal Lognormal Exponential\nNormal Behavior (Nor)\n\uf06d 0 0 1\n\uf073 1 1 -\nSmall Mean Shift (SM)\n\uf06d 1 1 2\n\uf073 1 1 -\nLarge Mean Shift (LM)\n\uf06d 3 3 4\n\uf073 1 1 -\nSmall Variance Shift (SV)\n\uf06d 0 0 -\n\uf073 2 2 -\nLarge Variance Shift (LV)\n\uf06d 0 0 -\n\uf073 3 3 -\n0 20 40 60 80 100 120 140\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\nExponential Distribution\n(a) 1\uf06d \uf03d\n0 20 40 60 80 100 120 140\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n2\n2.5\nNormal Distribution\n(b) 0, 1\uf06d \uf073\uf03d \uf03d\n0 20 40 60 80 100 120 140\n0\n2\n4\n6\n8\n10\n12\nLognormal Distribution\n(c) 0, 1\uf06d \uf073\uf03d \uf03d\nFigure 6: Example time Series. a) IID Exponential, b) IID Normal, c) IID Lognormal.\nWe trained rk-Chart in two ways: First, with only in-control data, and second, with in-control\nand limited out-of-control data as well. In the latter case, we used 10 abnormal patterns. As\nmentioned before, rk-Chart does not require out-of-control data, but having some helps to\nimprove the accuracy of the method. Type-I and Type-II errors are defined as rejecting a true\nhypothesis and accepting a false hypothesis, respectively. In a statistical process control context,\nType-I error refers to rejecting an in-control process as if it is out-of-control and Type-II error\nrefers to accepting an out-of-control process as if it is in-control. The format of reporting Type-I\nand Type-II errors is given in the Table 3. The results of rk-Chart implementation on processes\nthat follow normal, lognormal, and exponential distributions are summarized in Table 3.\n21\nTable 3: Type-I and Type-II errors defined.\nHypothesis Test\nEstimated\nIn-control Out-of-control\nA\nct\nua\nl In-control Correct Type-I error\nOut-of-control Type-II error Correct\nTable 4: Classification accuracy for: 1) (Left side of the table) Training only with 25 in-control patterns or\nsub-groups, each of which sub-group sample size of 10. 2) (Right side of the table) Training with 25 in-\ncontrol and 10 out-of-control patterns, each with a sub-group sample size of 10; a) Normal b) Lognormal c)\nExponential distributions; SM: Small Mean Shift, SV: Small Variance Shift, LM: Large mean shift, LV:\nLarge variance shift.\n1) Training only with in-control data 2) Training with in-control and limited out-of-control data\na.1\nEstimated\na.2\nEstimated\nIn-control Out-of-control In-control\nOut-of-\ncontrol\nA\nct\nua\nl\nIn-control 88.3% 11.7%\nA\nct\nua\nl\nIn-control 86.6% 13.4%\nO\nut\n-o\nf-\nco\nnt\nro\nl SM 7.4% 92.6%\nO\nut\n-o\nf-\nco\nnt\nro\nl SM 9.0% 91.0%\nLM 0.0% 100.0% LM 0.0% 100.0%\nSV 6.4% 93.6% SV 4.1% 95.9%\nLV 0.2% 99.8% LV 0.1% 99.9%\na) Normal Distribution\nb.1\nEstimated\nb.2\nEstimated\nIn-control Out-of-control In-control\nOut-of-\ncontrol\nA\nct\nua\nl\nIn-control 73.2% 26.8%\nA\nct\nua\nl\nIn-control 81.3% 18.7%\nO\nut\n-o\nf-\nco\nnt\nro\nl SM 4.5% 95.5%\nO\nut\n-o\nf-\nco\nnt\nro\nl SM 9.5% 90.5%\nLM 0.0% 100.0% LM 0.0% 100.0%\nSV 14.8% 85.2% SV 16.9% 83.1%\nLV 4.1% 95.9% LV 3.0% 97.0%\nb) Lognormal Distribution\nc.1\nEstimated\nc.2\nEstimated\nIn-control Out-of-control In-control\nOut-of-\ncontrol\nA\nct\nua\nl In-control 88.8% 11.2%\nA\nct\nua\nl In-control 80.8% 19.2%\nO\nut\n-\nof\n-\nco\nnt\nr\nol\nSM 23.9% 76.1%\nO\nut\n-\nof\n-\nco\nnt\nr\nol\nSM 16.0% 84.0%\nLM 0.7% 99.3% LM 0.4% 99.6%\nc) Exponential Distribution\nAs seen from Table 4, the non-parametric rk-Chart technique is able to detect out-of-control\nprocesses with no available out-of-control data with Type-I errors ranging from a highest of\n26.8% to a low of 11.2%. Type-II errors ranged from 23.9% down to 0% in the case of no\navailable out-of-control data. When limited training data from faulty process states are available,\n22\nType-I errors improve to a high of 19.2% and a low of 13.4%. Type-II errors also improve with\nlimited faulty process training data, to a high of 16.9% and a low of 0%.\nWe will also demonstrate the effectiveness of rk-Chart for detecting mean and variance shifts\nin using the benchmarking dataset generated by Smith (Smith 1994) and compare the results from\na general support vector machine (SVM) method proposed by (Chinnam 2002) and a multi-layer-\nperceptron (MLP) neural network model (Smith 1994). The dataset has 300 samples from in-\ncontrol state and out-of-control states of large mean shift (LM), small mean shift (SM), large\nvariance shift (LV), and small variance shift (SV). The parameters are given in Table 5. The\nresults will be reported in three different categories. In the first category, the results of rk-Chart\nwill be compared with results of Shewhart chart, MLP and SVM. Note that MLP and SVM create\na distinct model for each abnormality type (e.g. small mean shift, large variance shift). In the\nsecond category, two different rk-Chart results are reported: rk-Chart(1), which is trained with\nonly in-control data and rk-Chart(2), which is trained with in-control and limited out-of-control\ndata. Note that MLP and SVM cannot be implemented in case of absence of out-of-control\nsamples. In the third category, the results of rk-Chart that is trained with very limited in-control\n(i.e. 25 samples) and out-of-control data (i.e. 10 samples) are reported.\nTable 5: Parameters of testing datasets.\nState Label \uf06d \uf073\nNormal Behavior (Nor) 0 1\nSmall Mean Shift (SM) 1 1\nLarge Mean Shift (LM) 3 1\nSmall Variance Shift (SV) 0 2\nLarge Variance Shift (LV) 0 3\nTable 6: Classification Accuracy of rk-Chart versus MLP, Shewhart Charts and SVM charts using the\nbenchmarking dataset from Smith (1994). *in: In-control data, out: Out-of-control data.\nrk-Chart MLP ShewhartControl Charts SVM\nTest Train Test Test Test Train\nSmall Shift 91% 92% 72% 73% 93% 91%\nLarge Shift 96% N\/A 100% 100% 100% 99%\n23\nIn the first category, classification accuracy is reported in methods such as MLP, Shewhart\nchart and SVM instead of Type-I and Type-II errors. Thus, the classification accuracy of rk-Chart\nis calculated in order to compare the results with these methods. The weighted average of Type-I\nand Type-II errors are calculated, considering the number of patterns used for in-control and out-\nof-control data. As seen from Table 6, rk-Chart performs better than MLP and Shewhart charts.\nSVM is only 1% to 4% better than rk-Chart. Even though SVM method gives better results than\nrk-Chart, there are two difficulties of implementing SVM and MLP in real world settings:\n1. A distinct model is developed for each of the out-of-control state for SVM and MLP.\nThere are four out-of-control states (SM, LM, SV, LV) resulting in four SVM and MLP\nmodels. Even though each model works well for developed out-of-control states, they\ncannot effectively work for other out-of-control states that are yet to have a model. In\naddition, SVM and MLP may be sensitive to an undefined out-of-control state. In\ncontrast, rk-Chart characterizes the in-control state of the process and it has the ability\nto detect any undefined and unseen type of out-of-control state.\n2. SVM and MLP are trained with 300 samples from in-control state and 300 samples\nfrom each modeled out-of-control state resulting in 4 models using a total of 2400\nsamples. On the other hand, rk-Chart uses only 300 samples from in-control state and\n200 samples from out-of-control states.\nTable 7: Type-I and Type-II errors for the benchmarking dataset from Smith (1994) using rk-Chart.\nIn*: in-control state, Out*: Out-of-control state. rk-Chart(1): Training with 300 normal samples. rk-Chart(2):\nTraining with 300 normal and 200 abnormal samples (100 small mean shift, 100 small variance shift)\nEstimated\nrk-Chart(1) rk-Chart(2)\nIn* Out* In* Out* In* Out* In* Out*\nTrain Test Train Test\nA\nct\nua\nl\nIn-cont. 100% 0% 84% 16% 88% 12% 90% 10%\nO\nut\n*\nSM N\/A N\/A 5% 95% 1% 99% 7% 93%\nLM N\/A N\/A 0% 100% N\/A N\/A 0% 100%\nSV N\/A N\/A 15% 85% 4% 96% 9% 91%\nLV N\/A N\/A 15% 85% N\/A N\/A 8% 92%\n24\nIn the second category, we will report rk-Chart results with no and limited out-of-control\ndata. In the former case (i.e. rk-Chart(1)), only 300 samples of in-control data are used for training,\nin the latter case (i.e. rk-Chart(2)) 300 samples of in-control and 100 samples of small mean shift\nand 100 samples of small variance shift are employed for training. The Type-I and Type-II errors\nas given in Table 7 are promising with 16% Type-I error and highest 15% Type-II error in rk-\nChart(1) and 12% Type-I error and highest 9% Type-II error with rk-Chart(2).\nIn the third category, rk-Chart is implemented with very limited in-control data. We\nimplemented rk-Chart with 25 training patterns from the benchmarking Smith (Smith 1994)\ndataset and results are shown in Table 8.\nTable 8: Process state classification for non-correlated Smith data with limited number of in-control and\nout-of-control samples (i. e. 25 in-control and 10 out-of-control patterns)\nData size of 25\nrk-Chart\nin* out*\nA\nct\nua\nl\nin 88% 12%\nou\nt\nSM 9% 91%\nLM 1% 99%\nSV 8% 92%\nLV 4% 96%\nAs seen from Table 8, in the worst of the cases tested, rk-Chart had the power to effectively\ndetect out-of-control conditions 91% of the time (9% Type-II error), with false alarm rate of only\n12% (Type-I error) even with limited data size.\nAs for parameter selection in initializing rk-Charts, when the control chart is initialized with\njust in-control data, two parameters are required: a penalty value for misclassification of in-\ncontrol data ( C ) and compactness (\uf056 ). The recommended default parameter values are 1.0 and\n0.2, for C and \uf056 , respectively. An additional penalty parameter ( 0C ) is required for cases with\nany available out-of-control data for initialization. The recommended default parameter value is\nonce again 1.0. All the experimental results reported here are based on these default settings.\nExtensive experimentation with these parameters suggests that the rk-Chart is relatively robust\n25\nwith respect to parameter value selection and that the recommended default values work well in\nmost cases. However, individual applications can call for different types of tradeoffs between\nType-I and Type-II errors, which call for appropriate selection or tuning of rk-Chart parameters.\nV. CONCLUSION\nA new process control technique based on support vector machine principles, called robust\nkernel-distance control chart (rk-Chart), is proposed for both univariate and multivariate\nprocesses. rk-Chart has several advantages over the conventional SPC techniques and pattern\nrecognition methods in the literature. rk-Chart does not make any assumption about the data\ndistribution, which is a fundamental restriction for conventional SPC techniques. In addition,\nconventional SPC techniques cannot benefit from available out-of-control data, whereas rk-Chart\ncan learn from out-of-control samples, where applicable. Pattern recognition methods used for\nprocess control in the literature based on other support vector machine (SVM) principles, radial-\nbasis function (RBF) networks, and multi-layer-perceptron (MLP) neural networks require\nexcessive amount of in-control data as well as out-of-control data. Furthermore, a distinct model\nfor each type of out-of-control process needs to be created and trained using both in-control and\nout-of-control data with SVM, RBF and MLP. These models are not sensitive to out-of-control\nconditions other than those on which they are trained. In contrary, rk-Chart characterizes in-\ncontrol-processes and requires only in-control data. However, it makes an explicit provision to\naccommodate any available out-of-control data. Thus, it is sensitive to all types of out-of-control\nprocesses. It is also shown that rk-Chart is able to give very reasonable results with limited in-\ncontrol and out-of-control data when tested using a variety of datasets.\nACKNOWLEDGEMENTS\nThis research is partially funded by National Science Foundation under grant DMI-0300132.\n26\nREFERENCES\nAradhye, H. B., Bakshi, B. R., Strauss, R. A. and Davis, J. F. (2001). Multiscale\nstatistical process control using wavelets: Theoretical analysis and properties. Columbus,\nOH, Ohio State University.\nBakshi, B., Multiscale PCA with application to multivariate statistical process\nmonitoring. Journal of the American Institute of Chemical Engineers, 1998, 44, 1596-\n1610.\nChakraborti, S., Van der Laan, P. and Bakir, S. T., Nonparametric control charts: An\noverview and some results. Journal of Quality Technology, 2001, 33, 304-315.\nChakraborti, S., Van der Laan, P. and Van de Wiel, M., A class of distribution-free\ncontrol charts. Journal of the Royal Statistical Society, Series C, 2004, 55(3), 443-462\nChen, Q., Kruger, U., Meronk, M. and Leung, A. Y. T., Synthetic of t2 and q statistics for\nprocess monitoring, control engineering practice. Control Engineering Practice, 2004,\n12, 745-755.\nChinnam, R. B., Support vector machines for recognizing shifts in correlated and other\nmanufacturing processes. International Journal of Production Research, 2002, 40, 4449-\n4466.\nChinnam, R. B. and Kolarik, W. J., Automation and the total quality paradigm, in\nProceedings of the 1st IERC, 1992, Chicago, IL, IIE.\nCook, D. F. and Chiu, C., Using radial basis function neural networks to recognize shifts\nin correlated manufacturing process parameters. IIE Transactions, 1998, 30, 227-234.\nCristianini, N. and Taylor, J. S., An introduction to support vector machines and other\nkernel-based learning methods. (Cambridge: Cambridge University Press).\nEickelmann, N. and Anant, A., Statistical process control: What you don't measure can\nhurt you! Software, IEEE, 2003, 20, 49-51.\nHotelling, H. (1947). Multivariate quality control-illustrated by the air testing of sample\nbombsights. Techniques of statistical analysis. C. Eisenhart, M. W. Hastay and W. A.\nWallis. New York, McGraw-Hill: 111-184.\nKuhn, H. and Tucker, A., Nonlinear programming, in Proceedings of 2nd Berkeley\nSymposium on Mathematical Statistics and Probabilistics, 1951, Berkely, CA, University\nof California Press, 481-492.\nLiu, R. Y. and Singh, K., A quality index based on data depth and multivariate rank tests.\nJournal of the American Statistical Association (JASA), 1993, 88, 252-260.\n27\nLowry, C. A. and Montgomery, D. C., A review of multivariate control charts. IIE\nTransactions, 1995, 27, 800-810.\nLowry, C. A., Woodall, W. H., Champ, C. W. and Rigdon, S. E., A multivariate\nexponentially weighted moving average control chart. Technometrics, 1992, 34, 46-53.\nMacGregor, J. F. and Kourti, T., Statistical process control of multivariate processes.\nControl Engineering Practice, 1995, 3, 403-414.\nManabu, K., Shinji, H., Iori, H. and Hiromu, O., Evolution of multivariate statistical\nprocess control: Application of independent component analysis and external analysis.\nComputers & Chemical Engineering, 2004, 28, 1157-1166.\nMangasarian, O. L., Nonlinear programming. (Philadelphia, PA: Society for Industrial\nand Applied Mathematics).\nMartin, E. B., Morris, A. J. and Zhang, J., Process performance monitoring using\nmultivariate statistical process control. IEE Proceedings, 1996, 143, 132-144.\nMercer, J., Functions of positive and negative type and their connection with the theory\nof integral equations. Philosophical Transactions of the Royal Society of London, Series\nA, 1909, 209, 415-446.\nMessaoud, A., Weihs, C. and Hering, F. (2004). A nonparametric multivariate control\nchart based on data depth. Dortmund, Germany, Department of Statistics, University of\nDortmund.\nMontgomery, D. C., Introduction to statistical quality control. (New York: Wiley).\nM\u00fcller, K. R., Mika, S., R\u00e4tsch, G., Tsuda, K. and Sch\u00f6lkopf, B., An introduction to\nkernel-based learning algorithms. IEEE Neural Networks, 2001, 12, 181-201.\nNgai, H. and Zhang, J., Multivariate cumulative sum control charts based on projection\npursuit. Statistica Sinica, 2001, 11, 747-766.\nNimmo, I., Adequately address abnormal situation operations. Chemical Engineering\nProgress, 1995, 91, 36-45.\nPolansky, A. M., A smooth nonparametric approach to multivariate process capability.\nTechnometrics, 2001, 43, 199-211.\nPugh, G. A., A comparison of neural networks to spc charts. Computers and Industrial\nEngineering, 1991, 21, 253-255\n28\nRaich, A. and Cinar, A., Statistical process monitoring and disturbance diagnosis in\nmultivariate continuous processes. Journal of the American Institute of Chemical\nEngineers, 1996, 42, 995-1009.\nRose, K., Mathematics of success and failure. Circuits and Devices, IEEE, 1991, 7, 26-\n30.\nRunger, G. C. and Prabhu, S. S., A markov chain model for the multivariate\nexponentially weighted moving averages control chart. Journal of the American\nStatistical Association (JASA), 1996, 91, 1701-1706.\nRunger, G. C. and Testik, M. C., Multivariate extensions to cumulative sum control\ncharts. Quality and Reliability Engineering International, 2004, 20, 587 - 606.\nSchilling, E. G. and Nelson, P.R., The effect of non-normality on the control limits of X\ncharts. Journal of Quality Technology, 1976, 8, 183-187.\nShewhart, W. A., Quality control charts. Bell System Technical Journal, 1926, 22, 593-\n603.\nSmith, A. E., X-bar and r control chart interpretation using neural computing.\nInternational Journal of Production Research, 1994, 32, 309-320.\nStoumbos, Z. G. and Reynolds, M. R., On shewhart-type nonparametric multivariate\ncontrol charts based on data depth. Frontiers in Statistical Quality Control, 2001, 6, 207-\n227.\nSun, R. and Tsung, F., A kernel-distance-based multivariate control chart using support\nvector methods. International Journal of Production Research, 2003, 41, 2975-2989.\nTax, D. M. (2001). One class classification. Delft, The Netherlands, Delft Technical\nUniversity.\nTax, D. M. J. and Duin, R. P. W., Support vector domain description. Pattern\nRecognition Letters, 1999, 20, 1191-1199.\nTax, D. M. J. and Duin, R. P. W., Support vector data description. Machine Learning,\n2004, 54, 45-66.\nTeppola, P. and Minkkinen, P., Wavelet-pls regression models for both exploratory data\nanalysis and process monitoring. Journal of Chemometrics, 2000, 14, 383-399.\nTestik, M. C. and Borror, C. M., Design strategies for the multivariate exponentially\nweighted moving average control chart. Quality and Reliability Engineering\nInternational, 2004, 20, 571-577.\n29\nVapnik, V., Statistical learning theory. (New York: Wiley).\nWoodall, W. H. and Montgomery, D. C., Research issues and ideas in statistical process\ncontrol. Journal of Quality Technology, 1999, 31, 376-386.\nYoon, S. and MacGregor, J. F., Principal-component analysis of multiscale data for\nprocess monitoring and fault diagnosis. Journal of the American Institute of Chemical\nEngineers, 2004, 50, 2891-2903.\nYuan, C. and Casasent, D., Support vector machines for class representation and\ndiscrimination, in International Joint Conference on Neural Networks, 2003, Portland,\nOR, 1610-1615.\n"}