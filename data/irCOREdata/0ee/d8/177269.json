{"doi":"10.1016\/J.ARTINT.2007.04.006","coreId":"177269","oai":"oai:aura.abdn.ac.uk:2164\/2056","identifiers":["oai:aura.abdn.ac.uk:2164\/2056","10.1016\/J.ARTINT.2007.04.006"],"title":"Subjective Logic and Arguing with Evidence","authors":["Oren, Nir","Norman, Timothy J","Preece, Alun"],"enrichments":{"references":[{"id":3899,"title":"A generalization of Dung\u2019s abstract framework for argumentation: Arguing with sets of attacking arguments.","authors":[],"date":"2006","doi":"10.1007\/978-3-540-75526-5_4","raw":"S. H. Nielsen and S. Parsons. A generalization of Dung\u2019s abstract framework for argumentation: Arguing with sets of attacking arguments. In Proc. of ArgMAS 2006, pages 7\u201319, Hakodate, Japan, 2006.","cites":null},{"id":3895,"title":"A logic for uncertain probabilities.","authors":[],"date":"2001","doi":"10.1016\/s0218-4885(01)00083-1","raw":"A. J\u00f8sang. A logic for uncertain probabilities. Int. Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 9:279\u2013311, 2001.","cites":null},{"id":3903,"title":"A study of accrual of arguments, with applications to evidential reasoning.","authors":[],"date":"2005","doi":"10.1145\/1165485.1165500","raw":"H. Prakken. A study of accrual of arguments, with applications to evidential reasoning. In Proc. of the 10th Int. Conf. on Arti\ufb01cial Intelligence and Law, pages 85\u201394, 2005.","cites":null},{"id":3900,"title":"Arguing with con\ufb01dential information.","authors":[],"date":"2006","doi":"10.1016\/j.artint.2007.04.006","raw":"N. Oren, T. J. Norman, and A. Preece. Arguing with con\ufb01dential information. In Proc. of the 18th European Conf. on Arti\ufb01cial Intelligence, pages 280\u2013284, Riva del Garda, Italy, August 2006.","cites":null},{"id":3901,"title":"Argumentation based contract monitoring in uncertain domains.","authors":[],"date":"2007","doi":"10.1007\/11839354_25","raw":"N. Oren, T. J. Norman, and A. Preece. Argumentation based contract monitoring in uncertain domains. In Proc. of the 20th Int. Joint Conf. on Arti\ufb01cial Intelligence, pages 1434\u20131439, Hyderabad, India, 2007.","cites":null},{"id":3907,"title":"Argumentation in Bayesian belief networks.","authors":[],"date":"2004","doi":"10.1007\/978-3-540-32261-0_8","raw":"G. A. W. Vreeswijk. Argumentation in Bayesian belief networks. In Proc. of ArgMAS 2004, number 3366 in LNAI, pages 111\u2013129. Springer, 2004.","cites":null},{"id":3909,"title":"Argumentation Schemes for Presumptive Reasoning. Erlbaum,","authors":[],"date":"1996","doi":"10.4324\/9780203811160","raw":"D. N. Walton. Argumentation Schemes for Presumptive Reasoning. Erlbaum, 1996.","cites":null},{"id":3908,"title":"Burden of proof.","authors":[],"date":"1988","doi":"10.1007\/BF00178024","raw":"D. N. Walton. Burden of proof. Argumentation, 2:233\u2013254, 1988.","cites":null},{"id":3902,"title":"Cognitive Carpentry.","authors":[],"date":"1995","doi":"10.1017\/S0269888999221030","raw":"J. L. Pollock. Cognitive Carpentry. Bradford\/MIT Press, 1995.","cites":null},{"id":3904,"title":"Computational Logic: Logic Programming and Beyond. Essays In Honour of","authors":[],"date":"2002","doi":"10.1007\/3-540-45632-5_14","raw":"H. Prakken and G. Sartor. Computational Logic: Logic Programming and Beyond. Essays In Honour of Robert A. Kowalski, Part II, volume 2048 of LNCS, pages 342\u2013380. Springer-Verlag, 2002.","cites":null},{"id":3897,"title":"Conditional deduction under uncertainty.","authors":[],"date":"2005","doi":"10.1007\/11518655_69","raw":"A. J\u00f8sang, S. Pope, and M. Daniel. Conditional deduction under uncertainty. In Proc. of the 8th European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty, pages 824\u2013835, Barcelona, Spain, 2005.","cites":null},{"id":3906,"title":"Dialectical argumentation with argumentation schemes: An approach to legal logic.","authors":[],"date":"2003","doi":"10.1023\/B:ARTI.0000046008.49443.36","raw":"B. Verheij. Dialectical argumentation with argumentation schemes: An approach to legal logic. Arti\ufb01cial intelligence and Law, 11:167\u2013195, 2003.","cites":null},{"id":3898,"title":"Dialogue games in multi-agent systems.","authors":[],"date":"2002","doi":"10.1007\/s10458-009-9108-7","raw":"P. McBurney and S. Parsons. Dialogue games in multi-agent systems. Informal Logic, 22(3):257\u2013274, 2002.","cites":null},{"id":3905,"title":"editors. Argumentation Machines: New frontiers in argumentation and computation.","authors":[],"date":"2003","doi":"10.1162\/0891201053630282","raw":"C. A. Reed and T. J. Norman, editors. Argumentation Machines: New frontiers in argumentation and computation. Kluwer, 2003.","cites":null},{"id":3896,"title":"Multiplication and comultiplication of beliefs.","authors":[],"date":"2004","doi":"10.1016\/j.ijar.2004.03.003","raw":"A. J\u00f8sang and D. McAnally. Multiplication and comultiplication of beliefs. Int. Journal of Approximate Reasoning, 38(1):19\u201351, 2004.","cites":null},{"id":3893,"title":"On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games.","authors":[],"date":"1995","doi":"10.1016\/0004-3702(94)00041-X","raw":null,"cites":null},{"id":3890,"title":"On the bipolarity in argumentation frameworks.","authors":[],"date":"2004","doi":"10.1002\/int.20307","raw":"L. Amgoud, C. Cayrol, and M.-C. Lagasquie-Schiex. On the bipolarity in argumentation frameworks. In Proceedings of the 10th International Workshop on Non-monotonic Reasoning, pages 1\u20139, Whistler, Canada, 2004.","cites":null},{"id":3891,"title":"Reaching agreement through argumentation: a possiblistic approach.","authors":[],"date":"2004","doi":null,"raw":"L. Amgoud and H. Prade. Reaching agreement through argumentation: a possiblistic approach. In Proc. of the 9th Int. Conf. on the Principles of Knowledge Representation and Reasoning, pages 175\u2013182, 2004.","cites":null},{"id":3892,"title":"Towards a formal account of reasoning about evidence: Argumentation schemes and generalisations.","authors":[],"date":"2003","doi":"10.1023\/B:ARTI.0000046007.11806.9a","raw":"F. Bex, H. Prakken, C. Reed, and D. Walton. Towards a formal account of reasoning about evidence: Argumentation schemes and generalisations. Arti\ufb01cial Intelligence and Law, 11(2-3):125\u2013165, 2003. 25[4] P. M. Dung. On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Arti\ufb01cial Intelligence, 77(2):321\u2013357, 1995.","cites":null},{"id":3894,"title":"Towards a unifying theory of logical and probabilistic reasoning.","authors":[],"date":"2005","doi":"10.1007\/11518655_66","raw":"R. Haenni. Towards a unifying theory of logical and probabilistic reasoning. In Proc. of the 4th Int. Symp. on Imprecise Probabilties and Their Applications, pages 193\u2013202, Pittsburgh, Pennsylvania, USA, 2005.","cites":null}],"documentType":{"type":1}},"contributors":["University of Aberdeen, Natural & Computing Sciences, Computing Science"],"datePublished":"2007-07","abstract":"Peer reviewedPreprin","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:aura.abdn.ac.uk:2164\/2056<\/identifier><datestamp>\n                2018-01-06T06:13:05Z<\/datestamp><setSpec>\n                com_2164_673<\/setSpec><setSpec>\n                com_2164_370<\/setSpec><setSpec>\n                com_2164_331<\/setSpec><setSpec>\n                com_2164_705<\/setSpec><setSpec>\n                col_2164_674<\/setSpec><setSpec>\n                col_2164_706<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nSubjective Logic and Arguing with Evidence<\/dc:title><dc:creator>\nOren, Nir<\/dc:creator><dc:creator>\nNorman, Timothy J<\/dc:creator><dc:creator>\nPreece, Alun<\/dc:creator><dc:contributor>\nUniversity of Aberdeen, Natural & Computing Sciences, Computing Science<\/dc:contributor><dc:subject>\nargumentation<\/dc:subject><dc:subject>\ndialogue game<\/dc:subject><dc:subject>\nevidence<\/dc:subject><dc:subject>\nsubjective logic<\/dc:subject><dc:subject>\nQA76 Computer software<\/dc:subject><dc:subject>\nQA76<\/dc:subject><dc:description>\nPeer reviewed<\/dc:description><dc:description>\nPreprint<\/dc:description><dc:date>\n2011-05-19T15:53:01Z<\/dc:date><dc:date>\n2011-05-19T15:53:01Z<\/dc:date><dc:date>\n2007-07<\/dc:date><dc:type>\nJournal article<\/dc:type><dc:identifier>\nOren , N , Norman , T J & Preece , A 2007 , ' Subjective Logic and Arguing with Evidence ' Artificial Intelligence , vol 171 , no. 10-15 , pp. 838-854 . DOI: 10.1016\/J.ARTINT.2007.04.006<\/dc:identifier><dc:identifier>\n0004-3702<\/dc:identifier><dc:identifier>\nPURE: 159634<\/dc:identifier><dc:identifier>\nPURE UUID: f79b7711-1a3f-4143-9a2d-f063255deb2f<\/dc:identifier><dc:identifier>\nWOS: 000249757000009<\/dc:identifier><dc:identifier>\nScopus: 34548040661<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/2164\/2056<\/dc:identifier><dc:identifier>\nhttp:\/\/dx.doi.org\/10.1016\/J.ARTINT.2007.04.006<\/dc:identifier><dc:language>\neng<\/dc:language><dc:relation>\nArtificial Intelligence<\/dc:relation><dc:format>\n16<\/dc:format>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":[{"title":null,"identifiers":["issn:0004-3702","0004-3702"]}],"language":{"code":"en","id":9,"name":"English"},"relations":["Artificial Intelligence"],"year":2007,"topics":["argumentation","dialogue game","evidence","subjective logic","QA76 Computer software","QA76"],"subject":["Journal article"],"fullText":"Subjective Logic and Arguing with Evidence\nNir Oren a,\u2217, Timothy J. Norman a, Alun Preece a\naDepartment of Computing Science, University of Aberdeen, AB24 3UE, Scotland\nAbstract\nThis paper introduces a Subjective Logic based argumentation framework primar-\nily targeted at evidential reasoning. The framework explicitly caters for argument\nschemes, accrual of arguments, and burden of proof; these concepts appear in many\ntypes of argument, and are particularly useful in dialogues revolving around eviden-\ntial reasoning. The concept of a sensor is also useful in this domain, representing a\nsource of evidence, and is incorporated in our framework. We show how the frame-\nwork copes with a number of problems that existing frameworks have difficulty\ndealing with, and how it can be situated within a simple dialogue game. Finally,\nwe examine reasoning machinery to enable an agent to decide what argument to\nadvance with the goal of maximising its utility at the end of a dialogue.\nKey words: Argumentation, Dialogue Game, Heuristic, Evidence\n1 Introduction\nIt has long been recognised that argumentation research can be divided into\ntwo main strands [16]. The first involves the analysis of argument, while the\nsecond borrows ideas from argumentation theory in an attempt to create pow-\nerful reasoning mechanisms. In this paper, we follow the latter strand, using\nargument to create a powerful framework for evidential and diagnostic rea-\nsoning. Informally, we are trying to address situations where different agents,\neach with their own goals and viewpoints, are attempting to reach a shared\nagreement about the state of a subset of their environment. We further assume\nthat the environment is partially observable, and that any information about\n\u2217 Corresponding Author.\nEmail addresses: noren@csd.abdn.ac.uk (Nir Oren),\ntnorman@csd.abdn.ac.uk (Timothy J. Norman), apreece@csd.abdn.ac.uk\n(Alun Preece).\nPreprint submitted to Elsevier 24 March 2007\nit is obtained through the use of fallible sensors. Finally, we assume that the\nagents are self interested, and that different agents may have opposing goals.\nWithout a trusted third party, a centralised solution to this problem is dif-\nficult. Our proposed approach involves the agents engaging in dialogue with\neach other, exchanging arguments, and probing sensors for additional informa-\ntion about the environment. By combining the information from sensors and\narguments, a shared world view can be constructed. To tackle the problem,\ntherefore the following is needed:\n\u2022 A representation mechanism for the environment, agents\u2019 knowledge, argu-\nments and any other components with which interaction is required.\n\u2022 A technique for determining which conclusions are justified when opposing\narguments interact.\n\u2022 A specification detailing how agents should engage in dialogue with each\nother.\n\u2022 A way for the agents to decide which arguments to advance and what sensors\nto probe.\nPrakken [15] identified these as the logical, dialectic, procedural and heuristic\nlayers of an argument framework. Our logical layer is built around Subjective\nLogic [6], allowing us to represent concepts such as likelihood and uncertainty\nin a concise and elegant manner. The way in which arguments are constructed\nin our framework and used at the dialectic level is intended to support a rich\nrepresentation of arguments; we are able to represent concepts such as accrual\nof arguments, argument schemes and argument reinforcement in a natural\nmanner. While the logical and dialectic layers are domain independent, acting\nas a general argument framework, the explicit introduction of sensors at the\nprocedural level allows us to attack our problem.\nA sensor refers to anything that can determine the state of a subset of the\nenvironment. Multiple sensors may exist for certain parts of the environment,\nand some of these sensors may be more accurate than others. Finally, sensors\nmay not perform their services for free. Thus, sensors capture an abstract\nnotion of a source of evidence within our framework.\nAt the procedural level, agents engaging in dialogue, taking turns to advance\narguments and probe sensors in an attempt to achieve their goals. In this\ncontext, an agent\u2019s goal involves showing that a certain environment state\nholds. We assume that an agent associates a utility with various goal states.\nOur heuristic layer guides an agent and tells it what arguments to advance,\nand which sensors to probe during its turn in the dialogue game.\nUsing argumentation for evidential reasoning has a number of advantages over\nother approaches, including:\n2\n\u2022 Understandability. It is much easier to follow the reasoning behind a dialogue\nthan to attempt to interpret a complicated formula.\n\u2022 Resource bounded reasoning. It is possible to plug in different agents with\ndifferent capabilities (and hence different computational costs) and still ob-\ntain some (possibly non-accurate) answers.\n\u2022 Anytime. Related to the previous point, it is possible to terminate the dia-\nlogue at any time, with the provision that inaccurate, or incorrect answers\nmay be obtained.\n\u2022 Ease of knowledge engineering. At any point in the argument process, it is\neasy to introduce additional facts and see how they alter the dialogue.\nIn this section, we provided a brief overview of the problem we are trying to\ntackle, and outlined our proposed solution. Next, we discuss Subjective Logic,\nas it forms a core part of our formalism. Once this is done, we proceed to\ndescribe our formalism, following which an illustrative example is provided.\nWe then examine the strengths and weaknesses of our approach in more detail,\nand compare it with existing techniques. We also examine possible areas for\nfuture work before concluding the paper.\n2 Subjective Logic\nSubjective Logic [6] provides a standard set of logical operators (such as nega-\ntion, conjunction and disjunction), intended for use in domains containing\nuncertainty, and, more specifically, domains in which opinions regarding the\ntruth or falsehood of a (set of) domain elements differ. Subjective logic also\ncontains a number of other operators, designed to combine opinions in an in-\ntuitively correct manner. The semantics of our formalism, presented in Section\n3, are based on Subjective Logic (hereafter abbreviated SL), and we therefore\nnow provide a brief overview of the area. Most of this description is taken\ndirectly from J\u00f8sang\u2019s original paper [6].\nSince SL is based on Dempster-Shafer evidence theory, it operates on a frame\nof discernment, denoted by \u0398. A frame of discernment contains the set of\npossible system states, only one of which represents the actual system state.\nThese are referred to as atomic system states.\nIn many situations, it is difficult to determine what state one is in, and it\nthus makes sense to talk about non-atomic states, consisting of the union of\na number of primitive states. If the system is in primitive atomic state xi, it\nis also in all states xj such that xi \u2286 xj. The powerset of \u0398, denoted by 2\u0398,\nconsists of all possible unions of primitive states.\nAn observer assigns a belief mass to various states based on its strength of\n3\nbelief that the state (or one of its substates) is true:\nDefinition 1 (Belief Mass Assignment) Given a frame of discernment\n\u0398, one can associate a belief mass assignment 1 m\u0398(x) with each substate\nx \u2208 2\u0398 such that\n(1) m\u0398(x) \u2265 0\n(2) m\u0398(\u2205) = 0\n(3)\n\u2211\nx\u22082\u0398 m\u0398(x) = 1\nFor a substate x, m\u0398(x) is its belief mass.\nBelief mass is an unwieldy concept to work with. When one speaks of belief\nregarding a certain state, one refers not only to the belief mass in the state, but\nalso to the belief masses of the state\u2019s substates. Similarly, when one speaks\nabout disbelief, that is, the total belief that a state is not true, one needs to\ntake substates into account. Finally, subjective logic introduces the concept\nof uncertainty, that is, the amount of belief that one might be in a superstate\nor a partially overlapping state. We can define these concepts formally as:\nDefinition 2 (Belief, Disbelief and Uncertainty) Given a frame of dis-\ncernment \u0398 and a belief mass assignment m\u0398 on \u0398, we can define the belief\nfunction for a state x as\nb(x) =\n\u2211\ny\u2286x\nm\u0398(y) where x, y \u2208 2\u0398\nThe disbelief function as\nd(x) =\n\u2211\ny\u2229x=\u2205\nm\u0398(y) where x, y \u2208 2\u0398\nAnd the uncertainty function as\nu(x) =\n\u2211\ny \u2229 x 6= \u2205\ny 6\u2286 x\nm\u0398(y) where x, y \u2208 2\u0398\nThese functions have a number of features that should be noted. First, they\nall range between zero and one. Second, they always sum to one, meaning that\nit is possible to deduce the value of one function given the other two. If the\nentire belief mass is assigned to \u0398, then u(x) = 1 if x 6= \u0398. This situation is\nanalogous to total uncertainty. Dogmatic beliefs occur when no belief mass is\nassigned to \u0398.\n1 A belief mass assignment is often also referred to as a basic belief assignment, or\nbba, within belief theory. Since our framework is Subjective Logic based, we will\nrefer to belief mass assignment within this paper.\n4\nAnother concept introduced in subjective logic is relative atomicity. The rela-\ntive atomicity of two states x and y is the ratio of the number of states shared\nbetween them and the number of states in y. Relative atomicity is needed\nto compute the expected probability of an outcome, and captures the idea\nof prior probabilities. However, by taking into account the provision that we\ncannot translate directly from opinions to probabilities, we ignore atomicity\nin the interests of simplifying our framework.\nA focused frame of discernment for a state x is a frame of discernment con-\ntaining only the states x and x\u00af, the complement of x. J\u00f8sang provides a trans-\nformation from a frame of discernment to a focused frame of discernment.\nHowever, for a state x, the only value that changes between the two frames is\nits atomicity.\nAn opinion consists of the belief, disbelief, uncertainty (and relative atomicity)\nas computed over a binary frame of discernment:\nDefinition 3 (Opinion) Given a binary frame of discernment \u0398 containing\nx and its complement x\u00af, and assuming a belief mass assignment m\u0398 with\nbelief, disbelief, uncertainty and relative atomicity functions on x in \u0398 of\nb(x),d(x),u(x) and a(x), we define an opinion over x, written \u03c9x as\nwx \u2261 \u3008b(x), d(x), u(x), a(x)\u3009\nSince we ignore atomicity, we write an opinion for a state x as the triple\n\u3008b(x), d(x), u(x)\u3009. When the context is clear, we may refer to (for example)\nthe belief component of an opinion \u03c9x as b(\u03c9x). For compactness, we may\noccasionally write bx, dx, ux instead of b(x), d(x) and u(x).\nJ\u00f8sang has defined a large number of operators that are used to combine\nopinions, some of which are familiar such as conjunction and disjunction, and\nsome less so such as abduction. We look at three operators, namely negation,\ndiscounting, and consensus.\nThe propositional negation operator calculates the opinion that a proposition\ndoes not hold, and is defined as follows:\nDefinition 4 (Propositional Negation) For a \u03c9x = \u3008bx, dx, ux\u3009, the propo-\nsitional negation is computed as \u03c9\u00acx = \u3008dx, bx, ux\u3009.\nGiven an agent \u03b1, we represent its opinion on a proposition x as \u03c9\u03b1x . Discount-\ning is a model of hearsay. That is, given that an agent \u03b1 holds an opinion \u03c9\u03b1\u03b2\nabout agent \u03b2\u2019s reliability, and given that \u03b2 has an opinion \u03c9\u03b2x about propo-\nsition x, \u03c9\u03b1\u03b2x gives the opinion \u03b1 has about x.\nDefinition 5 (Discounting) Given two opinions \u03c9\u03b1\u03b2 = \u3008b\u03b1\u03b2 , d\u03b1\u03b2 , u\u03b1\u03b2\u3009, and\n5\n\u03c9\u03b2x = \u3008b\u03b2x, d\u03b2x, u\u03b2x\u3009, the discounted opinion is \u03c9\u03b1\u03b2x = \u3008b\u03b1\u03b2b\u03b2x, b\u03b1\u03b2d\u03b2x, d\u03b1\u03b2 + u\u03b1\u03b2 + b\u03b1\u03b2u\u03b2x\u3009\nThe independent consensus operator represents the opinion an imaginary\nagent would have about x if it had to assign equal weighting to the opin-\nions \u03c9\u03b1x and \u03c9\n\u03b2\nx . Later work [7] suggests how one can handle situations where\n\u03ba = 0, but, by assuming that sensors reflect reality, we can ensure that \u03ba is\nnever 0 in our framework.\nDefinition 6 (Independent Consensus) Given two independent opinions\n\u03c9\u03b1x and \u03c9\n\u03b2\nx about the same proposition x, the independent consensus opinion\nis defined as \u03c9\u03b1,\u03b2x = \u3008(b\u03b1xu\u03b2x + b\u03b2xu\u03b1x)\/\u03ba, (d\u03b1xu\u03b2x + d\u03b2xu\u03b1x)\/\u03ba, u\u03b1xu\u03b2x\/\u03ba\u3009 Where \u03ba =\nu\u03b1x + u\n\u03b2\nx \u2212 u\u03b1xu\u03b2x such that \u03ba 6= 0.\nTo simplify notation, we may represent the operators as follows:\n\u03c9\u00acx \u2261 \u00ac\u03c9x\n\u03c9\u03b1\u03b2x \u2261 \u03c9\u03b1\u03b2 \u2297 \u03c9\u03b2x\n\u03c9\u03b1,\u03b2x \u2261 \u03c9\u03b1x \u2295 \u03c9\u03b2x\nWith this grounding, we are now in a position to describe our framework.\n3 The Framework\nFollowing Prakken\u2019s model[15], we build our framework in layers, starting at\nthe logical layer, where we describe how an argument is constructed. In the\ndialectic layer, we look at how arguments interact, and then show how agents\nmay engage in dialogue in the procedural layer. Finally, in the heuristic layer,\nwe show how agents can decide which lines of argument should be advanced\nin a dialogue.\nOne concept that cuts across a number of layers is that of an argument scheme\n[20]. Argument schemes are common, stereotypical patterns of reasoning which\nare, typically, non-deductive and nonmonotonic. In our framework, arguments\nare instantiated instances of argument schemes. Thus, our universe of discourse\nis a tuple U = (PF , AS) where PF contains (a finite number of) possible facts\nabout our universe, and AS is the set of argument schemes. We also assume\nthat we have two distinct sets of symbols \u03a3 and \u03a6.\nFacts are represented as grounded predicates, and have an associated opinion.\nA set of predicates with an identical name acts as a frame of discernment.\nSince only a single state within a frame of discernment can be true, we cannot\nsimply match individual predicates to states. Instead, we define the atomic\n6\nstates by computing the powerset of the individual predicates, and associate\na non-atomic state with the predicate that encapsulates all atomic states in\nwhich the predicate holds. The left hand side of Figure 1 provides an example\nof this.\nDefinition 7 (Predicate) Given a universe of discourse U = (PF , AS), a\npredicate is a tuple (Name,Parameters) \u2208 PF where Name \u2208 \u03a3. Parameters\nis itself a tuple of finite arity whose members are elements of PF . Given\ntwo predicates (N1,Parameters1), and (N2,Parameters2), |Parameters1| =\n|Parameters2| if N1 = N2.\nWe refer to the frame of discernment 2P , where P is the set\n{Parameters|(Name,Parameters) \u2208 PF and Name = N} as \u0398N . This frame\nof discernment has additional states pi \u2208 P containing all states s \u2208 2P such\nthat pi \u2286 s.\nThe atomic states of a frame of discernment \u0398N are referred to as atomic(\u0398N).\nWe differentiate between atomic and non-atomic states by writing the latter\nin bold.\nA predicate is thus embedded within a frame of discernment, which in itself is\nanother predicate. We assume the existence of an anonymous, top level frame\nof discernment in which predicates reside. While it is possible to remove this\nnesting, and store all predicates within a single universal frame of discernment,\nembedding them in this way helps reduce the exponential explosion in the\nnumber of atomic states.\nWe place one restriction on the nesting of frames of discernment, and that is\nthat the graph of nestings must be acyclic, that is, a predicate may not nest\nother predicates such that it eventually nests itself.\nAs an example, consider the symbols a, b, fred , holds, geologist, expert and\ngeology. We may have the following predicates:\n(holds, {a}), (holds, {b}), (holds, {geologist})\n(geologist, {fred}), (expert, {(geologist, geology)})\nFor convenience, we rewrite a predicate of the form (A,B) as A(B). Thus,\nsome of our possible facts include holds(a), expert(geologist, geology), as well\nas holds(geologist) and holds(geologist(fred)). Note that atomic(\u0398holds) is\n{a, b, geologist}.\nLet us examine a subset of the holds frame of discernment, containing the\npredicates geologist (which in turn contains the predicate fred) and a. Figure 1\n7\nag\na,g\nf\n??\n??\nHolds\nGeologist\nA\nG\n0.1\n0.4\n0.3\n0.60.2\n0.3\n0.1\nFig. 1. The frames of discernment and associated belief mass assignments for the\npredicates (holds, {a, geologist}), (geologist, {fred}). g stands for geologist, and f\nfor fred . Capitalised letters show non-atomic states.\na\ngfa\ngf\na\ng{}\ng{}\n??\nG\nA\n0.2\n0.1\n0.18 0.07\n0.09\n0.12\n0.24\nF\nFig. 2. The merged frame of discernment for the predicates\n(holds, {a, geologist}), (geologist, {fred}). gf stands for the predicate\ngeologist(fred), and g is the primitive state obtained from the empty set of\nthe original (unmerged) geologist frame of discernment. The dotted ellipses,\ntogether with the associated capitalised letters show non-atomic states.\nshows these frames of discernment, together with a belief mass assignment to\nthe various states.\nOne question that we must answer is how nested belief masses (and from\nthem opinions) are computed. For example, what would the belief mass of\nholds(geologist(fred)) be? To answer this question, we need to merge our\nframes of discernment. Figure 2 shows the merged frame of discernment.\nTo create a merged frame of discernment from the frame of discernment of\ntwo predicates \u0398pr1 ,\u0398pr2 where pr1 = (n1, {p11, . . . , p1m}), pr2 = p1i for some\n1 < i < m, and pr2 = (n2, {p21, . . . , p2n}), we first define pa, the reduced set\n8\nof parameters as {p11, . . . , p1m}\\p2n.\nThen the atomic states in the merged frame of discernment are\n\u0398pa \u222a ((\u0398pa\\{})\u00d7\u0398pr2) \u222a\u0398pr2\nNote that we must differentiate between the empty sets found in both predi-\ncates when in the merged state, and will thus refer to the former as {}1 and\nthe latter as {}2.\nThe new frame of discernment contains n +m non-atomic states. As before,\nthese non-atomic states encapsulate all atomic states containing a parameter of\nthe same name. The only exception to this is the state p1i, which encompasses\nany atomic state with elements in p2j for j = 1 . . . n.\nThe belief mass assignments (BMAs) for all atomic states from pr1 remain\nthe same. BMAs for atomic states containing elements from pr2 are computed\nby multiplying the BMA from the relevant state in pr1 with the state from\npr2. Thus for example, the BMA for state a, f in Figure 2 is 0.4\u00d7 0.6 = 0.24.\nAs can be seen from the figure, compound state G was also assigned a BMA.\nThis was because the frame of discernment in the original predicate had an\nassociated BMA. The BMA for the p1i compound state is computed as the\nBMA assigned to the original frame of discernment multiplied by the BMA\nassigned to all its substates in pr1.\nConverting BMAs to opinions, we see that an opinion about a proposition\nremains the same in both the merged and unmerged models. This is an\nimportant result for the rest of our framework, as arguments are based on\nopinions rather than BMAs. It should be noted that \u03c9(geologist(fred)) 6=\n\u03c9(holds(geologist(fred)), even though the belief mass assignments remain\nthe same. However, \u03c9(a) remains the same in both cases.\nArgument schemes, while mainly used in the higher levels of our framework,\nform the second part of our universe of discourse. They are instantiated to\nform concrete arguments. We assume that our framework contains only a\nfinite number of argument schemes:\nDefinition 8 (Argument Scheme) Given a universe of discourse U =\n(PF , AS), an argument scheme as \u2208 AS is a tuple\n(Name,Premises ,Conclusions , F, A)\nName uniquely identifies the argument scheme, Premises and Conclusions are\ntuples of the form (li, (s1, . . . , sn)) such that \u2203(li,Parameters) \u2208 PF where n =\n|Parameters|. si \u2208 \u03a6 are symbols. All tuples within Premises and Conclusions\nmust differ from each other. A : 2Premises \u2192 {true, false} is the applicability\n9\nfunction. F is a user-defined function mapping opinions over all premises to\nopinions over all conclusions.\nThe admissibility function A is used to compute whether an argument (defined\nbelow) can be used given a specific set of premises. If it can, the F function is\nused to assign a set of opinions to the conclusions of the argument based on\nthe opinions assigned to its premises. Both the A and F functions can make\nuse of the following:\n\u2022 The standard arithmetic operators (+,\u2212, etc).\n\u2022 Comparison operators (<,\u2265, etc).\n\u2022 Boolean operators (not, or, etc).\n\u2022 The functions b(\u03c9), d(\u03c9), u(\u03c9) on an opinion \u03c9.\n\u2022 Subjective logic boolean (Definition 4), independent consensus and dis-\ncounting (Definitions 5 and 6), and other [7] operators, operating on opin-\nions.\n\u2022 References to predicates mentioned in Premises and Conclusions.\n\u2022 An if\/then conditional.\n\u2022 An \u201cunpacking\u201d operator to refer to the parameters of a predicate.\nAs an example, Modus Ponens can be represented with the argument scheme\n(ModusPonens , {holds(A), implies(A,B)}, {holds(B))}, F, true). Here, F is:\n\u03c9(holds(B)) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u30080, 0, 1\u3009 b(holds(A)) < 0.5 or\nb(implies(A,B)) < 0.5\n\u03c9(holds(A)) b(holds(A) < b(implies(A,B))\n\u03c9(implies(A,B) otherwise\nThe first condition is not strictly necessary, as the applicability function can\nbe crafted to prevent it from ever being evaluated. The second and third\nconditions choose an opinion based on the strength of the premises. Clearly,\nother F functions are also possible.\nWe make use of first order unification to transform an argument scheme into\na concrete argument. Symbols found in the argument scheme\u2019s premises and\nconclusions are replaced with symbols found in the predicate\u2019s frame of dis-\ncernment. As in most forms of unification, identical symbols are transformed\ninto identical variables. Arguments are thus instantiated argument schemes.\nDefinition 9 (Instantiated Argument) An instantiated argument for an\nargument scheme (Name,Premises ,Conclusions , F, A) is a tuple of the form\nA = (Name,M) where M is a set of symbol pairs (s, l) such that s \u2208 \u03a3 and\nl \u2208 \u03a6. Given that the argument scheme\u2019s Premises and Conclusions are of the\n10\nform (li, (s1, . . . , sn)), we have the restriction that s \u2208 {s1, . . . , sn}. Finally,\ngiven two pairs (s1, l1), (s2, l2) \u2208M , such that s1, s2 \u2208 S and l1, l2 \u2208 L, l1 = l2\niff s1 = s2.\nWe name the set of all possible instantiated arguments Args.\nThus, for example, given the argument scheme for Modus Ponens defined\nabove, and assuming a knowledge base containing the predicates holds(a) and\nimplies(a, b) associated with sufficiently high levels of belief, we may generate\nthe instantiated argument (ModusPonens , {(A, a), (B, b))}. Using instantiated\narguments in this form is unwieldy as references to premises and conclusions\nfrom the argument scheme must constantly be made. We can write an instan-\ntiated argument in an abbreviated form. For example, given the previously\ndescribed argument scheme for Modus Ponens, and the previously described\ninstantiated argument, we will write\n(ModusPonens , {holds(a), implies(a, b)}, {holds(b)}, A, F )\nIf the A and F functions are not used in the context in which we refer to the\nargument, we may leave them out.\nUntil now, we have described what individual arguments look like. However,\narguments do not exist in isolation. Instead, they interact with each other,\nreinforcing or weakening opinions about predicates in the process. Unlike most\nother argumentation frameworks, we do not explicitly model rebutting and\nundercutting attacks to show how arguments interact. Instead, we use the\nconcept of accrual of arguments to allow for both argument strengthening and\nweakening. To represent interactions between arguments, we must be able to\nanswer the following question: what happens when two different arguments\nhave opinions about a (partially shared) set of predicates in their conclusions?\nThe independent consensus operator gives us a default technique for applying\naccrual. Thus, given a set of arguments for and against a certain conclusion,\nand given no extra information, we apply the consensus operator based on\nthe opinions garnered from the arguments to arrive at a final opinion for the\nconclusion. While the consensus operator works well in most cases, it fails in a\nnumber of situations. Specifically, accruals do not always accrue in the manner\ncaptured by this operator. We now explore some situations where this occurs.\nPrakken [14] lays out three principles that the scheme of accrual of arguments\nadheres to. These are:\n(1) Accruals are sometimes weaker than their elements.\n(2) An accrual makes its elements inapplicable.\n(3) Flawed reasons or arguments may not accrue.\n11\nThe weakening of an accrual, according to Prakken, is due to the possibility\nthat the accruing reasons are not independent. As an example, he suggests\nthe case where two reasons exist not to go running, namely that it is hot,\nand that it is raining. However, for some runners, this specific combination of\nconditions may be less unpleasant than either condition alone, or may even be\na reason to go running. Prakken claims that the interactions between reasons\nmeans that, in general, it is impossible to calculate the strength of an accrual\nfrom is accruing elements. We believe that while this claim is true in specific\ncases, in the general case the consensus operator combines accrual elements in\nan intuitively correct manner. Our framework is, however, designed to cater\nfor special cases where the consensus operator should not be used. In these\ncases, a custom function is used instead of the consensus operator to perform\nthe accrual.\nWhile relatively obvious, the second principle is critical. Any framework sup-\nporting accrual of arguments must not count evidence twice. However, if the\naccrual is defeated, its component parts should be reinstated.\nThe final point states that if a component within an accrual is defeated, it\nshould not be counted when performing the accrual; otherwise this might mean\nthat the accrual as a whole becomes invalid.\nWhile some researchers have suggested that accrual of arguments is an argu-\nment scheme and can be treated as such (arguably, for example [13]), Prakken\u2019s\nview, in our understanding, is that the best way to handle accrual of argu-\nments is by following a two stage process. First, determine what arguments\nmay enter into an accrual, and second compute the effects of the accrual. We\nagree that accrual of arguments cannot be treated as \u201cjust another\u201d argument\nscheme due to its role and nature. We believe, however, that in certain situ-\nations (usually obeying principle 1), accrual of evidence can be treated as an\nargument scheme. The way in which our framework aligns these two views is\none of its most unique aspects.\nWe will provide an informal outline of how we approach accrual of arguments\nbefore giving a formal description of the process. Informally, given multiple\narguments for a conclusion, we apply the standard consensus rule. However, if\nan argument is advanced which subsumes (some of the) arguments which take\npart in the consensus, the subsumed argument\u2019s conclusions are ignored, and\nthe subsuming rule is used instead. If any of those arguments are attacked and\ndefeated, then our accrual rule is itself defeated, allowing all its undefeated\n(and previously subsumed) members to act again. If some of the newly acti-\nvated sub-members were, in turn, part of accruals, those accruals would enter\ninto force again.\nWe claim that an argument subsumes another if the subsumed argument\u2019s\n12\npremises are a subset of the subsuming argument\u2019s premises, and at least one\nconclusion is shared. However, any conclusions that are not shared are still in\nforce. Thus, for example, given the three arguments \u201cif it is raining, we do not\nrun\u201d, \u201cif it is hot, we do not run\u201d, and \u201cif it is hot and raining, we do run\u201d,\nit is clear that the third argument would subsume the other two. It is also\nclear that if the first argument is changed to read \u201cif it is raining, we do not\nrun and do not hang washing out to dry\u201d, we would run, but still not hang\nwashing out to dry. Formally,\nDefinition 10 (Argument Subsumption) Given two instantiated argu-\nments (written in abbreviated form),\n(Arg1, {p11, . . . , pl1}, {c11, . . . , cm1}), and (Arg2, {p12, . . . , pn2}, {c12, . . . , co2}),\nwe say that Arg2 subsumes Arg1 for a set of conclusions C iff the following\ntwo conditions hold:\n(1) for all pi1, i = 1 . . . l there is a j \u2208 {1 . . . l} such that pi1 = pj2.\n(2) for some 1 < i < m and 1 < j < o, ci1 = cj2 and ci1, cj2 \u2208 C\nIf Arg2 subsumes Arg1 for a set of conclusions C, we may write Arg2 \u001dC\nArg1.\nArg2 maximally subsumes Arg1 for a set of conclusions C if Arg2 subsumes\nArg1 for a set of conclusions C and there is no set of conclusions D for which\nArg2 subsumes Arg1 such that |D| > |C|.\nWhen given multiple arguments for a conclusion, we apply only the argument\nthat maximally subsumes all other arguments for that conclusion. If any ar-\nguments remain that can be applied, they are combined using the Subjective\nLogic independent consensus operator.\nWe are now in a position to provide an algorithm for evaluating how sets of\n(instantiated) arguments interact. Our algorithm is shown in Figure 3; it is\ninspired by the way reasoning is performed in probabilistic networks, and, in\nfact, is best explained by thinking of our sets of arguments and predicates\nas a graph. Both predicates and arguments can be thought of as nodes, with\na directed edge between the two if the predicate appears in the premises or\nconclusions of an argument. The edge enters the argument in the case of the\npredicate being a premise, and exits the argument otherwise.\nOne weakness of our approach is the assumption that our argument graph is\nacyclic. This makes it difficult to represent certain classes of arguments such\nas \u201ca holds iff b holds\u201d. Another family of cycles that can arise in the graph\ninvolves self reinforcing, or self defeating chains of argument. However, due\nto the nature of our algorithm, this class of argument does not pose as big a\nproblem. Some possible solutions to this issue are discussed in Section 5.\n13\nGiven: a set of instantiated arguments A\na set of possible facts and associated opinions PF , \u03c9PF\nVariables:\nvisitedFacts VF\nvisitedArguments VA\nVF = PF\nrepeat until A = VA\n\u2200a = (p, c, applicable, F ) \u2208 A\\VA\nif \u2200pi \u2208 p, pi \u2208 VF\nif admissible(a)\nVA = VA \u222a A\nelse\nA = A\\a\n\u2200r \u2208 PF\\VF\nif 6 \u2203a \u2208 A\\VA such that r \u2208 c(a)\n\u2200a \u2208 VA such that r \u2208 c(a) and 6 \u2203a2 \u2208 VA such that a2(c(a))\u001d a(c(a))\n\u03c9PF (r) =\n\u2295\n(F (a))\nVF = VF \u222a r\nelse if 6 \u2203a \u2208 A such that r \u2208 c(a)\n\u03c9PF (r) = \u30080, 0, 1\u3009\nVF = VF \u222a r\nFig. 3. An algorithm to compute conclusions given a set of argument schemes, in-\nstantiated arguments, and optionally, some opinions. c(a) represents the conclusions\nof instantiated argument a, F (a) the application of a\u2019s F function, and a2(c(a)) and\na(c(a)) represent the arguments whose conclusions are c(a). Note that the abbrevi-\nated form of an instantiated argument is used in the algorithm.\nTo operate, our algorithm requires an argument graph, as well as a starting\nset of opinions. We assume that these opinions are not under dispute, and\nthe associated nodes must, therefore, have no edges leading into them. Our\nalgorithm then propagates these opinions forward through the graph, until all\napplicable arguments in the graph have been taken into account. A few issues\nneed to be taken into consideration to ensure the proper functioning of our\nalgorithm:\n\u2022 Only justified arguments should be used.\n\u2022 Defaults must be taken into account.\n\u2022 It must be possible to differentiate between visited and unvisited nodes.\n\u2022 All, or only some of an argument\u2019s conclusions may participate in an accrual.\nA predicate node is assigned an opinion if all the argument nodes leading into\nit have associated opinions (taking into account accrual of arguments). An\n14\nargument node is evaluated if all predicate nodes leading into it are assigned\nan opinion, unless the predicate node has no arguments leading into it (for\nexample due to argument defeat) in which case they are assigned a default\nopinion of \u30080, 0, 1\u3009. Our algorithm terminates in O(n) time, where n is the\nnumber of edges.\nIf an argument is not admissible, it is removed from evaluation. It should be\nnoted that once an argument is removed, it cannot be reinstated. However,\narguments are only removed when there is no chance that they will be ad-\nmissible, so the framework yields the same results as our intuition. As we\nwill discuss in Section 5, there is only a weak relation between our seman-\ntics and Dung\u2019s argumentation semantics. We also defer discussion of other\nrepresentational issues relating to the underlying frameworkto Section 5.\nAt this point, we have a way of determining which conclusions hold given a set\nof arguments. Next, we describe a procedure for how the set of arguments is\ngenerated. This is done in two parts. We assume that our argument framework\nis used within the context of a dialogue. The utterances made in the course\nof the dialogue result in the set of arguments. Thus, we begin by formalising\nthe dialogue process, after which we provide a decision rule which dialogue\nparticipants can use to determine which arguments they should advance at any\npoint in the dialogue. Once this is done, we can show how agents, arguments\nand argument schemes interact to form our complete framework.\nTo specify the dialogue, we need to further constrain and describe the environ-\nment in which it takes place. We assume that dialogue occurs between two or\nmore agents, each of which has a private knowledge base, opinions about the\nenvironment, and goals. The dialogue environment contains a public commit-\nment store into which the agent\u2019s arguments are inserted, as well as the set of\nvalid argument schemes. Since we are interested in arguing about evidence in\npartially observable domains, we make the assumption that the environment\nholds a set of sensors. These sensors may be probed to obtain opinions about\nthe value of various relations. In practise, sensors may be agents, static parts of\nthe environment, or some other entity capable of providing an opinion about\nthe environment. We assume that multiple sensors can give opinions about\nthe same relations, and that some sensors are more reliable than others.\nDefinition 11 (Environment) Given a universe of discourse (PF,AS),\nThe environment Env is a tuple (Agents, CS, S, PC) where Agents is the set\nof agents operating in the environment, CS \u2286 PF is the commitment store\n(a public knowledge base of arguments), and S is the set of sensors present in\nthe environment. PC : 2S \u2192 R is the sensor probing cost function.\nDefinition 12 (Agents) Given environment Env = (Agents, CS, S, PC),\nan agent \u03b1 \u2208 Agents is a tuple (Name,KB,G,C) consisting of the agent\u2019s\n15\nname, Name, a private knowledge base KB \u2286 PF containing opinions about\nthe environment, a goal function G : \u0398 \u2192 R mapping combinations of opin-\nions on predicates (obtained by looking at the frame of discernment) to utility\nvalues, and a variable C \u2208 R to keep track of the agent\u2019s utility cost.\nDefinition 13 (Sensors) A sensor s is a structure (\u2126s,\u2126p). \u2126s is a set\ncontaining predicate, opinion pairs representing the reliability of a sensor with\nrespect to the predicate. \u2126p is another predicate, opinion pair which stores the\nsensor\u2019s opinion regarding the state of the predicate.\nAgents take turns to advance a line of argument (consisting of one or more\ninstantiated arguments), and probe sensors to obtain more information about\nthe environment. Such an action is called an utterance. In each turn, the\ncontents of an agent\u2019s utterance is added to the commitment store; any sensors\nprobed are marked as such (a sensor may not be probed more than once for\nthe value of a specific relation), and costs are updated. Once made, there is\nno way to withdraw the contents of an utterance from the commitment store.\nDefinition 14 (Utterances) The utterance function\nutterance : Environment\u00d7Name\u2192 2Args \u00d7 Probes\ntakes in an environment and an agent (via its name), and returns the utterance\nmade by the agent. The first part of the utterance lists the arguments advanced\nby the agent, while the second lists the probes the agents would like to undertake\nwhere Probes \u2208 2S.\nDefinition 15 (Turns) The turn function\nturn : Environment\u00d7Name\u2192 Environment\ntakes in an environment and an agent label, and returns a new environment\ncontaining the effects of an agent\u2019s utterance.\nIn our framework, the turn function is defined as turn = (NewAgents, CS \u222a\nAr,NewSensors, PC) where Ar,NewAgents and NewSensors are computed\nfrom the results of the utterance function. Assuming that the agent mak-\ning the utterance is agent \u03b1, if utterance(Env,Name) = (Ar, Probes) then\nNewAgents = (Agents\\{\u03b1})\u222a(Name,KB,G,C+PC(Probes)) and, \u2200s, l \u2208\nProbes, where l is a predicate that sensor s is able to probe, NewSensors =\n(Sensors \\ {s}) \u222a (\u2126s,\u2126p \u222a \u03c9p(l))).\nIt should be noted that the utterance depends on agent strategy; one possible\nutterance function was described in [11], and will be described briefly later.\nBefore doing so, we must define a protocol which agents may use to argue with\neach other. This protocol, often referred to as a dialogue game [9], contains only\none locution (in which agents advance an argument and probe sensors), and\n16\nallows agents to alternate in making utterances. More complicated dialogue\ngames are also possible, but are not examined here as they are auxiliary to\nthe focus of this paper.\nWe may assume that our agents are named Agent0, Agent1, . . . , Agentsn\u22121\nwhere n is the number of agents participating in the dialogue. We can de-\nfine the dialogue game in terms of the turn function by setting turn0 =\nturn((Agents, CS0, S, admissible, PC), Agent0), and then having turni+1 =\nturn(turni, Agenti mod n). The game ends if turni . . . turni\u2212n+1 = turni\u2212n.\nWhen the dialogue starts, CS0 contains publicly known arguments. It is usu-\nally empty. It should be noted that an agent may make a null utterance {, }\nduring its turn to (eventually) bring the game to an end. In fact, given a fi-\nnite number of arguments and sensors, it should be clear that the dialogue\nis guaranteed to terminate, as, eventually, no utterances will be possible that\nwill modify the public knowledge base CS.\nAt any time, we may compute an agent\u2019s utility by combining its utility gain\n(for achieving its goals) with its current costs. At any stage of the dialogue,\ngiven the environment\u2019s CS, and the set of all opinions probed by the sensors\n{\u2126p|s = (\u2126s,\u2126p) \u2208 S}, as well as the set of legal argument schemes, we can\nrun the reasoning algorithm to compute the set of \u201cproven\u201d relations; that is,\nrelations for which an opinion exceeds a predetermined admissibility bound.\nSimilarly, we can determine which relations have their negation proven, and\nwhich relations are simply unproven.\nGiven an environment CS, the set of all opinions probed by the sensors\n{\u2126p|s = (\u2126s,\u2126p) \u2208 S)} and an admissibility function on opinions\nAdmissible(\u03c9)\u2192 [true, false, unknown], we can run the reasoning algorithm\nover all possible facts to create a set of true, false and unproven predicates. If\nwe name these sets ftrue, ffalse and funknown, then the agent\u2019s net utility gain\nis G(ftrue, ffalse , funknown)\u2212 C, where C is the agent\u2019s utility cost.\nAt the end of the dialogue, we assume that agents agree that literals in the\nftrue and ffalse sets hold in the environment.\nOne simple decision procedure for an agent (described in detail in [11]) involves\nit performing one step look-ahead to decide which utterance to make. The\nagent computes what probes it can make by looking at what sensors have\nnot yet been probed, and what arguments it can advance (by looking at its\nknowledge base, the commitment store, and the set of argument schemes). It\nthen calculates the utility gain for each combination of probes and advanced\narguments, advancing the ones that maximise its utility.\n17\n4 Example\nIn this section, we describe a dialogue in a hypothetical bridge building sce-\nnario. Two agents, \u03b1 and \u03b2 must use these argument schemes to have a discus-\nsion about the amount of concrete and steel needed to build a bridge. Agent\n\u03b1\u2019s goal involves attempting to minimise the amount of steel needed \u2014 \u03b1\nis responsible for the supply of steel. This may be achieved by showing that\nthe environment is in such a state where little steel, and lots of concrete is\nneeded. \u03b2 is responsible for providing the bridge\u2019s concrete, and would like to\nminimise the amount of concrete used. In the interests of clarity, our descrip-\ntion is semi-formal.\nAssume we have the following general argument schemes (here, ArgExpertOp\nis the scheme for an argument from expert opinion [20]:\nName Premises Conclusions A F\nModusPonens {A, implies(A,B)} {B} A1 F1\nArgExpertOp {expert(E,D), claims(E,A), {A} A2 F2\ninDomain(A,D)}\nD1 {sand(L), support(X,L)} {concrete(X)} A3 F3\nD2 {rock(L), support(X,L)} {steel(X)} A4 F4\nD3 {mud(L), support(X,L)} {concrete(X)} A5 F5\nD4 {rock(L), sand(L), {steel(X), A6 F6\nsupport(X,L)} concrete(X)}\nwhere\nA1 : true if b(A) and b(implies(A,B)) are both \u2265 0.5 else false\nF1 : \u03c9(B) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u30080, 0, 1\u3009 if b(holds(A)) < 0.5 or\nb(implies(A,B)) < 0.5\n\u03c9(holds(A)) if b(holds(A) < b(implies(A,B))\n\u03c9(implies(A,B)) otherwise\nA2 : true if d(expert(E,D)) < 0.5 & d(inDomain(A,D)) < 0.5\nF2 : \u03c9(A) = claims(E,A)\nFor D1 . . . D4, the admissibility function requires that the belief in rock, sand\n18\nor mud be greater than 0.5, and the F function sets the conclusions to the\nsame strength as the premises, except for D4. Here, steel(X) is set to the\naverage value of rock(L) and sand(L), while concrete(X) is set to \u00acsteel(X).\nThe top level frame of discernment is created using the predicates\nsand, rock,mud, implies, expert, claims, inDomain, fastWater\nThe sand, rock,mud and fastWater frames of discernment contain l (rep-\nresenting a location). The implies frame of discernment contains the tuple\n(fastWater ,mud), while expert contains geologist. inDomain contains the\ntuple (geology, sand), and claims contains\n(geologist, sand), (geologist,mud), (geologist, rock)\nLet \u03b1 have an opinion of \u30080.9, 0, 0.1\u3009 regarding sand(l) stored in it\u2019s knowledge\nbase. Assume that it also believes fastWater(l) and claims(geologist, sand(l)).\nFinally, let the environment contain sensors s1, s2, s3, s4, with the first two\nbeing able to monitor the status of sand(l), s3 able to observe all states in\nthe claims frames of discernment, and s4 being able to detect fastWater(l).\nLet s1 also be able to discern the status of rock(l). We associate opinions\n\u30080.7, 0.2, 0.1\u3009,\u30080.8, 0.2, 0.1\u3009,\u30080.9, 0, 0.1\u3009 and \u30080.8, 0.1, 0.1\u3009 regarding the sensors\u2019\nrespective reliabilities.\nAgent \u03b1 begins the conversation by making the utterance\n((D1, {sand(l), support(bridge, l)}, {concrete(bridge)}), {(s1, sand(l))})\nThat is, it probes whether sand exists in the location the bridge is to be built,\nand instantiates an argument based on D1 claiming that since it is sandy, a\nlarge amount of concrete is required for the bridge 2 . Assume that s1 returns\na value of \u30080.7, 0, 0.3\u3009. This means that sand(l) is now associated with an\nopinion of \u30080.56, 0.07, 0.37\u3009 in the CS.\nAgent \u03b2 responds by probing another sensor\n(, {(s2, sand(l))})\nto show that the bank is not in fact sandy. This sensor returns an opinion\n\u30080.3, 0.6, 0.1\u3009, meaning that the \u03c9(sand(l)) is now \u30080.45, 0.36, 0.09\u3009. Agent \u03b1\u2019s\nargument is now no longer admissible.\n2 The probe may return values, contrary to the agent\u2019s belief, that cause the argu-\nment to be inapplicable.\n19\nNo more sensors exist that can be used to determine whether sand(l) holds\nor not. Agent \u03b1 thus advances the argument\n((ArgExpertOp, {expert(geologist, geology),\nclaims(geologist, sand(l)), indomain(geology, sand(l))}, {sand(l)}),\n{(s3, claims(geologist, sand(l)))})\nThe probe here represents asking the witness for their testimony. Assume\nthat the witness returns an opinion of \u30080.8, 0.1, 0.1\u3009. sand(l) would now once\nagain be a justified conclusion. At this point, it should be noted that the\nadmissibility requirements for the argument from expert opinion mean that\nthe burden of proof is assigned to the agent challenging the argument. More\ncomplex behaviour, such as requiring the agent introducing the argument to\njustify their assumptions, is easily introduced by changing the form of A and\nF .\nAgent \u03b2 now turns to an argument using accruals. It points out that\n((D4, {rock(l), support(bridge, l)}, {concrete(bridge), steel(bridge)}),\n{(s1, rock(l))})\nIn other words, the presence of rock together with sand means that steel\nrather than concrete is required. Since this argument scheme\u2019s F function is\nnot dogmatic, the conclusion for concrete is weakened, but not eliminated.\nFinally, \u03b1 responds with the argument\n({(ModusPonens , {fastWater(l), implies(fastwater(l),mud(l))},\n{mud(l)}), ((D3, {mud(l), support(bridge, l)},\n{concrete(bridge)})}, {(s4, fastWater(l))})\nThat is, by showing that there is fast water at location l, it supports the\nconclusion that mud exists at l. The existence of mud means that argument\nscheme D3 can be used, which accrues (via the default consensus operator)\nwith the current opinion regarding the need for concrete.\nAt this point in the conversation, the agents have no further arguments to\nadvance, and the dialogue terminates. predicates concrete(l) and steel(l) have\nopinions associated with them. Depending on the form of the admissibility\nfunction, they, or their negation, may be judged as proven or unproven. If, for\n20\nsand(L),support(X,L)->concrete(X)sand(l)\nconcrete(bridge)\nexpert(E,D),claims(E,A),\ninDomain(A,D)->A\nexpert(geologist,geology)\nclaims(geologist,sand(l))\nindomain(geology,sand(l))\nsupport(bridge,l)\n1\n2\n3\nsand(L),support(X,L)->concrete(X)sand(l)\nconcrete(bridge)\nsupport(bridge,l)\n1 sand(l)1\n2\nFig. 4. The argument graph obtained after the first three utterances are made. The\nnumbers show during which turn a sensor probe was made. Solid arrows indicate\nsupport for an argument or predicate, while dashed lines represent an attack.\nexample, concrete(l) is judged to be admissible, both agents would agree that\nmore concrete should be used at the site.\nThe first three utterances of the dialogue are shown in Figure 4. It is assumed\nthat any unprobed predicates were in the commitment store at the start of\nthe dialogue, together with their associated opinions. As can be seen, after\nthe second utterance, the low opinion associated with sand(l) means that the\nargument advanced at the start of the dialogue is no longer deemed applicable.\nThus, the opinion associated with concrete(bridge) would revert to its default\nvalue. The argument graph for the entire dialogue is shown in Figure 5.\n5 Discussion\nIn this section, we examine some of the novel features of our framework in\ndetail, as well as looking at related research and possible future work.\nOur framework was designed to allow for complex argument to take place, par-\nticularly in the domain of evidential reasoning. Uncertainty is a key feature of\nsuch domains, hence our decision to base our framework on Subjective Logic.\nCatering for uncertainty in argumentation frameworks is by no means new.\nPollock [13] made probability a central feature of his OSCAR architecture.\n21\nsand(L),support(X,L)->concrete(X)\nsand(l)\nconcrete(bridge)\nexpert(E,D),claims(E,A),\ninDomain(A,D)->A\nexpert(geologist,geology)\nclaims(geologist,sand(l))\nindomain(geology,sand(l))\nrock(L),sand(L),\nsupport(X,L)->steel(X),concrete(X)\nrock(l)\nsupport(bridge,l)\nsteel(bridge)\nfastWater(l)\nimplies(fastwater(l),mud(l))\nmud(l)\nA,implies(A,B)->B\nsupport(bridge,l)\nmud(L),support(X,L)->concrete(X)\n1\n2\n3\n4\n5\nFig. 5. The complete argument graph for the dialogue. The numbers show during\nwhich turn a sensor probe was made. Solid arrows indicate support for an argument\nor predicate, while dashed lines represent an attack.\nWe disagree with his extensive use of the \u201cweakest link\u201d principle, however,\nbelieving that, while it may hold in general, it is not always applicable (as\nmentioned in [14]. His use of probability, rather than uncertainty is another\npoint at which our approaches diverge. Other notable work includes that of\nVreeswijk [18], and Haenni [5]. The latter suggests an approach called \u201cProb-\nabilistic argumentation systems\u201d. These systems are designed to perform in-\nference under uncertainty within conflicting knowledge bases. While lacking a\ndialogical aspect, he shows a relation between his work and Dempster-Schafer\ntheory.\nOur use of Subjective Logic as the basis of the framework provides us with\na large amount of representational richness. Not only are we able to repre-\nsent probability (via belief), but we are also able to speak about ignorance\n(via uncertainty). Differentiating between these two concepts lets us repre-\nsent defaults in a natural, and elegant way. A default can be represented by\nspecifying, within the A function, that a conclusion may hold as long as the\ndisbelief for a premise remains below a certain threshold. By requiring that\nbelief remain above some threshold, normal premises can also be represented.\n22\nA simple example of this was provided in the previous section, where everyone,\nby default, is assumed to be an expert. Burden of proof [19] is very closely\nrelated to defaults, and we model it in the same way.\nArgument schemes have been extensively discussed in the literature (see for\nexample [3,20]). A small, but growing number of argumentation frameworks\nprovide explicit support for argument schemes (e.g. [17]). We believe that\nsupporting argument schemes in our framework not only enhances argument\nunderstanding, but that such support also provides clear practical advantages,\nincluding the separation of domain and argument knowledge, re-usability, and\na possible reduction in computational complexity when deciding what argu-\nments to advance. The separation between arguments and agent knowledge\ncreated by argument schemes raises the intriguing possibility of the modifica-\ntion and dynamic creation of argument schemes during a dialogue.\nWe have separated out the F and A functions within our representation of\nargument schemes as we believe that in some situations, the decision regarding\nwhether an argument scheme is applicable, and how strongly it supports its\nconclusions, are independent of each other. Separating out the two functions is\nalso practically useful; by explicitly excluding an argument based on the result\nof its A function, we can avoid extra calculations in our algorithm. Agents can\nalso use the A function in the heuristic to avoid considering the application\nof invalid argument schemes.\nJ\u00f8sang has proposed a large number of additional operators for use in Sub-\njective Logic which have not been mentioned in this paper (see for example\n[8]). Many of these operators appear to encapsulate common forms of reason-\ning about evidence, and can thus (with appropriate restrictions on premises\nand conclusions) form the basis of an argument scheme\u2019s F function. We be-\nlieve that using our framework as a basis for investigating additional possible\nSubjective Logic operators may be fruitful.\nAnother area in which we plan to extend the framework involves unification\nand quantification. At the moment, we perform universal quantification over\nall the elements of a frame of discernment. That is, if the expert frame of\ndiscernment contains (geologist,mud), we could deduce\nexpert(geologist(fred),mud(l)), expert(geologist(mary),mud(m))\nwith no way of specifying that in fact, fred is only an expert in mud at location\nl, not location m.\nThe interplay between sensors and arguments is an area in which little formal\nwork has been done [12]. While our model is very simple, it elegantly captures\nthe fact that sensor data is inherently unreliable in many situations. Enriching\nour model of sensors is one area in which we plan to do future work.\n23\nOur model was designed with support for accrual of arguments in mind. The\nway in which we deal with accrual of arguments, while powerful, is still limited,\nand overcoming these limitations is a priority. We are unable to handle accruals\nin which the accrued and accruing arguments share no conclusions. While it is\noften possible to add an explicit negated conclusion so as to negate the accrued\nargument\u2019s conclusions, this is incorrect in some situations. Instead, we recom-\nmend extending the framework by introducing an explicit accrual relationship\nbetween argument schemes. Representing such situations requires inputting\nadditional domain knowledge into the framework, the domain-specific nature\nof such accruals means that no other way to handle them exists.\nWe are currently investigating what effects a mapping between our model and\na Dung-like abstract model [4] will have. By allowing for linked arguments [10]\nand support between arguments [1], the translation of the graph that results\nfrom an application of the model to one embedded in an abstract model allows\nus to cater for some types of loops. However, this approach does not allow us to\ndeal with argument strength in a satisfactory manner, meaning that techniques\nfor representing self-reinforcing arguments must still be investigated.\nThe dialogue game we have proposed is very simple, and only guarantees\ndialogue termination (given a finite number of argumentation schemes, finite\nknowledge bases, and a finite number of sensors sensors). Dispute focus, that\nis, ensuring that agents advance arguments relevant to the conversation, is\nprovided by the utility based argument heuristic. Also related to the dialogue\ngame, as well as the structure of the agents and environment, is the problem\nof advancing an argument without proof. Depending on the structure of an\nargument scheme\u2019s F function, a sensor must always be probed before an\nargument may be advanced. Thus, the initial burden of proof always falls on\nthe agent making an utterance. While some may claim that such an approach\nmakes sense (after all, even commonsense knowledge requires some sort of\nshared experience, which could be viewed as a sensor), others may view this\nas a weakness of the system. There are a number of ways to avoid this issue.\nThe most obvious is to set up initial predicates in the commitment store with\nthe appropriate belief values. Another way would involve the addition of a\nsingle zero utility cost sensor that an agent may probe to set a predicate\u2019s\ninitial value. It is also possible to craft the F function to allow for certain\nclaims with no proof. None of these approaches are fully satisfying, as they\nallow for only one side of a claim to be made without the need for proof.\nAllowing agents to act as sensors might be a better way of overcoming this\nproblem, and the addition of explicit \u201cclaim\u201d and \u201cchallenge\u201d moves in the\ndialogue is another way to attack this issue. The decision procedure we have\ndescribed is based on some of our earlier work [11]. Other researchers have\nadvanced other possible approaches to argument selection [2], and it would be\ninteresting to integrate these techniques into our work.\n24\n6 Conclusions\nArgumentation is a well recognised, powerful reasoning technique. With a few\nnotable exceptions however, argument frameworks have had difficulty operat-\ning in domains where uncertainty was present. Furthermore, most argument\nframeworks have examined only a single aspect of the problem of argument,\nbe it the underlying logical representation, the interaction between arguments,\nor an illustration of a protocol for argument.\nIn this paper, we presented a framework for argumentation in domains contain-\ning uncertainty. The concept of argument schemes is built into the framework,\nallowing for a rich set of primitives to be utilised in the argumentation process.\nWe have also attempted to cater for other important concepts in argument\nsuch as accrual of arguments, defaults, and burden of proof. While the lowest\nlevels of the framework are general enough to be applied to almost area in\nwhich argument is used, the higher levels are aimed at evidential reasoning.\nTo this end, we introduced the concept of sensors, abstracting the notion of\nobtaining information from the environment. Finally, we introduced a dia-\nlogue game and a very simple decision procedure allowing agents using the\nframework to decide which arguments to advance.\nAcknowledgements\nThis research was partly funded by the DTI\/EPSRC E-Science Core Program\nand British Telecom, via a grant for the CONOISE-G project. It is continuing\nthrough participation in the International Technology Alliance sponsored by\nthe U.S. Army Research Laboratory and the U.K. Ministry of Defence.\nReferences\n[1] L. Amgoud, C. Cayrol, and M.-C. Lagasquie-Schiex. On the bipolarity in\nargumentation frameworks. In Proceedings of the 10th International Workshop\non Non-monotonic Reasoning, pages 1\u20139, Whistler, Canada, 2004.\n[2] L. Amgoud and H. Prade. Reaching agreement through argumentation: a\npossiblistic approach. In Proc. of the 9th Int. Conf. on the Principles of\nKnowledge Representation and Reasoning, pages 175\u2013182, 2004.\n[3] F. Bex, H. Prakken, C. Reed, and D. Walton. Towards a formal account\nof reasoning about evidence: Argumentation schemes and generalisations.\nArtificial Intelligence and Law, 11(2-3):125\u2013165, 2003.\n25\n[4] P. M. Dung. On the acceptability of arguments and its fundamental role in\nnonmonotonic reasoning, logic programming and n-person games. Artificial\nIntelligence, 77(2):321\u2013357, 1995.\n[5] R. Haenni. Towards a unifying theory of logical and probabilistic reasoning. In\nProc. of the 4th Int. Symp. on Imprecise Probabilties and Their Applications,\npages 193\u2013202, Pittsburgh, Pennsylvania, USA, 2005.\n[6] A. J\u00f8sang. A logic for uncertain probabilities. Int. Journal of Uncertainty,\nFuzziness and Knowledge-Based Systems, 9:279\u2013311, 2001.\n[7] A. J\u00f8sang and D. McAnally. Multiplication and comultiplication of beliefs. Int.\nJournal of Approximate Reasoning, 38(1):19\u201351, 2004.\n[8] A. J\u00f8sang, S. Pope, and M. Daniel. Conditional deduction under uncertainty. In\nProc. of the 8th European Conference on Symbolic and Quantitative Approaches\nto Reasoning with Uncertainty, pages 824\u2013835, Barcelona, Spain, 2005.\n[9] P. McBurney and S. Parsons. Dialogue games in multi-agent systems. Informal\nLogic, 22(3):257\u2013274, 2002.\n[10] S. H. Nielsen and S. Parsons. A generalization of Dung\u2019s abstract framework for\nargumentation: Arguing with sets of attacking arguments. In Proc. of ArgMAS\n2006, pages 7\u201319, Hakodate, Japan, 2006.\n[11] N. Oren, T. J. Norman, and A. Preece. Arguing with confidential information.\nIn Proc. of the 18th European Conf. on Artificial Intelligence, pages 280\u2013284,\nRiva del Garda, Italy, August 2006.\n[12] N. Oren, T. J. Norman, and A. Preece. Argumentation based contract\nmonitoring in uncertain domains. In Proc. of the 20th Int. Joint Conf. on\nArtificial Intelligence, pages 1434\u20131439, Hyderabad, India, 2007.\n[13] J. L. Pollock. Cognitive Carpentry. Bradford\/MIT Press, 1995.\n[14] H. Prakken. A study of accrual of arguments, with applications to evidential\nreasoning. In Proc. of the 10th Int. Conf. on Artificial Intelligence and Law,\npages 85\u201394, 2005.\n[15] H. Prakken and G. Sartor. Computational Logic: Logic Programming and\nBeyond. Essays In Honour of Robert A. Kowalski, Part II, volume 2048 of\nLNCS, pages 342\u2013380. Springer-Verlag, 2002.\n[16] C. A. Reed and T. J. Norman, editors. Argumentation Machines: New frontiers\nin argumentation and computation. Kluwer, 2003.\n[17] B. Verheij. Dialectical argumentation with argumentation schemes: An\napproach to legal logic. Artificial intelligence and Law, 11:167\u2013195, 2003.\n[18] G. A. W. Vreeswijk. Argumentation in Bayesian belief networks. In Proc. of\nArgMAS 2004, number 3366 in LNAI, pages 111\u2013129. Springer, 2004.\n[19] D. N. Walton. Burden of proof. Argumentation, 2:233\u2013254, 1988.\n[20] D. N. Walton. Argumentation Schemes for Presumptive Reasoning. Erlbaum,\n1996.\n26\n"}