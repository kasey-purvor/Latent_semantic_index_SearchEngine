{"doi":"10.1007\/s007790170029","coreId":"70485","oai":"oai:eprints.lancs.ac.uk:11932","identifiers":["oai:eprints.lancs.ac.uk:11932","10.1007\/s007790170029"],"title":"Teaching context to applications","authors":["Van Laerhoven, Kristof","Aidoo, Kofi"],"enrichments":{"references":[{"id":16303798,"title":"An Architecture for Multi-Sensor Fusion in Mobile Environments,","authors":[],"date":"1999","doi":null,"raw":"Chen, D., Schmidt, A. & Gellersen, H.-W. (1999) An Architecture for Multi-Sensor Fusion in Mobile Environments, In Proceedings International Conference on Information Fusion, Sunnyvale, CA, USA.","cites":null},{"id":16303801,"title":"Esprit Project 26900. Technology for Enabling Awareness (TEA).http:\/\/tea.starlab.net,","authors":[],"date":"1999","doi":null,"raw":"Esprit Project 26900. Technology for Enabling Awareness (TEA).http:\/\/tea.starlab.net, 1999.","cites":null},{"id":16303804,"title":"Self-Organizing Maps,","authors":[],"date":"1997","doi":"10.1007\/978-3-642-97966-8_3","raw":"Kohonen, T. (1997) Self-Organizing Maps, Springer Springer-Verlag Heidelberg,","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2001-02","abstract":"Although mobile devices keep getting smaller and more powerful, their interface with the user is still based on that of the regular desktop computer. This implies that interaction is usually tedious, while interrupting the user is not really desired in ubiquitous computing. We propose adding an array of hardware sensors to the system that, together with machine learning techniques, make the device aware of its context while it is being used. The goal is to make it learn the context-descriptions from its user on the spot, while minimising user-interaction and maximising reliability","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/70485.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/11932\/1\/pt_2000.pdf","pdfHashValue":"fa1b4c48f48320fd4d84c6ce5de23eb30f72cca4","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:11932<\/identifier><datestamp>\n      2018-01-24T00:04:25Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Teaching context to applications<\/dc:title><dc:creator>\n        Van Laerhoven, Kristof<\/dc:creator><dc:creator>\n        Aidoo, Kofi<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        Although mobile devices keep getting smaller and more powerful, their interface with the user is still based on that of the regular desktop computer. This implies that interaction is usually tedious, while interrupting the user is not really desired in ubiquitous computing. We propose adding an array of hardware sensors to the system that, together with machine learning techniques, make the device aware of its context while it is being used. The goal is to make it learn the context-descriptions from its user on the spot, while minimising user-interaction and maximising reliability.<\/dc:description><dc:date>\n        2001-02<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1007\/s007790170029<\/dc:relation><dc:identifier>\n        Van Laerhoven, Kristof and Aidoo, Kofi (2001) Teaching context to applications. Personal and Ubiquitous Computing, 5 (1). pp. 46-49. ISSN 1617-4909<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/11932\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1007\/s007790170029","http:\/\/eprints.lancs.ac.uk\/11932\/"],"year":2001,"topics":["QA75 Electronic computers. Computer science"],"subject":["Journal Article","PeerReviewed"],"fullText":"Teaching Context to Applications1\nKristof Van Laerhoven, Kofi A. Aidoo\nStarlab Research NV\/SA\nSt-Michielslaan 47\n1040 Brussels\n{kristof, kofi}@starlab.net\n1 The final version of this paper can be found in: Personal Technologies: Situated Interaction and Context-\nAware Computing, A. Schmidt, G. Kortuem, D. Morse and A. Dey (Eds). Springer-Verlag, Berlin, 2000.\nkeywords : adaptive context awareness, Kohonen map,\nadaptive clustering, sensor data fusion.\nAbstract\nAlthough mobile devices keep getting smaller\nand more powerful, their interface with the user\nis still based on that of the regular desktop\ncomputer. This implies that interaction is usually\ntedious, while interrupting the user is not really\ndesired in ubiquitous computing. We propose\nadding an array of hardware sensors to the\nsystem that, together with machine learning\ntechniques, make the device aware of its context\nwhile it is being used. The goal is to make it\nlearn the context-descriptions from its user on\nthe spot, while minimizing user-interaction and\nmaximizing reliability.\n1 INTRODUCTION\n1.1 FROM SENSORS TO CONTEXTS\nSome applications enhance their user interface by adding\na sensor and using the sensors\u2019 value in some simple rule.\nA typical example is connecting a light sensor to a screen-\nbased device and adjusting the contrast and brightness of\nthe screen according to the value of the light sensor.\nOther applications can change their behavior only when\nthe user explicitly tells them to. It is also possible to use\nuser-defined profiles that describe the devices\u2019 behavior.\nProfiles in mobile phones, for example, can be set to\nmake the phone ring very loud outside or on the train, but\nonly vibrate in a meeting. This approach leads to a lot of\nuser-involvement, though: the user first needs to program\nthese profiles, and then these profiles must be set in every\ncontext (\u2018in a meeting\u2019, \u2018in the train\u2019, \u2026).\nThe combination of all of the approaches mentioned\nearlier leads to an automated profiles selection: context\nrecognition based on simple sensors sets the behavior of\nthe device (see [1] and [5]). Knowing the context\nusually leads to being able to improve the application\nand particularly enhancing the interaction with the user.\nThis approach is far from simple, however: how can a\ndevice, equipped with sensors, recognize a context?\n1.2 CONTEXT\nThe notion of context is very broad and incorporates\nlots of information, not just about the current location,\nbut also about the current activity, or even the inner\nstate of the person describing it. As a consequence,\nmultiple people can describe their contexts in different\nways, even if they are in the same location doing the\nsame thing. Someone familiar with a building might\nknow a room as \u2018classroom 402B\u2019, while a visitor\nwould probably describe it as just \u2018a classroom\u2019.\nIn addition, the application defines the description of\nthe context as well. Some applications require more\nlocation-based contexts, while others need contexts that\ngive more information about the user. Since contexts\ndepend heavily on both user and application, context\nawareness should be adaptive. Furthermore, to make\nthe device usable the user should be able to give\nminimal feedback to the learning module.\n1.3 CONTEXT DESCRIPTION\nThe sensors we have experimented with are small, low-\nlevel, and cheap. The hardware boards (see [2]) that\nwere used (Figure 1) include light sensors, temperature\nsensors, accelerometers for movement, microphones,\npressure-, IR-, touch-, and CO sensors.\nThe simplest method for giving a context description\nwould be to sum up all the values from the sensors to a\nformatted description, like for instance \u201cmovement:\n(87%, 29%), light: 78%, humidity: 69%, temperature:\n50%, \u2026\u201d. A simple, rule-based architecture could be\nused to enhance this description into \u201cmoving slowly in a\ncold, humid, well-lit room\u201d.\nThe architecture described here works the opposite way:\nthe system merges the output from the sensors and maps\nthem to a description given by the user. The description\ncould then be something like \u201cwalking in the basement\u201d.\nFigure 1: One of the sensor boards\n2 ONLINE ADAPTIVE CONTEXT\nAWARENESS\nInstead of just using the raw sensor values as input for the\nnext layer, small pre-processing routines were used to\nenhance the future clustering. For example, instead of just\nlooking at the brightness of the light, it is also possible to\nlook at its frequency, which results in easier\ndistinguishing of several types of artificial light. Taking\nthe standard deviation of the accelerometer values can\nalso give more qualitative information. Other sensors like\nmicrophones and infrared sensors have similar mini-\ntransformations from the raw sensor data to one, usually\nmultiple, values, which are often called cues or features.\nAnother advantage of the cues is that that they are sent\nless frequently to the next layer. The light sensor, for\ninstance, is read a few hundred times per second. The\ncues from this sensor (light level and frequency) are sent\nevery second. Cues are very significant for a fast, but\naccurate context recognition system. However, using cues\nresults in a large input dimension, which makes the\nmapping-algorithm very slow in learning. This difficulty\narises when many irrelevant inputs are present and is\nusually referred to as the curse of dimensionality (see [4]\nfor a definition).\n2.1 SELF-ORGANIZATION\nWhen a rat has learned its location in a labyrinth,\ncertain braincells on the hippocampal cortex respond\nonly when it is in a particular location. Self-\norganization of neuronal functions seems to exist on\nvery abstract levels (like geographic environments) in\nthe brain. The Kohonen Self-Organizing Map (SOM)\n[3] has a similar principle: neurons (artificial, this time)\nare activated topologically for tasks depending on the\nsensory input. The SOM is also known to handle noisy\ndata relatively well, which makes it a sensible choice\nfor clustering the inputs.\nIt is possible to monitor the activation of the neurons\nand plot the resulting matrix as a landscape, where\ndifferent hills ideally represent different contexts. This\nmight be a way to provide the user an insight in the\nlearning capabilities of the system (see Figure 2).\nFigure 2: Example of an activity-plot of a SOM.\nThe traditional algorithm starts out highly adaptive (a\nlarge learning rate and huge neighborhood radius) and\ngradually becomes fixed. After this stage, it is not\ncapable of learning anymore, which poses an obstacle if\nthe system needs to remain adaptive. This is a problem\nalso known as the stability-plasticity dilemma. It\ntherefore is necessary to add some supervision\nmechanism that controls the flexibility of the SOM.\nThe only necessary user interaction is the labeling of\nclusters produced by the SOM. This means that when a\ncluster gets activated, two possible situations can occur:\n(1) the cluster is labeled, so classification is possible, or\n(2) there is no label. In the latter case, we use a distance\nweighted K-Nearest Neighbors algorithm to search for\nthe closest label on the Self-Organizing Map. The\ntopology preserving property of the SOM makes it very\nprobable that the nearest label will indeed be the right\ncontext.\n2.2 SUPERVISION AND USER BEHAVIOR\nThe next layer is primarily intended to supervise\ntransitions from one context to another. It uses a\nprobabilistic finite state machine architecture where each\ncontext is represented by a state, and transitions are\nrepresented by edges between states. The model keeps a\nprobability measure for each transition, so every time a\ntransition occurs, the supervision model can check if this\nreally is likely. If a transition is not really probable, the\nnext state is not entered yet, but a buffer mechanism is\ninitiated so that it does become more likely after several\ntries in a row. Each transition to a state is thus dependent\non the previous state, which makes this model a first-\norder Markov model. Every state also keeps track of how\nmuch time was spend in a particular context, which\ncontrols the flexibility of the SOMs: the newer a context,\nthe more flexible and adaptive the map should be.\nwalking\nstanding\nsitting\ntalking\nrunning\nkitchen\nstairs\nNN lab\nHardw .\nlab\noutside\nactivity\nlocation\nMovement Light Sound Misc.\nAC1, AC2,\nAC3, AC4,\nIR\nLevel,\nFrequency\nN ratio,\nzerocross .,\nvolume\nTemp,\npressure,\nCO\nTEA sensor boardAcceleration board\nUser interaction\nSO\nM\ns\nFigure 3: Overall architecture. User interaction is only\nnecessary after the clustering.\nThe result is that after some time this model generates a\ngraph depicting the behavior of a user with relation to the\ncontexts visited. When the user tends to go from A to B\nrather than to C, then this will be reflected in the graph\u2019s\nconnection strengths. Figure 3 depicts the typical layout\nof the final architecture.\n3 RESULTS AND FUTURE WORK\nFor context awareness to be effectively user-friendly, it\nis necessary that the system gets feedback from the user\nwhenever the user would like to give it. These\nconstraints are both hard and challenging from a\nmachine-learning point of view. The combination of\nunsupervised neural networks and a context model\ngives promising results, without creating a bulky\noverhead on the user-computer interaction. Simple\nactivities like sitting, walking and running are usually\nrecognized within tens of seconds if the accelerometers\nare placed on the user\u2019s leg or hip. For locations, the\nlight sensor has proven to be very efficient, especially\nwhen cues like light-frequency are used. However, as a\nconsequence, this also means that recognition\ndeteriorates as lighting conditions change. Combination\nof light sensors, GPS and\/or beacons would be very\ninteresting in that regard.\nIn the future, we would like to boost the performance\nby improving both sensors and cues in both quality and\nquantity. The experiments up until now used about 10\nsensors, but we expect to increase this number\nsignificantly. Other important issues we are researching\nare placement of sensors (on both devices and\nclothing), the grouping of sensors for the clustering, and\nredundancy of sensors to make the system truly robust.\nFinally, the Kohonen map also offers an intuitive\nrepresentation to the user of how contexts are stored\nand learned, which is not obvious in machine learning,\nespecially in neural networks. \nAcknowledgements\nThe framework of this paper was given by the TEA\nproject [2], which is sponsored by the European\nCommissions \u2018Fourth Framework\u2019. Thanks go out to\nthe people from all project-partners: Starlab Research\n(Belgium), Nokia Mobile Phones (Finland), TecO\n(Germany) and Omega Generation (Italy).\nReferences\n[1] Chen, D., Schmidt, A. & Gellersen, H.-W. (1999) An\nArchitecture for Multi-Sensor Fusion in Mobile\nEnvironments, In Proceedings International\nConference on Information Fusion, Sunnyvale, CA,\nUSA.\n[2] Esprit Project 26900. Technology for Enabling\nAwareness (TEA). http:\/\/tea.starlab.net, 1999.\n[3] Kohonen, T. (1997) Self-Organizing Maps, Springer\nSpringer-Verlag Heidelberg,\n[4] Mitchell, T.M.(1997) Machine Learning , McGraw-\nHill.\n[5] Schmidt, A., Aidoo, K.A., Takaluoma, A.,\nTuomela, U., Van Laerhoven, K. & Van de Velde,\nW. (1999) Advanced Interaction in Context, in H.\nGellersen (Ed.) Handheld and Ubiquitous\nComputing, Lecture Notes in Computer Science\nNo. 1707, Springer-Verlag Heidelberg, p. 89-101.\n"}