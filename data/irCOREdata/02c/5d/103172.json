{"doi":"10.1007\/3-540-44938-8_24","coreId":"103172","oai":"oai:epubs.surrey.ac.uk:3030","identifiers":["oai:epubs.surrey.ac.uk:3030","10.1007\/3-540-44938-8_24"],"title":"Combining multiple modes of information using unsupervised neural classifiers","authors":["Ahmad, K","Casey, M","Vrusias, B","Saragiotis, P","Windeatt, T","Roli, F"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2003-01-01","abstract":null,"downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:3030<\/identifier><datestamp>\n      2017-10-31T14:07:35Z<\/datestamp><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:436F6D707574696E67<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/3030\/<\/dc:relation><dc:title>\n        Combining multiple modes of information using unsupervised neural classifiers<\/dc:title><dc:creator>\n        Ahmad, K<\/dc:creator><dc:creator>\n        Casey, M<\/dc:creator><dc:creator>\n        Vrusias, B<\/dc:creator><dc:creator>\n        Saragiotis, P<\/dc:creator><dc:creator>\n        Windeatt, T<\/dc:creator><dc:creator>\n        Roli, F<\/dc:creator><dc:date>\n        2003-01-01<\/dc:date><dc:type>\n        Conference or Workshop Item<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        attached<\/dc:rights><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/3030\/2\/2003_ahmad_casey_vrusias_saragiotis_combining_multiple_modes_of_information.pdf<\/dc:identifier><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/3030\/4\/licence.txt<\/dc:identifier><dc:identifier>\n          Ahmad, K, Casey, M, Vrusias, B, Saragiotis, P, Windeatt, T and Roli, F  (2003) Combining multiple modes of information using unsupervised neural classifiers  In: 4th International Workshop on Multiple Clasifier Systems (MCS 2003), 2003-06-11 - 2003-06-13, UNIV SURREY, GUILDFORD, ENGLAND.     <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1007\/3-540-44938-8_24<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/3030\/","http:\/\/dx.doi.org\/10.1007\/3-540-44938-8_24"],"year":2003,"topics":[],"subject":["Conference or Workshop Item","PeerReviewed"],"fullText":"Combining Multiple Modes of Information using \nUnsupervised Neural Classifiers  \nKhurshid Ahmad1, Matthew Casey1, Bogdan Vrusias1 and Panagiotis Saragiotis1 \n \n1\n Neural Computing Group, Department of Computing, School of Electronics and Physical Sci-\nences,  \nUniversity of Surrey, Guildford, Surrey, GU2 7XH, UK \n{K.Ahmad, M.Casey, B.Vrusias, P.Saragiotis}@surrey.ac.uk \nhttp:\/\/www.computing.surrey.ac.uk\/ \n \nAbstract. A modular neural network-based system is presented where the compo-\nnent networks learn together to classify a set of complex input patterns.  Each pat-\ntern comprises two vectors: a primary vector and a collateral vector.  Examples of \nsuch patterns include annotated images and magnitudes with articulated numerical \nlabels.  Our modular system is trained using an unsupervised learning algorithm.  \nOne component learns to classify the patterns using the primary vectors and an-\nother classifies the same patterns using the collateral vectors.  The third combiner \nnetwork correlates the primary with the collateral.  The primary and collateral \nvectors are mapped on a Kohonen self-organising feature map (SOM), with the \ncombiner based on a variant of Hebbian networks.  The classification results ap-\npear encouraging in our attempts to classify a set of scene-of-crime images and in \nour attempts to investigate how pre-school infants relate magnitude to articulated \nnumerical quantities.  Certain features of SOM\u2019s, namely the topological \nneighbourhoods of specific nodes, allow for one to many mappings between the \nprimary and collateral maps, hence establishing a broader association between the \ntwo vectors when compared with the association due to synchrony in a conven-\ntional Hebbian association. \n1 Introduction \nThere are a number of problems currently advocated in the image understanding \/ \nretrieval literature and in the developmental psychology literature, which will benefit \nfrom the use of a system comprising a set of classifiers, capable of learning key fea-\ntures in the input, and using a decision combination function that can learn to com-\nbine the output of the different classifiers. \n1.1 Image Recognition and Multiple Classifiers \nImage understanding and retrieval systems, especially those discussed under the rubric \nof content-based image retrieval (CBIR) systems, focus on how a system can identify \nthe components of an image by dealing, almost exclusively, on physical and\/or per-\nceptual features of the image. Typical multiple classifier ensembles used so far com-\nprise individual \u2018expert\u2019 classifiers that can deal with one physical feature at a time: \ncolour, texture, shape or illumination. The CBIR literature is increasingly using image \nexternal features, for instance, texts collateral to the image \u2013 a caption, or a news story \nin which the image is embedded.  Schettini et al [15] have used the CART methodol-\nogy to construct tree classifiers, which in turn use a majority voting rule, for classify-\ning a set of indoor, outdoor and close up images. \nIn an oft-cited paper, Kittler et al have presented \u2018a common theoretical framework \nfor combining classifiers which use distinct pattern representation\u2019 ([10] p.226). The \nframework uses the Bayesian relationship between the posterior and prior probability \ndensity functions that model one of the many possible classes, to the measurement \nvectors used by a given classifier. What is of interest to us here is the use of multiple \nclassifiers that deal with perceptually (quasi-) independent biometric sensing modali-\nties, such as frontal facial features, face profile features and (characteristic) voice fea-\ntures, and a combination function that is expected to lead to the establishment of per-\nsonal identity. Each classifier matches an input with a stored template and an identity \nis produced.  In a handwriting recognition experiment, Kittler et al use four different \nclassifiers operating on the same input, with the sum rule again achieving one of the \nbest combining schemes. Here, the authors have used a large feed forward neural \nnetwork as one of the classifiers. \nJing and Zhang [8] have used genetic algorithms for evaluating the \u2018correctness of \nthe result from [\u2026] combined classifiers\u2019 (p. 486) in dealing with recognition of faces. \nThe authors use four classifiers dealing with sub-images based on low frequency fil-\ntered images and three orientation sub-images: horizontal, vertical and diagonal. A \ngenetic algorithm is used for determining the weights for combining the output from \nthe classifiers. The authors show that individual classifiers, except for low frequency \nfiltered sub-image classifiers, perform poorly when compared to a majority voting \nmethod combined classifier \u2013 and the results of a generic-algorithm (GA) weighted \nclassifier shows a 96% recognition rate. \nThe use of neural networks in the classification of complex images, for instance, \nhuman faces, is limited to less than 20 classes.  There are exceptions to such a limited \napproach: Lawrence et al [12] have developed a multiple neural network system com-\nprising a Kohonen self-organising feature map (SOM) [11], for quantizing a set of \ninput image samples into a topological space, and a backpropagation network, also \ncalled a convolution network, that learns to incorporate constraints that allow it to deal \nwith an image in an invariant manner.  Such multiple neural network systems have \nbeen termed multi-net systems by Sharkey [16], but there are some nuances added to \nconcept of multi-nets [17]. \nA backpropagation network is in itself multi-layered (planes in parallel) and capa-\nble of detecting multiple features in an image.  Lawrence et al have compared the \nperformance of their multi-net system with another combined classifier comprising a \nprincipal component analysis (PCA) system and multi-layer perceptron (MLP).  The \nconvolutional approach outperforms the MLP in the classification of 400 images of 40 \nindividuals.  The performance of the MLP, containing up to 200 hidden nodes, is not \nsurprising.  The interesting thing for us is that this multi-net system, convolutional \nplus Kohonen, with substantial ab initio unsupervised learning capability performs \nwell in the difficult task of face recognition. \n1.2 Combining Unsupervised Classifiers \nIn a recent paper on the limitations on research in multiple classifier systems, Sharkey \n[17] suggests that this may be due to a lack of \u2018awareness of the range of modular \ndesigns that could be employed\u2019 (p. 108).  Sharkey advocates a modular approach and \nillustrates the advantages with a system of classifiers that only provides a partial solu-\ntion to the classification task and the combination of the partials providing the full \nsolution.  She uses a number of neural networks, multi-layer perceptrons (MLP), or-\nganised in different topologies as unitary classifiers of fault patterns in diesel engines, \nand compares these with modular and ensemble approaches.  An approximately 2% \nimprovement in performance is achieved with both modular and ensemble systems \nover the unity solutions, not quite advocating the use of modular and ensemble sys-\ntems.  However, intuitively we believe that modular networks should outperform \nensemble networks. \nIn this paper, we describe a modular co-operative network, where the component \nnetworks learn through an unsupervised process, in contrast to other supervised ap-\nproaches, such as Sharkey\u2019s.  The unsupervised learning regimen is used to train net-\nworks that classify the input patterns and a combiner network that produces the out-\nput. \nWillshaw & von der Malsburg [20] developed a model that applied Hebbian learn-\ning [6] between a two-dimensional pre-synaptic (input) layer and a post-synaptic (out-\nput) layer of neurons to form a topological map of activations; Hebbian learning is \nbased on the postulate that to affect learning in connections (synapses) between two \nneuronal cells, the strengths of the connections are increased when both sides of the \nconnection are active. This associative learning pre-empts the use of a teacher. The \nproduction of a topological map as a consequence of a learning rule can be regarded as \na rule that maximises the average mutual information between input and output sig-\nnals. Kohonen\u2019s [11] SOM is based upon a similar principle to Willshaw & von der \nMalsburg\u2019s, producing a statistical approximation of the input space by mapping the \ninput to a two-dimensional output layer. The approximation is achieved by selection \nof features (or prototypes) that characterise the data, which are output in a topologi-\ncally ordered map. \nIn this paper we discuss how a multi-net system can learn to classify a set of com-\nplex input patterns; the complexity lies in the fact that the input comprises a number \nof independent components. Each component has different information about a given \ninput pattern. Consider, for example the case of an annotated image \u2013 an image plus \nits collateral description. The image features may include a texture vector together \nwith a textual description. The texture vector informs a system about the idiosyncratic \nvisual features of an image independently of the description and in a different manner. \nCollateral information helps to sharpen the query to an image database [19]. \nOur combination of neural networks is so organised that one network each can \nlearn to classify an input pattern based on the sole knowledge of one component only, \nin contrast to other such schemes such as the so-called mixture-of-experts (ME) net-\nwork [7] in which a number of networks are trained on the same set of input patterns. \nThe behaviour of the individual networks, taken two at a time, is learnt by another \nnetwork which, in turn, learns to associate classes in the individual network that are \nactive at a given time.  This synchronous activity is used to interrelate two independ-\nent components and is learnt through association whilst the individual classifiers \nlearn. \nThe co-operative multi-net architecture that we propose extends both Willshaw & \nvon der Malsburg [20] paradigm and Kohonen\u2019s [11] formulation, this time by con-\nnecting two SOMs together with a Hebbian network.  A multi-net system that uses a \nHebbian network to interrelate the output of two (or more) SOMs, can exploit some or \nall of the properties of Hebbian synapses. The properties of time dependence, local \nmodification, interaction between pre- and post-synaptic signals and their correlation \nprovides justification for the combination of independent modalities of information \n(images and collateral text or numbers and articulation). When combining two SOMs, \nthe clustering of activation in either map relates an input signal to similar prototype \ninputs. This local information on either side of the Hebbian connection allows local \nmodification of signals to be achieved through corresponding interaction at pre-set \ntime steps. Furthermore, since inputs to both SOMs are related during training, the \nHebbian connection modification provides correlation between the two modalities, \nwhich can be exploited during recall. These properties enable information modalities \nto be combined effectively to improve classification of signals through multiple, dis-\ntinct classifiers, allowing the translation of classification from one modality to an-\nother. \n2 Self-organised Combinations \nA SOM uses an algorithm that transforms a continuous n-dimensional input signal \nonto a one- or two-dimensional discrete space of neurons.  For each input pattern, the \nweight vectors in the SOM that connect the input layer to the output layer, attempt to \nrelate to the input by computing respective values of a discriminant function.  The \nweight vector closest to the input is deemed the winner and represents the non-linear \ntransformation of the input onto the output.  The process of competition complete, the \nco-operative phase takes over with the winning neuron determining the spatial loca-\ntion of a set of (less) excited neurons accompanying the highly excited winner.  The \nwinner\u2019s \u2018halo\u2019, visually created by a Gaussian neighbourhood function, displays co-\noperation.  The topological neighbourhood is of considerable import to our method. \nThe input vectors for the cases we discuss comprise two vectors \u2013 a primary vector \nand a collateral vector; for example, an annotated image\u2019s primary vector may com-\nprise the visual features of colour, texture and shape, and the collateral vector holds \nindications of the presence or absence of keywords in the image\u2019s collateral text.  We \ntrain two SOMs: one for classifying input patterns based entirely on the basis of the \nprimary vectors, one on the collateral vectors (Fig 1).  Here, the primary vector is \npresented to the primary SOM, which is then trained on the input using Kohonen\u2019s \nweight update rule [11], using a Gaussian neighbourhood function, an exponentially \ndecreasing learning rate and neighbourhood radius.  Similarly, the collateral vector is \npresented to the collateral SOM, which in turn is trained on the input. \n Primary \nSOFM \nBi-directional \nHebbian Network \n. \n. \n. \n. \n. \n. \nPrimary \nVector \nCollateral \nVector \nCollateral \nSOFM \n \nFig 1: Architecture showing the connections between the primary and collateral SOMs.  Note that \ntraining is performed in-situ with association between neighbourhoods. \nSubsequent to this, the output of both the primary and collateral SOMs are input to \nthe Hebbian network.  In order to preserve the topological information provided by \nboth SOMs, the output from each is represented as a range of activations centred on \nthe winning neuron, demonstrating the active neighbourhood of the winner and allow-\ning a cluster of activity to be presented to the Hebbian network.  This is achieved by \nusing the Euclidean distance for each unit in the SOM for an input, and inverting this \nvia the function xexf \u2212=)( , which provides a sufficient radius around the winner.  \nWith both primary and collateral SOM output represented in this way, the Hebbian \nconnections were allowed to train on both of these patterns, with the cycle continuing \nfor subsequent input patterns until sufficient learning has occurred in the SOMs to \nform clusters. \nEach SOM shows the effects of the competition and co-operation processes used in \ntraining by assigning each pattern\u2019s neuron in the output layer.  Each winning neuron \nhas a topological neighbourhood of active neurons.  The neighbours may or may not \nbe associated with other patterns.  Therefore, if during the testing phase, the presenta-\ntion of a test pattern leads to its recognition by the excitation of a neuron that has won \nsimilar or the same pattern during training, the neighbourhood will also be activated. \nRecall that we link the primary and collateral vectors through Hebbian learning.  \nDuring the training phase, the presentation of the two vectors comprising a pattern \ntrivially leads to a weighted one-to-one association simply because both patterns are \npresent at the same time.  However, in addition to this association by synchrony, the \nwinning unit of a primary vector gets associated not only with the winning unit of the \ncollateral (and vice versa), but also the neighbourhood of the winner gets associated \nwith that of the neighbourhood of the winning unit of the collateral (and vice versa). \n2.1 Classifying Images and Collateral Text \nWe first look at the combination of image and text classifiers, the image forming the \nprimary vector, and the text associated with the image the collateral vector, taken from \nthe scene-of-crime domain. An image is represented by a 112-dimensional vector that \nconsists of extracted physical features.  The intention when extracting these 112 com-\nponents was to create vectors that described various properties such as colours, edges \nand textures. In these vectors we used 21 dimensions to describe the colour distribu-\ntion as an intensity histogram, 19 dimensions to describe edges by applying an edge \nfilter and the water-filling algorithm [21], and 72 dimensions to describe texture using \nGrey Level Co-occurrence Matrices [5].  Here, the primary 15 by 15 unit SOM is \nintended to organise the images into clusters that share similar image features. \nCollateral text is represented by a 50-dimensional (term based) binary vector. \nThese are generated by extracting significant terms from the text, ignoring punctua-\ntion, numerical expressions and closed-class words.  These significant terms are gen-\nerated using the frequency of a term and a weirdness coefficient describing the sub-\nject-specificity of a term [2]. Textual vectors with common key-terms are clustered \ntogether in a 15 by 15 unit SOM. \nTo evaluate the proposed architecture, we trained three separate systems to allow \nfor a comparison of results: the combined architecture, two separate SOMs, one each \nfor images and texts, and a single SOM trained on combined image and text vectors.  \nA total of 66 images and associated texts where used, pre-classified into 8 ideal \nclasses by experts on the scene-of-crime domain: \u2018bar area\u2019, \u2018exhibits\u2019, \u2018fingerprints\u2019, \n\u2018footmarks\u2019, \u2018fruit machine area\u2019, \u2018body\u2019, \u2018window area\u2019 and \u2018general\u2019. One vector was \nthen selected at random from each class for use in testing and the remaining 58 were \nused for training. \nEach system was trained for 1000 epochs with training vectors, with initial random \nweights, Gaussian neighbourhood function with initial radius 8, reducing to 1, and \nexponential learning rate starting at 0.9 reducing to 0.1. The Hebbian weight connec-\ntions were normalised and a learning rate of 10 was used.  For the combined system, \nwe first tested the performance of the Hebbian network on the training data by trans-\nlating one SOM\u2019s output to the other SOM (image to text, text to image). We calcu-\nlated the Euclidean distance of the actual SOM\u2019s winning node for a particular input, \nto the node that the Hebbian link activated. The results showed that the Hebbian net-\nwork managed to identify all images correctly for a given textual input, and only \nmissed 1 out of 58 texts for a given image input. \nThe system was then tested for its accuracy on classifying each of the 8 test inputs.  \nHere, of the 8 test vectors, the image SOM correctly classified 4.  For the remaining \nmisclassified vectors, the collateral text vectors were input to the text SOM, and sub-\nsequently provided activation through the Hebbian link to obtain an image classifica-\ntion.  This technique correctly classified 3 of the remaining 4 vectors.  A similar ap-\nproach was applied to the text SOM, giving 5 initially correct classifications, and 2 \nmore via the image SOM and Hebbian linkage. \nIn comparison, the independently trained SOMs were tested with the same test vec-\ntors. The image SOM showed correct classification of 4 out of 8 test vectors, the text \nSOM 5 out of 8.  The combined system therefore shows the benefit of combining \nmodalities of information to improve classification, allowing an improved response by \nselecting the best possible via the Hebbian connection.  However, the multi-net ap-\nproach shows benefit over a monolithic solution, as demonstrated by the single SOM \nthat was trained with the input vectors formed by concatenating the image and text \nvectors together.  Test results show worse classification ability, with only 3 out of 8 \ncorrectly classified, demonstrating that combined modalities can only be used if ap-\npropriate selection of response and separation of signals is possible.  Whilst building a \nmonolithic network to process combined modality vectors may not seem intuitive, this \nis one approach to using multiple modalities of information for classification.  Our \nmulti-net approach is an alternative that seems to offer benefit. \n2.2 Classifying Number Magnitude and Articulation \nThe combination of modalities to improve classification is a subject that has relevance \nto developmental psychology. For example, the study of mathematical abilities has \nconcentrated on the way different numerical modalities are processed [4], [13], [14]. \nDehaene\u2019s triple code model of numerical abilities includes processing of Arabic and \nspoken numbers, together with an internalised magnitude representation of number \n[4]. Such an internal representation of number can be obtained through a process of \nsubitization or estimation [9]. Of specific interest here is the way in which the internal \nmagnitude representation of number interacts with number articulation.  We simulate \nthe linkage of the internal representation of number and its phonetic representation \n[1], [3].  \nWe use number magnitude as the primary vector, using a 1-dimensional SOM with \n66 neurons as a number line.  Input to the SOM is formed as a 66-dimensional binary \nvector where each magnitude is identified with a block of three digits.  The collateral \nvector is formed as a number articulation, with a SOM consisting of 16 by 16 neurons, \nshowing the representation between the spoken form of the numbers one to twenty-\ntwo.  For example, the numbers seven, seventeen and eleven all have similar sounds, \nand are hence clustered together within the SOM.  Input to the SOM is formed as a \n16-dimensional vector representing the phonemes used in the number word, with \nvalue taken as the order found within the Collins English Dictionary and Thesaurus \n[18], with zero representing the absence of a phoneme for shorter number words. \nThe training and testing data sets where selected randomly from the numbers 1 to \n22, with 16 numbers in the training set and 6 in the test set (2,3,10,14,15,19).  The \nentire system was trained for 1000 epochs on the training data, with initial random \nweights, Gaussian neighbourhood function with initial radius for the primary map as \n33, and the collateral 8, reducing to 1, and exponential learning rate starting at 0.5.  \nThe Hebbian weight connections were normalised and a learning rate of 0.5 was used. \nOnce trained, the system was tested on both the magnitude and phonetic forms of \nall 22 numbers.  Looking at the magnitude SOM in isolation, the system correctly \nordered all 22 magnitudes such that 1 was at one end of the map and 22 at the other, \nwith all intermediate values in ascending order, including those that were not in the \ntraining set.  This demonstrates the SOMs ability to generalise to new patterns based \nupon their apparent magnitude.  However, the testing of the phonetic SOM shows the \ninability to correctly classify 2, 3, 10, 14, 15 and 19 (those not in the training set).  \nThe SOM is able to associate these values with those based upon their phonetic simi-\nlarity, for example 2 is classified as 1, however the SOM cannot distinguish between \nvalues, attempting to provide an existing specific label rather than a distinct value. \nTo determine if the combined system can improve upon this classification, the six \nmisclassified values were presented to the magnitude SOM, and its output used to \nactivate the Hebbian link to produce a corresponding articulation output.  The corre-\nsponding Hebbian output provides new associations for the six values.  Here 14, 15 \nand 19 are now distinct whilst being associated with the \u2018teens\u2019 cluster, improving \nclassification by distinguishing these values whilst retaining their similarity to their \nphonetic counterparts.  However, 2, 3 and 10 are associated with 20, 12 and 21, re-\nspectively, a worse classification than previously achieved.  The improvement in clas-\nsification is marginal, having both beneficial and detrimental affects.  However, the \ncombined system has the advantage that we can improve classification with another \nmodality of information. \nTo understand if this can be achieved in a monolithic solution rather than the \nmulti-net we describe, we trained a single SOM on a combined set of input modali-\nties.  Here, both the magnitude and phonetic representations for the training and test-\ning sets described above where combined into appropriate 82-dimensional vectors.  A \nsingle SOM consisting of 16 by 16 units was trained using the same parameters as \nabove for 1000 epochs, with an initial neighbourhood radius of 8 units.  The results \nshow a map that demonstrates the relationship between the phonetic representations \nalone, with no magnitude information portrayed.  The responses for the six test values \ngives a similar level of classification as that for the single phonetic SOM described \nabove, with all six values being associated with those that are phonetically similar.  \nHowever, the loss of magnitude information means that there is no understanding of \nthe relationship between the numbers, nor how to translate between a magnitude and \nan articulation, or vice-versa.  This loss of modality information supports the use of \nsplit modalities in a combined system, such as we have described. \n3 Conclusion \nWe have presented a modular co-operative neural network system that incorporates \nunsupervised networks, of the same architecture and topology, that learn to classify a \nset of patterns based on partial information, either primary information or collateral \ninformation, about the patterns.  The combiner, a Hebbian network, learns to associate \nnot only the two winning units on the primary and collateral maps, but between the \nrespective neighbourhoods of the two winners. \nNote that the number of units in our networks is smaller than, say, in Sharkey\u2019s su-\npervised multi-net simulations.  We have been parsimonious perhaps compensating \nfor manually assigning the classifiers to the primary or the collateral sub-vectors.  \nHowever, this manual assignment is offset partially by the fact that the two classifying \nnetworks have exactly the same topology and aspects of the training, including regi-\nmen, that is the way the neighbourhood distances and learning rates are changed, for \nthe two networks are exactly the same.  The combiner in our system shares the same \nlearning paradigm, unsupervised learning, and indeed the SOM architecture is a vari-\nant of the Hebbian architecture [20]. \nA comparison between our modular system and that of a unity network demon-\nstrates improvement.  In classifying images and text the unity network does not dem-\nonstrate any linkage between images and text, whereas the modular system allow the \nneighbourhood of images to be associated with neighbourhoods of text.  For magni-\ntudes and articulated numerical labels, a unity network provides a comparable classifi-\ncation to that of just the phonetic representations, demonstrating that the phonetic \ninformation dominates the magnitude information.  \nAcknowledgements \nThe authors acknowledge the support of the EPSRC sponsored Scene of Crime Infor-\nmation System project (Grant No.GR\/M89041) jointly undertaken by the Universities \nof Sheffield and Surrey and supported by five police forces in the UK.  The authors \nwould also like to thank the UK Police Training College at Hendon for supplying the \nscene-of-crime images and Mr C. Handy for transcribing the image collateral text.  \nLastly, the authors would like to thank the anonymous reviewers of this paper for their \nconstructive comments. \nReferences \n[1] Ahmad, K., Casey, M. & Bale, T. (2002).  Connectionist Simulation of Quantification Skills.  \nConnection Science, vol. 14(3), pp. 165-201. \n[2] Ahmad, K., Vrusias, B. & Tariq, M. (2002). Co-operative Neural Networks and 'Integrated' \nClassification. Proceedings of the 2002 International Joint Conference on Neural Networks \n(IJCNN'02), vol.2, pp. 1546-1551. \n[3] Bale, T.A. (1998).  Modular Connectionist Architectures and the Learning of Quantifica-\ntion Skills.  Unpublished doctoral thesis.  Guildford, UK: University of Surrey. \n[4] Dehaene, S. (1992).  Varieties of Numerical Abilities.  In Numerical Cognition (1993), \npp.1-42.  Cambridge, MA.: Blackwell Publishers. \n[5] Haralick, R.M., Shanmugam, K. & Dinstein, I. (1973).  Textural Features for Image Classi-\nfication.  IEEE Transactions on Systems, Man, and Cybernetics, vol. SMC-3(6), pp.610-\n621. \n[6] Hebb, D.O. (1949).  The Organization of Behavior: A Neuropsychological Theory.  New \nYork: John Wiley & Sons. \n[7] Jacobs, R.A., Jordan, M.I., & Barto, A.G. (1991).  Task Decomposition through Competition \nin a Modular Connectionist Architecture: The What and Where Vision Tasks.  Cognitive \nScience, vol. 15, pp. 219-250. \n[8] Jing, X., & Zhang, D. (2003) Face recognition based on linear classifiers combination. Neu-\nrocomputing, vol. 50, pp. 485-488. \n[9] Kaufman, E.L., Lord, M.W., Reese, T.W. & Volkmann, J. (1949).  The Discrimination of \nVisual Number.  American Journal of Psychology, vol. 62, pp. 498-525. \n[10] Kittler, J., Hatef, M. Duin, R.P.W. & Matas, J. (1998). On Combining Classifiers. IEEE \nTransactions on Pattern Analysis and Machine Intelligence, vol. 20(3), pp. 226-239. \n[11] Kohonen, T. (1997).  Self-Organizing Maps, 2nd Ed.  Berlin, Heidelberg, New York: \nSpringer-Verlag. \n[12] Lawrence, S., Giles, C.L., Ah Chung Tsoi & Back, A.D.  (1997).  Face Recognition: A \nConvolutional Neural Network Approach.  IEEE Transactions on Neural Networks, vol.  8 \n(1) pp 98-113. \n[13] McCloskey, M. (1992). Cognitive Mechanisms in Number Processing and Calculation: \nEvidence from Dyscalculia. In Dehaene, S. (Ed), Numerical Cognition (1993), pp. 107-157.  \nCambridge, MA.: Blackwell Publishers. \n[14] McCloskey, M., Caramazza, A. & Basili, A. (1985).  Cognitive Mechanisms in Number \nProcessing and Calculation: Evidence from Dyscalculia.  Brain and Cognition, vol. 4, pp. \n171-196. \n[15] Schettini, R., Brambilla, C. & Cusano, C. (2002). Content-based Classification of Digital \nPhotos. In Roli, F. & Kittler, J. (Eds), Proceedings of the Third International Workshop on \nMultiple Classifier Systems (MCS 2002), LNCS 2364, pp. 272-282. \n[16] Sharkey, A.J.C. (1999).  Multi-net Systems.  In Sharkey, A.J.C. (Ed), Combining Artificial \nNeural Nets: Ensemble and Modular Multi-Net Systems, pp.1-30.  Berlin, Heidelberg, New \nYork: Springer-Verlag. \n[17] Sharkey, A.J.C. (2002).  Types of Multinet System.  In Roli, F. & Kittler, J. (Eds), Proceed-\nings of the Third International Workshop on Multiple Classifier Systems (MCS 2002), \nLNCS 2364, pp. 108-117. \n[18] Sinclair, J. M. (Ed.) (1993) Collins English Dictionary and Thesaurus. Harper Collins. \n[19] Srihari R.K., (1995).  Use of Collateral Text in Understanding Photos, Artificial Intelli-\ngence Review, special issue on Integrating Language and Vision, Volume 8, pp. 409-430. \n[20] Willshaw, D.J. & von der Malsburg, C. (1976).  How Patterned Neural Connections can be \nset up by Self-Organization.  Proceedings of the Royal Society, Series B, vol. 194, pp. 431-\n445. \n[21] Zhou, X.S., Rui Y. & Huang S.T. (1999).  Water-filling: A Novel Way for Image Structural \nFeature Extraction. Proceedings of the IEEE International Conference on Image Process-\ning, pp. 570-574. \n"}