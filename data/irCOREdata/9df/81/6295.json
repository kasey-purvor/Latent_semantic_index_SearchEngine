{"doi":"10.1080\/0968776030110204","coreId":"6295","oai":"oai:generic.eprints.org:404\/core5","identifiers":["oai:generic.eprints.org:404\/core5","10.1080\/0968776030110204"],"title":"Cost\u2010effectiveness analysis of computer\u2010based assessment","authors":["Loewenberger, Pauline","Bull, Joanna"],"enrichments":{"references":[{"id":439511,"title":"A Guide to the Cost-effectiveness of Technology-based Training,","authors":[],"date":"1997","doi":null,"raw":"Hunt, M. and Clarke, A. (1997), A Guide to the Cost-effectiveness of Technology-based Training, Coventry: National Council for Educational Technology.","cites":null},{"id":191583,"title":"CAA Centre Annual Report, CAA Centre, University of Luton. Available from http:\/\/www.caacentre.ac.uk","authors":[],"date":"2000","doi":null,"raw":"Bull, J. (2000), CAA Centre Annual Report, CAA Centre, University of Luton. Available from http:\/\/www.caacentre.ac.uk Bull, J. and McKenna, C. (2000), Computer-assisted Assessment Centre (TLTP3) Update, Proceedings of the Fourth International CAA Conference, Loughborough, July 2000.","cites":null},{"id":191579,"title":"CNL Handbook: Guidelines and Resources for Costing Courses using Activity Based Costing,","authors":[],"date":"2001","doi":null,"raw":"Ash, C., Heginbotham, S. and Bacsich, P. (2001), CNL Handbook: Guidelines and Resources for Costing Courses using Activity Based Costing, Telematics in Education Research Group on behalf of the Virtual Campus Programme and School of Computing and Management Sciences, Sheffield Hallam University, September 2001.","cites":null},{"id":191584,"title":"Cost-benefit analysis of telelearning: developing a methodological framework',","authors":[],"date":"1997","doi":"10.1080\/0158791970180110","raw":"Cukier, J. (1997), 'Cost-benefit analysis of telelearning: developing a methodological framework', Distance Education, 18 (1), 137-52.","cites":null},{"id":191585,"title":"Cost-effectiveness analysis of computer-based assessment","authors":[],"date":"1997","doi":"10.1080\/0968776030110204","raw":"39Pauline Loewenberger and Joanna Bull Cost-effectiveness analysis of computer-based assessment Dearing, R. (1997), Report of the National Committee of Inquiry into Higher Education, London: HMSO.","cites":null},{"id":439512,"title":"Effectiveness and cost-effectiveness of online education: a review of the literature',","authors":[],"date":"2000","doi":null,"raw":"Jung, I. and Rha, I. (2000), 'Effectiveness and cost-effectiveness of online education: a review of the literature', Educational Technology, 40 (4), 57-60.","cites":null},{"id":191580,"title":"Evaluating interactive technologies for learning',","authors":[],"date":"1993","doi":"10.1080\/0022027930250403","raw":"Atkins, M. (1993), 'Evaluating interactive technologies for learning', Journal of Curriculum Studies, 25 (4), 333-42.","cites":null},{"id":1041704,"title":"Evaluating the Costs and Benefits of Changing to CAA,","authors":[],"date":"2000","doi":null,"raw":"Pollock, M. J., Whittington, C. D. and Doughty, G. F. (2000), Evaluating the Costs and Benefits of Changing to CAA, Proceedings of the Fourth International CAA Conference, Loughborough, July 2000.","cites":null},{"id":439510,"title":"Information Technology Assisted Teaching and Learning in Higher Education, Collaborative research by the Consortium of Telematics for Education,","authors":[],"date":"1997","doi":null,"raw":"HEFCE Research Series (1997), Information Technology Assisted Teaching and Learning in Higher Education, Collaborative research by the Consortium of Telematics for Education, University of Exeter with the NatWest Financial Literacy Centre, University of Warwick and the Institute of Learning and Research Technology, July 1997.","cites":null},{"id":1041703,"title":"The CAA Centre National Survey of Computer-assisted Assessment, paper presented at the Association for Learning Technology Conference,","authors":[],"date":"1999","doi":null,"raw":"McKenna, C. and Bull, J. (1999), The CAA Centre National Survey of Computer-assisted Assessment, paper presented at the Association for Learning Technology Conference, Bristol, September 1999.","cites":null},{"id":191582,"title":"The costs of ICT use in higher education: what little we know',","authors":[],"date":"2000","doi":null,"raw":"Bakia, M. (2000), 'The costs of ICT use in higher education: what little we know', TechKnowLogia, http:\/\/www. techknowlogia. org Boucher, A. (1998), 'Information technology-based teaching and learning in higher education: a view of the economic issues', Journal of Information Technology for Teacher Education, 7(1), 87-111.","cites":null},{"id":191581,"title":"The Costs of Networked Learning - Phase Two,","authors":[],"date":"2001","doi":"10.1007\/978-1-4471-0181-9_3","raw":"Bacsich, P., Ash, C. and Heginbotham, S. (2001), The Costs of Networked Learning - Phase Two, Telematics in Education Research Group on behalf of the Virtual Campus Programme and School of Computing and Management Sciences, Sheffield Hallam University, September 2001.","cites":null},{"id":1041705,"title":"The Costs of Networked Learning: What Have We Learnt? h ttp:\/\/www. shu. ac. uk\/flish\/rumblep.htm","authors":[],"date":"2001","doi":null,"raw":"Rumble, G. (accessed 2001), The Costs of Networked Learning: What Have We Learnt? h ttp:\/\/www. shu. ac. uk\/flish\/rumblep.htm Scott, P. (1996), The Economic Costs and Benefits of Information Technology Assisted Teaching and Learning in Higher Education: A Literature Overview, internal report, University of Wales Institute, Cardiff.","cites":null},{"id":1041706,"title":"The economic costs and benefits of information technology assisted teaching and learning in higher education',","authors":[],"date":"1997","doi":null,"raw":"Scott, P. (1997), 'The economic costs and benefits of information technology assisted teaching and learning in higher education', in A. Boucher, N. Davis, P. Dillon, P. Hobbs and P. Teale (1997), Information Technology Assisted Teaching and Learning in Higher Education, Bristol: Higher Education Funding Council for England.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2003","abstract":"The need for more cost\u2010effective and pedagogically acceptable combinations of teaching and learning methods to sustain increasing student numbers means that the use of innovative methods, using technology, is accelerating. There is an expectation that economies of scale might provide greater cost\u2010effectiveness whilst also enhancing student learning. The difficulties and complexities of these expectations are considered in this paper, which explores the challenges faced by those wishing to evaluate the cost\u2010effectiveness of computer\u2010based assessment (CBA). The paper outlines the outcomes of a survey which attempted to gather information about the costs and benefits of CBA","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/6295.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/404\/1\/ALT_J_Vol11_No2_2003_Cost_effectiveness_analysis_of.pdf","pdfHashValue":"32c01d1712086627aa74effecfa9e7a85a3ebe27","publisher":"University of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:404<\/identifier><datestamp>\n      2011-04-04T09:10:11Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/404\/<\/dc:relation><dc:title>\n        Cost\u2010effectiveness analysis of computer\u2010based assessment<\/dc:title><dc:creator>\n        Loewenberger, Pauline<\/dc:creator><dc:creator>\n        Bull, Joanna<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        The need for more cost\u2010effective and pedagogically acceptable combinations of teaching and learning methods to sustain increasing student numbers means that the use of innovative methods, using technology, is accelerating. There is an expectation that economies of scale might provide greater cost\u2010effectiveness whilst also enhancing student learning. The difficulties and complexities of these expectations are considered in this paper, which explores the challenges faced by those wishing to evaluate the cost\u2010effectiveness of computer\u2010based assessment (CBA). The paper outlines the outcomes of a survey which attempted to gather information about the costs and benefits of CBA.<\/dc:description><dc:publisher>\n        University of Wales Press<\/dc:publisher><dc:date>\n        2003<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/404\/1\/ALT_J_Vol11_No2_2003_Cost_effectiveness_analysis_of.pdf<\/dc:identifier><dc:identifier>\n          Loewenberger, Pauline and Bull, Joanna  (2003) Cost\u2010effectiveness analysis of computer\u2010based assessment.  Association for Learning Technology Journal, 11 (2).  pp. 23-45.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776030110204<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/404\/","10.1080\/0968776030110204"],"year":2003,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"Cost-effectiveness analysis of computer-based\nassessment\nPauline Loewenberger* and Joanna Bull**\n*University of Luton, **Director, Eduology\nemail:pauline.loewenberger@luton.ac.uk\nThe need for more cost-effective and pedagogically acceptable combinations of teaching\nand learning methods to sustain increasing student numbers means that the use of\ninnovative methods, using technology, is accelerating. There is an expectation that\neconomies of scale might provide greater cost-effectiveness whilst also enhancing student\nlearning. The difficulties and complexities of these expectations are considered in this\npaper, which explores the challenges faced by those wishing to evaluate the cost-\neffectiveness of computer-based assessment (CBA). The paper outlines the outcomes of\na survey which attempted to gather information about the costs and benefits of CBA.\nIntroduction\nIn response to the opportunities created through widening participation and within an\nenvironment operating under the pressures of limited resources, the higher education\nsector needs more cost-effective and pedagogically acceptable combinations of teaching\nand learning methods to sustain increasing student numbers. With traditional assessment\nand delivery methods, economic frugality forces reductions in the staff-student ratio and\ndeclining unit costs per student, potentially resulting in a reduction in the quality of the\nlearning experience for students. The challenge is to find an alternative cost curve to allow\nfor increasing numbers without a loss of quality and with greater efficiency (Dearing,\n1997). Within this environment the use of innovative methods, utilizing developments in\ntechnology, is accelerating, in the expectation that economies of scale might provide\ngreater cost-effectiveness whilst also enhancing student learning.\nThe uptake of computer-assisted teaching, learning and assessment remains somewhat limited\nand measures designed to establish the true development and adoption costs and related\nbenefits (economic and educational) are equally restricted (Boucher, 1998). To date, little\n23\nPauline Loewenberger and Joanna Bull Cost-effectiveness analysis of computer-based assessment\nconclusive evidence exists for the cost-effectiveness of such approaches due to a lack of\nrigorous studies (Jung and Rha, 2000) and competing methodologies that make comparison\ndifficult (Bakia, 2000). Lack of consensus on costing models, observable variation in cost\nestimates (Scott, 1997), problems in quantifying costs and benefits and variable quality\n(Boucher, 1998) present a major challenge. The sector needs to develop appropriate method-\nologies to identify and quantify the educational, institutional and social benefits claimed.\nChallenges and issues\nAt an institutional level in the UK higher education sector, all investment in technology\nwill be used for many purposes and costs may be met across many sources and absorbed\nunder different budgets. Therefore, apportioning these costs specifically to computer-\nassisted assessment (CAA) is likely to be extremely complex. Another problem in\nevaluating cost-effectiveness is the differing time scales between the initial investment in the\ntechnology and supporting infrastructure, and the realization of benefits. Even if costs are\namortized over the expected life of the technology and duration of course materials\nproduced, insufficient data currently exists on which to base any firm assumptions.\nPolitical sensitivity to the 'cost of costing' (Bacsich, Ash, Boniwell and Kaplan, 1999) in\nthe academic community combined with the destabilizing effect of the exposure of costing\nprogrammes (Oliver, Conole and Bonetti, 2001) may reveal the cost of everything and the\nvalue of nothing and provide inadequate arguments for not costing (Bacsich, Ash and\nHeginbotham, 2001).\nThe immaturity of computer-assisted teaching, learning and assessment methods and the\nproblematic nature of evaluating cost-effectiveness means that there is not an accurate or\nreliable model that might predict the precise costs and benefits to be expected. A major\nstudy on the costs of networked learning (Bacsich et al., 1999) revealed many intangible, or\n'hidden', costs. Recent developments arising from this project (Bacsich et al., 2001) resulted\nin guidelines and resources for costing courses (Ash, Heginbotham and Bacsich, 2001).\nThe model uses an activity-based costing model (equally suitable to costing conventional\nand networked learning) applied at the level of the institution, faculty, course or module.\nHowever, this methodology is complex and extremely resource-intensive in terms of time\nscales, software and professional support costs.\nCost-benefit analyses of computerized teaching and learning have also proved\nproblematic. The procedure is intended as a decision tool prior to undertaking the\ninvestment on the assumption that all costs and benefits are known with certainty, and can\nbe quantified over the known lifetime of a project. Although cost-benefit analysis is based\non sound economic principles, at this stage there is no suitable procedure for evaluating the\ncost-effectiveness of computer-assisted methods. Not only are most benefits difficult to\nquantify and the lifetime of a project sometimes difficult to ascertain, but also the majority\nof evaluations tend to be carried out retrospectively. The existence of intangibles in\neducation requires modification, in the form of cost-effectiveness analysis. Although based\non similar methodology, benefits are not valued in the same terms as costs and need not be\nevaluated in purely financial terms, but include statements of aspiration and social\nconcerns, equality of opportunity, for example.\nBenefits may be pedagogical, social or psychological. Some of those uncovered in the\nliterature suggest that learning technologies may:\n24\nALT-J Volume 11 Number 2\n\u2022 be motivating, time-efficient and effective in learning conceptually difficult topics\n(Atkins, 1993);\n\u2022 reduce failure through self-paced practice (Pollock, Whittington and Doughty, 2000;\nScott, 1996);\n\u2022 enhance the quality of the learning experience;\n\u2022 promote a shift from passive to active learning;\n\u2022 improve transferable skills, such as independent study or IT knowledge;\n\u2022 lead to improved quality of teaching material;\n\u2022 allow increasingly flexible student access to learning materials;\n\u2022 allow learning experiences not otherwise ethically or economically practical (for\nexample, simulations);\n\u2022 represent an important culture shift, rather than a one-off, short-term benefit (Oliver et\naL, 2001);\n\u2022 develop greater institutional visibility or academic kudos (HEFCE, 1997).\nTherefore, evaluation of cost-effectiveness is more important than a purely financial\nevaluation due to the potential benefits to be gained - cost is only one element in the\nequation (Bacsich et aL, 1999; Rumble, 2001). Developmental costs for computer-assisted\nmethods are usually higher than for traditional methods (Hunt and Clarke, 1997; NCIHE,\n1997) but, if innovative methods allow for increased numbers of participants, have a longer\nshelf life and improve educational quality (Cukier, 1997), cost-effectiveness can still be\ndemonstrated.\nComputer-based assessment (CBA)\nThe Computer-assisted Assessment Centre {http:llcaacentre.ac.uk) was funded by the\nTeaching and Learning Technology Programme (Phase 3) to provide advice, guidance and\ngood practice in the implementation and evaluation of CBA in the UK higher education\nsector (Bull and McKenna, 2000). A review of published literature undertaken by the CAA\nCentre, regarding cost-effectiveness of CBA, revealed only one directly relevant article in\nthe field (Pollock et aL, 2000), while the CAA Centre's annual report in 2000 highlighted\nsome of the main issues as hidden costs, existing infrastructure and support costs (Bull,\n2000).\nAt this stage in the development and implementation of computerized assessment, for\nreasons outlined above, a retrospective, cost-effectiveness analysis is appropriate. An\ninstitutional approach to CBA has been in place at the University of Luton for a number\nof years and, in common with the ITATL project (HEFCE, 1997), it is unlikely that\ninstitutions, such as the University of Luton, will be able to provide accurate figures for the\ncost of equipment, maintenance and replacement, or in respect of the supporting infra-\nstructure and access costs.\nBenefits arising from the implementation of CBA may be more important than costs\nsuggested by responses to a national survey (McKenna and Bull, 1999). For example, the\nbenefits to academic staff include automatic marking, statistical analysis of results and\n25\nPauline Lcewenberger and Joanna Bull Cost-effectiveness analysis of computer-based assessment\nspeed of feedback to students. Further, formative or self-assessment can provide benefits\nfor students by allowing them to study at their own pace, as frequently as necessary and\nwith the advantage of instant feedback. At an institutional level this potentially equates to\nimproved access and retention. Other benefits which seem to be emerging include improved\nawareness of underlying pedagogy, increased efficiency through front-loading, spreading\nthe academic workload, testing of the full breadth of course material and contextual\ncongruence with other forms of online learning and student record and management\nsystems. It is also important to consider opportunity costs in relation to the failure to use\nthe resource in its best possible alternative use, particularly staff time. Time spent\ndeveloping and delivering computer-assisted methods could have been spent on teaching,\nresearch, professional development, updating course material, or with students, for'\nexample.\nCertainly, an important driver for the adoption of CBA is the time saved as a result of\nautomatic marking. Although, perhaps, not typical, Pollock et al (2000) suggest the time\nspent preparing and marking examinations has been, reduced to one-sixth of the original\ntime. If a comparison is to be made with more traditional approaches the methodology\nadopted needs to be sufficiently flexible to incorporate all types of assessment, particularly\nas evaluation of the cost-effectiveness of traditional methods at the same level of\ntransparency is rare (Bacsich et al, 1999).\nThe literature recognizes a number of significant challenges in the evaluation of cost-\neffectiveness and benefits of computer-based teaching, learning and assessment. There are\na range of issues which need to be evaluated in order to conduct a full cost-benefit analysis\nand which were beyond the scope and resourcing of this study. However it was seen as\nparticularly valuable to explore specific issues relating to time saved as a result of\nautomatic marking, hidden time costs and the level of staff experience in order to gain an\nunderstanding of the cost-effectiveness of CBA.\nThe CBA cost-effectiveness study\nThis paper reports the results of a study designed to evaluate the cost-effectiveness of\nCBA. The focus of this study was only those costs that are directly attributable to CBA,\nover and above the provision of other technology within the institution, including CBA\nspecific software costs. On the basis that academic staff time is the principal cost of\nteaching in higher education (Dearing, 1997), the major area of the study involved the\ntime, and therefore costs, associated with using CBA in comparison with more traditional\nforms of assessment. It was critical to evaluate cost-effectiveness in terms of staff time\nincluding all stages of the process from planning and implementation through testing,\npost-testing, moderation, analysis and evaluation.\nDuring the development and implementation stages, CBA is resource-intensive and costs\nwill be greater at this stage than for subsequent maintenance. In addition to software costs,\nexamples of hidden costs (Bacsich et al, 1999) specifically relevant to CBA were addressed\nincluding: time for question and test design, including the necessary staff development\n(trainers and trainees) and demand on technical support staff, which can increase overall\ncosts. However, once questions are developed they can be saved for future use as long as the\nmaterial stays relevant to the objectives of the course. For example, quality assurance\n26\nALT-] Volume I \/ Number 2\nguidelines at the University of Luton currently require a change in examination questions\nof at least 10 per cent per year although, in practice, it is often more. In the early stages of\nimplementation this requires the design of new questions while, in time, the development\nof question banks may provide a sufficient number of questions to facilitate the necessary\nvariation in examination content. Together with software costs, it therefore becomes\npossible to amortize these costs over their useful shelf life. Experience in CBA is also a\ncritical factor and it was anticipated that the history of CBA at this institution would allow\ninclusion of the temporal effects of experience. Approximations of economies of scale may\nalso have been possible.\nAims\nDrawing on the findings of the literature review and acknowledging the complexity of the\nissues surrounding the costs and benefits of CBA, the study involved a number of aims.\nThe aims sought to address the key issues in the definable quantitative costs and also more\nqualitative benefits, such as staff experience and improved quality of teaching and learning\nexperiences. The following aims were identified:\n1. evaluation of the cost-effectiveness of implementing and maintaining CBA at a\nmodular level;\n2. evaluation of benefits arising from implementing and maintaining CBA at a modular\nlevel;\n3. estimation of the effects of experience over time in relation to the cost-effectiveness\nCBA;\n4. estimation of cost-per-student for CBA;\n5. comparison of cost-effectiveness for conventional methods of assessment;\n6. evaluation of evidence for improved retention rates where CBA is used.\nMethodology\nOriginal methodology\nThe researchers' original intention was to explore cost-effectiveness within one institution,\nthe University of Luton. Based on expertise developed within the CAA Centre over the\nprevious three years and an extensive literature review, they identified that the most\nappropriate methodology to achieve the objectives of this evaluation would be focus\ngroups and semi-structured interviews involving key members of staff. This would allow\ncollection of historical data on implementation and maintenance of the CBA process\nthrough participation of staff with varying experience of using CBA. The aims were to\nevaluate the effects of module tenure and experience on the time, and therefore costs,\ninvolved as well as the benefits emerging. Data would be obtained for comparable modules\nusing alternative assessment methods. Retention data would be extracted from the student\nrecords system, management information system or from field managers.\nProblems with this original methodology emerged, however, due to resistance of staff\nwithin the two targeted faculties to participate in focus groups or interviews. Provision of\nhistorical retention data also proved problematic within the available time scales. It is likely\nthat the problems encountered were, perhaps, due less to the nature of the study and more\n27\nPauline Loewenberger and Joanna Bull Cost-effectiveness analysis of computer-based assessment\nto unforeseen restructuring within the institution, including major changes to the CBA\nprocess at an institutional level - coincidental timings, which could not have been\nanticipated at the outset.\nRevised methodology\nWithin the remaining faculties it was not possible to collect historical data on\nimplementation and maintenance of CBA or to identify staff with the varying levels of\nCBA expertise necessary to fulfil the original aims of this study. In anticipation that staff\nat the institution would be more amenable to less time-consuming data collection methods,\nthe only feasible alternative was to conduct a survey. Also, the use of a survey would\nprovide cost-effective collection of as much data as possible within limited remaining\nresources. This would allow access to all staff within the institution, both users and non-\nusers of CBA, and, more specifically, to the CBA User Group, a sub-group of the\nUniversity Teaching and Learning Committee. Given the changed nature of the study it\nwas decided also that there would be merit in attempting to collect similar data from other\nacademic institutions. A survey could effectively be administered to a network of CAA\npractitioners through the CAA Centre's national mailing list.\nThe survey was designed as far as practicable to obtain data that would closely match the\noriginal aims of this study. Survey questions requested details of position, faculty,\nacademic experience, module level, student numbers, instruction time, methods of\nassessment and time needed for each stage of the assessment process. Additional questions\nwere included for users of CBA with the aims of evaluating the effects of experience over\ntime. The survey was piloted to all staff within the- CAA Centre and to several users and\nnon-users of CAA within the institution. Following feedback on the survey, it was adapted\nprior to being made available more widely.\nThe survey (see Appendix ) was distributed online to the CBA User Group, comprising\nacademics and key support personnel reporting to the Teaching and Learning Committee,\nall academic staff within the University of Luton, and approximately 700 members of the\nCAA Centre's national mailing list. Both the CBA User Group and members of the CAA\nCentre's national mailing list provided relevant and convenient samples to obtain valuable\ndata in relation to computerized and other methods of assessment. At the institutional\nlevel, CBA has been in use at the University of Luton for a number of years with varying\nlevels of uptake and expertise within the individual faculties. Distribution of the survey to\nall staff at the institution provided the potential to obtain data on both CBA and other\nassessment methods. The survey was made available online for a period of two weeks\nfollowing email messages to the above sample. No personal identifiers were requested due\nto potential sensitivity of the type of data requested and, therefore, all responses remained\nanonymous.\nResults\nDemographic information\nParticipants\nEighty participants responded, thirty of whom comprised current users of CBA, the\nsurvey successfully reaching both users and non-users of CBA. This represents a low\nresponse rate that is likely to be attributable to the limited availability of the survey.\n28\nALT-J Volume 11 Number 2\nRespondents frequently failed to answer all questions, as reflected in the analysis of the\ndata. The breakdown of respondents is somewhat biased towards higher levels of seniority\nand experience, with three-quarters of respondents reporting more than ten years'\nacademic experience. Respondents' positions are shown in Table 1.\nPosition Number\nField manager\nHead of department\nLecturer\nModule leader\nPrincipal lecturer\nSenior lecturer\nOther\n12\n10\n13\n18\n13\nTable I: Academic positions o f survey respondents\nA breakdown of the frequencies of use for all assessment methods reported by respondents\nis given in Table 2. Frequencies reported exceed the total number of respondents as the\nmajority used more than one assessment method for each module.\nAssessment method Number Assessment method Number Assessment method Number\nComputer-based assessment 30\nEssay (course work) 43\nEssay (exam) 28\nShort answer questions 22 Lab Report\nPresentation 19 Worksheets\nObjective test 19 Other\n14\n22\nTable 2: Reported frequencies of assessment methods\n>\nu\n: r\ne\nqu\ner\n30 -\n25 -\n20 -\n15 -\n10 -\n5 J\n0\n26\nIB\n\u2022\n\u2022\nIs\n<50\n21\n\u2022l \u2022\nmm m\nmm mW$WmwB\u00ae\u00ae 1 ^\n<100 <150\nStudent numbers\n\u2022 All BCBA\n12\n\u2022 \u2022 1\n^ ^ H 8\nIs\n>150\nFigure I: Reported numbers of students\n29\nPauline Loewenberger and Joanna Bull Cost-effectiveness analysis of computer-based assessment\nStudent numbers\nDetails of student numbers for each module were requested from all respondents.\nReported frequencies are summarized in Figure 1. For all types of assessment, the majority\nof modules reported comprised student numbers of less than one hundred. It is, however,\ninteresting to note that the relative use of CBA increases proportionately to higher student\nnumbers.\nModule levels\nReported frequencies for the academic levels of the modules are summarized in Table 3.\nDisproportionate CBA usage at lower levels was reported, as would be expected from\nthe (mistaken) perception that CBA is only suited to the assessment of lower learning\nlevels.\nModule level\n0\n1\n2\n3\n4\nHND .\nAll\n4\n29\n15\n10\n18\n2\nCBA\n2\n18\n1\n4\n3\n2\nTable 3. Module levels for all assessment methods\nC B A\nQuestions numbered 8-23 in the survey were only relevant to current users of CBA (n=30),\nin order to obtain more detailed information about this method of assessment. The\nduration of CBA use and the level of experience of academic staff in using CBA clearly\nsuggest that over half the participants have more than five years' experience with this\nmethod of assessment.\nAlmost two-thirds of respondents reported module weighting of 50 per cent or less for\nCBA, leaving just over one-third that reported weighting of between 51 and 100 per cent.\nThe majority of respondents (24) reported using CBA for summative assessment closely\nfollowed by formative assessment (17), with lesser use for self (9) and diagnostic (8)\nassessment.\nPreparation time\nCBA users were asked to respond to questions on the time involved for CBA preparation,\nper module and per annum. Based on usable data provided by twenty-eight of the thirty\nrespondents using CBA, overall frequencies of preparation times reported in respect of\nCBA preparation are summarized in Figure 2. More than one-third of respondents\nreported preparation times of less than ten hours per module and per annum with almost\ntwo-thirds reporting preparation times of less than thirty hours.\nAnalysis of reported preparation times in relation to student numbers (Figure 3)\nnecessarily results in low frequencies for each of the resulting fifteen categories. Reported\npreparation times are clearly not dependent upon student numbers, as would be expected\n30\nVolume 11 Number 2\nFigure 2: Reported preparation times for C8A\nfor one of the main advantages of CBA. Analysis of reported preparation times based on\nlearning levels (Figure 4), although based on very low numbers within each of the twelve\ncategories, does start to form a pattern, For example, at level 0 both reports were for less\nthan ten hours. At postgraduate level there are no reports of less than ten hours. However,\nbetween these extremes, no clear relationships emerge. Similarly, analysis of reported\npreparation times according to the weighting of CBA (Figure 5) suggests no clear\nrelationships. It is likely that more complex relationships exist between these and other\nfactors.\n5 \u2022\n4 -\ns 3 '\na\n03\nu. *- .\n1 -\n0 -\n0-5\nu< 10 Hours\n\u2022 <70 hours\n0 51 - 100 Uk\n101Student Numbers\n150\nn< 30hours\ns\u00ab>100 hours\n\u20221\nLid\n15\n0+\n\u2014i\nFigure 3: CBA: reported times by student numbers\n31\nPauline Loewenberger and Joanna Bull Cost-effectiveness analysis of computer-based assessment\n9 -i\n8 -\n7 -\n5 5-\nK.\n2 -\n\u2022h\nLevel 0 Level 1\nHi ^\nLevel 2\nModule Level\n\u2022 <1Ohours ta< 30 hours\n\u2022 < 70 hours H<>100hour\nW\/A\nLevel 3\ns\nPG\nF\/'gure 4: CBA' reported preparation times by student learning levels\nQ -i\n5 -\na\n2 -\n1 \u2022\n0 \u2022\nV77iP\n1\n0- 25\n\u2022\n\u2022 <10 hours Q<30 hours\nC3<70 hours H<>100 hours\nmm\n26\n\u2022\n50\n\u2022i i\n51 75 u\n76 -100\n\u2014' .\nWeighting %\nF\/gure 5: Frequency of reported preparation time according to the weighting of CBA\nIn terms of CBA expertise, more than half the respondents reported more than five years'\nexperience. Contrary to expectations, no clear relationship emerges between experience and\npreparation times. Also, the length of time that CBA has been used in modules is not\nclearly related to preparation times (Figure 6).\nCBA users were also asked questions on the numbers of examinations used, new\nexaminations prepared, questions per examination, new questions written and question\nbank use, the reported data for which is summarized in Table 4.\nOne aim of the study was to evaluate the effects of experience of using CBA over time and\n32\nALT-] Volume 11 Number 2\n<3 <5\nCBA Tenure (Years)\n>5\nFigure 6: Number of years CBA used on this module\nPreparation\ntime (hours)\n< 10\n<20\n<30\n<50\n<60\n<70\n< 100\n> 100\nCBA module\ntenure (years)\n<3\n>5\n<5\n< 1\n>5\n< 3\n< 3\n< 3\n> 5\n> 5\n< 3\n< 3\n> 5\n< 1\n<5\n< 1\n<5\n< 1\n< 3\n< 3\n< 1\n< 1\n>5\n> 5\n<S\n< 1\n< 3\n< 3\nExperience in\nCBA(years)\n>5\n>5\n>5\n< 1\n<5\n>5\n> 5\n>S\n> 5\n> 5\n<5\n<5\n>5\n< 1\n> 5\n>5\n>5\n< 1\n<5\n<3\n<5\n< 3\n>5\n> 5\n> 5\n<5\n< 3\n>5\nQuestion\nbank\nYes\nNo\nNo\nYes\nYes\nYes\nNo\nYes\nNo\nYes\nNo\nNo\nNo\nYes\nYes\nYes\nNo\nYes\nYes\nYes\nYes\nYes\nNo\nNo\nYes\nYes\nNo\nYes\nExams\nNumber\n1\n3\n2\n> 3\n> 3\n> 3\n1\n2\n3\n> 3\n3\n> 3\n3\n2\n2\n> 3\n1\n2\n> 3\n1\n>3\n3\n>3\n3\n2\n>3\n> 3\nNew\n1-3\n>S\n1-3\n>S\n1-3\n>5\n>5\n>5\n-3\n-3\n-3\n-3\n-3\n-3\n-3\n>5\n4-5\n4-5\n1-3\n>5\nQuestions\nNumber\n0-30\n0-30\n31-50\n0-30\n0-30\n0-30\n0-30\n0-30\n0-30\n51-100\n0-30\n0-30\n0-30\n0-30\n51-100\n0-30\n0-30\n31-50\n0-30\n0-30\n31-50\n0-30\n0-30\n\u202231-50\n31-50\n31-50\n0-30\n0-30\nNew\n5-10\n5-10\n5-10\n>30\n5-10\n>30\n5-10\n11-20\n5-10\n5-10\n5-10\n11-20\n11-20\n5-10\n5-10\n5-10\n5-10\n11-20\n5-10\n11-2-\n11-20\n>30\n21-30\n21-30\n>30\n>30\nTable 4: Preparation times (modular level and per annum), experience, number of exams and questions\n33\nPauline Loewenberger and Joanna Bull Cost-effectiveness analysis of computer-based assessment\nto include other factors, such as weighting, student numbers and learning level of the\nmodule. Questions in the survey to explore this were based on the assumption that the\nfront-end loading of CBA means that it is likely to be more costly than other methods of\nassessment on implementation, but that with practice and experience in question design\nand test construction the process should become more cost-effective over time. It was\nfurther anticipated that the point at which this occurred could have been approximated\nfrom the data provided. Exploration of correlations between any number of these factors\nproved equally insignificant.\n7 -\n6 -\n! * \u2022h-\n2 -\n1 -\n\u2022\nMiMill\n5 10 20 30 40\nMinutes\nII\n50 60\nper student\n1\n70 80 90\n(maximum)\n1,\n100 120 More\nFigure 7: CBA: approximation of time per student\n8\n7-j\nS i\n\u00a7\u2022 4 1\n0)\n\u00a3 3-\n2 -\n1 -\nEssay (coursework)\nI I I 1\n10 20 30 40 50 60 70 80 90 100 120 More\nApproximate time per student (minutes)\nFigure 8: Approximate time per student \u2014 CBA and coursework essays\n34\nALT-] Volume 11 Number 2\nTimes per student\nDrawing on the data obtained it becomes possible to calculate approximations of time\n(and, therefore, cost) per student in respect of CBA preparation. As shown in Figure 7,\nbased on the data provided by half of the respondents who use CBA, the preparation time\ninvolved is less than twenty minutes per student. Data from only five respondents indicates\npreparation time in excess of one hour per student. Reference to the source data reveals\nthat two of these involved modules with low student numbers (0-50), and three of the\nmodules reported on involved between 50 and 100 students. Two of the modules were at\npostgraduate level and two represented weighting of up to 75 per cent. CBA had been used\nfor less than one year in respect of three of the modules. It should be noted that these\ncalculations are based on the minimum number of students and, therefore, represent\nmaximum preparation times based on reported data. Figure 8 shows these approximations\nin comparison to times per student in respect of the more traditional coursework essays. In\nso doing, the argument concerning the very different skills assessed between these\nassessment methods is recognized.\nAssessment method Preparation\nMin. Max.\nMarking\nMin. Max.\nCBA\nEssay (coursework)\nEssay (exam)\nSAQ\nObjective test\nLab. reports\nWorksheets\nPresentations\n5.00\n0.17\n0.35\n1.00\n0.35\n0.17\n1.00\n0.35\n120\n12\n20\n50\n24\n75\n25\n25\n0\n2\n1\n1\n0\n1\n3\n0\n0\n150\n60\n80\n75\n32\n18.75\n120\nTable 5: Preparation and marking times (hours per annum) for assessment methods\nCBA\nProcesses\nEssay Essay\n(course- (exam)\nwork)\nMethods of assessment\nSAQ Objective Lab Work\ntest report sheets\nPresentation Other\nQuestion design\/\ntest construction\nExam preparation\nMarking\nInvigilation\nModeration\nEvaluation\/analysis\nStaff development\nTotal\n20.00*\n0.00\n2.08**\n1.24**\n2.05**\n8.57**\n33.94\n2.00*\n25.00*'\n0.00\n2.22*\n1.12*\n4.00*\n34.34\n2.00*\n19.50*\n3.25*\n2.00*\n1.33*\n4.00*\n32.08\n3.00*\n11.25*\n0.74*\n1.32*\n5.00*\n9.50*\n30.31\n6.00*\n14.25*\n1.40*\n2.84*\n2.00*\n3.50*\n29.99\n4.00*\n10.00*\n6.00*\n2.00*\n3.00*\n8.00*\n33.00\n3.00*\n16.88*\n1.22*\n1.78*\n3.00*\n3.75*\n29.63\n3.00*\n12.00*\n2.63*\n1.70*\n1.67*\n3.75*\n24.75\n5.00*\n8.10*\n2.79*\n3.23*\n1.56*\n1.75*\n22.43\n*Median * * Mean\nTable 6: Average* total times for assessment methods (hours per annum at a modular level)\n35\nPauline Loewenberger and Joanna Bull Cost-effectiveness analysis of computer-based assessment\nTime savings\nRespondents were also asked direct questions (questions 18-23) on time savings and other\nbenefits arising from the use of CBA. Of CBA users, 75 per cent reported noticeable net\ntime savings (n=21), and 40 per cent reported making time savings within the first two\nyears (n=12). A further 40 per cent attributed time savings to both experience and the use\nof a question bank (n=12).\nOther methods of assessment\nQuestion 24 in the survey requested estimated times for examination preparation in\naddition to or, other than, CBA and questions 25-9 requested an estimation of times for\nmarking, invigilation, moderation, evaluation\/analysis of results and staff development for\nall methods of assessment. The purpose of this was to obtain comparable data for all\nstages of the assessment process across all assessment methods. The extent of responses to\nthe majority of these questions is evident from the range of preparation and marking times\nshown in Table 5.\nTable 6 provides a summary of total times reported for all stages of the process in respect\nof all assessment methods reported. This data is based on the mean or median as deemed\nmost appropriate for each set of data arising from responses. Extreme high and low values\nresults in a wide range for the majority of data sets. Therefore, the median, where equal\nnumbers of cases fall above and below the value used, is more representative. In many\ninstances, the median and the mode produced identical values. Almost all cases resulted in\nthe mean producing a higher value than the mode. Therefore, the use of the median\npresents a fair and realistic evaluation.\nDiscussion\nCursory interpretation of the results in Table 6 suggests that, in terms of time and cost,\nCBA is placed towards the higher end of the range of reported assessment methods,\nsecond only to coursework essays and, with the caveat that hardware and software costs\nhave been intentionally excluded on the basis that hardware is widely used for many other\npurposes in higher education, the costs of which, together with software costs, are likely to\nbe distributed across diverse budgets. At the modular level, costing of hardware and\nsoftware becomes even more difficult to quantify with any accuracy. Comparison between\ndifferent assessment methods is interesting and informative, although caution with\ninterpretation is advisable as reduction of data in this way necessarily results in a loss of\nrichness and complexity.\nOf greater direct relevance are the data on preparation times in respect of CBA use (Figure\n2). Over one-third of respondents (n=l 1) reported preparation times of less than ten hours\nper module, per annum. Almost two-thirds of respondents (n=19) reported preparation\ntimes of less than thirty hours per module, per annum. This needs to be considered\ntogether with the absence of marking time for CBA. Preparation and marking times for\nCBA are not dependent on student numbers and, therefore, are suitable for modules that\nattract large student numbers. This is supported by the analysis of reported preparation\ntimes by student numbers (Figure 3) and the relative proportional increased use of CBA\nfor higher student numbers (Figure 1).\nLarge variations were apparent between times reported for conventional assessment\n36\nALT-] Volume 11 Number 2\nmethods (Table 5). Variations in student numbers are reflected in the marking, moderation\nand, possibly, analysis\/evaluation stages of the assessment process but these are not\nrelevant at the preparation, invigilation or staff development stages. The extent of the\nvariations reported questions the validity of the reported data. For example, even for low\nstudent numbers it is unrealistic that essays can be marked in an hour or two. Some\nparticipants reported on the basis of 'time per student' and it is possible that this applies to\nmany of the reports. For these reasons, the data reported in Table 5 has been based upon\nthe median being representative of the data.\nFor conventional methods of assessment the only meaningful unit of analysis is cost-per-\nstudent. CBA is much less subject to variation based on student numbers and the mean\nbecomes far more meaningful but calculation of cost-per-student is more complex. This\nclearly demonstrates the major problem in comparing CBA with conventional assessment\nmethods, whilst at the same time highlighting one of the major advantages of CBA for\nhigh student numbers. Approximations of time-per-student based on reported data (Figure\n7) suggests maximum preparation times for CBA for almost half the respondents of less\nthan twenty minutes per student (n=14); only five respondents fall into categories in excess\nof one hour per student. This partially addresses aims 4 and 5 of this study. Given the\ngenerally low preparation time and the absence of marking time, CBA compares\nfavourably with more traditional assessment methods. -\nDetailed information was requested from CBA users in anticipation that the resulting data\nwould address aims 1 and 3 of this study. It was expected that preparation time for CBA\nwould decrease with experience after allowing for variations in the number of\nexaminations and number of questions per examination and giving consideration to other\nfactors such as the learning level of the module, weighting and question bank use. This\nshould have allowed estimations of preparation times for the assessment processes over\ntime for comparison with conventional methods of assessment. However, analysis of\nparticipants' responses quantifying all stages of the CBA process (Table 2) suggests the\nrelationship between these factors is far from direct. No relationships are suggested based\non experience and no clear relationships are apparent between any of the variables relevant\nto the aims of this study. Any relationship that may exist is likely to involve complex\ninteractions between any number, or all variables. A slight indication of the greater time\nneeded per question at higher learning levels is suggested, although the low numbers on\nwhich this is based results in this being far from conclusive. Inconsistencies in responses are\nevident. For example, some participants report no new questions or examinations per year,\nothers report new questions but not new examinations, whilst still others report a greater\nnumber of new examinations than examinations actually used per year.\nThese indications are far from conclusive and cannot be used as the basis for the level of\ncomplexity necessary to fulfil aims 1 and 3 of this study. The data resulting from this\nsurvey has, therefore, not been successful in achieving the aims of evaluating the effects of\nexperience nor of being able to amortize the costs of implementing CBA over time. This is\npartly due to a bias towards experience of respondents, 75 per cent of whom reported\nacademic experience in excess of ten years, and with 57 per cent of CBA users reporting\nexperience in excess of five years. On reflection of the survey design, this may also have\nbeen due to inadequate wording of some of the survey questions to enable differentiation\nbetween preparation times for examination preparation, and new question design and\n37\nPauline Loewenherger and Joanna Bull Cost-effectiveness analysis of computer-based assessment\nexamination preparation and insufficient differentiation between the implementation and\nmaintenance stages of the CBA process. What this study has been successful in doing is\nhighlighting complexities in evaluating the cost-effectiveness of CBA in comparison to\nconventional methods of assessment, in addition to those that were evident from the start,\nfrom the extensive literature review (see Bakia, 2000; Boucher, 1998; Jung and Rha, 2000;\nScott, 1997) and expertise within the CAA Centre.\nHowever, costs only represent one part of the equation for the cost-effectiveness of CBA,\nthe other side being benefits arising (Aim 2 of the study.). Responses to direct questions on\ntime savings and other benefits arising from the use of CBA (Q18-23) appeared very\npositive. Of the thirty CBA users who completed the survey, twenty-one (70 per cent)\nreported noticeable net time savings, and twelve (40 per cent) of these reported time savings\nhaving become apparent within two years of implementation, the same number attributing\nthese savings to both experience and the use of a question bank. Further potential benefits\nnot specifically addressed in this study are that the front-end loading of CBA spreads the\nworkload of assessment methods, enhancement of the student learning experience,\nimproved access through diagnostic assessment and improved retention rates through\nformative assessment, all of which potentially contribute to cost-effectiveness at an\ninstitutional level.\nRecommendations\nThis study has not been successful in achieving all of its original aims. Importantly, the\nneed emerges, congruent with the original methodology, for stratified sampling that\nprovides a balanced range of experiences (academic, CBA and module tenure) in order to\nestimate the effects of experience over time in the CBA process. A longitudinal approach\nacross a number of case studies may be more successful in achieving these aims, subject to\navailable resources as this methodology would be extremely resource-intensive. Sampling\nfor such studies presents a real problem. For example, whilst this study may be considered\nto have used convenience sampling, this was making use of the many contacts of the CAA\nCentre, all of whom have expertise in the use of CBA. The resistance of academic staff,\nwhich resulted in a forced change to the methodology as well as the limited resources\navailable, largely hindered the study. Whether staff resistance was due to coincidental\ntiming with other major changes within the institution, or reluctance to explore the costing\nissues cannot be known with certainty (see Bacsich et al, 1999; Oliver et ah, 2001).\nOn the basis of the study as it was conducted, the need to differentiate more clearly\nbetween the implementation and maintenance processes for CBA emerges. In particular,\ndistinguishing between examination preparation, new question design and incorporation\nand preparation of new examinations is important. Greater clarity is also needed in respect\nof data requested to evaluate more traditional assessment methods.\nThe relative immaturity of computerized teaching, learning and assessment methods,\ncombined with the institutionalization of traditional methods and resistance to change all\nremain problematic in any attempt to evaluate the cost-effectiveness of CBA at this point\nin time. For these reasons, it becomes extremely difficult to obtain hard data that\nconclusively demonstrate the cost-effectiveness of CBA. It also becomes necessary to\nconsider whether the ultimate goal is to demonstrate that CBA is efficient in terms of\n38\nALT-] Volume 11 Number 2\nutilization of available resources, or whether it is effective in contributing to the goals of\nquality education.\nAbove all, the success of CBA is dependent upon whether it is pedagogically acceptable.\nThis necessitates extensive research that demonstrates, not only that CBA enhances student\nlearning, but, specifically, how it does so. Current developments towards the assessment of\nall learning levels, possibly through a more holistic approach, combined with text analysis,\nfor example, may provide the key to the future of effectiveness and cost-effectiveness of\nCBA. To date, the uptake of CBA appears to have been adopted in a largely piecemeal\nfashion that also suggests a need for the management of change and of the diffusion of\ninnovation process through staff awareness and development.\nAcknowledgement\nThe authors would like to thank and acknowledge the support and advice of Ian Hesketh,\nUniversity of Strathclyde and former Research Fellow of the CAA Centre.\nReferences\nAsh, C., Heginbotham, S. and Bacsich, P. (2001), CNL Handbook: Guidelines and\nResources for Costing Courses using Activity Based Costing, Telematics in Education\nResearch Group on behalf of the Virtual Campus Programme and School of Computing\nand Management Sciences, Sheffield Hallam University, September 2001.\nAtkins, M. (1993), 'Evaluating interactive technologies for learning', Journal of Curriculum\nStudies, 25 (4), 333-42.\nBacsich, P., Ash, C., Boniwell, K. and Kaplan, L. (1999), The Costs of Networked\nLearning, Telematics in Education Research Group on behalf of the Virtual Campus\nProgramme and School of Computing and Management Sciences, Sheffield Hallam\nUniversity, October 1999.\nBacsich, P., Ash, C. and Heginbotham, S. (2001), The Costs of Networked Learning - Phase\nTwo, Telematics in Education Research Group on behalf of the Virtual Campus\nProgramme and School of Computing and Management Sciences, Sheffield Hallam\nUniversity, September 2001.\nBakia, M. (2000), 'The costs of ICT use in higher education: what little we know',\nTechKnowLogia, http:\/\/www. techknowlogia. org\nBoucher, A. (1998), 'Information technology-based teaching and learning in higher\neducation: a view of the economic issues', Journal of Information Technology for Teacher\nEducation, 7(1), 87-111.\nBull, J. (2000), CAA Centre Annual Report, CAA Centre, University of Luton. Available\nfrom http:\/\/www.caacentre.ac.uk\nBull, J. and McKenna, C. (2000), Computer-assisted Assessment Centre (TLTP3) Update,\nProceedings of the Fourth International CAA Conference, Loughborough, July 2000.\nCukier, J. (1997), 'Cost-benefit analysis of telelearning: developing a methodological\nframework', Distance Education, 18 (1), 137-52.\n39\nPauline Loewenberger and Joanna Bull Cost-effectiveness analysis of computer-based assessment\nDearing, R. (1997), Report of the National Committee of Inquiry into Higher Education,\nLondon: HMSO.\nHEFCE Research Series (1997), Information Technology Assisted Teaching and Learning in\nHigher Education, Collaborative research by the Consortium of Telematics for Education,\nUniversity of Exeter with the NatWest Financial Literacy Centre, University of Warwick\nand the Institute of Learning and Research Technology, July 1997.\nHunt, M. and Clarke, A. (1997), A Guide to the Cost-effectiveness of Technology-based\nTraining, Coventry: National Council for Educational Technology.\nJung, I. and Rha, I. (2000), 'Effectiveness and cost-effectiveness of online education: a\nreview of the literature', Educational Technology, 40 (4), 57-60.\nMcKenna, C. and Bull, J. (1999), The CAA Centre National Survey of Computer-assisted\nAssessment, paper presented at the Association for Learning Technology Conference,\nBristol, September 1999.\nNCIHE (1997), Higher Education in the Learning Society, Middlesex: NCIHE\nPublications. http:\/\/www. leeds.ac.uk\/educol\/ncihe.\nOliver, M., Conole, G. and Bonetti, L. (accessed 2001), The Hidden Costs of Change:\nEvaluating the Impact of Moving to On-line Delivery, http:\/\/www.shu.ac.uk\/flish\/oliverp.htm.\nPollock, M. J., Whittington, C. D. and Doughty, G. F. (2000), Evaluating the Costs and\nBenefits of Changing to CAA, Proceedings of the Fourth International CAA Conference,\nLoughborough, July 2000.\nRumble, G. (accessed 2001), The Costs of Networked Learning: What Have We Learnt?\nh ttp:\/\/www. shu. ac. uk\/flish\/rumblep.htm\nScott, P. (1996), The Economic Costs and Benefits of Information Technology Assisted\nTeaching and Learning in Higher Education: A Literature Overview, internal report,\nUniversity of Wales Institute, Cardiff.\nScott, P. (1997), 'The economic costs and benefits of information technology assisted\nteaching and learning in higher education', in A. Boucher, N. Davis, P. Dillon, P. Hobbs\nand P. Teale (1997), Information Technology Assisted Teaching and Learning in Higher\nEducation, Bristol: Higher Education Funding Council for England.\nAppendix: CBA effectiveness questionnaire\nQuestion 1: Which of the following best describes your position?\n\u2022 Head of Department\n\u2022 Module Leader\n\u2022 Principal Lecturer\n\u2022 Senior Lecturer\n\u2022 Lecturer\n\u2022 Teaching Associate\n\u2022 Other\n40\nALT-] Volume 11 Number 2\nQuestion 2. To which faculty do you belong?\n\u2022 Creative Arts and Technologies\nG Health and Social Sciences\n\u2022 Luton Business School\nG Other\nQuestion 3: What is the level of your academic experience?\nQ 0-3 years\n\u2022 4-6 years\n\u2022 7-9 years\n\u2022 10 or more years\nQuestion 4: What is the level of this module?\n\u2022 0\n\u2022 1\n\u2022 2\n\u2022 3\nQ Postgraduate\nQ HND\nQuestion 5: On average, how many students take this module?\n\u2022 Less than 50\n\u2022 51-100\n\u2022 101-150\n\u2022 150 or more\nQuestion 6: What is the weekly commitment for timetabled instruction on this module (including\nlectures, seminars, tutorials, workshops etc?\nQ 1-2 hours\nQ 3-4 hours\nQ 4-5 hours\nQ More\nQuestion 7: What methods of assessment are used in this module (please tick all that apply)?\nQ Computer-based assessment\n\u2022 Essay (coursework)\n\u2022 Essay (exam)\n\u2022 Short Answer Questions\nQ Lab Report\nQ Worksheets\nO Presentation\nQ Other\nPauline Loewenberger and Joanna Bull Cost-effectiveness analysis of computer-based assessment\nQuestions 8-23 refer to CBA users only\nQuestion 8: For how long has CBA been used to assess this module?\n\u2022 0-1 year\n\u2022 2-3 years\n\u2022 4-5 years\n\u2022 Longer\nQuestion 9: For how long have you been using CBA?\n\u2022 0-1 year\n\u2022 2-3 years\n\u2022 4-5 years\n\u2022 Longer\nQuestion 10: What is the weighting (%) of CBA within this module?\n\u2022 0-25\nQ 26-50\nQ 51-75\nQ 76-100\nQuestion 11: For what type of assessment(s) is CBA used in this module?\n\u2022 Summative\n\u2022 Formative\nO Diagnostic\n\u2022 Self-assessment\nQuestion 12: How many tests are used on this module?\n\u2022\n\u2022\nl\n2\n3\nMore\nQuestion 13: How many new CBA examinations are created each year?\n\u2022 0\nQ 1-3\n\u2022 4-5\n\u2022 More\nQuestion 14: How many questions are included per CBA test?\na\n\u2022\n\u2022\n\u2022\n30 or less\n31-50\n51-100\n100 or more\n42\nALT-] Volume I \/ Number 2\nQuestion 15: How many new questions are designed for CBA examinations in this module each\nyear?\n\u2022 5-10\n\u2022 11-20\n\u2022 21-30\n\u2022 More\nQuestion 16: Approximately how long (to the nearest hour) does it take to design the questions and\ncreate the tests for CBA in this module?\na\n\u2022\n\u2022\n\u2022\na\n\u2022\n\u2022\n\u2022\n\u2022\na\n\u2022\nQuestion 17: Dc\na\na\n0-10\n11-20\n21-30\n31-40\n41-50\n51-60\n61-70\n71-80\n81-90\n91-100\nMore\n) you use\nYes\nNo\nQuestion 18: In your experience of CBA do you notice any net time savings overall (i.e. extra time\nfor question design and test construction compared to time saved marking)?\nQ Yes\n\u2022 No\nQuestion 19: If you do notice net time savings, how long after implementation did this become\napparent?\n\u2022 0-2 years\nQ 3-5 years\nQ Longer\nQuestion 20: If you do notice net time savings, what would you attribute this to (please tick all that\napply)?\n\u2022 Use of a question bank\n\u2022 Experience in question design\nQ Both of the above\nQ Neither of the above\n43\nPauline Loewenberger and Joanna Bull Cost-effectiveness analysis of computer-based assessment\nQuestion 21: It has been suggested that the use of CBA for formative assessment enhances student\nlearning through flexible access and immediate feedback. Would you agree with this\nstatement?\n\u2022 Yes\n\u2022 No\n\u2022 Unsure\nQuestion 22: Do you think that the use of CBA, particularly formative, reduces the amount of\nstudent contact time with lecturers outside of timetabled instruction time?\n\u2022 Yes\n\u2022 No\nQuestion 23: Have you noticed any improvement in pass rates which may result from CBA?\n\u2022\na\na\nYes\nNo\nPossibly\nQuestion 24: For each assessment method used (as identified in Question 8, above), other than\nCBA, please provide an estimate of the time needed for examination preparation in\nrespect of each method.\n\u2022 Essay (coursework)\n\u2022 Essay (exam)\n\u2022 Short Answer Questions\n\u2022 Objective test\n\u2022 Lab Report\n\u2022 Worksheets\nQ Presentations\n\u2022 Other\nQuestion 25: For each assessment method identified in Question 8, above, please estimate the total\nmarking time required for each.\nD Essay (coursework)\n\u2022 Essay (exam)\nQ Short Answer Questions\n\u2022 Objective test\n\u2022 Lab Report\n\u2022 Worksheets\n\u2022 Presentations\n\u2022 Other\nQuestion 26: For each of the assessment methods identified in Question 8, above, please estimate\nthe time required for invigilation?\n\u2022 Computer-based assessment\n\u2022 Essay (coursework)\n\u2022 Essay (exam)\n\u2022 Short Answer Questions\n\u2022 Objective test\n\u2022 Lab Report\n\u2022 Worksheets\n44\nALT-J Volume 11 Number 2\nQ Presentations\n\u2022 Other\nQuestion 27: For each of the assessment methods identified in Question 8, above, what is the\napproximate time taken to moderate the results?\n\u2022 Computer-based assessment\n\u2022 Essay (coursework)\n\u2022 Essay (exam)\n\u2022 Short Answer Questions\n\u2022 Objective test\n\u2022 Lab Report\n\u2022 Worksheets\n\u2022 Presentations\n\u2022 Other\nQuestion 28: If you carry out evaluation and\/or analysis of the results, what is the approximate time\ntaken for each of the assessment methods used in this module?\n\u2022 Computer-based assessment\n\u2022\n\u2022\n\u2022\n\u2022\na\n\u2022\na\nEssay (coursework)\nEssay (exam)\nShort Answer Questions\nObjective test\nLab Report\nWorksheets\nPresentations\nOther\nQuestion 29. If you undertake staff development or need specialist support for any of the\nassessment methods used in this module, please specify which, and how much time is\ninvolved for each method.\n\u2022 Computer-based assessment\n\u2022 Essay (coursework)\n\u2022 Essay (exam)\n\u2022 Short Answer Questions\n\u2022 Objective test'\nQ Lab Report\nQ Worksheets\n\u2022 Presentations\n\u2022 Other\nQuestion 30: If you are prepared to participate in a follow-up session, please provide contact details\nbelow. Please also feel free to use this space for any other relevant comments.\nQuestion 31: For follow-up purposes, which of the following would you prefer?\na\n\u2022\nFocus Group\nShort Interview\n45\n"}