{"doi":"10.1177\/1754073908100441","coreId":"66900","oai":"oai:dro.dur.ac.uk.OAI2:5487","identifiers":["oai:dro.dur.ac.uk.OAI2:5487","10.1177\/1754073908100441"],"title":"Neuroscientific evidence for simulation and shared substrates in emotion recognition : beyond faces.","authors":["Heberlein,  A.S.","Atkinson,  A.P."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2009-04-01","abstract":"According to simulation or shared-substrates models of emotion recognition, our ability to recognize the emotions expressed by other individuals relies at least in part on processes that internally simulate the same emotional state in ourselves. The term \u201cemotional expressions\u201d is nearly synonymous, in many people\u2019s minds, with facial expressions of emotion. However, vocal prosody and whole-body cues also convey emotional information. What is the relationship between these various channels of emotional communication? We first briefly review simulation models of emotion recognition, and then discuss neuroscientific evidence related to\\ud\nthese models, including studies using facial expressions, whole-body cues, and vocal prosody. We conclude by discussing these data in the context of simulation and shared-substrates models of emotion recognition","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66900.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/5487\/1\/5487.pdf","pdfHashValue":"8a12bedf5337c7c0d8d583ce8d9e47be184d966e","publisher":"Sage","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:5487<\/identifier><datestamp>\n      2017-03-07T14:23:53Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Neuroscientific evidence for simulation and shared substrates in emotion recognition : beyond faces.<\/dc:title><dc:creator>\n        Heberlein,  A.S.<\/dc:creator><dc:creator>\n        Atkinson,  A.P.<\/dc:creator><dc:description>\n        According to simulation or shared-substrates models of emotion recognition, our ability to recognize the emotions expressed by other individuals relies at least in part on processes that internally simulate the same emotional state in ourselves. The term \u201cemotional expressions\u201d is nearly synonymous, in many people\u2019s minds, with facial expressions of emotion. However, vocal prosody and whole-body cues also convey emotional information. What is the relationship between these various channels of emotional communication? We first briefly review simulation models of emotion recognition, and then discuss neuroscientific evidence related to\\ud\nthese models, including studies using facial expressions, whole-body cues, and vocal prosody. We conclude by discussing these data in the context of simulation and shared-substrates models of emotion recognition.<\/dc:description><dc:subject>\n        Amygdala<\/dc:subject><dc:subject>\n         Empathy<\/dc:subject><dc:subject>\n         Simulation<\/dc:subject><dc:subject>\n         Somatosensory cortex.<\/dc:subject><dc:publisher>\n        Sage<\/dc:publisher><dc:source>\n        Emotion review, 2009, Vol.1(2), pp.162-177 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2009-04-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:5487<\/dc:identifier><dc:identifier>\n        issn:1754-0739<\/dc:identifier><dc:identifier>\n        issn: 1754-0747<\/dc:identifier><dc:identifier>\n        doi:10.1177\/1754073908100441<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/5487\/<\/dc:identifier><dc:identifier>\n        https:\/\/doi.org\/10.1177\/1754073908100441<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/5487\/1\/5487.pdf<\/dc:identifier><dc:rights>\n        The final definitive version of this article has been published in the journal 'Emotion Review' 1\/2 2009. \u00a9 2009 by ISRE and SAGE at the Emotion Review page: http:\/\/emr.sagepub.com\/ on SAGE Journals Online: http:\/\/online.sagepub.com\/\\ud\n<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["1754-0739","issn: 1754-0747","issn:1754-0739"," 1754-0747"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2009,"topics":["Amygdala","Empathy","Simulation","Somatosensory cortex."],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n12 May 2010\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nHeberlein, A.S. and Atkinson, A.P. (2009) \u2019Neuroscientific evidence for simulation and shared substrates in\nemotion recognition : beyond faces.\u2019, Emotion review., 1 (2). pp. 162-177.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1177\/1754073908100441\nPublisher\u2019s copyright statement:\nThe final definitive version of this article has been published in the journal \u2019Emotion Review\u2019 1\/2 2009. 2009 by ISRE\nand SAGE at the Emotion Review page: http:\/\/emr.sagepub.com\/ on SAGE Journals Online:\nhttp:\/\/online.sagepub.com\/\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\nNeuroscience of Emotion Recognition 1 \n \nRunning head: NEUROSCIENCE OF EMOTION RECOGNITION \n \n \n \nNeuroscientific evidence for simulation and shared substrates  \nin emotion recognition: Beyond faces \n \nAndrea S. Heberlein \nHarvard University \n \nAnthony P. Atkinson \nDurham University \n \n \nCorrespondence to: \nAndrea S. Heberlein \nDepartment of Psychology \nHarvard University \nPhone: (617) 495-1933 \nE-mail: heberlein@wjh.harvard.edu \nPre-print of penultimate copy accepted for publication in Emotion Review. Please do not cite \nor quote without first contacting the authors. \n Neuroscience of emotion recognition 2 \nAbstract \nAccording to simulation or shared-substrates models of emotion recognition, our ability to \nrecognize the emotions expressed by other individuals relies at least in part on processes that \ninternally simulate the same emotional state in ourselves. The term \u201cemotional expressions\u201d is \nnearly synonymous, in many people\u2019s minds, with facial expressions of emotion. However, \nvocal prosody and whole-body cues also convey emotional information. What is the relationship \nbetween these various channels of emotional communication? We first briefly review simulation \nmodels of emotion recognition, and then discuss neuroscientific evidence related to these \nmodels, including studies using facial expressions, whole-body cues, and vocal prosody. We \nconclude by discussing these data in the context of simulation and shared-substrates models of \nemotion recognition. \n \nKeywords: empathy, amygdala, somatosensory cortex, simulation \n \n Neuroscience of emotion recognition 3 \nNeuroscientific evidence for simulation and shared substrates  \nin emotion recognition: Beyond faces \nAccording to simulation or shared-substrates models of emotion recognition, our ability to \nrecognize the emotions expressed by other individuals relies at least in part on processes that \ninternally simulate the same emotional state in ourselves. The term \u201cemotional expressions\u201d is \nnearly synonymous, in many people\u2019s minds, with facial expressions of emotion, and the \npreponderance of studies of emotion recognition has focused on faces. However, other nonverbal \ncues such as vocal prosody, body posture, gestures, and locomotor patterns all convey emotional \ninformation\u2014a fact well known to actors, dancers, mimes, and animators. What is the \nrelationship between these various channels of emotional communication? Do the same neural \nstructures play the same roles in recognizing, for example, fear from a facial expression as from \na trembling voice, a cowering posture, or a hesitant, halting gait? Models of emotion recognition \nthat were developed largely based on data from studies of facial expression may also work for \nother channels or cues\u2014but evidence from these other cues might also necessitate refinement of \nface-based models. In the present paper, we will first briefly review simulation models of \nemotion recognition, and then will discuss neuroscientific evidence related to these models. In \neach section, we will first review evidence from studies using facial expressions, and then \nfindings based on whole-body and, to a lesser extent, vocal prosody cues. We conclude by \ndiscussing these data in the context of simulation and shared-substrates models of emotion \nrecognition. \nSimulation or Shared-Substrates Models of Emotion Recognition \nSimulation models of emotion recognition propose that at least part of the mechanism by \nwhich we recognize another individual\u2019s emotional state relies on internally simulating the same \n Neuroscience of emotion recognition 4 \nemotional state in ourselves. As such, these models are consonant with theories of embodied \ncognition, which emphasize the utilization of body representations in a variety of behaviors from \nmemory to attitude formation (Barsalou, Kyle Simmons, Barbey, & Wilson, 2003; Niedenthal, \nBarsalou, Winkielman, Krauth-Gruber, & Ric, 2005; Ping, Dhillon, and Beilock, 2009; \nCrawford, 2009), as well as with perception-action models of movement perception and \nimitation (Prinz, 1997). Simulation models can be contrasted with their logical alternative: one \nmight imagine that emotion recognition is possible by a learned rule-based system whereby \nconfigurations of facial features, vocal prosody patterns, and body gestures, postures, or \nmovements come to be associated with knowledge acquired about the associated experience \nthrough other, perhaps verbal, means. However, as we will review below, sufficient evidence \nexists for simulation processes that it is difficult or impossible to believe they do not play at least \nsome role in emotion recognition.  \nEmotion recognition via simulation might take at least two different forms, which are not \nmutually incompatible: emotional contagion, in which perceiving another\u2019s emotional expression \ngives rise somehow directly to an emotional experience in the perceiver (Preston & de Waal, \n2002); and simulating the viewed emotional state by referencing representations of the associated \nbody state (Adolphs, 2002) or motor programs (Carr, Iacoboni, Dubeau, Mazziotta, & Lenzi, \n2003; Gallese, Keysers, & Rizzolatti, 2004). Both models are presumed to be fairly automatic, \nand both rely at least to some extent on the idea of shared substrates, or overlapping neural \ncircuitry subserving both experience and recognition. Another related term is vicarious \nresponding (Morrison, 2007), which refers to the activation of body-related brain areas by \nanother person\u2019s expressions or imagined situations. As such, they are conceptually related to \n Neuroscience of emotion recognition 5 \ntwo other ideas relying on shared substrates for perception and action: mirror neurons, and pain \nempathy. \nMirror neurons were originally reported based on neurophysiological studies in macaques, in \nwhich single cells in region F5 (homologous to premotor cortex in humans) fired both when the \nmonkey performed a goal directed action (e.g., reaching and grasping) and when it viewed \nanother individual performing the same action (e.g., di Pellegrino, Fadiga, Fogassi, Gallese, & \nRizzolatti, 1992)1. Presumably homologous and analogous regions in humans\u2014in premotor \ncortex near the frontal operculum\u2014 are active in functional neuroimaging studies when humans \nview actions with the intent to imitate (e.g., Decety & Gr\u00e8zes, 1999; Iacoboni et al., 1999). This \npattern of responding, not surprisingly, has been interpreted to underlie empathy (Blakemore & \nDecety, 2001; Decety & Lamm, 2006) and even other social behaviors (Gallese et al., 2004). \nNotably, premotor regions in humans have also been implicated in emotion recognition (e.g., \nCarr et al., 2003; Winkielman, McIntosh & Oberman, 2009). The link between motor behavior \nand perception is also critical in the perception of body movement more generally, a process in \nwhich posterior superior temporal sulcus (pSTS) is known to play a key role (reviewed in Blake \n& Shiffrar, 2007). \nEvidence for self-other overlap for pain empathy initially came from a serendipitous finding: \nHutchison and colleagues (1999) were testing anterior cingulate cortex (ACC) responses to \npainful stimuli such as cold and pinpricks in neurosurgical patients who had consented to \nexperimental studies while awaiting surgery. When an experimenter accidentally pricked herself \nwith a testing probe, the ACC neuron from which Hutchison et al. were recording responded as \nthough the patient had been similarly pricked (Hutchison, Davis, Lozano, Tasker, & Dostrovsky, \n1999). Two contemporaneous studies examined this dual coding of self- and other-pain in \n Neuroscience of emotion recognition 6 \nhealthy subjects using slightly different paradigms: the same region of dorsal ACC is active both \nwhen subjects\u2019 fingers are pricked with a sharp probe and when they see videos of anonymous \nfingers being stabbed with a hypodermic needle (Morrison, Lloyd, di Pellegrino, & Roberts, \n2004), and both dorsal ACC and anterior insula are commonly active when female participants \nreceive painful (vs. mild) electric shocks to the hand and when they believe that their partners, \nseated outside but right next to the scanner, are receiving similar shocks (Singer et al., 2004). All \nof these findings (and several others not included here) imply a degree of overlap between \nrepresenting one\u2019s own pain and representing the pain of a known or imagined other. \nEvidence for some overlap between representations of self and other facial expressions is not \nunique to neuroscientific studies: several studies have demonstrated that people move their facial \nmuscles to mirror observed facial expressions. For example, Dimberg (1982) used facial \nelectromyography to record facial muscle contractions, some too subtle to be detected otherwise, \nwhen subjects viewed emotional facial expressions. Such mimicry has been correlated with \nrecognition (Wallbott, 1991) and with empathy scores (Sonnby-Borgstr\u00f6m, 2002), and its effects \nneed not be conscious: EMG studies revealed subtle contractions even when emotional facial \nexpressions were presented to the subject subliminally (Dimberg, Thunberg, & Elmehed, 2000). \nAmazingly, viewers even spontaneously contract or dilate their pupils to match those of viewed \nsad faces (Harrison, Singer, Rotshtein, Dolan, & Critchley, 2006). Inhibiting mimicry has also \nbeen shown to impair emotion recognition in at least one study: When subjects were asked to \nhold a pencil laterally between their teeth, a manipulation which forces a smile-like contraction \nof the zygomaticus muscles in the cheek (Strack, Martin, & Stepper, 1988), their emotion \nrecognition performance was reduced (Niedenthal, Brauer, Halberstadt, & Innes-Ker, 2001)2. \nThat said, Hess and Blairy (2001) did not find any relation between emotion recognition \n Neuroscience of emotion recognition 7 \nperformance, as measured by an emotion rating task, and subjects\u2019 tendency to mimic dynamic \ndisplays of happiness, sadness, anger, and disgust. Subjects did show evidence of mimicry, but \nmimicry was not correlated with self-reported emotional contagion. In addition, patients with \nfacial paralysis have not shown impaired discrimination or recognition of emotional facial \nexpressions, as measured by matching and naming tasks with standardized facial expressions \n(Calder, Keane, Cole, Campbell, & Young, 2000; Keillor, Barrett, Crucian, Kortenkamp, & \nHeilman, 2002)3. In summary, though people spontaneously mimic observed facial expressions, \nand this mimicry may often play a role in recognition of others\u2019 emotions, the evidence regarding \nthe necessity of mirroring movements for emotion recognition is still mixed.  \nA long, though sparser, history of research has examined how emotional information is \nconveyed by whole-body cues. Darwin famously included postural descriptions in his Expression \nof the Emotions (Darwin, 1872\/1965), and William James examined emotion recognition from \nwhole-body posture photographs in 1932. In that latter series of experiments, James noted that \nsubjects viewing photographs of posed whole bodies frequently were seen to imitate the posture \nthey were looking at, and noted further that this \u201ctendency rarely resulted in an outright \nassumption of the posture, but... was either directly reported or was revealed by reports of \nkinaesthesis in the arms, legs, back, neck or other part of the body significant for the assumption \nof the posture\u201d (James, 1932, p. 419).  James also reported evidence for emotion contagion: \u201cIn \nsome instances the observers also experienced the feeling or emotion which was attributed to the \npostural expression. The feeling ... resulted from the tendency of the observer to put himself into \nthe same situation and posture as that of the figure\u201d (p. 419).  \nJames was skeptical, however, about the possibility of getting high inter-subject agreement \nabout the emotion portrayed by any given posture\u2014or, for that matter, facial expression. In \n Neuroscience of emotion recognition 8 \ncontrast, more recent studies with both static and dynamic whole-body stimuli have indeed found \na high degree of agreement, using a combination of (1) stimuli which were specifically \nconstructed to convey emotional expressions, often using trained actors (as opposed to James\u2019 \nmore open-ended approach in stimulus construction and selection) and (2) more constrained \nmethods than James\u2019 subjects\u2019 free responses (e.g., Atkinson, Dittrich, Gemmell & Young, 2004; \nde Meijer, 1989; Dittrich, Troscianko, Lea, & Morgan, 1996; Wallbott, 1998). Much of this \nresearch has focused on identifying the visual cues used to identify bodily expressed emotions \nand, as we discuss below, the neural mechanisms underpinning bodily emotion recognition. For \nexample, it is clear from studies that have isolated body or body-part motion using techniques \nsuch as the point-light display (see Figure 1) that such motion cues (especially kinematics) \nconvey important information about others\u2019 emotional states.  \n[Figure 1 about here] \nBelow, we review further evidence for simulation models of emotion recognition from \nneuroscientific studies. Such evidence falls into two general categories: support for the role of \nspecific neural structures in both recognition and experience measures (such as for the amygdala \nin both fear experience and fear recognition) and support for the role of known motor or sensory \nstructures in emotion recognition (such as for premotor and somatosensory cortices in \nrecognizing emotional faces). Because many of the same regions are implicated in emotion \nrecognition based on facial, prosodic and\/or whole-body cues, we will organize this discussion \nby neural structure or region, focusing first on faces and then on prosody or whole-body emotion \nrecognition. Along the way we briefly and somewhat selectively review functional imaging and \nneuropsychological studies of face emotion recognition and, more completely (because the \nrelevant literature is smaller), whole-body emotion recognition. We also include discussions of \n Neuroscience of emotion recognition 9 \ndata regarding emotional prosody recognition in sections where these enable a more complete \nunderstanding of the role of a particular region in emotion recognition processes. (For recent, \nmore comprehensive reviews of emotion recognition from vocal cues, see Bachorowski & \nOwren, 2008 and Grandjean, Banziger, & Scherer, 2006.) We will focus particularly on three \nanatomical structures or regions: the amygdala and right somatosensory cortex, primarily \nbecause their roles in emotion recognition based on other nonverbal cues have been explored, \nand the face- and body-selective areas of occipitotemporal cortex. Both the amygdala and right \nsomatosensory regions have played prominent roles in simulation models of emotion \nrecognition, although as we will discuss below, the evidence for amygdala involvement in at \nleast simple simulation models is quite mixed. We will also briefly discuss the roles of two \nfrontal cortical regions in emotional face recognition: left inferior frontal cortex (about which \nmore is said in the Winkielman et al. paper in this volume) and ventromedial frontal cortex. Note \nthat other regions, which we will not discuss here, are known to be critical for processing at least \ncertain emotions in facial expressions, prominent among them the insula and basal ganglia \n(Calder et al., 2001; for more comprehensive reviews of the neuroscience of face emotion \nrecognition please see Adolphs, 2002 and Atkinson, Heberlein, & Adolphs, In Press). \nThe Amygdala in Emotion Perception and Experience \nThe Amygdala in Face Emotion Processing \nThe amygdala, a paired, almond-sized and \u2013shaped nucleus buried within the medial \ntemporal lobe, has long been known to be important for emotional behavior, and particularly for \naversive or \u201cfear\u201d conditioning in both animal models (Fanselow & LeDoux, 1999) and in \nhumans (e.g., Bechara et al., 1995; LaBar, LeDoux, Spencer, & Phelps, 1995). The importance \nof the amygdala in social behavior was established by studies of nonhuman primates with \n Neuroscience of emotion recognition 10 \nanterior temporal ablations or, in more recent studies, selective damage to the amygdala, who \nshowed impairments in social interactions (Amaral, 2002; Kl\u00fcver & Bucy, 1939\/1997). \nHowever, a series of studies of rare patients with bilateral amygdala damage, conducted and \nreported in the 1990s, highlighted an apparently specific deficit in fear recognition consequent to \namygdala damage in humans (e.g., Adolphs, Tranel, Damasio, & Damasio, 1995; Calder, Young, \nPerrett, Hodges, & Etcoff, 1996; Sprengelmeyer et al., 1999). These and other patient studies \nused standardized emotional face stimuli such as those created by Ekman and Friesen (1976), \nand required participants either to label the faces (usually by choosing from a list of emotion \nwords) or to rate them (usually rating the level of each of several \u2018basic\u2019 emotions in each of a \nset of faces).  \nFunctional neuroimaging studies of neurologically intact participants yielded convergent \nresults regarding the role of the amygdala in fearful face processing, though notably most did not \ndirectly address emotion recognition: participants were generally passively viewing emotional \nfaces, or performing a task such as gender categorization which required attention to the faces \nbut not to their expressions. Amygdala activity was greater when participants viewed fearful vs. \nneutral faces (e.g., Morris et al., 1996; Pessoa, McKenna, Gutierrez, & Ungerleider, 2002) or \nhappy faces (e.g., Morris et al., 1996; Pessoa et al., 2002; see Phan, Wager, Taylor, & Liberzon, \n2002 for a review). In some studies, amygdala activity was observed even during presentation of \nsubliminally presented faces (e.g., Whalen, 1998 -- but see Phillips et al., 2004) or during faces \nthat were viewed at the same time as, but incidental to, an attention-demanding task (e.g., \nVuilleumier, Armony, Driver, & Dolan, 2001 \u2013 but see Pessoa et al., 2002). This activation may \nbe due to input from the subcortical pathway to the amygdala, i.e. from the superior colliculus to \nthe pulvinar and thus to the amygdala (Morris, Ohman, & Dolan, 1999). \n Neuroscience of emotion recognition 11 \nThis collection of lesion and imaging findings has been interpreted in light of amygdalar \ninvolvement in aversive conditioning and in other measures of fear-related experience. For \nexample, in addition to the amygdala\u2019s established role in conditioning, intracranial stimulation \nof the amygdala has been associated with a feeling of anxiety (Halgren, Walter, Cherlow, & \nCrandall, 1978). This dual involvement in recognition and experience was thus interpreted as \nsupport for a model of the amygdala as a \u201cconvergence zone\u201d for fear (Damasio, 1989): the \namygdala was thought to coordinate representations of and responses to fear-related stimuli in \ncortical and subcortical regions (Adolphs et al., 1995). One fairly recent study cast doubt on the \nrole of the amygdala specifically in fear experience: one subject with bilateral amygdala damage \nas well as several with unilateral damage reported normal affective states both in a retrospective \nconsideration of the past month of experience, and in a day-by-day measure over the course of a \nmonth. This was true even for anxiety and fear states, implying that the amygdala is not critical \nfor normal experience of a range of emotional feelings, even though it may be recruited during \nsuch experience in intact brains (Anderson & Phelps, 2002). However, the results of another \nstudy suggest that the emotional lives of individuals with amygdala lesions might not be entirely \nnormal. Independent clinical interviews of another individual with complete bilateral amygdala \ndamage indicated that she experienced a normal range of affect and emotion, and yet she \nrecounted the considerable amount of adversity in her life in a matter-of-fact way, without any \nsign of dysphoria, and she denied having felt strong emotions at the time (Tranel, Gullickson, \nKoch, & Adolphs, 2006). In contrast, when talking of positive life events, the same individual \nwas much more animated and positive. She was also judged by clinicians na\u00efve to her \nneuropsychological condition to lack a normal sense of distrust and danger of others (Tranel et \nal., 2006), which is consistent with the finding that she rates as highly trustworthy and \n Neuroscience of emotion recognition 12 \napproachable those faces that are normally judged to look the least trustworthy and approachable \n(Adolphs, Tranel, & Damasio, 1998). Consider also the recent finding that individuals with \namygdala lesions showed abnormally low levels of self-reported arousal in response to negative \n(but not positive) picture stimuli (Berntson, Bechara, Damasio, Tranel, & Cacioppo, 2007). \nThus, Anderson and Phelps\u2019 (2002) finding of no emotional experience differences consequent \nto amygdala damage may have been due to the use of an insufficiently sensitive measure \nrequiring self-report and introspection. The amygdala may in fact be important for the experience \nof at least certain types of negative affect (though perhaps not specifically fear). \nTo further muddy the \u201cconvergence zone for fear\u201d picture, recent data suggest that the link \nbetween fearful face processing and the amygdala is not as straightforward as the initial lesion \nand imaging reports suggested. Amygdala activity is observed not just during viewing of fearful \nfaces, but also when participants view happy faces (relative to neutral ones; e.g., Pessoa et al., \n2002). Furthermore, patterns of face emotion recognition impairment after amygdala damage can \nbe variable, depending both on the task used and on individual differences. For example, \nAdolphs and colleagues (1999) compared nine people with bilateral amygdala damage on the \nsame sensitive emotion-rating task. They found that while fear was the most commonly and most \nseverely impaired emotion, only four of the nine were impaired on fear recognition, and some \nwere impaired on other emotions (3 each impaired on sadness, disgust, and surprise, and 4 on \nanger). Functional imaging studies explicitly examining individual differences in amygdala \nactivity have found that variance in personality traits and in gene polymorphisms has a large \neffect on amygdala response to emotional faces (e.g., Canli, Sivers, Whitfield, Gotlib, & \nGabrieli, 2002; Hariri et al., 2002).  \nRecent work focusing on the amygdala\u2019s role in facial expression recognition has focused on \n Neuroscience of emotion recognition 13 \na role in directing attention to and within emotional faces. Whalen and colleagues (2004) found \namygdala activity when healthy subjects viewed backward-masked \u201ceye whites\u201d isolated from \nfearful facial expressions (vs. eye whites isolated from happy expressions), but not inverted \u201ceye \nblacks.\u201d Careful testing of a single bilateral-amygdala-damaged patient, SM, provided \nconvergent evidence about the necessity of the amygdala for normal direction of attention to the \neyes in emotional face processing (Adolphs et al., 2005): SM and a group of demographically \nmatched control subjects were asked to make two-alternative emotion judgments based on \nviewing only small, variably-placed and -sized regions of the face at once (the \u201cbubbles\u201d \ntechnique developed by Schyns, Bonnar, & Gosselin, 2002). Normal controls use information \nfrom the eye region to make judgments of happiness, fear, sadness and anger, and especially the \nlatter three of these (Smith, Gosselin, & Schyns, 2004). In contrast, Adolphs and colleagues \nshowed that SM did not use information from the eyes when making two-alternative (happy vs. \nfear) judgments. This finding parsimoniously explains SM\u2019s known impairment in fearful face \nrecognition. In a direct test of this, her fear recognition was \u2018rescued\u2019 when she was instructed to \nattend to the eyes in a free-viewing emotion recognition task (Adolphs et al., 2005). Thus, rather \nthan merely responding to fearful faces in a sort of large-scale pattern recognition, the amygdala \nmay participate in emotion recognition by responding to the presence of certain facial features, \nsuch as eyes, and directing attention for further processing to those features and the rest of the \nobject\u2014or even the environment\u2014surrounding them. This interpretation is consistent with \nfindings indicating that subjects with bilateral amygdala damage perform poorly on other social \njudgments requiring attention to the eyes, such as inferring intention or attention from eye gaze \n(Young et al., 1995) or making judgments about social emotions such as jealousy or guilt from \nthe eye region of facial expressions (Adolphs, Baron-Cohen, & Tranel, 2002).  \n Neuroscience of emotion recognition 14 \nTo summarize: The amygdala is reliably recruited during both fearful experience and fearful \nface processing, thus serving at least nominally as a \u201cshared substrate\u201d for both experience and \nrecognition. However, its role relating these processes is not as clearcut as was once thought. \nThis is due both to inconsistencies in data linking the amygdala specifically to fearful experience \nas opposed to threat-related emotional experience or negative emotional experience more \ngenerally, and to the increasing support for models positing a more general role for the amygdala \nin directing attention to environmental stimuli indicative of potential threat, including but not at \nall limited to (certain features of) fearful faces (Whalen et al., 2004). We return to this point \nbelow, in discussing links between the amygdala and visual face-processing regions in the \nservice of attending to emotional faces. However, recent data linking emotional experience to \ntrait negative affect and amygdala activity help to indicate what sort of role the amygdala might \nplay in a shared substrates model of emotion experience and recognition: Barrett and colleagues \n(2007) specifically addressed this issue by correlating trait negative affect, measured by \nexperience sampling over several weeks, with amygdala reactivity when subjects viewed briefly \npresented fearful faces months later inside the scanner. Their results indicate that amygdala \nreactivity might underlie both perceptual sensitivity to negative environmental stimuli\u2014perhaps \nespecially social stimuli\u2014and the affective sequelae of perceiving negative information (Barrett \net al., 2007). The kind of attentional processes that the amygdala mediates in the service of \nemotion recognition may mediate experience as well, because awareness of negative \nenvironmental stimuli leads to negative emotion.  \nThe Amygdala in Prosodic  and Whole-Body Emotion Recognition \nGiven the role that the amygdala is now thought to play in facial emotion recognition, it may \nnot be surprising that the evidence for amygdala involvement in recognizing emotion from vocal \n Neuroscience of emotion recognition 15 \ncues is mixed. If at least part of the amygdala\u2019s role in emotion recognition consists of low-level \ncue recognition and directing attention to stimuli thus detected (e.g., wide eye whites), would one \npostulate a corresponding role in vocal emotion recognition? One bilaterally amygdala-damaged \nsubject (DR) was found to be impaired at labeling fear and anger from both prosody and non-\nverbal vocal sounds in addition to faces (Scott et al., 1997), and another (NE) was impaired in \nrecognizing fear (forced choice labeling) from faces, prosody, and body postures (Sprengelmeyer \net al., 1999). However, three different bilaterally amygdala-damaged subjects (SP, SM and RH) \ntested in two separate studies showed no selective impairment recognizing fear when labeling in \nprosodic stimuli (Anderson and Phelps, 1998; Adolphs and Tranel, 1999). While the exact \nregions of extra-amygdaloid damage differ between these subjects, the anatomical differences do \nnot line up with the differences in their impairments (Calder, Lawrence & Young, 2001), so it is \nhard to draw conclusions about the importance of the amygdala in recognition of fear, or any \nother emotions, from vocal prosody or non-verbal vocal sounds. Imaging studies have not \nresolved this question: Hearing either fearful vocal expressions or laughter and crying activated \namygdalar nuclei in two studies (Phillips et al., 1998; Sander and Scheich, 2001), but another \nstudy found a decrease in amygdala activation while participants listened to nonverbal \nexpressions of fear (Morris et al., 1999). It is at present unclear if these discrepancies, like those \nfound for faces (see above), are due to individual differences, to task differences, or, in the case \nof the imaging studies, to limitations in BOLD fMRI resolution. Nonetheless, it is not at present \npossible to conclude that the amygdala is critically involved in recognizing fear from vocal cues. \nThe evidence for amygdala involvement in whole-body emotional expression recognition is \nsimilarly mixed: Many imaging studies find amygdala activity during viewing of emotional \nwhole-body stimuli, but lesion studies have not provided converging evidence regarding the \n Neuroscience of emotion recognition 16 \nnecessity of the amygdala for whole-body emotion recognition, and in fact have provided some \nevidence against such a necessity. Bonda and colleagues (1996) may have been the first to \nexamine neural activations in response to viewing emotional body movements. Using PET \nimaging, they found greater activity in both amygdala and superior temporal sulcus when \nsubjects looked at emotional whole-body movements (with the instruction to attend for future \nrecognition memory testing), as compared with goal-directed hand-and-arm movements (Bonda, \nPetrides, Ostry, & Evans, 1996), but they did not specifically examine fearful body movements. \nTwo recent functional imaging studies by de Gelder and colleagues have also yielded amygdala \nactivity in response to whole-body emotion cues and did focus on fear. In one, passively viewing \nphotos of fearful body postures with blurred faces, as compared to neutral postures, yielded right \namygdala activity and fusiform activity (in the separately-localized fusiform face area; \nHadjikhani & de Gelder, 2003), implying a role for the amygdala in the processing of fearful \nvisual expressions other than faces. In another analysis, right amygdala was more active during \npassive viewing of fearful stimuli than during viewing of neutral photographs, as were a suite of \nother regions including orbitofrontal cortex, body-representing regions including fusiform \ncortex, and motor planning regions (de Gelder, Snyder, Greve, Gerard, & Hadjikhani, 2004).  \nDe Gelder and colleagues interpreted these findings as supporting a model of emotional body \ncue processing with two separate but connected circuits: The first is a more automatic system \ninvolving the subcortical route to the amygdala (i.e. via the superior colliculus and pulvinar \nnucleus of the thalamus) and the striatum, with a critical role in preparing appropriate motor \nresponses to environmental threat, as communicated by another\u2019s expression; this system is \nconceptualized in reflexive terms. The second, in contrast, is a primarily cortical system \ninvolving amygdala, body-representing perceptual areas including fusiform and lateral \n Neuroscience of emotion recognition 17 \noccipitoparietal regions, and motor planning areas, with a role that encompasses detailed \nperceptual processing of another\u2019s emotional body expressions, connecting this perception to \nstored knowledge (i.e. recognition), and planning an appropriate motor response in light of this \ndetailed information (de Gelder, 2006). Note that the descriptions of both of these circuits \nemphasize links between the amygdala and motor planning structures, highlighting these \nauthors\u2019 focus on the connection between emotion perceived in others\u2014particularly from the \nrich cues of whole-body stimuli\u2014and behavioral responses. However, three recent studies have \nfailed to replicate the specificity of the amygdala response to fearful whole-body stimuli: Pichon, \nde Gelder, and Gr\u00e8zes (in press) found increased amygdala activity for passive viewing of angry \nrelative to neutral static and dynamic bodies, and Gr\u00e8zes, Pichon, and de Gelder (2007) found \ngreater amygdala activity during viewing of both fearful and neutral whole-body stimuli (both \nstatic and dynamic) relative to scrambled versions of the same stimuli; regions with greater \nactivity during viewing of fearful whole-body stimuli included bilateral temporal pole and \nposterior superior temporal sulcus, as well as right premotor area and a lateral occipital region \nwhich may be the extrastriate body area (EBA), a visual region selective for bodies and body \nparts over other objects (Downing, Jiang, Shuman, & Kanwisher, 2001). Pichon et al. and Gr\u00e8zes \net al. interpret the discrepancy between their findings and those reported in the previous studies \nwith static stimuli as resulting from the more closely matched stimuli: In the first two \nexperiments, emotional expressions included still shots from such actions as opening a door and \nexpressing fear at an imagined robber, whereas neutral stimuli consisted of instrumental actions \nsuch as combing one\u2019s hair or pouring water. In contrast, Gr\u00e8zes et al. focused on the door-\nopening action but varied only the emotion expressed by the actor upon opening the door; they \nsuggest that this \u201cinstrumental action... invite[s] by itself... a social meaning,\u201d which by \n Neuroscience of emotion recognition 18 \nimplication is sufficient to recruit the amygdala\u2014and to do so comparably to the presence of \nfearful expression. The absence of increased amygdala activity during observation (for \nsubsequent rating of intensity) of fearful compared to neutral whole-body movements has also \nbeen reported by Peelen, Atkinson, Andersson and Vuilleumier (2007), who nevertheless did \nfind greater amygdala activation for both happy and angry (but not sad or disgusted) body \nmovements.  \nEvidence from lesion studies that would confirm the necessity of the amygdala for whole-\nbody emotion processing has generally not supported such a role. As noted above, \nSprengelmeyer et al. (1999) found impaired fearful posture recognition in a subject with bilateral \namygdala damage, consistent with this individual\u2019s impairments recognizing fear from faces and \nvocal expressions. However, two other studies have demonstrated normal emotion recognition \nfrom several differing whole-body stimulus sets in another subject with bilateral amygdala \ndamage, SM, who has been previously demonstrated to have impairments recognizing fearful \nfaces. In one of these studies, photographs of dramatic, emotionally-charged movie scenes were \naltered so that the facial expressions were not visible. Though this alteration significantly \nreduced normal subjects\u2019 ability to recognize all emotions, including fear, from the characters, \nSM recognized fear normally in a forced-choice paradigm\u2014in fact, her performance was better \nfor these stimuli than for the intact photographs (Adolphs & Tranel, 2003; note that three other \nparticipants with bilateral damage that included amygdala as well as further medial temporal \nlobe structures were similarly normal on both the masked and intact photographs). In the second \nstudy, SM was tested with four different whole-body stimulus sets (see Figure 1): a set of \nemotional body posture photographs with the faces blurred; dynamic stimuli in which the actors \nfaced forward and expressed emotion with a full-body gesture; the same stimuli edited to be \n Neuroscience of emotion recognition 19 \npatch-light (similar to point-light; Atkinson et al., 2004); and a set of emotional point-light \nwalkers in which the actors were filmed walking, creeping, dancing, or otherwise locomoting \nacross the field of view (Heberlein, Adolphs, Tranel, & Damasio, 2004; Heberlein & Saxe, \n2005). Another bilaterally amygdala-damaged subject, AP, was tested on two of these (the \nposture photographs and the point-light walkers). Both participants recognized fear normally, in \nforced-choice tasks, in all the stimulus sets that they judged (Atkinson, Heberlein, & Adolphs, \n2007). These two findings make it impossible to claim that the amygdala is necessary for normal \nrecognition of emotional body movements. Consistent with the data of de Gelder and colleagues, \nit may be the case that in intact brains the amygdala serves to associate perceived bodily \nexpressions of fear with relevant motor plans. By this view, SM\u2014though she is able to know that \nthe perceived individual is afraid\u2014would not prepare escape behavior in response to seeing \nanother person\u2019s fearful body expression. Potential future experiments examining the role of the \namygdala in responding to whole-body expressions might include an examination of evoked \nmotor responses or motor-related activity in patients with bilateral amygdala damage as well as \nfurther studies of amygdala responses to dynamic and static whole-body emotional expressions, \nideally with the inclusion of individual differences measures. \nRight Somatosensory Cortex in Emotion Perception and Experience \nRight Somatosensory Cortex in Face Emotion Processing \nDamage to cortices in right hemisphere has long been known to result in impairments \nrecognizing emotional expressions (e.g., Bowers, Bauer, Coslett, & Heilman, 1985). Recent \nevidence from both functional neuroimaging studies (Winston, O'Doherty, & Dolan, 2003) and \nfrom lesion overlap studies (Adolphs, Damasio, Tranel, Cooper, & Damasio, 2000) suggests that \nthe critical regions within right hemisphere lie within somatosensory cortices, broadly defined to \n Neuroscience of emotion recognition 20 \ninclude not just primary but also more posterior secondary somatosensory regions. Adolphs and \ncolleagues tested 108 brain-damaged individuals on a face emotion-rating task, then compared \nthe overlap of brain lesion locations of impaired (i.e. abnormal) patients with the overlap of brain \nlesion locations of unimpaired patients. The region of maximal lesion overlap among impaired \npatients was in right posterior postcentral gyrus, bordering on supramarginal gyrus (Adolphs et \nal., 2000). Notably, single patients with lesions to right somatomotor cortices, but sparing \nsomatosensory cortices, were unimpaired, whereas patients with lesions to somatosensory \ncortices sparing motor cortices were impaired. A nearly identical region was highlighted in a \nfunctional imaging study in which participants attended either to the emotional content or the \ngender of pairs of morphed faces (morphed from a neutral face of one gender to an emotional \nface of the other gender). When participants attended to the emotional content (answering \n\u201cWhich is more emotional?\u201d), this region of right somatosensory cortex was significantly more \nactive than when participants viewed the same faces but attended to gender (\u201cWhich is more \nmale?\u201d; Winston et al., 2003).  \nA recent study using transcranial magnetic stimulation (TMS) has confirmed the critical role \nof right somatosensory regions in emotion recognition. Single pulse TMS over right \nsomatosensory cortex slows facial emotion discrimination (in a match-detection paradigm), \nrelative to similar stimulation over superior lateral temporal cortex (the converse pattern was \nobtained for eye gaze discrimination; Pourtois et al., 2004). Interestingly, right somatosensory \nstimulation slowed judgments of fearful faces, but not happy faces; such a distinction has not \nbeen observed for this region in lesion or neuroimaging studies. However, it is unclear how to \nreconcile these TMS findings with the lesion and functional imaging findings implicating right \nsomatosensory regions in emotion recognition across categories. \n Neuroscience of emotion recognition 21 \nMore generally, however, one might ask why somatosensory cortex\u2014a region of the brain \nnamed for its role in representing bodily sensations such as touch and pressure\u2014is important for \nrecognizing emotion in others. Adolphs (e.g., 2002) and others have interpreted this involvement \nby appealing to simulation models of emotion recognition, specifically the idea that at least one \ncomponent of emotion recognition processes entails a reliance on internal representations of the \nbody state associated with an observed emotion-- in other words, what it feels like to be \nexperiencing the emotion that we view another person expressing. Right somatosensory cortex \nmight thus be involved in the representation of a \u201csomatosensory image\u201d associated with a felt \nemotion (Adolphs, 2002); we return to this point below.4 \nRight Somatosensory Cortex in Prosodic and Whole-Body Emotion Recognition \nGiven such a model of right somatosensory involvement in emotion recognition, it is not \nsurprising that other forms of nonverbal emotional expressions also appear to rely at least partly \non right somatosensory regions. In contrast to the amygdala\u2019s posited role\u2014detecting certain \nfeatures of environmental stimuli relevant to threat, or other emotional salience, and focusing \nfurther attention on these stimuli\u2014right somatosensory regions are posited to represent either the \nfeeling-state associated with an observed emotional expression or the actual sensations \nassociated with producing an emotional expression. Right hemisphere cortices have consistently \nbeen implicated in studies of emotional vocal prosody recognition (e.g., Kucharska-Pietura, \nPhillips, Gernand, & David, 2003; Ross, Thompson, & Yenkosky, 1997), and a lesion overlap \nstudy like the one discussed above for facial expression similarly found a focus of maximal \noverlap in posterior regions of right somatosensory cortex, including portions of both postcentral \nand supramarginal gyri, for subjects impaired on an emotional prosody rating task (Adolphs et \nal., 2002). Convergent evidence comes from TMS, with a twist\u2014a dissociation between \n Neuroscience of emotion recognition 22 \napproach and avoidance emotions: In a study similar to the Pourtois et al. paper described above \nfor face recognition, van Rijn et al. (2005) delivered repetitive pulse TMS to participants\u2019 right \nfrontoparietal operculum, encompassing the somatosensory representation of the mouth region. \nImmediately afterwards, the same participants detected \u201cwithdrawal emotions,\u201d i.e. fear and \nsadness, significantly slower in heard sentences (compared to after sham stimulation). In \ncontrast, no effect was observed for the \u201capproach emotions,\u201d i.e. happiness and anger.  Van Rijn \nand colleagues interpret the laterality effects that they and Pourtois et al. (2004) observed in light \nof Davidson\u2019s (e.g., 1992) theories regarding hemispheric differences in approach vs. withdrawal \nemotion experience, in which right frontal regions are more involved in withdrawal-related \nemotions, and left frontal regions are more involved in approach-related emotions. \nThe role of right somatosensory regions in emotion recognition extends also to recognizing \nwhole-body emotional expressions. Benowitz and colleagues (1983) tested both right- and left-\nhemisphere-damaged patients using the Profile of Nonverbal Sensitivity (PONS; Rosenthal et al., \n1979), which requires emotion labeling of both faces and body movements\/gestures in short \ndynamic stimuli. Interestingly, though they found face recognition impairments in five of six \nright-hemisphere damaged subjects, only one of these five was also impaired at the whole-body \nemotion recognition task\u2014and notably, this patient had parietal damage, potentially \ncorresponding to somatosensory cortex involvement. More recent work has attempted to localize \nregions critical for whole-body emotion recognition more precisely: A lesion overlap study \nexamining impairments in emotion recognition (forced-choice labeling) based on whole-body \npoint-light walker cues (Heberlein et al., 2004) identified similar right-somatosensory regions to \nthose implicated in the earlier studies of facial (Adolphs et al., 2000) and vocal (Adolphs et al., \n2002) expressions. As in the initial lesion overlap study on face emotion recognition, \n Neuroscience of emotion recognition 23 \nexaminations of cases with precentral vs. postcentral gyrus damage found impairments in \nindividuals with damage to sensory but not motor cortex, and preserved performance in \nindividuals with the reverse pattern of damage, indicating that motor impairments could not \nexplain this pattern of results.  \nThe involvement of right somatosensory regions in whole-body emotion recognition was \nsupported by an fMRI study using the same stimuli as the lesion overlap study (Heberlein & \nSaxe, 2005): A region at the border of right postcentral and supramarginal gyri was more active \nwhen subjects made emotion judgments about point-light walkers (given one of a known set of \nemotion words, rating how well it fit the stimulus) than when they made personality trait \njudgments (comparable task with trait words) based on the same stimuli. Further, a region of \ninterest based on the maximal lesion overlap of the previous study\u2014i.e., posterior right \nsomatosensory cortex\u2014was significantly more active for emotion, as compared to personality \ntrait, judgments (a separate region, in left inferior frontal gyrus, was associated with personality \ntrait judgments in both the lesion and fMRI papers; Heberlein & Saxe, 2005). \nIn summary, right somatosensory cortices appear to play a role in emotion recognition based \nnot just on faces, but also on emotional prosody and whole-body emotional expressions. This \nevidence is consistent with two different simulation roles: modeling what a specific body part \nfeels like when it produces an emotional expression, or representing the overall (or perhaps just \nvisceral) body state associated with a felt emotion. In either case, we recognize the emotional \nexpressions that we see or hear from others by relying (to some extent) on representations of \nwhat our own bodies feel like. While both lesion and functional imaging data implicate right and \nnot left somatosensory cortex, at least two \u201cfunctional lesion\u201d TMS studies suggest instead that \n Neuroscience of emotion recognition 24 \nright and left regions both participate, depending on the specific emotion in question; additional \nwork is necessary to explore this possible dissociation further. \nCategory-Selective Visual Regions in Emotion Perception \nVisual Cortex Regions in Face Emotion Recognition: Fusiform, Occipital and Superior \nTemporal Face Areas \nFaces constitute a category of visual object for which there is both selectivity (a preference \nfor faces over other visual objects) and functional specialization in higher-level visual cortices \n(for reviews, see e.g. Atkinson et al., In Press; Kanwisher & Yovel, 2006). Prominent amongst \nthese regions are the occipital face area (OFA) and the fusiform face area (FFA), whose \nfunctions are principally in the extraction and integration of structural information from faces \n(featural and configural cues). Face selectivity is also evident in posterior regions of superior \ntemporal cortex, which, in contrast to the processing of relatively invariant aspects of faces in \nfusiform cortex, are more involved in processing facial movements and other, relatively \nchangeable aspects of faces (Haxby, Hoffman, & Gobbini, 2000), or in the multimodal \nintegration of changeable and invariant aspects of social information signaled by the face, body \nor voice (Calder & Young, 2005). What are the roles of these regions in emotion recognition, \nand in particular, in simulation processes underlying the ability to infer emotional states from \nfacial expressions?  \nIn the influential face-processing model of Haxby et al. (2000), the OFA sends outputs to \nboth the FFA and posterior superior temporal cortex (principally the superior temporal sulcus or \nSTS). Facial expression perception is underpinned by the activity of posterior STS in association \nwith other regions involved in emotion perception generally, including the amygdala and insula. \nThe FFA, on the other hand, has little or no role in processing emotional expressions, in this \n Neuroscience of emotion recognition 25 \nmodel, being more involved in processing identity, sex, and other more invariant aspects of \nfaces. However, a more recent survey of the evidence indicates that this bifurcation of identity \nand expression processing reflects more of a bias or relative segregation than a categorical \ndissociation (Calder & Young, 2005). Consistent with this proposal is evidence that the FFA\u2019s \nactivity is modulated by the emotional expressions of viewed faces, which we discuss below. \nConsider also the very recent finding of early discrimination of emotional expression in ventral \ntemporal cortex (including fusiform), as revealed by intracranial recordings in humans viewing \nemotional faces: cells in ventral temporal cortex responded differentially to emotions in dynamic \nmorphing faces, and did so more quickly and accurately than did a region of lateral temporal \ncortex that included STS (Tsuchiya, Kawasaki, Howard, & Adolphs, 2008). \nConsistent with the suggestion that the OFA encodes structural properties of faces at an early \nstage of visual processing, the first face-selective response over posterior sites occurs at around \n100ms after stimulus onset (e.g., Liu, Harris, & Kanwisher, 2002). Indeed, there is evidence that \nthe OFA is critically involved in the processing of individual facial features but not configural \ncues at this very early stage: repetitive TMS applied to right (but not left) OFA activity within a \nwindow of 60-100ms from stimulus onset disrupted accurate discrimination of individual facial \nfeatures but not of the spacing between those features in a delayed match-to-sample task \n(Pitcher, Walsh, Yovel, & Duchaine, 2007). Furthermore, a recent study showed that TMS \napplied to right OFA impaired discrimination of emotional facial expressions in a similar task \n(Pitcher, Garrido, Walsh, & Duchaine, 2008). This study also confirmed previous findings of \nTMS to right somatosensory cortex disrupting facial emotion recognition. Importantly, for \npresent purposes, Pitcher et al. (2008) found different critical periods for right OFA and right \nsomatosensory cortex involvement in emotion expression processing: emotion discrimination \n Neuroscience of emotion recognition 26 \naccuracy was impaired only when TMS was delivered over right OFA at 60-100ms or over right \nsomatosensory cortex at 100-140ms and 130-170ms post-stimulus onset\u2014notably early, but later \nthan OFA. Thus, the OFA and somatosensory cortices contribute to emotion recognition at \ndifferent times, consistent with hierarchical models of facial emotion processing according to \nwhich information about structural properties of faces relevant to the perception of emotional \nexpressions is fed forward to somatosensory regions implicated in the simulation of body states \nassociated with the viewed emotional state. It remains to be seen what roles, if any, the FFA and \nposterior STS have in such a processing hierarchy. \nA consistent finding from functional imaging studies of emotional face perception is an \nenhanced activation of occipital and temporal regions, including the FFA and OFA, in response \nto viewing faces expressing emotions, relative to emotionally neutral faces (for a review, see e.g. \nVuilleumier & Driver, 2007). Emotional enhancement of FFA activation has been shown to \ncorrelate with activity in the amygdala (e.g. Morris et al., 1998), which is consistent with \nfindings from animal studies demonstrating substantial bidirectional connections between the \namygdala and much of ventral temporal cortex (e.g., Amaral, Behniea, & Kelly, 2003), and thus \nimplicates the amygdala as the source of the emotional modulation of visual cortex. More direct \nevidence of the amygdala\u2019s role as the source of the emotional modulation comes from a \ncombined lesion and fMRI study. Vuilleumier et al. (2004) found that individuals with \nhippocampal damage but spared amygdala showed the normal modulation of fusiform cortex by \nviewing fearful compared to neutral facial expressions while making a same\/different judgment, \nwhereas individuals with amygdala and hippocampal damage did not; furthermore, in an \nimportant control condition, fusiform cortex activity was modulated by attention to faces as \nopposed to houses in both groups of patients. Vuilleumier and colleagues have proposed that this \n Neuroscience of emotion recognition 27 \nemotional modulation of occipitotemporal face-processing regions serves an essentially \nattentional role, albeit one that is functionally and anatomically distinct from task-related or \npurely stimulus-related attentional modulation; specifically, the emotional significance of stimuli \ntriggers, via feedback connections from the amygdala, an enhancement or prioritization of their \nvisual processing (e.g., Vuilleumier, 2005; Vuilleumier & Driver, 2007). \nModulation of occipitotemporal activity during viewing of emotional relative to neutral faces \noccurs around 170ms after stimulus onset, as indexed by a modulation of the N170 or M170 \npeak of the face-selective evoked-response potential (although some studies have failed to show \nsuch emotional modulation of the face-selective ERP; for reviews, see Vuilleumier & Pourtois, \n2007). It has recently been reported that not only does the N170 show different latencies for \nviewing different facial emotions, but also that it reflects an integration of visual information \nspecific to each expression, beginning 50ms prior to and ending at the peak of this ERP (Schyns, \nPetro, & Smith, 2007). For all facial expressions tested, this integration begins at the eyes and \nmoves down the face, stopping once the diagnostic information for the relevant expression has \nbeen resolved (e.g., at the eyes for fear expressions, the corners of the nose for disgust, and the \nmouth for happiness). It is the behavioral goal of emotion classification that determines when the \nintegration of facial information ceases, which suggests top-down cognitive control of processing \nin occipitotemporal cortex, perhaps from prefrontal regions (Schyns et al., 2007). \nVisual Cortex Regions In Whole-Body Emotion Recognition: Extrastriate Body Area and \nFusiform Body Area \nLike faces, the form of the human (or primate) body is a category of visual object for which \nthere is both selectivity and functional specialization in higher-level visual cortices. Evidence for \nbody-selective visual mechanisms comes from studies of both humans and non-human primates \n Neuroscience of emotion recognition 28 \n(reviewed by Peelen & Downing, 2007). In humans, the evidence points to two distinct regions, \ndubbed the extrastriate body area (EBA), located in lateral occipitotemporal cortex (Downing et \nal., 2001), and the fusiform body area (FBA), located in fusiform gyrus (Peelen & Downing, \n2005; Schwarzlose, Baker, & Kanwisher, 2005). The EBA and FBA respond selectively to \nviewing human bodies and body parts compared with objects, faces, and other control stimuli, \ndespite considerable anatomical overlap between the FBA and the face-selective FFA (Peelen & \nDowning, 2005; Schwarzlose et al., 2005) and between the EBA, motion processing area \nV5\/MT, and object-form-selective lateral occipital complex (Downing, Wiggett, & Peelen, \n2007).  \nParalleling the findings with facial expressions, research on emotional body perception has \nsimilarly reported enhanced activation in visual cortex when people perceive emotional bodies \nand body parts. Such modulation by emotional bodies was consistently reported in the fusiform \ngyrus (e.g., de Gelder et al., 2004), lateral occipitotemporal cortex (e.g., Gr\u00e8zes et al., 2007) and \nSTS (Gr\u00e8zes et al., 2007; Pichon et al., in press). For example, de Gelder and colleagues have \nshown that fusiform and occipital gyri, as well as the amygdala, are activated during passive \nviewing of static whole-body postures of fear, relative to emotionally neutral postures (de Gelder \net al., 2004; Hadjikhani & de Gelder, 2003). Fearful body actions have also been shown to \nactivate right middle temporal gyrus, in the region of the EBA and overlapping MT\/V5, \nirrespective of whether these were presented as static or dynamic images (Gr\u00e8zes et al., 2007). \nTaken together, these findings raise the intriguing possibility that perceiving emotion signals \nfrom the body might modulate precisely those populations of neurons that code for the viewed \nstimulus category (see Sugase, Yamane, Ueno, & Kawano, 1999), instead of reflecting \n\u2018synergies\u2019 between the perception of facial and bodily expressions (de Gelder et al., 2004), or a \n Neuroscience of emotion recognition 29 \nglobal boost to all visual processing in extrastriate visual cortex. There is now evidence of just \nsuch category-specific emotional modulation. Peelen et al. (2007) found increased activation in \nthe EBA when people viewed angry, disgusted, happy, and fearful (but not sad) body movements \n(for subsequent intensity ratings on emotion scales), compared to neutral controls, and increased \nactivation in the FBA for angry, disgusted, and happy (but not fearful or sad) body movements. \nImportantly, multi-voxel pattern analysis showed that the strength of this emotional modulation \nwas related, on a voxel-by-voxel basis, to the degree of body selectivity, while there was no \nrelation with the degree of selectivity for faces, supporting the idea that emotional cues from \nbody movements produce topographically selective influences on category-specific populations \nof neurons in visual cortex. Furthermore, across subjects, amygdala responses to emotional \nbodies positively correlated with the modulation of the EBA and FBA but not the FFA. This \nresult parallels the findings of correlations between amygdala and fusiform activity to facially \nexpressed emotions, discussed above, and thus implicates feedback modulatory influences on \nvisual cortex from the amygdala, but in addition suggests that this modulatory feedback is \ncategory-specific. \nWe suggest that the role of the EBA in emotion perception is similar to that of the OFA \n(bearing in mind that their respective functions in body and face processing per se are not \nentirely analogous). In particular, this body-selective region is critically involved in processing \nstructural cues related to the form of the human body, information which may then be fed to \nsomatosensory cortices to aid emotion recognition via simulation (although there is currently no \ndirect evidence to support this latter proposal). It might also be the case that the FBA and FFA \nplay analogous roles in emotion perception from bodies and faces, respectively, acting in concert \nwith the EBA (for bodily expressions) and OFA (for facial expressions) to extract expression-\n Neuroscience of emotion recognition 30 \nrelevant structural cues. Presumably, later emotional modulation of the EBA and FBA \nspecifically by bodily expressions reflects prioritized or enhanced visual processing of those \nbodily forms so as to allow more efficient or accurate discrimination of others\u2019 emotions. \nYet there is also evidence suggesting a possible additional role for at least the EBA in \nemotion recognition via simulation. This evidence shows that the EBA is sensitive to certain \ndifferences between self and other; in particular, right EBA activity is increased when (1) bodies \n(Chan, Peelen, & Downing, 2004) or body parts (Saxe, Jamal, & Powell, 2006) are presented in \nallocentric relative to egocentric views, regardless of whether the image shows the observer\u2019s \nown body or that of another person (Chan et al., 2004); and (2) when visual information (the \nmovement of a cursor) is consistent with an assumed other person\u2019s joystick movements rather \nthan with one\u2019s own (David et al., 2007). Indeed, this latter finding has recently been confirmed \nin a TMS study, suggesting that the EBA plays a critical role in identifying ourselves as agents of \nself-generated movements (David et al., 2007). Taken together, this evidence raises the \nintriguing possibility that the EBA could be part of an emotion simulation network, helping to \ndistinguish the simulated body states or emotional actions of another from one\u2019s own body states \nor actions. Such a network could also involve posterior parietal cortex, which has been \nimplicated in the detection of sensorimotor congruence and the sense of agency (e.g. Chaminade \n& Decety, 2002; Jeannerod, 2004), and which, in David et al.\u2019s (2007) study, showed enhanced \nfunctional connectivity with the EBA when visual feedback was incongruent with the \nparticipants\u2019 own movements. Despite this evidence of EBA involvement in distinguishing self \nfrom others and the evidence of emotional modulation of EBA, a recent study indicates that it is \nnot specifically involved in empathy for pain (Lamm & Decety, In Press), which raises doubts as \nto whether it has a direct role in simulating the emotional states of others. \n Neuroscience of emotion recognition 31 \nPrefrontal Cortex in Emotion Perception and Experience \nPrefrontal Cortex in Face Emotion Recognition: Ventromedial Frontal Cortex and Left \nInferior Frontal\/Premotor Cortex \nPrefrontal cortex has repeatedly been implicated in emotional face processing, with two \nregions most frequently identified as critical: ventromedial and (left or bilateral) inferior frontal \nor premotor regions. Functional imaging studies have yielded somewhat inconsistent results, due \nperhaps to differences in task requirements, stimuli used, specific emotions tested, or differential \ninvolvement of PFC regions in the processing of specific emotions. However, three regions are \nreported most consistently during viewing, categorizing, matching, or holding in memory \nemotional faces (with tasks varying between studies): orbitofrontal cortex or OFC (generally but \nnot always ventral, and therefore included within the designation of ventromedial prefrontal \ncortex; Blair, Morris, Frith, Perrett, & Dolan, 1999; Dolan et al., 1996; Vuilleumier et al., 2001), \nanterior cingulate cortex (ACC; e.g., Blair et al., 1999; Dolan et al., 1996; Vuilleumier et al., \n2001), and left or bilateral inferior frontal gyrus (e.g., Carr et al., 2003). Loss-of-function studies \nof subjects with prefrontal damage due to stroke, surgery, or fronto-temporal dementia have also \nimplicated these regions: ventromedial and OFC (e.g., Heberlein, Padon, Gillihan, Farah, & \nFellows, 2008; Hornak et al., 2003; Marinkovic, Trebon, Chauvel, & Halgren, 2000); ACC \n(Hornak et al., 2003) and inferior frontal gyrus or the frontal opercular region more generally \n(e.g., Adolphs et al., 2000; Marinkovic et al., 2000).  \nTwo somewhat different explanations for the involvement of ventromedial and\/or \norbitofrontal regions in emotion recognition both relate to simulation, albeit of a somewhat \ndifferent form than is played by the regions described above. For example, OFC and ACC \nregions are thought to play a role in representing arousal, and this role has been invoked to \n Neuroscience of emotion recognition 32 \nexplain medial PFC involvement in recognizing high-arousal emotions (Adolphs, 2002; \u00d6ngur & \nPrice, 2000). Two recent studies have examined the effects of PFC damage on both emotional \nexperience and recognition (Heberlein et al., 2008; Hornak et al., 2003), and both of these studies \nfound a relationship between measures of specifically sad mood experience and sadness \nrecognition. Heberlein et al. (2008) found that in people with ventromedial frontal (but not other \nprefrontal) cortex damage, there was a relationship between ratings of sad faces and lab-induced \nsad emotion. They explained these findings by positing a model which bears some similarity to \nthat proposed by Barrett et al. (2007), discussed above, for the amygdala: People with damage to \nVMF cortices may be less sensitive to others\u2019 emotional expressions, including of distress, and \ntherefore may show less sad mood in situations including laboratory mood inductions. In support \nof this idea, there was a higher correlation between sad face recognition and induced sad mood \nfor a film-based sadness induction, than between sad face recognition and induced mood for an \nautobiographical recall-based sadness induction (Heberlein et al., 2008). This fits theoretically \nwith a general role for VMF cortex in the top-down direction of attention to others\u2019 emotions and \nother salient emotional stimuli, which we later incorporate in a model of emotion recognition \n(see the section, Summary and a Simulation Model of Emotion Recognition Incorporating Both \nFace and Whole-Body Data). \nIn contrast, the role of premotor or frontal opercular areas has generally been understood in \nterms of \u201cmirror-neuron-like\u201d activity (e.g., Carr et al., 2003; Winkielman et al., 2009). In \nsupport of this model, a posterior section of left inferior frontal gyrus at the frontal operculum is \nactive bilaterally when people imitate emotional facial expressions (Lee, Josephs, Dolan, & \nCritchley, 2006). It is unclear, at present, how to integrate these findings with the model of \n Neuroscience of emotion recognition 33 \nemotional face processing that we described above, relating the roles of the amygdala, \nextrastriate cortex, and right somatosensory regions. \nPrefrontal Cortex in Voice or Whole-Body Emotion Recognition: Ventromedial Frontal \nCortex and Left Inferior Frontal\/Premotor Cortex \nGiven its repeated, if inconsistent, implication in emotion recognition from faces, it is \nsomewhat surprising how little evidence there is for prefrontal cortical involvement in emotion \nrecognition from other nonverbal cues. A detailed lesion study (Hornak et al., 2003) compared \nemotion recognition from faces and vocal prosody in groups with damage to several different \nprefrontal regions. Though they found no group-wise impairment on facial emotion identification \nin a labeling task (some individuals with OFC or unilateral dorsomedial damage were impaired), \nboth OFC and dorsomedial PFC were reliably associated with voice emotion recognition. The \nauthors posit that vocal prosody is a more sensitive test of emotion recognition ability than facial \nexpressions because it is less easily translated into verbalizable cues (Hornak et al., 2003).  \nPremotor and inferior frontal regions have been implicated in several studies of emotion \nrecognition based on both prosodic and whole-body cues. Thus, premotor cortex is active, \nespecially on the right side, when subjects process emotional prosody (George et al., 1996; \nImaizumi et al., 1997). These data converge with evidence from Adolphs and colleagues\u2019 (2002) \nlesion overlap study of emotional prosody recognition, which found a focus of lesion overlap \naround the left frontal operculum (i.e. posterior left inferior frontal gyrus), in addition to the \nabove-mentioned right somatosensory regions. It is unclear, however, why these findings do not \nagree with respect to laterality. Though Heberlein and colleagues\u2019 (2004) lesion overlap study \ndid not identify inferior frontal regions as critical for whole-body emotion recognition (rather, \nthese were found to be critical for personality trait judgments from dynamic whole-body stimuli), \n Neuroscience of emotion recognition 34 \nimaging studies have reported inferior frontal and premotor activity during viewing of dynamic \nwhole-body emotion cues (e.g., Gr\u00e8zes et al., 2007). \nSummary and a Simulation Model of Emotion Recognition Incorporating Both Face and \nWhole-Body Data \nThe evidence we have reviewed here suggests considerable overlap between the neural \nsystems involved in recognizing emotion from face cues and from other nonverbal cues, and \nsuggests as well that not all instances of \u201cshared substrates\u201d between emotion recognition and \nemotional experience are evidence for simulation, in the sense of representing another\u2019s behavior \nand\/or feelings in one\u2019s self. In addition, based on the evidence reviewed here, we might usefully \ndistinguish between different senses of simulation by focusing on the varied roles suggested for \ndifferent emotion-recognition-relevant neural regions. \n(1) Right somatosensory cortex and representing body sensation: Because of their defined \nrole in representing self body sensations, it is difficult to conceive of a role for somatosensory \ncortices that is not somehow a simulation account: part of how one knows what another person \nfeels like when that person acts a certain way is by representing what oneself feels like when one \nacts in a similar way. This presumed role is bolstered by the findings of somatosensory cortex \ninvolvement across multiple stimulus types and, indeed, multiple modalities of nonverbal cues. \nHowever, it is at present unclear whether the contribution of these regions is body-part specific \n(i.e. a more proprioceptive \u201cwhat my arm feels like when my arm is expressing anger\u201d role) or \nmore global (i.e. a whole-body gestalt and\/or somatovisceral representation). In either case, this \nrole in emotion recognition appears to extend across at least multiple emotions; further evidence \nfrom studies that follow up the TMS laterality-specific results will be welcome.  \n Neuroscience of emotion recognition 35 \n(2) The amygdala, fear detection and fear response: simulation or just shared substrates? In \ncontrast to somatosensory cortex, the amygdala\u2019s role in emotion recognition processes appears \nto be most extensive for, although not restricted to, negative or potentially threat-related \ninformation. Furthermore, the amygdala and related structures play many different roles related \nto both sensory processes (e.g. responding to specific threat-related environmental features\u2014\nincluding facial features\u2014and directing visual attention to these objects) and endocrine and \nmotor processes (e.g. coordinating responses to detected threats). As may have emerged from the \nmultiple instances of the phrase \u201cthe evidence... is mixed\u201d in this section of our review, it is at \npresent unclear how to extend what is known about the role of the amygdala in facial expression \nprocessing to other forms and modalities of nonverbal emotional communication; results are \nstrikingly variable across studies. Thus, while the amygdala surely plays roles in both the \nexperience of fear and the recognition of at least certain types of fearful expressions, these roles \nmay depend on fairly unrelated processes in which the amygdala participates, making it fit less \nneatly into a \u201csimulation\u201d model, despite still fitting a \u201cshared substrates\u201d model.5 Ventromedial \nprefrontal cortices may similarly play roles in both emotion recognition and emotional \nexperience without an actual overlapping \u201csimulation\u201d process. That said, it remains to be \nconfirmed whether the amygdala has a more direct role in simulation processes underlying \nemotion recognition via activating somatic states associated with the viewed emotional \nexpression. This role would facilitate recognition via \u201cas-if\u201d circuits (Damasio, 1999) or even \nactual motor behavior, a form of simulation like that in which somatosensory cortices are posited \nto play a role.  \n(3) Premotor areas and simulating expressions: Premotor regions are thought to play a role \nsomewhat similar to that of somatosensory cortices: while somatosensory cortices represent what \n Neuroscience of emotion recognition 36 \none feels like in associating an observed expression with an emotional concept or label, premotor \ncortices may represent one\u2019s own motor plans toward the same end. in order to associate an \nobserved expression with an emotional concept or label. However, unlike for somatosensory \nregions, there is less evidence that this role extends to whole-body emotional expressions; further \nwork is needed to address this discrepancy.  \n(4) Visual regions: participating via connections to regions involved in simulation: The \nrelevance to simulation of the visual regions we reviewed above lies in their interactions with \nother regions, rather than direct roles in simulation per se. Note, however, that the EBA results \n(e.g. activity differentiating self- vs. other-centered views of body parts) may prove an exception \nto this generalization. \n \nTo briefly summarize all of the above, current evidence suggests the following hierarchical \nmodel, involving both feedforward and feedback connections between visual and somatosensory \ncortices and regions involved in processing emotional information (note that we might postulate \na similar model for auditory cues, but that very little evidence currently exists to support such a \nmodel): Visual cues important for discriminating facial emotional expressions are extracted at \nearly stages of visual processing, largely in cortices specialized for face and body processing, \nand are fed forward to somatosensory cortices. These regions are implicated in simulating certain \naspects of the body states associated with the viewed emotional state\u2014possibly including \nproprioceptive (and thus body part-specific) and\/or somatovisceral sensations.  \nOverlapping in time are at least two other separate sets of processes, which may also \ninterlink. In one, the amygdala responds to specific features signaling the emotional significance \nof the viewed expressions, enabling, via feedback connections, enhanced or prioritized visual \n Neuroscience of emotion recognition 37 \nprocessing of those expressions (a consequence of which may be to allow more efficient or \naccurate discrimination of others\u2019 emotions).6 Thus, for example, when the presence of widened \neye whites is detected, the facial features (and other context) surrounding this feature are \nprocessed in greater detail. In another set of processes, information about the current processing \ngoals (e.g. that one\u2019s task is to identify the viewed expression) and other motivational \ninformation (e.g. the proclivity to attend to others\u2019 emotional states) are fed back from prefrontal \nto early visual cortices to influence the extent to which information specific to different \nemotional expressions is integrated. In theory, the results of these latter two sets of processes \ncould in turn influence continuing or subsequent simulation processes in premotor and \nsomatosensory cortices. Providing evidence for (and more specific models of) such feedback \nprocesses would be yet another welcome area of future research.  \n Neuroscience of emotion recognition 38 \nReferences \nAdolphs, R. (2002). Recognizing emotion from facial expressions: psychological and \nneurological mechanisms. Behavioral and Cognitive Neuroscience Reviews, 1(1), 21\u201361. \nAdolphs, R., Baron-Cohen, S., & Tranel, D. (2002). Impaired recognition of social emotions \nfollowing amygdala damage. J Cogn Neurosci, 14(8), 1264\u20131274. \nAdolphs, R., Damasio, H., Tranel, D., Cooper, G., & Damasio, A. R. (2000). A role for \nsomatosensory cortices in the visual recognition of emotion as revealed by three-\ndimensional lesion mapping. The Journal of Neuroscience, 20(7), 2683\u20132690. \nAdolphs, R., Gosselin, F., Buchanan, T. W., Tranel, D., Schyns, P., & Damasio, A. R. (2005). A \nmechanism for impaired fear recognition after amygdala damage. Nature, 433(7021), 68\u2013\n72. \nAdolphs, R., & Tranel, D. (2003). Amygdala damage impairs emotion recognition from scenes \nonly when they contain facial expressions. Neuropsychologia, 41, 1281\u20131289. \nAdolphs, R., Tranel, D., & Damasio, A. R. (1998). The human amygdala in social judgment. \nNature, 393, 470\u2013474. \nAdolphs, R., Tranel, D., Damasio, H., & Damasio, A. R. (1995). Fear and the human amygdala. \nThe Journal of Neuroscience, 15(9), 5879\u20135891. \nAdolphs, R., Tranel, D., Hamann, S., Young, A. W., Calder, A. J., Phelps, E. A., et al. (1999). \nRecognition of facial emotion in nine individuals with bilateral amygdala damage. \nNeuropsychologia, 37, 1111\u20131117. \nAmaral, D. G. (2002). The primate amygdala and the neurobiology of social behavior: \nimplications for understanding social anxiety. Biol Psychiatry, 51, 11\u201317. \n Neuroscience of emotion recognition 39 \nAmaral, D. G., Behniea, H., & Kelly, J. L. (2003). Topographic organization of projections from \nthe amygdala to the visual cortex in the macaque monkey. Neuroscience, 118(4), 1099\u2013\n1120. \nAnderson, A. K., & Phelps, E. A. (2002). Is the human amygdala critical for the subjective \nexperience of emotion? Evidence of intact dispositional affect in patients with amygdala \nlesions. J Cogn Neurosci, 14(5), 709\u2013720. \nAtkinson, A. P., Dittrich, W. H., Gemmell, A. J., & Young, A. W. (2004). Emotion perception \nfrom dynamic and static body expressions in point-light and full-light displays. \nPerception, 33(6), 717\u2013746. \nAtkinson, A. P., Heberlein, A. S., & Adolphs, R. (2007). Spared ability to recognise fear from \nstatic and moving whole-body cues following bilateral amygdala damage. \nNeuropsychologia, 45(12), 2772\u20132782. \nAtkinson, A. P., Heberlein, A. S., & Adolphs, R. (In Press). Are people special? A brain's eye \nview. In R. B. Adams, Jr., N. Ambady, K. Nakayama & S. Shimojo (Eds.), The Science \nof Social Vision: Oxford University Press. \nBachorowski, J., & Owren, M. J. (2008). Vocal expressions of emotion. In M. Lewis, J. M. \nHaviland-Jones & L. F. Barrett (Eds.), Handbook of Emotions, Third Edition (pp. 196\u2013\n210). New York: The Guilford Press. \nBarrett, L. F., Bliss-Moreau, E., Duncan, S. L., Rauch, S. L., & Wright, C. I. (2007). The \namygdala and the experience of affect. Soc Cogn Affect Neurosci, 2, 73\u201383. \nBarsalou, L. W., Kyle Simmons, W., Barbey, A. K., & Wilson, C. D. (2003). Grounding \nconceptual knowledge in modality-specific systems. Trends Cogn Sci, 7(2), 84\u201391. \n Neuroscience of emotion recognition 40 \nBechara, A., Tranel, D., Damasio, H., Adolphs, R., Rockland, C., & Damasio, A. R. (1995). \nDouble dissociation of conditioning and declarative knowledge relative to the amygdala \nand hippocampus in humans. Science, 269(5227), 1115\u20131118. \nBenowitz, L. I., Bear, D. M., Rosenthal, R., Mesulam, M. M., Zaidel, E., & Sperry, R. W. \n(1983). Hemispheric specialization in nonverbal communication. Cortex, 19, 5\u201311. \nBerntson, G. G., Bechara, A., Damasio, H., Tranel, D., & Cacioppo, J. T. (2007). Amygdala \ncontribution to selective dimensions of emotion. Soc Cogn Affect Neurosci, 2, 123\u2013129. \nBlair, R. J., Morris, J. S., Frith, C. D., Perrett, D. I., & Dolan, R. J. (1999). Dissociable neural \nresponses to facial expressions of sadness and anger. Brain, 122 ( Pt 5), 883\u2013893. \nBlake, R., & Shiffrar, M. (2007). Perception of human motion. Annu Rev Psychol, 58, 47\u201373. \nBlakemore, S.-J., & Decety, J. (2001). From the perception of action to the understanding of \nintention. Nature Reviews Neuroscience, 2, 561\u2013567. \nBonda, E., Petrides, M., Ostry, D., & Evans, A. (1996). Specific involvement of human parietal \nsystems and the amygdala in the perception of biological motion. The Journal of \nNeuroscience, 16(11), 3737\u20133744. \nBowers, D., Bauer, R. M., Coslett, H. B., & Heilman, K. M. (1985). Processing of faces by \npatients with unilateral hemisphere lesions: I. Dissociation between judgments of facial \naffect and facial identity. Brain and Cognition, 4, 258\u2013272. \nCalder, A. J., Keane, J., Cole, J., Campbell, R., & Young, A. W. (2000). Facial expression \nrecognition by people with Mobius syndrome. Cognitive Neuropsychology, 17, 73\u201387. \nCalder, A. J., Lawrence, A. D., & Young, A. W. (2001). Neuropsychology of fear and loathing. \nNature Reviews Neuroscience, 2, 352\u2013363. \n Neuroscience of emotion recognition 41 \nCalder, A. J., & Young, A. W. (2005). Understanding the recognition of facial identity and facial \nexpression. Nat Rev Neurosci, 6(8), 641\u2013651. \nCalder, A. J., Young, A. W., Perrett, D. I., Hodges, J. R., & Etcoff, N. L. (1996). Facial emotion \nrecognition after bilateral amygdala damage: Differentially severe impairment of fear. \nCognitive Neuropsychology, 13, 699\u2013745. \nCanli, T., Sivers, H., Whitfield, S. L., Gotlib, I. H., & Gabrieli, J. D. (2002). Amygdala response \nto happy faces as a function of extraversion. Science, 296(5576), 2191. \nCarr, L., Iacoboni, M., Dubeau, M. C., Mazziotta, J. C., & Lenzi, G. L. (2003). Neural \nmechanisms of empathy in humans: A relay from neural systems for imitation to limbic \nareas. Proc Natl Acad Sci U S A, 100, 5497\u20135502. \nChaminade, T., & Decety, J. (2002). Leader or follower? Involvement of the inferior parietal \nlobule in agency. Neuroreport, 13. \nChan, A. W., Peelen, M. V., & Downing, P. E. (2004). The effect of viewpoint on body \nrepresentation in the extrastriate body area. Neuroreport, 15(15), 2407\u20132410. \nChouchourelou, A., Matsuka, T., Harber, K., & Shiffrar, M. (2006). The visual analysis of \nemotional actions. Social Neuroscience, 1(1), 63\u201374. \nCrawford, L. E. (2009). Conceptual metaphors of affect. Emotion Review, 1(2). \nDamasio, A. R. (1989). Time-locked multiregional retroactivation: a systems-level proposal for \nthe neural substrates of recall and recognition. Cognition, 33(1\u20132), 25\u201362. \nDamasio, A. R. (1999). The feeling of what happens: Body and emotion in the making of \nconsciousness. New York: Harcourt Brace. \n Neuroscience of emotion recognition 42 \nDarwin, C. (1872\/1965). The expression of the emotions in man and animals. (Reprinted from \nthe D. Appleton and Company authorized edition). Chicago: The University of Chicago \nPress. \nDavid, N., Cohen, M. X., Newen, A., Bewernick, B. H., Shah, N. J., Fink, G. R., et al. (2007). \nThe extrastriate cortex distinguishes between the consequences of one's own and others' \nbehavior. Neuroimage, 36(3), 1004\u20131014. \nde Gelder, B. (2006). Towards the neurobiology of emotional body language. Nat Rev Neurosci, \n7(3), 242\u2013249. \nde Gelder, B., Snyder, J., Greve, D., Gerard, G., & Hadjikhani, N. (2004). Fear fosters flight: A \nmechanism for fear contagion when perceiving emotion expressed by a whole body. Proc \nNatl Acad Sci U S A, 101(47), 16701\u201316706. \nDecety, J., & Gr\u00e8zes, J. (1999). Neural mechanisms subserving the perception of human actions. \nTrends in Cognitive Sciences, 3(5), 172\u2013178. \nDecety, J., & Lamm, C. (2006). Human empathy through the lens of social neuroscience. \nScientificWorldJournal, 6, 1146\u20131163. \nde Meijer, M. (1989). The contribution of general features of body movement to the attribution of \nemotions. Journal of Nonverbal Behavior, 13(4), 247\u2013268. \ndi Pellegrino, G., Fadiga, L., Fogassi, L., Gallese, V., & Rizzolatti, G. (1992). Understanding \nmotor events: a neurophysiological study. Exp Brain Res, 91(1), 176\u2013180. \nDimberg, U. (1982). Facial reactions to facial expressions. Psychophysiology 19(6): 643\u20137. \nDimberg, U., Thunberg, M., & Elmehed, K. (2000). Unconscious facial reactions to emotional \nfacial expressions. Psychol Sci, 11(1), 86\u201389. \n Neuroscience of emotion recognition 43 \nDittrich, W. H., Troscianko, T., Lea, S. E., & Morgan, D. (1996). Perception of emotion from \ndynamic point-light displays represented in dance. Perception, 25(6), 727\u2013738. \nDolan, R. J., Fletcher, P. C., Morris, J. S., Kapur, N., Deakin, J. F. W., & Frith, C. D. (1996). \nNeural activation during covert processing of positive emotional facial expressions. \nNeuroimage, 4, 194\u2013200. \nDowning, P. E., Jiang, Y., Shuman, M., & Kanwisher, N. (2001). A cortical area selective for \nvisual processing of the human body. Science, 293(5539), 2470\u20132473. \nDowning, P. E., Wiggett, A. J., & Peelen, M. V. (2007). Functional magnetic resonance imaging \ninvestigation of overlapping lateral occipitotemporal activations using multi-voxel pattern \nanalysis. J Neurosci, 27(1), 226\u2013233. \nEkman, P., & Friesen, W. (1976). Pictures of facial affect. Palo Alto, CA: Consulting \nPsychologists. \nFanselow, M. S., & LeDoux, J. E. (1999). Why we think plasticity underlying Pavlovian fear \nconditioning occurs in the basolateral amygdala. Neuron, 23(2), 229\u2013232. \nGallese, V., Keysers, C., & Rizzolatti, G. (2004). A unifying view of the basis of social \ncognition. Trends Cogn Sci, 8(9), 396\u2013403. \nGeorge, M. S., Parekh, P. I., Rosinsky, N., Ketter, T. A., Kimbrell, T. A., Heilman, K. M., et al. \n(1996). Understanding emotional prosody activates right hemisphere regions. Archives of \nNeurology, 53, 665\u2013670. \nGlenberg, A., Webster, B., Mousilo, E., Havas, D. & Lindeman, L. (2009). Gender, emotion, and \nthe embodiment of language comprehension. Emotion Review, 1(2). \nGrandjean, D., Banziger, T., & Scherer, K. R. (2006). Intonation as an interface between \nlanguage and affect. Progress in Brain Research, 156, 235\u2013268. \n Neuroscience of emotion recognition 44 \nGr\u00e8zes, J., Pichon, S., & de Gelder, B. (2007). Perceiving fear in dynamic body expressions. \nNeuroimage, 35(2), 959\u2013967. \nHadjikhani, N., & de Gelder, B. (2003). Seeing fearful body expressions activates the fusiform \ncortex and amygdala. Curr Biol, 13, 2201\u20132205. \nHalgren, E., Walter, R. D., Cherlow, D. G., & Crandall, P. H. (1978). Mental phenomena evoked \nby electrical stimulation of the human hippocampal formation and amygdala. Brain, \n101(1), 83\u2013117. \nHariri, A. R., Mattay, V. S., Tessitore, A., Kolachana, B., Fera, F., Goldman, D., et al. (2002). \nSerotonin transporter genetic variation and the response of the human amygdala. Science, \n297(5580), 400\u2013403. \nHarrison, N. A., Singer, T., Rotshtein, P., Dolan, R. J., & Critchley, H. D. (2006). Pupillary \ncontagion: central mechanisms engaged in sadness processing. Soc Cogn Affect Neurosci, \n1(1), 5\u201317. \nHaxby, J. V., Hoffman, E. A., & Gobbini, M. I. (2000). The distributed human neural system for \nface perception. Trends Cogn Sci, 4(6), 223\u2013233. \nHeberlein, A. S., Adolphs, R., Tranel, D., & Damasio, H. (2004). Cortical regions for judgments \nof emotions and personality traits from point-light walkers. Journal of Cognitive \nNeuroscience, 16(7), 1143\u20131158. \nHeberlein, A. S., Padon, A. A., Gillihan, S. J., Farah, M. J., & Fellows, L. K. (2008). \nVentromedial frontal lobe plays a critical role in facial emotion recognition. J Cogn \nNeurosci, 20(4), 721\u2013733. \nHeberlein, A. S., & Saxe, R. (2005). Dissociation between emotion and personality judgments: \nConvergent evidence from functional neuroimaging. Neuroimage, 28, 770\u2013777. \n Neuroscience of emotion recognition 45 \nHess, U., & Blairy, S. (2001). Facial mimicry and emotional contagion to dynamic emotional \nfacial expressions and their influence on decoding accuracy. Int J Psychophysiol, 40(2), \n129\u2013141. \nHornak, J., Bramham, J., Rolls, E. T., Morris, R. G., O'Doherty, J., Bullock, P. R., et al. (2003). \nChanges in emotion after circumscribed surgical lesions of the orbitofrontal and cingulate \ncortices. Brain, 126(Pt 7), 1691\u20131712. \nHutchison, W. D., Davis, K. D., Lozano, A. M., Tasker, R. R., & Dostrovsky, J. O. (1999). Pain-\nrelated neurons in the human cingulate cortex. Nat Neurosci, 2(5), 403\u2013405. \nIacoboni, M., Woods, R., Brass, M., Bekkering, H., Mazziotta, J., & Rizzolati, G. (1999). \nCortical mechanisms of human imitation. Science, 286, 2526\u20132528. \nImaizumi, S., Mori, K., Kiritani, S., Kawashima, R., Sugiura, M., Fukuda, H., et al. (1997). \nVocal identification of speaker and emotion activates different brain regions. \nNeuroreport, 8(12), 2809\u20132812. \nJames, W. (1932). A study of the expression of bodily posture. Journal of General Psychology, \n7, 405\u2013437. \nJeannerod, M. (2004). Visual and action cues contribute to the self-other distinction. Nat \nNeurosci, 7(5), 422\u2013423. \nJohansson, G. (1973). Visual perception of biological motion and a model of its analysis. \nPerception and Psychophysics, 14, 202\u2013211. \nKanwisher, N., & Yovel, G. (2006). The fusiform face area: a cortical region specialized for the \nperception of faces. Philos Trans R Soc Lond B Biol Sci, 361(1476), 2109\u20132128. \n Neuroscience of emotion recognition 46 \nKeillor, J. M., Barrett, A. M., Crucian, G. P., Kortenkamp, S., & Heilman, K. M. (2002). \nEmotional experience and perception in the absence of facial feedback. J Int \nNeuropsychol Soc, 8(1), 130\u2013135. \nKl\u00fcver, H., & Bucy, P. C. (1939\/1997). Preliminary analysis of functions of the temporal lobes \nin monkeys. 1939. J Neuropsychiatry Clin Neurosci, 9(4), 606\u2013620. \nKucharska-Pietura, K., Phillips, M. L., Gernand, W., & David, A. S. (2003). Perception of \nemotions from faces and voices following unilateral brain damage. Neuropsychologia, \n41, 1082\u20131090. \nLaBar, K. S., LeDoux, J. E., Spencer, D. D., & Phelps, E. A. (1995). Impaired fear conditioning \nfollowing unilateral temporal lobectomy in humans. J Neurosci, 15(10), 6846\u20136855. \nLamm, C., & Decety, J. (In Press). Is the extrastriate body area (EBA) sensitive to the perception \nof pain in others? Cerebral Cortex, doi: 10.1093\/cercor\/bhn006. \nLee, T. W., Josephs, O., Dolan, R. J., & Critchley, H. D. (2006). Imitating expressions: emotion-\nspecific neural substrates in facial mimicry. Soc Cogn Affect Neurosci, 1(2), 122\u2013135. \nLiu, J., Harris, A., & Kanwisher, N. (2002). Stages of processing in face perception: an MEG \nstudy. Nat Neurosci, 5(9), 910\u2013916. \nMarinkovic, K., Trebon, P., Chauvel, P., & Halgren, E. (2000). Localized face processing by the \nhuman prefrontal cortex: Face-selective intracerebral potentials and post-lesion deficits. \nCognitive Neuropsychology, 17, 187\u2013199. \nMorris, J. S., Friston, K. J., Buchel, C., Frith, C. D., Young, A. W., Calder, A. J., et al. (1998). A \nneuromodulatory role for the human amygdala in processing emotional facial \nexpressions. Brain, 121(Pt 1), 47\u201357. \n Neuroscience of emotion recognition 47 \nMorris, J. S., Frith, C. D., Perrett, D. I., Rowland, D., Young, A. W., Calder, A. J., et al. (1996). \nA differential neural response in the human amygdala to fearful and happy facial \nexpressions. Nature, 383, 812\u2013815. \nMorris, J. S., \u00d6hman, A., & Dolan, R. J. (1999). A subcortical pathway to the right amygdala \nmediating \"unseen\" fear. Proc Natl Acad Sci U S A, 96(4), 1680\u20131685. \nMorrison, I. (2007). A motivational perspective on the neural bases of empathy. In T. F. D. \nFarrow & P. W. R. Woodruff (Eds.), Empathy in Mental Illness and Health. Cambridge, \nUK: Cambridge University Press. \nMorrison, I., Lloyd, D., di Pellegrino, G., & Roberts, N. (2004). Vicarious responses to pain in \nanterior cingulate cortex: Is empathy a multisensory issue? Cognitive, Affective, and \nBehavioral Neuroscience, 4(2), 270\u2013278. \nNiedenthal, P. M., Barsalou, L. W., Winkielman, P., Krauth-Gruber, S., & Ric, F. (2005). \nEmbodiment in attitudes, social perception, and emotion. Pers Soc Psychol Rev, 9(3), \n184\u2013211. \nNiedenthal, P. M., Brauer, M., Halberstadt, J. B., & Innes-Ker, A. H. (2001). When did her smile \ndrop? Facial mimicry and the influences of emotional state on the detection of change in \nemotional expression. Cognition and Emotion, 15, 853\u2013864. \n\u00d6ngur, D., & Price, J. L. (2000). The organization of networks within the orbital and medial \nprefrontal cortex of rats, monkeys and humans. Cereb Cortex, 10(3), 206\u2013219. \nPeelen, M. V., Atkinson, A. P., Andersson, F., & Vuilleumier, P. 2007). Emotional modulation \nof body-selective visual areas. Social Cognitive and Affective Neuroscience, 2(4), 274\u2013\n283. \n Neuroscience of emotion recognition 48 \nPeelen, M. V., & Downing, P. E. (2005). Selectivity for the human body in the fusiform gyrus. J \nNeurophysiol, 93(1), 603\u2013608. \nPeelen, M. V., & Downing, P. E. (2007). The neural basis of visual body perception. Nat Rev \nNeurosci, 8(8), 636\u2013648. \nPessoa, L., McKenna, M., Gutierrez, E., & Ungerleider, L. G. (2002). Neural processing of \nemotional faces requires attention. Proc Natl Acad Sci U S A, 99(17), 11458\u201311463. \nPhan, K. L., Wager, T., Taylor, S. F., & Liberzon, I. (2002). Functional neuroanatomy of \nemotion: a meta-analysis of emotion activation studies in PET and fMRI. Neuroimage, \n16(2), 331\u2013348. \nPhillips, M. L., Williams, L. M., Heining, M., Herba, C. M., Russell, T., Andrew, C., et al. \n(2004). Differential neural responses to overt and covert presentations of facial \nexpressions of fear and disgust. Neuroimage, 21(4), 1484\u20131496. \nPichon, S., de Gelder, B., & Gr\u00e8zes, J. (in press). Emotional modulation of visual and motor \nareas by dynamic body expressions of anger. Social Neuroscience, doi: \n10.1080\/17470910701394368. \nPing, R., Dhillon, S., & Beilock, S. (2009). Reach for what you like: The body\u2019s role in shaping \npreferences. Emotion Review, 1(2). \nPitcher, D., Garrido, L., Walsh, V., & Duchaine, B. (2008). TMS disrupts the perception and \nembodiment of facial expressions [Abstract]. Journal of Vision, 8(6), 700\u2013700. \nPitcher, D., Walsh, V., Yovel, G., & Duchaine, B. (2007). TMS evidence for the involvement of the \nright occipital face area in early face processing. Current Biology, 17(18), 1568\u20131573. \nPollick, F. E., Paterson, H. M., Bruderlin, A., & Sanford, A. J. (2001). Perceiving affect from \narm movement. Cognition, 82, B51\u2013B61. \n Neuroscience of emotion recognition 49 \nPourtois, G., Sander, D., Andres, M., Grandjean, D., Reveret, L., Olivier, E., et al. (2004). \nDissociable roles of the human somatosensory and superior temporal cortices for \nprocessing social face signals. Eur J Neurosci, 20(12), 3507\u20133515. \nPreston, S. D., & de Waal, F. B. M. (2002). Empathy: its ultimate and proximate bases. \nBehavioral and Brain Sciences, 25, 1\u201372. \nPrinz, W. (1997). Perception and action planning. European Journal of Cognitive Psychology, 9, \n129\u2013154. \nRoss, E. D., Thompson, R. D., & Yenkosky, J. (1997). Lateralization of affective prosody in \nbrain and the callosal integration of hemispheric language functions. Brain Lang, 56(1), \n27\u201354. \nSawada, M., Suda, K., & Ishii, M. (2003). Expression of emotions in dance: relation between \narm movement characteristics and emotion. Percept Mot Skills, 97(3 Pt 1), 697\u2013708. \nSaxe, R., Jamal, N., & Powell, L. (2006). My body or yours? The effect of visual perspective on \ncortical body representations. Cereb Cortex, 16(2), 178\u2013182. \nSchwarzlose, R. F., Baker, C. I., & Kanwisher, N. (2005). Separate face and body selectivity on \nthe fusiform gyrus. J Neurosci, 25(47), 11055\u201311059. \nSchyns, P. G., Bonnar, L., & Gosselin, F. (2002). Show me the features! Understanding \nrecognition from the use of visual information. Psychol Sci, 13(5), 402\u2013409. \nSchyns, P. G., Petro, L. S., & Smith, M. L. (2007). Dynamics of visual information integration in \nthe brain for categorizing facial expressions. Curr Biol, 17(18), 1580\u20131585. \nSinger, T., Seymour, B., O'Doherty, J., Kaube, H., Dolan, R. J., & Frith, C. D. (2004). Empathy \nfor pain involves the affective but not sensory components of pain. Science, 303(5661), \n1157\u20131162. \n Neuroscience of emotion recognition 50 \nSmith, M. L., Gosselin, F., & Schyns, P. G. (2004). Receptive fields for flexible face \ncategorizations. Psychological Science, 15(11), 753\u2013761. \nSonnby-Borgstr\u00f6m, M. (2002). Automatic mimicry reactions as related to differences in \nemotional empathy. Scand J Psychol, 43(5), 433\u2013443. \nSprengelmeyer, R., Young, A., Schroeder, U., Grossenbacher, P. G., Federlein, J., Buettner, T., \net al. (1999). Knowing no fear. Proceedings of the Royal Society of London B., 266, \n2451\u20132456. \nStrack, F., Martin, L. L., & Stepper, S. (1988). Inhibiting and facilitating conditions of the \nhuman smile: a nonobtrusive test of the facial feedback hypothesis. J Pers Soc Psychol, \n54(5), 768\u2013777. \nSugase, Y., Yamane, S., Ueno, S., & Kawano, K. (1999). Global and fine information coded by \nsingle neurons in the temporal visual cortex. Nature, 400(6747), 869\u2013873. \nTranel, D., Gullickson, G., Koch, M., & Adolphs, R. (2006). Altered experience of emotion \nfollowing bilateral amygdala damage. Cognit Neuropsychiatry, 11(3), 219\u2013232. \nTsuchiya, N., Kawasaki, H., Howard, M. A., & Adolphs, R. (2008). Decoding frequency and \ntiming of emotion perception from direct intracranial recordings in the human brain \n(abstract). Journal of Vision, 8(6), 962\u2013962. \nvan Rijn, S., Aleman, A., van Diessen, E., Berckmoes, C., Vingerhoets, G., & Kahn, R. S. \n(2005). What is said or how it is said makes a difference: role of the right fronto-parietal \noperculum in emotional prosody as revealed by repetitive TMS. Eur J Neurosci, 21(11), \n3195\u20133200. \nVuilleumier, P. (2005). How brains beware: neural mechanisms of emotional attention. Trends in \nNeurosci, 21(11), 585\u2013594. \n Neuroscience of emotion recognition 51 \nVuilleumier, P., Armony, J. L., Driver, J., & Dolan, R. J. (2001). Effects of attention and \nemotion on face processing in the human brain: an event-related fMRI study. Neuron, \n30(3), 829\u2013841. \nVuilleumier, P., & Driver, J. (2007). Modulation of visual processing by attention and emotion: \nwindows on causal interactions between human brain regions. Philos Trans R Soc Lond B \nBiol Sci, 362(1481), 837\u2013855. \nVuilleumier, P., & Pourtois, G. (2007). Distributed and interactive brain mechanisms during \nemotion face perception: Evidence from functional neuroimaging. Neuropsychologia, \n45(1), 174\u2013194. \nVuilleumier, P., Richardson, M. P., Armony, J. L., Driver, J., & Dolan, R. J. (2004). Distant \ninfluences of amygdala lesion on visual cortical activation during emotional face \nprocessing. Nat Neurosci, 7(11), 1271\u20131278. \nWallbott, H. G. (1991). Recognition of emotion from facial expression via imitation? Some \nindirect evidence for an old theory. Br J Soc Psychol, 30 ( Pt 3), 207\u2013219. \nWallbott, H. G. (1998). Bodily expression of emotion. Eur J Soc Psych, 28, 879\u2013896. \nWhalen, P. J., Kagan, J., Cook, R. G., Davis, F. C., Kim, H., Polis, S., et al. (2004). Human \namygdala responsivity to masked fearful eye whites. Science, 306(5704), 2061. \nWinston, J. S., O'Doherty, J., & Dolan, R. J. (2003). Common and distinct neural responses \nduring direct and incidental processing of multiple facial emotions. Neuroimage, 20(1), \n84\u201397. \nYoung, A. W., Aggleton, J. P., Hellawell, D. J., Johnson, M., Broks, P., & Hanley, J. R. (1995). \nFace processing impairments after amygdalotomy. Brain, 118(Pt 1), 15\u201324. \n Neuroscience of emotion recognition 52 \nAuthor Note \nWe are grateful to India Morrison, Paula Niedenthal, Piotr Winkielman, and an \nanonymous reviewer for comments on an earlier version of this manuscript; to Daniel M. \nWegner and Mahzarin R. Banaji for advisory and financial support (A.S.H.); and to Ralph \nAdolphs for numerous discussions which have shaped both of our thinking on these issues. \n Neuroscience of emotion recognition 53 \nEndnotes \n1 Please note that due to space constraints, we cited just one or two papers to support \nnumerous findings which have been documented by several independent replications and \nextensions. These are generally denoted by an \u201ce.g.\u201d before a reference or abbreviated list of \nreferences.   \n2 As reviewed in another paper in this volume (Glenberg, Webster, Mousilo, Havas & \nLindeman, 2009), manipulating spontaneous facial expressions also affects judgments of \nsentence valence. \n3 The term \u2018perception\u2019 typically refers to processes that make explicit the distinct \nfeatures of stimuli and their geometric configurations to allow discrimination among different \nstimuli on the basis of their appearance. By \u2018discrimination\u2019 of emotional expressions we are \nreferring to abilities such as deciding whether two (or more) stimuli (e.g. faces) have the same or \ndifferent emotion, or matching a target face with one of several other faces based on their \nexpression. In contrast, \u2018recognition\u2019 refers to processes that require additional knowledge that \ncould not be obtained solely from an inspection of the features of the stimulus, such as \nconceptual knowledge of a particular emotion, the retrieval of appropriate lexical labels, and the \nperception of the emotional response (or a central representation thereof) that the stimulus \ntriggers in the observer. Tasks that tap recognition, rather than purely perceptual, processes \ninclude those in which participants are asked to use selected emotion words to identify emotional \nexpressions, or to label or describe the viewed emotional state in a more open-ended way, or to \nrate how much of a given emotion is represented in a stimulus. As our main interest here is in the \nrecognition of emotional expressions, we shall reserve the term \u2018perception\u2019 for processes that \noccur relatively early in time subsequent to the onset of the stimulus that are presumed to rely \n Neuroscience of emotion recognition 54 \nlargely on early sensory cortices and that do not rely on or necessarily invoke conceptual or \nlexical knowledge. \n4 Note, however, that in none of these studies were participants with right somatosensory \ncortex damage at chance levels of performance; the role played by these regions in emotion \nrecognition extends across multiple specific emotions, but humans must have multiple, at least \npartly independent, pathways for recognizing emotion. \n5 As noted in the section, The Amygdala in Emotion Perception and Experience, there \nmay well be correlations between these roles across individuals: for example, Barrett\u2019s recent \nfinding of amygdala sensitivity to negative social cues correlating with month-long records of \nexperienced mood, explained via a correlation between amygdala-mediated increased awareness \nof negative social cues and a corresponding increase in the normal, negative response to such \nawareness. \n6 Note that the amygdala may be involved at an even earlier stage, somehow directing the \nvisual system to seek out and attend to certain salient visual features; a key topic for ongoing \nresearch is in elucidating the nature of this interaction. \n Neuroscience of emotion recognition 55 \nFigure Caption \nFigure 1. Examples of whole-body emotional expression stimuli.  \nA. Examples of emotional posture photographs used in Atkinson, Heberlein & Adolphs, \n2007 (created by A. S. Heberlein). Both of these are labeled \u201cfearful\u201d by the majority of \nnormal participants. \nB. Still shots from full-light and corresponding patch-light emotional movement movies \n(first described in Atkinson et al., 2004). These patch-light displays are similar to the \npoint-light displays originally developed by Johansson (1973) to study the perception of \nbody and other biological motion. Small lights or reflective patches are placed on the \nmajor joints of a person, who is then filmed while making body movements. Video clips \nare then produced such that the only visible elements are these moving small light points \nor patches. The dynamic stimuli are instantly recognizable as human motion, even when \na human form is not readily recognizable in stills from such stimuli. Point-light and \npatch-light stimuli can be ideal for studies of movement-based emotion perception \nbecause both face and morphological cues are absent or minimal, but kinematic cues are \npreserved. The movement of the figure shown in B is recognized as fearful by normal \nparticipants, whether viewed in full-light or patch-light; still shots, in contrast, are only \nrecognizable as fearful in the full-light version, as is evident here. \nC. Still shots from two point-light emotional movement movies (first described in Heberlein \net al., 2004). Both movements are recognized as fearful by normal participants, but \nwithout kinematic information the figures are unlikely to even be recognizable as human \nforms, especially without the context of the other figures. \n \n"}