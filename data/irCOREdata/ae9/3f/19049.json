{"doi":"10.1080\/00131911.2010.537315","coreId":"19049","oai":"oai:eprints.bham.ac.uk:525","identifiers":["oai:eprints.bham.ac.uk:525","10.1080\/00131911.2010.537315"],"title":"A randomised controlled trial of the use of a piece of commercial software for the acquisition of reading skills","authors":["Khan, M","Gorard, Stephen"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2011","abstract":"We report here the overall results of a cluster randomised controlled trial of the use of computer-aided instruction with 672 year 7 pupils in 23 secondary school classes in the north of England. A new piece of commercial software, claimed on the basis of publisher testing to be effective in improving reading after just six weeks of use in the classroom, was compared over ten weeks (one term) with standard practice in literacy provision. Pupil literacy was assessed before and after the trial, via another piece of commercial software testing precisely the kinds of skills covered by the pedagogical software. Both the treatment group and the comparison group improved their tested literacy. In a sense the publisher\u2019s claim was justified. However, the comparison group improved their literacy scores considerably more than the treatment group, with a standardized improvement of +0.99 compared to +0.56 (overall \u2018effect\u2019 size of -0.37), suggesting that the software approach yields no relative advantage for improvements, and may even disadvantage some pupils. \\ud\nOn this evidence, the use of software, of a kind that is in very common use across schools in England, was a waste of resource. This could be an important corrective finding for an area of schooling that has been the focus of intense policy and practice attention. Of course, the software used has now been superseded by the same and different publishers. But the paper discusses the implications of these results for the use of such software to teach literacy more widely, for the way in which publisher claims are worded, and for the research community in relation to the feasibility of conducting pragmatic trials in school settings","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"Taylor & Francis","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.bham.ac.uk:525<\/identifier><datestamp>\n      2012-01-17T14:10:01Z<\/datestamp><setSpec>\n      7374617475733D696E7072657373<\/setSpec><setSpec>\n      7375626A656374733D4C:4C31<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        A randomised controlled trial of the use of a piece of commercial software for the acquisition of reading skills<\/dc:title><dc:creator>\n        Khan, M<\/dc:creator><dc:creator>\n        Gorard, Stephen<\/dc:creator><dc:subject>\n        L Education (General)<\/dc:subject><dc:description>\n        We report here the overall results of a cluster randomised controlled trial of the use of computer-aided instruction with 672 year 7 pupils in 23 secondary school classes in the north of England. A new piece of commercial software, claimed on the basis of publisher testing to be effective in improving reading after just six weeks of use in the classroom, was compared over ten weeks (one term) with standard practice in literacy provision. Pupil literacy was assessed before and after the trial, via another piece of commercial software testing precisely the kinds of skills covered by the pedagogical software. Both the treatment group and the comparison group improved their tested literacy. In a sense the publisher\u2019s claim was justified. However, the comparison group improved their literacy scores considerably more than the treatment group, with a standardized improvement of +0.99 compared to +0.56 (overall \u2018effect\u2019 size of -0.37), suggesting that the software approach yields no relative advantage for improvements, and may even disadvantage some pupils. \\ud\nOn this evidence, the use of software, of a kind that is in very common use across schools in England, was a waste of resource. This could be an important corrective finding for an area of schooling that has been the focus of intense policy and practice attention. Of course, the software used has now been superseded by the same and different publishers. But the paper discusses the implications of these results for the use of such software to teach literacy more widely, for the way in which publisher claims are worded, and for the research community in relation to the feasibility of conducting pragmatic trials in school settings.<\/dc:description><dc:publisher>\n        Taylor & Francis<\/dc:publisher><dc:date>\n        2011<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.bham.ac.uk\/525\/1\/Gorard_2010_Educational_Review.pdf<\/dc:identifier><dc:relation>\n        public<\/dc:relation><dc:relation>\n        http:\/\/eprints.bham.ac.uk\/525\/1.hassmallThumbnailVersion\/Gorard_2010_Educational_Review.pdf<\/dc:relation><dc:relation>\n        http:\/\/www.tandfonline.com\/doi\/abs\/10.1080\/00131911.2010.537315?prevSearch=gorard&searchHistoryKey=<\/dc:relation><dc:identifier>\n        Khan, M and Gorard, Stephen (2011) A randomised controlled trial of the use of a piece of commercial software for the acquisition of reading skills. Educational Review, 64. pp. 21-35. ISSN 0013-1911 (In Press)<\/dc:identifier><dc:relation>\n        http:\/\/eprints.bham.ac.uk\/525\/<\/dc:relation><dc:language>\n        English<\/dc:language><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["public","http:\/\/eprints.bham.ac.uk\/525\/1.hassmallThumbnailVersion\/Gorard_2010_Educational_Review.pdf","http:\/\/www.tandfonline.com\/doi\/abs\/10.1080\/00131911.2010.537315?prevSearch=gorard&searchHistoryKey=","http:\/\/eprints.bham.ac.uk\/525\/"],"year":2011,"topics":["L Education (General)"],"subject":["Article","PeerReviewed"],"fullText":"A randomised controlled trial of the use of a piece of commercial software for the \nacquisition of reading skills \n \n \nMuhammad Khan and Stephen Gorard \nSchool of Education, University of Birmingham, B17 9SX \ns.gorard@bham.ac.uk \n \n \nAbstract \n \nWe report here the overall results of a cluster randomised controlled trial of the use of \ncomputer-aided instruction with 672 year 7 pupils in 23 secondary school classes in the \nnorth of England. A new piece of commercial software, claimed on the basis of publisher \ntesting to be effective in improving reading after just six weeks of use in the classroom, \nwas compared over ten weeks (one term) with standard practice in literacy provision. \nPupil literacy was assessed before and after the trial, via another piece of commercial \nsoftware testing precisely the kinds of skills covered by the pedagogical software. Both \nthe treatment group and the comparison group improved their tested literacy. In a sense \nthe publisher\u2019s claim was justified. However, the comparison group improved their \nliteracy scores considerably more than the treatment group, with a standardized \nimprovement of +0.99 compared to +0.56 (overall \u2018effect\u2019 size of -0.37), suggesting that \nthe software approach yields no relative advantage for improvements, and may even \ndisadvantage some pupils. \n \nOn this evidence, the use of software, of a kind that is in very common use across schools \nin England, was a waste of resource. This could be an important corrective finding for an \narea of schooling that has been the focus of intense policy and practice attention. Of \ncourse, the software used has now been superseded by the same and different publishers. \nBut the paper discusses the implications of these results for the use of such software to \nteach literacy more widely, for the way in which publisher claims are worded, and for the \nresearch community in relation to the feasibility of conducting pragmatic trials in school \nsettings. \n \nKeywords \n \nRandomised controlled trial, literacy, reading skills, technology based instruction \n \n 1\nLiteracy as an issue \n \nReading is a fundamental skill for later life and forms a basis for any child\u2019s subsequent \nlearning at school (Good et al. 1998). Pupils who read well in early stages of their \neducation are more successful in later years compared to those who fall behind (Hirsch \n2007). Differential reading ability is a key determinant of patterns of subsequent learning \n(Wolf and Katzir-Cohn 2001, Pikulski and Chard 2005). Poor reading ability can have \nharmful psychological, social and economic consequences, with implications far beyond \nthose directly associated with education (Adams and Bruck 1993). Societal demands on \nreading ability are increasing in the information age (Cunningham et al. 2004), and a \nminimal level of literacy is an entitlement for all in a civilized nation. \n \nIn the UK, concern has been expressed for a decade or more about poor or even, \naccording to some reports, declining levels of literacy. The Department for Education and \nEmployment (1999) famously reported that an estimated \u2018seven million adults in England \ncannot locate the page reference for plumbers in Yellow Pages\u2019 (p.12). This is clearly an \nexaggeration. Nevertheless, at least 10% of the children in England used to leave primary \nschool with apparently deficient reading and writing skills (Brooks et al. 2002). Over \n30% of children in their first year of secondary school were found not to be able to read \nat a level suitable for their age in 1997, althouugh this dropped to around 20% by 2009 \n(National Literacy Trust 2010). At least 30% of 14-year-old boys did not reach the \ngovernment-specified target level in reading and writing (National Reading Campaign \nSurvey 2005). These apparently high levels of functional illiteracy, for a developed \ncountry, are sometimes reflected in the results from international surveys of attainment. \nFor example, England was ranked 19th in the 2006 Progress in International Reading \nLiteracy Study (PIRLS 2007). The figures for low literacy levels differ depending on the \nage, test used, and the precise standards or targets imposed. It does not even matter for \nour purposes here whether these claims are true or not. What is clearly true is that policy-\nmakers, educational authorities, many in the media, and some researchers and teachers \nbelieve that standards of literacy in schools in England need to improve (ETI 2003, DfES \n2003). Can technology-based instruction be part of the solution? \n 2\n  \nThe claims for technology-based instruction \n \nAccess to technology in schools in England has grown exponentially since the 1980s. It is \nnow routine for most schools to use technology-based products such as software \npackages and websites in teaching and learning \u2013 both in literacy and other core subject \nskills. For literacy alone, we estimate that by 2005 there were 300 pieces of software and \nmore than 500 instructional websites available or on the market aimed at improving \nprimary or early secondary reading skills. Part of the reason for this growth has been \nenhanced government funding for technology-based purchases and for staff development \nin the use of ICT. Between 1998 and 2002, for example, spending on ICT doubled in \nsecondary schools, and continues to rise. Government bodies in England strongly \nrecommend using technology solutions in all school subjects (DfES 2003). \n \nHowever, the evidence on the educational benefits of these various technology products \nis not particularly clear. ICT is used in the classroom, and independently and less \nformally by individual learners. ICT can be used in the classroom in isolation or as part \nof a blended learning approach. It may be used for different tasks over different lengths \nof time, even within one school. The beneficial outcomes sought could be enjoyment, \nautonomy of learning, future participation, personalization, freeing up teachers to deal \nwith other issues, cost effectiveness, or simply enhanced learning outcomes. It is, \ntherefore, hard to say whether and to what extent technology-based instruction \u2018works\u2019.  \n \nIn addition, many of the studies directly addressing the efficacy of ICT in literacy \neducation have been descriptive in nature, relying on the impressions of participants. \nThese studies often find an apparently positive impact on the acquisition of pupil literacy \nskills (Blok et al. 2002, Silverstein et al. 2000, Cox et al. 2003, Pittard et al. 2003, \nOFSTED 2004, Baron 2001, Rose and Dalton 2002, Pelgrum 2001, Sivin-Kachala and \nBialo 2000). But others have argued that the small sample sizes, the lack of comparators, \n 3\nindeed the lack of research design, and the passive retrospective nature of some of this \nwork combine to offer a potentially misleading picture (Waxman et al. 2003).  \n \nIn this light, it is interesting that experimental studies of the effectiveness of software \npackages in improving literacy skills tend to show rather different results. Rigorous \nintervention studies with suitable controls often find little or no positive impact from the \nuse of technology-based instruction compared to standard or traditional practice. A \nnumber of studies and systematic reviews have found that software packages had no \neffect on reading achievement (Borman et al. 2009, Rouse and Krueger 2004, Andrews et \nal. 2005, Torgerson and Zhu 2003, Angrist and Lavy 2002, Goolsbee and Guryan 2005, \nDynarski et al. 2007, Lei and Zhao 2005). An overview of reading instruction \ninterventions by Slavin et al. (2008) suggested that mixed methods and co-operative \napproaches are more effective than technology alone, although this conclusion is itself \nthe subject of some dispute (e.g. Greenleaf and Petrosino 2008).  \n \nGiven that computers and associated software impose a cost, are frequently updated, and \nare in widespread use in schools, it is important to have evidence of their impact. The \nexpenditure on ICT may have an impact but not proportionate to the costs. It may have \nno impact and so be a cost with no benefits. And it may even have deleterious impacts. \nUntested educational initiatives can frequently be harmful for children (Boruch et al. \n2002). \u2018There are many examples of education interventions that have been widely \ndisseminated on the basis that they are driven by good intentions, seem plausible and are \nunlikely to do any harm, yet when they have been rigorously evaluated have been found \nto be ineffective or positively harmful\u2019 (Gorard with Taylor 2004, pp.92-93 ).  \n \nThis new study examined the claims of a widely used commercial software publisher \nconcerning the impact of a specific literacy program for mainstream year 7 secondary \nschool pupils. At the time of this study, the software was in use in around 400 schools \nacross the UK. Depending on the size of any school, a one year license for this software \nwas between \u00a3375 and \u00a3600. The publishers claimed that this software was research \nbased, and had been developed with guidance from some of the leading reading experts, \n 4\nto lead pupils aged 11 to 15 through the essential steps in becoming successful readers. It \nwas considered especially valuable for pupils transferring to secondary school at age 11 \nor 12, even those with relatively poor reading skills. How did it fare in a pragmatic trial? \n \n \nMethods used \n \nThe participants \n \nOur sample was purposive in the sense that it was regional to Yorkshire in the north of \nEngland, and consisted of those state-maintained schools agreeing to co-operate with the \nresearch and possessing a minimum level of technology access and support (agreed \nbetween the software publishers and the school ICT coordinators). As shown below, the \npupils involved in the research were diverse in terms of ethnicity, sex and family poverty, \nwhile the schools differed in terms of size, aggregate public examination results and the \nmix of their pupils. However, because of the replacements necessary we do not claim that \nthese schools and their pupils are statistically representative of a larger known population. \nThe achieved overall sample is the population for this study. This population consists of \nnine secondary schools in the Bradford, Leeds and York areas. These nine schools agreed \nthat their entire year 7 pupil body would take part in either the treatment or control, as \nlong as the allocation to these groups was for full teaching classes rather than for \nindividuals, and as long as the parents raised no objection. Eight classes out of the total of \n31 did not take part because at least one parent objected to their child taking part in the \nstudy. This left 23 classes containing 672 pupils at the outset. Four pupils moved to \nschools in another area before the pre-test, and a further three moved before the post-test. \nIt was not possible to conduct an intention-to-treat analysis using these, since we could \nnot follow the seven missing pupils. Nevertheless, their numbers are small and divided \nbetween both groups. A simple sensitivity analysis suggests that their inclusion could \nmake no difference to the clear results of this trial (see below). The resulting 665 pupils \nranged in age from 11 years and one month to 12 years and four months.  \n \n 5\nThe 23 classes were ranked in order of size (number of pupils), and a random number \ngenerator was used to select one of each successive pair of classes starting with the \nlargest two. The final odd class was randomly allocated to one of the groups. Thus, each \nclass had the same chance of ending up in the treatment or in the control group, once \nstratified by size. In the end, 11 classes of 319 pupils were in the treatment group selected \nto use commercial software in their literacy lessons for one term, and 12 classes of 346 \npupils were destined to receive standard practice in their literacy lessons over the same \nperiod. Table 1 illustrates the outcome of this cluster randomisation. The two groups \nachieved were very similar in terms of all observed and measured characteristics. The \ncontrol group had a slightly higher proportion of boys and of pupils not reported as being \n\u2018White\u2019 in ethnicity. These characteristics may be loosely related to literacy levels in \nEnglish, and are discussed further below.  \n \nTable 1 - Demographic characteristics of treatment and control group pupils in the study \nGroup Total \nPercentage \nFSM \nPercentage \nmale \nPercentage \nnon-White \nTreatment 319 21 53 33 \nControl  346 21 56 34 \nN=655 \nNote: FSM represents eligibility for free school meals, an important and widely used \nmeasure of family poverty in the UK. \n \nSchools had classes in both groups, and while this might be thought to lead to \ncontamination this was not a strong possibility with this specific intervention (see below), \nit does lead to some control for a possible school effect. Of course, with only 23 clusters \nto allocate we cannot be sure that the randomisation had coped with any unobserved \nsystematic differences between the groups. However, the pre- and post-test design (see \nbelow) should deal with any revealed differences in literacy levels themselves. Therefore, \nwe should be able to conclude that the major difference between the two groups will be \nthe way in which literacy lessons are conducted for the two groups.  \n \n 6\nThe treatment \n \nThe intervention took place for 10 weeks, over a single term, during the course of the \nacademic year 2006\/07. The control group remained in routine teaching practice using a \nmore traditional paper and teacher based format, with no specified ICT component.  \n \nWe do not name here the software used in the treatment (or its publisher). While \nregrettable, this is what was agreed at the outset, and it anyway makes little difference to \nthe implications of this research (see below). The publishers claimed that their reading \nsoftware was \u2018award winning\u2019, and that if 11 to 15 year-olds work on this program for \none hour a day, spread over six weeks, the program will quickly improve their reading \nskills including single word reading, sentence reading and non-word reading. It also \nreportedly improves reading speed, reading fluency, vocabulary, comprehension and \nreading stamina. The software is multi-sensory in nature, combining vision, sound and \ntouch. It allows pupils to progress at their own pace, with consistent and immediate \nfeedback, and progress is measured. It was designed to be used in conjunction with \nstandard reading exercises, based on the National Literacy Strategy in England.  More \nthan 100 starter texts are provided, including poems, tales, recipes, articles, descriptions, \nletters, points of view, instructions, and official documents, and new content can be \nadded via authoring tools. The standard of difficulty of each exercise, and the look and \nfeel of the program can be adapted to suit the pupil, making it suitable for all ages and \nabilities including children with learning challenges. It was aligned with National \nCurriculum standards, developed with guidance from some of the leading reading experts, \nand grounded in the most current research on literacy, using a carefully structured \nwhole\/part\/whole approach to reading instruction. It has customised professional \ndevelopment ranging from CD-ROM and online courses to on-site workshops. A \ncomprehensive Teacher\u2019s Guide with activities and lesson plans are included in the \npackage. Free technical support is available during office hours.  \n \nThe number of computers available for use, and access to them, were discussed with the \nICT technicians in all participating schools. We found that all the schools had enough \n 7\ncomputers and headphones for the treatment group pupils to use during the trial. Trial \norientation sessions were made available to all teachers, head teachers, and school \ngovernors in the participating schools before the start of the study. A detailed description \nof the software and its learning activities, the timelines, study purpose, research questions, \nand expectations relating to participation were provided to all participating teachers. In \nearly September 2006 an agreed final trial procedure was displayed on school notice \nboards, and a copy of the procedure was provided for all of the teachers involved in the \ntrial. Teachers were provided with log books in which to keep records of implementation, \nand notes on progress of the trial. To ensure that the treatment implementation was as \nintended by the software publisher, the researcher tested the software installation on both \nstand-alone and networked computers in all treatment classes.  \n \nOngoing technical support was agreed with the software publisher for the period of the \ntrial. All treatment teachers received software training about how to use the software \nfrom consultants sent by the publisher. The training included a demonstration of the most \neffective ways of using the software. Teachers were provided with a copy of all materials. \nThey were trained to view individual pupils\u2019 feedback, assigned a code for the software \npublisher\u2019s records, and given access to the consultant as well as a telephone number to \nuse if they had any technical problems with the software or with any of the associated \nactivities.  \n \nThe treatment group used the computer software for a designated time on three to four \ndays each week. In most of the classes, the treatment group was monitored by the teacher \nwhile they were working on the computer software. A time was allocated at which pupils \nwere to go to computer laboratories before the intervention started. Headphones were \nsupplied for every pupil to counter distraction, thereby maximising the pupils\u2019 attention. \n \nThe software, the treatment schedule and the training all encouraged teachers to help \npupils complete all of the learning activities provided by the software, over the ten weeks \nof implementation. The software itself automatically logged the records of each activity \ncompleted by each pupil and class. The results appear in Table 2. Most pupils in all \n 8\nclasses completed the bulk of the activities. However, there is some variation, as might be \nexpected in a pragmatic trial. This could be due to differences in ability of the pupils, \nboth in literacy and competence in the use of the software, differences in the length of \nclasses per week, and partly because of the expressed preferences of pupils for some \nactivities over others. The class labeled 1 here had some technical difficulties with their \ncomputer system early in the term. This issue is discussed further below.  \n \nTable 2 \u2013 Percentage of each of 11 classes completing each activity, according to \nguidelines \nClasses 1 2 3 4 5 6 7 8 9 10 11 \nActivity 1 50 65 96 94 90 100 100 100 100 100 100 \nActivity 2 45 45 68 96 100 90 79 95 90 90 100 \nActivity 3 76 76 88 89 97 80 81 79 90 90 100 \nActivity 4 97 90 90 100 100 81 65 96 90 94 100 \nActivity 5 79 76 89 80 90 76 100 90 81 100 90 \n \nAssessing literacy levels \n \nBoth groups were given a pre-test of their existing literacy levels in the first week of \nSeptember 2006. An equivalent post-test was given to both groups after ten weeks of \nteaching in December 2006. The assessment was computer-based, perhaps thereby \nfavouring the treatment group slightly, and the items tested were directly linked to the \nmaterial covered in the treatment software activities. The software used for assessment \nwas the Lucid Assessment System for Schools (LASS secondary, see http:\/\/www.lucid-\nresearch.com\/sales\/esales.htm?category_id=31&product_id=183). This looked at eight \nrelated reading skills, forming a suite of three attainment tests (single-word reading, \nsentence reading and spelling), one ability test (reasoning), and four diagnostic tests \n(auditory memory, visual memory, phonic skills and phonological processing).  \n \nThree of the eight tests (sentence reading, spelling, and reasoning) were adaptive, based \non statistical item response theory, where each test item is selected from a large bank of \n 9\nitems of known difficulty for 11 to 15 year old pupils. The remaining tests are \nprogressive in format and utilise a graded series of items of increasing difficulty for 11 to \n15 year old pupils. For each test, instructions are spoken by the computer, and practice \nitems are given to familiarise the pupil with the test requirements. The LASS software \nclaims to conform to the British Psychological Society\u2019s guidelines for the development \nand use of computer based assessments. Calibration was originally carried out using a \nrepresentative sample of UK pupils, with ages ranging from 11 to 15 years, and \nsubsequently with pupils aged 8 to 11. There is a reasonable correlation between the \nLASS tests and the NFER Sentence Completion Test of reading comprehension. Each of \nthe tests shows reasonably high internal consistency between items (around 0.9), and also \ntest-retest reliability (around 0.75).  \n \nThe LASS software generated test results in raw and z-score form, with percentiles and \nage-expected equivalents. The z-scores were converted to standard scores, by adding 100 \nto 15 times the z-score. The total of the eight standard scores, with an expected mean of \n800, were then used to calculate changes over time or between groups.  \n \nFurther data collection \n \nFor any research design, but perhaps especially for a trial, it is important to collect more \nthan the presumed outcome data (Gorard with Taylor 2004). In-depth and contextual data \ncan help explain why an intervention does or does not work, how to improve it, or which \nsub-groups of learners it is most appropriate for.  \n \nBefore the trial, we gathered the age, sex, free school meal eligibility and ethnic origin of \nall pupils involved (as these measures would appear on the pupil-level annual schools \ncensus in England).  \n \nA template was developed for the study in conjunction with the software publisher, in \norder to record classroom observations. This template was used by teachers to summarise \npupil learning activities, their use of software, and the general environment of the ICT \n 10\nrooms. Implementation logs were developed for the treatment teachers to record the pupil \nactivities (such as games, activities, units and sections covered in their ICT laboratories \nand classes), and information about the way in which activities were delivered and any \ntechnical problems associated with the software. The teachers were requested to log the \ninformation regarding the number of software activities started and completed by the \npupils, the type of software learning activities used by the pupils, the role of the teachers \nduring the treatment session, and the number of pupils absent from the sessions. The \nresearch team visited all schools on a regular basis during the 10 weeks, and ensured that \nno other instructional software was used in the treatment group classes. \n \nUnstructured discussions about the trial, lasting 20 to 30 minutes, took place with \nteachers, head teachers, and school governors. Their chief purpose with the treatment \ngroup was to find out if the software was considered effective in improving reading skills \nand, if so, why it was effective and what actually made it effective. All respondents also \ndiscussed the attitude of pupils towards using the software. For the control group one of \nthe purposes was to find out about any other software used.  \n \nAnalysis \n \nIn this paper, we present the overall results from the pre- and post-tests of reading skills. \nThese are the mean (and standard deviation) scores for each group. We present a \nstandardised improvement for each group, calculated as the gain from pre- to post-test \ndivided by the overall standard deviation at the pre-test. Some concerns have been \nexpressed at the use of this standard deviation in cluster trials (White and Thomas 2005). \nHowever, this concern is about comparability with individual trials in a meta-analysis \u2013 a \nrather esoteric topic \u2013 and anyway only serves to warn us that the improvement of both \ngroups might be underestimated. As is made clear below, such an underestimation for \nboth groups would make no difference to the substantive findings here.  We also \ncalculate the standard \u2018effect\u2019 size as the difference between the gain scores for the \ntreatment and control, divided by the pooled (average) post-test standard deviation of \nboth groups. Although our calculation does not, in itself, identify any cause:effect model, \n 11\nour clear intention was to design the study so that any major difference between the \ngroups could be attributed to the intervention.  \n \nWe make mention of other results, such as the gains and effect sizes for sub-groups of \npupils such as boys and girls separately. We also present an overall picture of the \nunstructured interview and observation data. These will all be dealt with more fully in \nanother paper. For more on trial design and analysis see the resources available via \nhttp:\/\/www.tlrp.org\/capacity\/rcbn.html. \n \n \nThe overall findings \n \nAt the outset of the trial, the pre-test scores show that both groups had similar \nstandardised mean scores and deviations (Table 3). Both means were above the expected \nmean score of 800 (see above). The treatment group was very slightly superior at this \nstage. After 10 weeks of software use in literacy lessons the treatment group improved \ntheir standardised mean score substantially, just as the software publishers had claimed. \nTherefore, a simple before and after design with no control could easily, but falsely, \nconclude that the use of commercial software was an especially effective approach to \nliteracy teaching and learning. This illustrates again the danger for educational research \nof conclusions drawn from what constitutes the majority of published work, conducted \nwithout suitable comparators (see examples in Gorard et al. 2007).  \n \nTable 3 \u2013 Pre- and post-test scores for both groups \n Pre-test mean Pre-test SD Post-test mean Post-test SD \nTreatment 823 68 863 88 \nControl 817 72 886 78 \nN=655 \n \nAs Table 3 illustrates, the control group exposed to standard practice in literacy lessons, \nno routine access to computers, and no access to the treatment software, also improved \n 12\ntheir standardised mean score substantially. There is no prime facie case here that the \nimprovement for the treatment group was due to the software used. In fact, it would be \neasier to mount an argument that pupils using the software were disadvantaged. Insofar as \nthe methods used in this study are accepted, we have shown that the software package \nused was ineffective in comparison to standard practice. This is an important conclusion, \nwith wider implications than might be imagined at first sight. The implications are \ndiscussed at the end of the paper. Another noticeable finding in Table 3 is the increase in \nthe standard deviation relative to the mean score for the treatment group at post-test \ncompared both to pre-test and to the control group. This means that the eventual results \nfor the treatment group were more varied, and raises the possibility that the treatment was \nless effective for only some pupils.  \n \nThe gain score for each group in the trial is converted to a standardised improvement, as \ndescribed above. This shows even more clearly the greater improvement for pupils in the \ncontrol group (Table 4). The precise figure used as the standard deviation for calculating \nthe effect size does not matter. The substantive findings are the same whether the overall \nstandard deviation (71) is used, as here, or whether each group uses its own pre- or post-\ntest standard deviation (see Table 3). In all cases, the improvement score for the control \ngroup is nearly twice as large as for the treatment group. This translates into an effect size \nof -0.37 (or 30\/81).  \n \nTable 4 \u2013 Gain scores for both groups \n Gain score Overall pre-test SD Standardised \nimprovement \nTreatment (319) 40 71 0.56 \nControl (346) 70 71 0.99 \nN=655 \n \nIntriguingly, the in-depth data collected routinely as part of the trial suggested a high \nlevel of satisfaction with the treatment. The technology-based instruction reportedly \nprovided teaching groups with a range of information, links and activities in an accessible \n 13\nand entertaining way. Nine of the 11 teachers involved in the treatment said that the \nsoftware had an encouraging focus on language for early Key Stage 3 and that the \nactivities were stimulating for pupils and teachers alike. They believed that it offered a \nreliable way to help pupils improve their reading skills. The pupils were satisfied with the \ntechnology-based reading materials, and were observed getting heavily involved in the \nactivities. When asked, all teachers indicated that they would use the same or similar \nsoftware in the future, and almost all of them said that they would recommend it to other \nteachers.  \n \nOne limitation that was repeated by teachers was that the software did not cover \neverything the teachers would have wanted to cover in the term it was used. Several \nteachers reported minor technical problems with the software or more often with their \nschool computer systems, and one class (class 1 above) apparently wasted several lessons \nbecause of a school computer network failure. \n \n \nDiscussion \n \nThe trial design eliminates several alternative interpretations of our results but, as with \nany piece of research, it is important to consider the warrant for our claim that the \nsoftware was ineffective in comparison to standard practice (Gorard 2002a). If that \nconclusion were not, in fact, true how else could we explain our findings?  \n \nThe class that had extended difficulty with their computer system had a low gain score \nwhich contributed in part to the poor showing of the treatment to the control. However, it \ndoes not explain all of the difference. It certainly does not suggest that the treatment was \nmore effective than the control. And in a pragmatic trial we expect these differences. \nSuch technical difficulties are a real consideration for technology-based instruction and \nshould not be \u2018cleaned\u2019 away to enhance the apparent effect of the treatment, any more \nthan poor teaching of the control would be ignored on the basis that it \u2018should not \nhappen\u2019. \n 14\n The sample of 23 classes with 655 pupils could be considered quite small, although the \ntrial is one of the larger ones of its kind in the UK. Brooks et al. (2006), for example, had \nan achieved sample of only 130 pupils. Perhaps it was not of the scale (did not have the \n\u2018power\u2019) to demonstrate the advantage of the software. But there are two reasons why we \nsuggest this is not so. First, as our results show this is not really an issue of power. It is \nnot that the treatment group did slightly better but that we cannot be sure whether the \ndifference is statistically \u2018significant\u2019 (with all of the flaws inherent in such a claim, \nGorard 2010). The treatment group did substantially worse. Second, the sample size is \nlarger than most studies of this type, and is an order of magnitude higher than the samples \nused by the publisher to test the software initially. Third, and most obviously, there is an \nasymmetry in the burden of scientific proof. The publishers claim that the software will \nbe more effective than standard approaches (else why would anyone buy it?). We tested it \nand found no evidence that it was more effective. Until someone does another controlled \nstudy with randomisation of treatment but showing a different result, we must assume \nthat this treatment is ineffective.  \n \nSchools had classes in both the treatment and control conditions, and this can lead to \n\u2018contamination\u2019 whereby treatment pupils share their new approach with control pupils, \nso reducing the apparent effect size of the treatment. This is a standard concern in cluster \nrandomised trials using teaching groups, and is partly handled by intention to treat \nanalysis. More specifically, the treatment was based on content presented using the \ntechnology, was not made available in the system to other pupils (login name and \npassword-protected), and it is unlikely that the treatment pupils would remember the \ndetails of the activities or discuss them in detail during free time. It is not like passing \nover a pill, or lending a textbook. Contamination is still possible, but this could not \nexplain the substantially lower level of progress made by the treatment group. The \nintervention was also more natural than in some prior studies, such as Brooks et al. (2006) \nwhere hardware shortages meant some distortion of the way in which pupils were \ngrouped in classes.  \n \n 15\nThe control group started from a slightly lower base score. In some tests this would give \nthem more room for improvement. This could perhaps explain some of the greater gain, \nbut does not explain the larger absolute score for the control in post-test. Pupils declaring \na non-White ethnicity were very slightly more prevalent in the control, and showed \nhigher gains in both groups. But the scale of these small differences cannot explain the \nresults. If non-White pupils are excluded from analysis the result stands. There were \nmore boys than girls in the control, but their gains were equivalent. Only boys in the \ntreatment group showed higher gains than girls. This is an interesting result, perhaps due \nto boys\u2019 expressed preference for using ICT in lessons, and we will discuss the in-depth \nresults in more detail in future papers. For the present, we simply note that the group-\nlevel result works against explaining why the treatment group showed such small gains.  \n \nOur result depends heavily on the validity of the test. If this did not test the learning \noutcomes fairly, then our conclusion is in doubt. We conducted lengthy face validity \nchecks, comparing the material in the tests with that in the exercises and found a good \nmatch. We were interested to note that our result would hold for any one of the eight \nindividual tests (such as spelling) as well as for the overall scores reported here. And the \nfact that the test was computer-based should not disadvantage the treatment group more \nused to handling such material on the computer (rather the reverse). Finally, and perhaps \nmore importantly for a pragmatic trial, the software publisher accepted the test as valid \nbefore the start. \n \nThe test, and the trial it supports, is only intended to take into account the eight reading \nskills involved. There may be unmeasured gains in other respects \u2013 beneficial unintended \nconsequences perhaps. But there might also be unmeasured harm caused by the treatment. \nWe do not know. We could, of course, have conducted other trials \u2013 perhaps comparing \nstandard classes to a blended approach of ICT and traditional teaching, or comparing the \nsoftware classes to having no classes at all. All of these might generate different results. \nBut what are the implications of what we did find? \n \n \n 16\nImplications \n \nThe simplest and most obvious implication of the study is that this software, for these \nclasses, at this stage, was not effective. In fact, it was markedly less effective than \nstandard classroom practice not involving technology. Unfortunately, like most \nreasonably secure claims in the social science of policy or practice, this implication is of \nlimited practical value since it is retrospective. We are naturally in sympathy with the \ndemand of Brooks et al. (2006) that \u2018Before schools adopt ICT packages for teaching \nspelling or other literacy skills, these need to be evaluated in randomised controlled trials\u2019 \n(p.142). But we do not think this is always feasible. Our fieldwork was conducted in the \nacademic year 2006\/07, and at time of writing the specific version of the software used in \nthe trial is no longer marketed. With ICT and education, in particular, Moore\u2019s law means \nthat once a longitudinal study is complete the technology tested will have been \nsuperseded. By the time this paper is published, even those schools which had already \npurchased the precise version of the software tested will likely have moved to a new \nproduct. There is no obvious way around this limitation that would also apply, for \nexample, to rigorous evaluation of a national policy. Does this mean, then, that we should \nnot bother to conduct longitudinal evaluations of this kind? \n \nWe would argue that evaluations such as these have a wider purpose, and more general \nimplications. Most interventions in education are not rigorously evaluated before \nimplementation. Where they are evaluated post hoc, many are found to have been \nineffective or worse (see Gorard 2006 for examples). It is then too late for any such \nintervention, and the missed opportunities and even harm they may have caused to \nlearners. But as in the fuller design science process (Gorard and Cook 2007), we can use \nwhat we have learnt about the nature of effective and ineffective interventions to help \ndesign better ones for the future, to make formative \u2018corrections\u2019 to live implementations, \nand to provide clear guidelines about the kinds of claims that can legitimately be made \nabout any educational artefact (like a piece of software). We suggest some of these in the \nfollowing paragraphs. \n \n 17\nMany educational programs have in-built assessment and record-keeping applications. \nSoftware publishers then make their marketing research claims based on these records. \nThe problem is that in most of the cases these findings are not based on a comparison \nwith anything else. So, for example, the software publisher claims about the effectiveness \nof their product should be tempered by a caution that no suitable comparator was used, or \nperhaps all such claims should only be allowed by advertising standards when a suitable \ncomparator has been used. Otherwise, teachers and educational authorities are in danger \nof being duped by a claim that is the same in real terms as the kind that is rightly banned \nin the medical literature. For example, since the common cold is of short duration we are \nnot allowed to advertise a pill that is 100% effective in clearing up colds after one week. \n \nAnother important implication is the warning against over-reliance on impressions about \nthe treatment efficacy, as reported by people involved in any intervention. The \noverwhelming view of staff and pupils involved in the treatment group was that learning \nwas proceeding well, that pupils were better motivated and enjoying lessons, and teachers \nhad been freed to deal with specific literacy difficulties within their classes. These reports \nare valuable in themselves, and open up possible areas to investigate ways in which \ntechnology-based instruction could lead to other beneficial outcomes. But the reports do \nnot agree with the results of the central testable claim made by the publishers that use of \nthe software will lead to enhanced performance in assessment of literacy skills after just \nsix weeks. If our results are accepted, then many of the impressions of staff and pupils \nabout the efficacy of the product were incorrect, just as the publisher claims were found \nto be incorrect.  \n \nMore generally, the results of this study add to the body of research that shows concerns \nabout the effectiveness of technology-based instruction. Several prior studies have shown \nlimited or no beneficial impact of ICT-based approaches on pupil learning in reading \nskills (as here) and in other core subjects such as mathematics (see above). But in \nisolation these studies do not reveal the scale of wasted opportunities and possible harm \ndone to pupils. In general, marketing teams in the UK offer software to schools on a trial \nbasis. During the trial they show how pupils are making progress by using the in-built \n 18\nassessment process (without appropriate comparator). Teachers can then see pupil \nprogress over learning activities and may be persuaded to purchase. Once teachers have \nbought the software they tend to use the convenient in-built assessment process regularly. \nThis makes all involved part of a reinforcing cycle. The software publishers make money. \nTeachers have a record of progress made by pupils, for their own and others satisfaction. \nPupils generally enjoy working on computers and playing with different technology \napplications. Parents will be pleased to see a record of their child\u2019s progress. Local and \nnational government is content that their funding of technology initiatives is justified, and \nschools are persuaded to spend that funding on technology products, making money for \nthe companies to develop new products.  \n \nOur final suggested implication concerns the future of evaluation research. This new \nstudy demonstrates again the feasibility of classroom-level randomized controlled trials \n(and similar active rigorous research designs). It is possible for a small team or even a \nlone researcher to conduct a trial of this scale within one year. The cost of this unfunded \nstudy was minimal, mostly related to travel between schools for the fieldwork. \nExperimental designs are not inherently expensive. Most of the cost of an intervention \nstudy is for the intervention, which means that pragmatic evaluation of an intervention \nthat is already scheduled to take place costs almost nothing additionally. No special \nethical or practical issues arose. There was a prime facie case for implementing the \nintervention, which would have taken place in schools without the research anyway. It \ncannot be unethical to deny the intervention to the control when the treatment was \ninferior anyway. And it cannot be ethical to permit an inferior and costly treatment to be \nrolled out without testing (Gorard 2002b). Relatively small trials such as this one are an \nimportant part of the mixed methods necessary to conduct education research. If \neducation researchers really care for their ultimate charges (the learners) then, as part of \ntheir wider approach, they will embrace this simple, cheap and ethical way of evaluating \nthe many interventions taking place routinely in schools. \n \n \nAcknowledgements \n 19\n Thanks go to all schools agreeing to participate in this study. Special thanks go to the \npublishers of the literacy software and assessment software used, for their generosity in \nsupporting this study through provision of software and with technical support \nthroughout.  \n \n \nReferences \n \nAdams, M. and Bruck, M. (1993) Word recognition: The interface of educational Policies \nand scientific research, Reading and Writing: An interdisciplinary Journal, 5, pp. \n113-139 \nAndrews, R., Dan, H., Freeman, A., McGuinn, N., Robinson, A. and Zhu, D. (2005) The \neffectiveness of different ICTs in the teaching and learning of English (written \ncomposition) 5\u201316, Research Evidence in Education Library, London: EPPI-Centre, \nSocial Science Research Unit, Institute of Education, University of London, \nhttp:\/\/eppi.ioe.ac.uk\/EPPIWeb\/home.aspx?&page=\/reel\/reviews.htm, accessed \n26\/3\/06 \nAngrist J. and Lavy, V. (2002) New evidence on classroom computers and pupil \nlearning, The Economic Journal, 112, pp. 735\u2013765 \nBaron, D. (2001) From pencils to pixels: the stages of literacy technologies, pp. 70-84 in \nLiteracy: A Critical Sourcebook. Boston, MA: Bedford\/St. Martin's \nBlok, H., Oostdam, R., Otter, M., and Overmaat, M. (2002) Computer-assisted \ninstruction in support of beginning reading instruction: a review, Review of \nEducational Research, 72, 1, 101-130 \nBorman, G., Benson, J. and Overman, L. (2009) A randomised field trial of the Fast \nForWord Language computer-based training program, Educational Evaluation and \nPolicy Analysis, 31, 82-106 \nBoruch, R., De Moya, R. and Snyder, B. (2002) The importance of randomised field \ntrials in education and related areas, pp. 50-79 in F. Mosteller and R. Boruch (Eds.) \n 20\nEvidence Matters: Randomised Trials in Education Research, Brookings, \nWashington DC \nBrooks, G., Cole, P., Davies, P., Davis, B., Frater, G., Harman, J. and Hutchison, D. \n(2002) Keeping Up with the Children, Evaluation for the Basic Skills Agency by \nthe University of Sheffield and the National Foundation for Educational Research. \nLondon: Basic Skills Agency \nBrooks, G., Miles, J., Torgerson, C. and Torgerson, D. (2006) Is an intervention using \ncomputer software effective in literacy learning? A randomised controlled trial, \nEducational Studies, 32, 2, 133-43 \nCox, M., Abbott, C., Webb, M., Blakeley, B., Beauchamp, T. and Rhodes, V. (2003) ICT \nand pedagogy, A review of the research literature, ICT in Schools Research and \nEvaluation Series No. 18, Coventry\/London: Becta\/DfES, \nhttp:\/\/www.becta.org.uk\/page_documents\/research\/ict_pedagogy_summary.pdf, \naccessed 10\/2\/05 \nCunningham, M., Kerr, K., McEune, R., Smith, P. and Harris, S. (2004) Laptops for \nTeachers, an Evaluation of the First Year of the Initiative,, ICT in Schools Research \nand Evaluation Series No. 19. Coventry\/London: Becta\/DfES, \nhttp:\/\/www.becta.org.uk\/page_documents\/research\/lft_evaluation.pdf, accessed \n18\/12\/06 \nDepartment for Education and Employment (1999) Radical change needed to boost basic \nskills. A briefing paper on the report \u2018A Fresh Start - Improving Literacy and \nNumeracy\u2019, Skills and Enterprise briefing, Issue 5\/99, London: Department for \nEducation and Employment \nDfES (2003) Towards a unified learning e-learning strategy, London: HMSO, \nhttp:\/\/www.dfes.gov.uk\/consultations\/downloadableDocs\/towards%20a%20unified\n%20e-learning%20strategy.pdf, accessed 22\/3\/05 \nDynarski, M., Agodini, R., Heaviside, S., Novak, T., Carey, N., Campuzano, L., et al. \n(2007) Effectiveness of reading and mathematics software products: findings from \nthe first pupil cohort, (Publication No. 2007-4005), Washington, DC: U.S, \nDepartment of Education, Institute of Education Sciences, available from \nhttp:\/\/ies.ed.gov\/ncee\/pdf\/20074005.pdf \n 21\nETI (2003) An evaluation by the education and training inspectorate of information and \ncommunication technology in post-primary schools 2001\u20132002, Education and \nTraining Inspectorate, \nhttp:\/\/www2.deni.gov.uk\/inspection_services\/surveys\/index.htm, accessed 9\/1\/05 \nGood, R., Simmons, D., and Smith, S. (1998), \u201cEffective academic interventions in the \nUnited States: Evaluating and enhancing the acquisition of early reading skills.\u201d \nSchool Psychology Review, vol. 27, pp. 45-56. \nGoolsbee, A. and Guryan, J. (2005) The impact of internet subsidies for public schools, \nReview of Economics and Statistics, 88, 2, 36-347 \nGorard, S. (2002a) Fostering scepticism: the importance of warranting claims, Evaluation \nand Research in Education, 16, 3, 136-149 \nGorard, S. (2002b) Ethics and equity: pursuing the perspective of non-participants, Social \nResearch Update, 39, 1-4 \nGorard, S. (2006) Does policy matter in education?, International Journal of Research \nand Method in Education, 29, 1, 5-21 \nGorard, S. (2010) All evidence is equal: the flaw in statistical reasoning, Oxford Review \nof Education, iFirst, pp.1-15 \nGorard, S. and Cook, T. (2007) Where does good evidence come from?, International \nJournal of Research and Method in Education, 30, 3, 307-323 \nGorard, S., with Adnett, N., May, H., Slack, K., Smith, E. and Thomas, L. (2007) \nOvercoming barriers to HE, Stoke-on-Trent: Trentham Books \nGorard, S., with Taylor, C. (2004) Combining methods in educational and social \nresearch, London: Open University Press \nGreenleaf, C. and Petrosino, A. (2008) Response to Slavin, Cheung, Groff and Lake, \nReading Research Quarterly, 43, 4, 349-354 \nHirsch, D. (2007) Chicken and Egg: child poverty and educational inequalities, London: \nCampaign to End Child Poverty, http:\/\/www.endchildpoverty.org.uk\/index.html, \naccessed 14\/9\/07 \nLei, J., and Zhao, Y. (2005) Technology uses and pupil achievement: A longitudinal \nstudy, Computers and Education, 49, pp. 284\u2013296 \n 22\nNational Literacy Trust (2010) Are children\u2019s literacy skills improving or getting worse, \nhttp:\/\/www.literacytrust.org.uk\/about\/faqs\/filter\/about%20literacy%20in%20the%2\n0uk#q713, accessed July 2010 \nNational Reading Campaign Survey (2005) Making every home a reading home, \nhttp:\/\/www.literacytrust.org.uk\/press\/FRC.html, accessed 28\/2\/06 \nOFSTED (2004) ICT in schools: the impact of government initiatives, School Portraits \nEggbuckland Community College, London: Ofsted, \nwww.ofsted.gov.uk\/publications\/index.cfm?fuseaction=pubs.displayfile&id=3704&\ntype=pdf, accessed 26\/3\/06 \nPelgrum, W. (2001) Obstacles to the integration of ICT in education: results from a \nworldwide educational assessment, Computers and Education, 37, pp. 163-178 \nPikulski, J. and Chard, D. (2005) Fluency: Bridge between decoding and comprehension, \nThe Reading Teacher, 58, 6, 510-519 \nPIRLS (2007) PIRLS 2006 reading achievement, \nhttp:\/\/news.bbc.co.uk\/1\/hi\/education\/7117231.stm, accessed 3\/9\/09 \nPittard, V, Bannister, P and Dunn, J (2003) The big pICTure: The impact of ICT on \nattainment, motivation and learning, London: DfES, \nhttp:\/\/www.dfes.gov.uk\/research\/data\/uploadfiles\/ThebigpICTure.pdf, accessed \n22\/11\/05 \nRose, D. and Dalton, B. (2002) Using technology to individualize reading instruction, pp. \n257-274 in C. Block, L. Gambrell and M. Pressley (Eds.) Improving comprehension \ninstruction: Rethinking research, theory, and classroom practice, San Francisco: \nJossey Bass Publishers \nRouse, C., and Krueger, A. (2004) Putting computerized instruction to the test: A \nrandomised evaluation of a \u201cscientifically-based\u201d reading program, Economics of \nEducation Review, 23, pp. 323\u2013338 \nSilverstein, G., Frechtling, J. and Miyoaka, A. (2000) Evaluation of the use of technology \nin Illinois public schools: Final report (prepared for Research Division, Illinois \nState Board of Education), Rockville, MD: Westat \nSivin-Kachala, J., and Bialo, E. (2000) 2000 research report on the effectiveness of \ntechnology in schools (7th ed.), Washington DC: Software and Information Industry \n 23\nSlavin, R., Cheung, A., Groff, C. and Lake, C. (2008) Effective reading programs for \nMiddle and High Schools: A best-evidence synthesis, Reading Research Quarterly, \n43, 3, 290-322 \nTorgerson C. and Zhu D. (2003) A systematic review and meta-analysis of the \neffectiveness of ICT on literacy learning in English, 5-16, Research Evidence in \nEducation Library. London: EPPI-Centre, Social Science Research Unit, Institute of \nEducation, University of London \nWaxman, H., Lin, M., and Michko, G. (2003) A meta-analysis of the effectiveness of \nteaching and learning with technology on pupil outcomes, North Central Regional \nEducational Laboratory Web site: http:\/\/www.ncrel.org\/tech\/effects2\/waxman.pdf, \naccessed 28\/2\/06 \nWhite, I. and Thomas, J. (2005) Standardized mean differences in individually-\nrandomized and cluster-randomized trials, with applications to meta-analysis, \nClinical Trials, 2, 141,151 \nWolf, M. and Katzir-Cohen, T. (2001) Reading fluency and its intervention, Scientific \nStudies of Reading, 5, 3, 211-239 \n \n 24\n"}