{"doi":"10.1186\/1748-5908-5-20","coreId":"177203","oai":"oai:aura.abdn.ac.uk:2164\/788","identifiers":["oai:aura.abdn.ac.uk:2164\/788","10.1186\/1748-5908-5-20"],"title":"Statistical considerations in a systematic review of proxy measures of clinical behaviour","authors":["Dickinson, Heather O","Hrisos, Susan","Eccles, Martin P","Francis, Jill","Johnston, Marie"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":["University of Aberdeen, Medicine, Medical Sciences & Nutrition, Institute of Applied Health Sciences"],"datePublished":"2010-02","abstract":"Peer reviewedPublisher PD","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:aura.abdn.ac.uk:2164\/788<\/identifier><datestamp>\n                2018-01-02T00:03:38Z<\/datestamp><setSpec>\n                com_2164_632<\/setSpec><setSpec>\n                com_2164_364<\/setSpec><setSpec>\n                com_2164_330<\/setSpec><setSpec>\n                com_2164_705<\/setSpec><setSpec>\n                col_2164_633<\/setSpec><setSpec>\n                col_2164_706<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nStatistical considerations in a systematic review of proxy measures of clinical behaviour<\/dc:title><dc:creator>\nDickinson, Heather O<\/dc:creator><dc:creator>\nHrisos, Susan<\/dc:creator><dc:creator>\nEccles, Martin P<\/dc:creator><dc:creator>\nFrancis, Jill<\/dc:creator><dc:creator>\nJohnston, Marie<\/dc:creator><dc:contributor>\nUniversity of Aberdeen, Medicine, Medical Sciences & Nutrition, Institute of Applied Health Sciences<\/dc:contributor><dc:subject>\nRA Public aspects of medicine<\/dc:subject><dc:subject>\nRA<\/dc:subject><dc:description>\nPeer reviewed<\/dc:description><dc:description>\nPublisher PDF<\/dc:description><dc:date>\n2010-12-20T10:54:01Z<\/dc:date><dc:date>\n2010-12-20T10:54:01Z<\/dc:date><dc:date>\n2010-02<\/dc:date><dc:type>\nJournal article<\/dc:type><dc:identifier>\nDickinson , H O , Hrisos , S , Eccles , M P , Francis , J & Johnston , M 2010 , ' Statistical considerations in a systematic review of proxy measures of clinical behaviour ' Implementation Science , vol 5 , 20 . DOI: 10.1186\/1748-5908-5-20<\/dc:identifier><dc:identifier>\n1748-5908<\/dc:identifier><dc:identifier>\nPURE: 1649848<\/dc:identifier><dc:identifier>\nPURE UUID: 620aeff7-c3ea-49cd-ac35-bb23761affe7<\/dc:identifier><dc:identifier>\nScopus: 77949351798<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/2164\/788<\/dc:identifier><dc:identifier>\nhttp:\/\/dx.doi.org\/10.1186\/1748-5908-5-20<\/dc:identifier><dc:language>\neng<\/dc:language><dc:relation>\nImplementation Science<\/dc:relation><dc:format>\n8<\/dc:format>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":[{"title":null,"identifiers":["issn:1748-5908","1748-5908"]}],"language":{"code":"en","id":9,"name":"English"},"relations":["Implementation Science"],"year":2010,"topics":["RA Public aspects of medicine","RA"],"subject":["Journal article"],"fullText":"RESEARCH ARTICLE Open Access\nStatistical considerations in a systematic review of\nproxy measures of clinical behaviour\nHeather O Dickinson1*, Susan Hrisos1*, Martin P Eccles1, Jill Francis2, Marie Johnston3\nAbstract\nBackground: Studies included in a related systematic review used a variety of statistical methods to summarise\nclinical behaviour and to compare proxy (or indirect) and direct (observed) methods of measuring it. The objective\nof the present review was to assess the validity of these statistical methods and make appropriate\nrecommendations.\nMethods: Electronic bibliographic databases were searched to identify studies meeting specified inclusion criteria.\nPotentially relevant studies were screened for inclusion independently by two reviewers. This was followed by\nsystematic abstraction and categorization of statistical methods, as well as critical assessment of these methods.\nResults: Fifteen reports (of 11 studies) met the inclusion criteria. Thirteen analysed individual clinical actions\nseparately and presented a variety of summary statistics: sensitivity was available in eight reports and specificity in\nsix, but four reports treated different actions interchangeably. Seven reports combined several actions into\nsummary measures of behaviour: five reports compared means on direct and proxy measures using analysis of\nvariance or t-tests; four reported the Pearson correlation; none compared direct and proxy measures over the\nrange of their values. Four reports comparing individual items used appropriate statistical methods, but reports that\ncompared summary scores did not.\nConclusions: We recommend sensitivity and positive predictive value as statistics to assess agreement of direct\nand proxy measures of individual clinical actions. Summary measures should be reliable, repeatable, capture a\nsingle underlying aspect of behaviour, and map that construct onto a valid measurement scale. The relationship\nbetween the direct and proxy measures should be evaluated over the entire range of the direct measure and\ndescribe not only the mean of the proxy measure for any specific value of the direct measure, but also the range\nof variability of the proxy measure. The evidence about the relationship between direct and proxy methods of\nassessing clinical behaviour is weak.\nBackground\nOver the past 15 years, there has been a concerted move\nto encourage the practice of evidence-based medicine\n[1]. The implementation of evidence-based recommen-\ndations and clinical guidelines often needs changes in\nthe behaviour of healthcare professionals. Evaluation of\nthe effectiveness of initiatives to change clinical beha-\nviour requires valid measures of such behaviour, which\nare relevant to policy-makers, practitioners, and\nresearchers.\nClinical practice can be measured by direct observa-\ntion, which is generally considered to provide an\naccurate reflection of the observed behaviour and there-\nfore represent a \u2018gold standard\u2019 measure. However,\ndirect measures can be intrusive and can alter the beha-\nviour of the individuals being observed, placing signifi-\ncant limitations on their use in any other than small\nstudies. As they are also time-consuming and costly,\nthey are not always a feasible option. Measurement of\nclinical behaviour has therefore commonly relied on\nindirect measures, including review of medical records\n(or charts); clinician self-report, and patient report.\nHowever, the extent to which these proxy measures of\nclinical behaviour accurately reflect a clinician\u2019s actual\nbehaviour is unclear. In a separate systematic review, we\nassessed the validity of proxy measures for directly\nobserved clinical behaviour [2]. The included studies\n* Correspondence: heather.dickinson@ncl.ac.uk; susan.hrisos@ncl.ac.uk\n1Institute of Health and Society, Newcastle University, 21 Claremont Place,\nNewcastle upon Tyne, NE2 4AA, UK\nDickinson et al. Implementation Science 2010, 5:20\nhttp:\/\/www.implementationscience.com\/content\/5\/1\/20\nImplementation\nScience\n\u00a9 2010 Dickinson et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative\nCommons Attribution License (http:\/\/creativecommons.org\/licenses\/by\/2.0), which permits unrestricted use, distribution, and\nreproduction in any medium, provided the original work is properly cited.\nused a variety of statistical methods both to summarize\nclinical behaviour and to compare proxy and direct\nmeasures. The estimated agreement between direct and\nproxy measures varied considerably not only between\ndifferent clinical actions but also between studies. It\nseems unlikely that all the methods used will have simi-\nlar validity: some of the heterogeneity in findings may\nbe due to inappropriate statistical methods. The plan-\nning of future studies would benefit from an evaluation\nof the range of approaches used. The objective of the\npresent paper is to evaluate the validity of the statistical\nmethods used by these studies and to recommend the\nmost appropriate methods.\nMethods\nIn a companion systematic review [2], evidence was\nsynthesised from empirical, quantitative studies that\ncompared a measure of the behaviour of clinicians (doc-\ntors, nurses, and allied health professionals) based on\ndirect observation (standardised patient, trained obser-\nver, or video\/audio recording) with a proxy measure\n(retrospective self-report; patient-report; or chart-review)\nof the same behaviour. The review searched PsycINFO,\nMEDLINE, EMBASE, CINAHL, Cochrane Central Reg-\nister of Controlled Trials, Science\/Social science citation\nindex, Current contents (social and behavioural med\/\nclinical med), ISI conference proceedings, and Index to\nTheses for studies that met the inclusion criteria. All\ntitles, abstracts, and full text articles retrieved by electro-\nnic searching were screened for inclusion, and data were\nabstracted independently by two reviewers. Disagree-\nments were resolved by discussion with a third reviewer\nwhere necessary.\nAll the studies identified as meeting the inclusion cri-\nteria for the review based their measures of behaviour\non whether a clinician had performed one or more clini-\ncal actions, e.g., prescribing a specific drug, ordering a\nspecific test, asking a patient whether s\/he smoked.\nHence, clinical actions were recorded as binary (yes\/no)\nvariables, which we refer to as \u2018items\u2019. Several studies\ncompared direct and proxy values of items, but others\ncombined items into summary scores that were treated\nas continuous variables and then compared the sum-\nmary scores based on direct and proxy measures. So, for\nthe purposes of assessing the statistical methods, we\ndivided the methods used into those that compared\nitems and those that compared summary scores.\nItem by item comparisons\nWe noted whether studies reported the sensitivity, spe-\ncificity, positive predictive value, or negative predictive\nvalue of the proxy measure (see Table 1); we noted any\nalternative methods used to summarise the relationship\nbetween direct and proxy measures.\nComparisons of summary scores\nA proxy measure of behaviour will not be a consistent\nsurrogate for a direct measure of behaviour unless both\nthe proxy and direct measures are valid. The companion\nreview assessed the face and content validity and relia-\nbility of these measures [2]. Here, we assessed four\naspects of the statistical validity of the measures.\nBias and variability\nWe noted whether studies reported the average relation-\nship between direct and proxy measures, described over\nthe entire range of possible values of the measures, and\nthe variability around the average relationship, e.g., by a\nBland and Altman plot [3-7] or a regression line, regres-\nsing the direct on the proxy measure, with a prediction\ninterval [7].\nFor all studies (comparing both items and summary\nscores), we also assessed the following:\n1. Estimation or hypothesis testing: We noted whether\nstudies treated comparisons between direct and proxy\nmeasures largely as estimation or hypothesis testing; we\nassumed that reporting of p-values indicated the latter.\n2. Confidence intervals and clustering: We noted\nwhether studies reported confidence intervals on statis-\ntics summarising the relationship between direct and\nproxy measures, and allowed for clustering of consulta-\ntions within clinicians.\nResults\nFifteen reports of eleven studies met the inclusion cri-\nteria [8-22]; three of these studies were reported in\nmore than one publication [10,11,9,19,8,12,16], but these\npublications used different statistical methods to com-\npare direct and proxy measures, so they are considered\nseparately.\nStudy designs\nAll included reports (except [13]) used identical check-\nlists and scoring procedures to rate both direct and\nproxy measures of behaviour. The number of items per\nconsultation considered by each report ranged from one\n[13] to 79 [19] (see Table 2). Thirteen reports compared\nthe direct and proxy measures item by item\nTable 1 Statistics summarising validity of binary (yes\/no)\nmeasures of behaviour\nDirect measure\nProxy measure: YES NO TOTAL\nYES a b a + b\nNO c d c + d\nTOTAL a + c b + d T = a + b + c + d\nThe sensitivity of the proxy measures is defined as: a\/(a+c); its specificity is as:\nd\/(b+d); its positive predictive value as a\/(a+b); and its negative predictive\nvalue as d\/(c+d).\nDickinson et al. Implementation Science 2010, 5:20\nhttp:\/\/www.implementationscience.com\/content\/5\/1\/20\nPage 2 of 8\nTable 2 Statistical methods used in the included papers to compare direct and proxy measures of behaviour\nReport ni nj nk Statistics used Notes\nItem-by-item comparisons: items treated as distinct\nFlocke, 2004[9] 10 19 138\nStange, 1998[19] 79 32 138\nWard, 1996[20] 2 26 41 Sensitivity = a\/(a + c)\nWilson, 1994[21] 3 20 16\nZuckerman, 1975[22] 15 17 3\nStange, 1998[19] 79 32 138\nWard, 1996[20] 2 26 41\nWilson, 1994[21] 3 20 16 Specificity = d\/(b + d)\nZuckerman, 1975[22] 15 17 3\nDresselhaus, 2000*[8]\nGerbert, 1988[11]\nPbert, 1999*[15]\nRethans, 1987*[18]\nWilson, 1994[21]\n7\n4\n15\n24\n3\n8\n3\n9\n1\n20\n20\n63\n12\n25\n16\nAgreement: comparison of:\n(i) (a + b)\/T, and (ii) (a + c)\/T\nAgreement was assessed by comparing the proportion\nof recommended behaviours performed as measured\nby the direct and proxy measures. Three reports\nperformed hypothesis tests, using analysis of variance\n[8], Cochran\u2019s Q-test [15], and McNemar\u2019s test [18].\nGerbert, 1988*[11]\nPbert, 1999*[15]\nStange, 1998[19]\n4\n15\n79\n3\n9\n32\n63\n12\n138\nkappa = 2(ad - bc)\/{(a + c)\n(c + d) + (b + d)(a + b)}\nAll three reports used kappa-statistics to summarise\nagreement; two reports [11,15] also used them for\nhypothesis testing.\nGerbert, 1988[11] 4 3 63 Disagreement = (i) c\/T (ii) b\/T\n(iii) (b + c)\/T\nDisagreement was assessed as the proportion of items\nrecorded as performed by one measure but not by the\nother.\nItem-by-item comparisons: items treated as interchangeable within categories of behaviour\nLuck, 2000[12] NR 8 20\nPage, 1980 [14] 16-17 1 30 Sensitivity = a\/(a + c)\nRethans, 1994[17] 25-36 3 35\nLuck, 2000[12]\nPage, 1980[14]\nNR 8\n1\n20\n30\nSpecificity = d\/(b + d)\nGerbert, 1986[10]\nPage, 1980[14]\n20\n16-17\n3\n1\n63\n30\nConvergent validity = (a + d)\/T Convergent validity was assessed as the proportion of\nitems showing agreement.\nComparisons of summary scores for each consultation: summary scores were the number (or proportion) of recommended items performed\nLuck, 2000*[12] NR 8 20 Analysis of variance to compare means of scores on\ndirect measure and proxy.\nPbert, 1999*[15] 15 9 12\nRethans, 1987*[18] 24 1 25 S xjk ijk\ni\n\uf03d \uf0e5 Paired t-tests to compare means of scores on direct\nmeasure and proxy.\nPbert, 1999*[15] 15 9 12 Pearson correlation of the scores on direct measure\nand proxy.\nComparisons of summary scores for each clinician: summary scores were the number (or proportion) of recommended items performed\nO\u2019Boyle, 2001[13] 1 NA 120 Comparison of means of scores on direct measure and\nproxy.\nO\u2019Boyle, 2001*[13] 1 NA 120 S xk ijk\ni j\n\uf03d \uf0e5\n,\nPearson correlation of scores on direct measure and\nproxy.\nRethans, 1994*[17] 25-36 3 25\nComparisons of summary scores for each consultation: summary scores were weighted sums of the number of recommended items\nperformed\nPeabody, 2000*[16] 21 8 28 Analysis of variance to compare means of scores on\ndirect measure and proxy.S xjk i ijk\ni\n\uf03d \uf0e5\uf077\nPage, 1980*[14] 16-17 1 30 Pearson correlation of scores on direct measure and\nproxy.\na, b, c, d, T are defined in Table 1; i = item, j = consultation, k = physician, ni = average number of items per consultation, nj = average number of consultations\nper clinician; nk = average number of clinicians assessed; \u03c9i = weight for i\nth item; xijk = 0 if item is not performed; xijk = 1 if item is performed;.\nNR = Not reported; NA = Not applicable.\n* This study used this method for hypothesis testing.\nDickinson et al. Implementation Science 2010, 5:20\nhttp:\/\/www.implementationscience.com\/content\/5\/1\/20\nPage 3 of 8\n[8-12,14,15,17-22]; seven reports combined the items\ninto summary scores for direct and proxy measures,\nwhich were then compared [12-15,17,18]; three reports\nused both methods [14,15,18].\nReports comparing items\nSeven reports [8,9,11,19-22] did not attempt to combine\nitems in any way. Two reports [15,18] analysed items\nboth as separate items and also after amalgamation into\na summary score for each consultation (see below).\nReports comparing items, but treating items as\ninterchangeable within categories of behaviour\nFour reports treated different items interchangeably\nwithin specific categories: necessary, unnecessary beha-\nviours [12]; assessing symptoms, assessing signs, order-\ning laboratory tests, delivering treatments, delivering\npatient education [10]; must do, should do, could do,\nshould not do, must not do actions [14]; taking a his-\ntory, performing a physical examination, ordering\nlaboratory examinations, giving guidance and advice,\ndelivering medication and therapy, specifying follow-up\n[17].\nReports combining items into summary scores for each\nconsultation\nFour reports constructed summary scores, essentially\ndefined as the number of recommended items that were\nperformed, for each consultation, using both the direct\nand proxy measures [12,14-16]. Two of these reports\n[14,16] weighted the items to reflect their perceived\nimportance.\nOne further report constructed summary scores for\neach consultation by category of item: obligatory, inter-\nmediate, and superfluous [18]. This study had only one\nconsultation\/clinician, so its summary score could\nequally well be regarded as describing the clinician or\nthe consultation.\nReports combining items into summary scores for each\nclinician\nTwo reports constructed summary scores for each\nclinician, using both the direct and proxy measures.\nOne report recorded only one item (hand washing)\nand constructed a summary score for each clinician by\ncalculating the number of times the item was per-\nformed in a two-hour period as a proportion of the\nnumber of times it should have been performed [13].\nThe other report recorded a clinician\u2019s behaviour on\nseveral items in up to four consultations and con-\nstructed a summary score for each clinician by sum-\nming the number of recommended items performed in\nall consultations [17].\nStatistical methods used to compare direct and proxy\nmeasures\nTable 1 summarises the statistical methods used in the\nincluded papers to compare direct and proxy measures\nof behaviour.\nItem by item comparisons\nSix reports presented sensitivity [9,12,17,19-21], and a\nfurther two presented sufficient data to allow calculation\nof the sensitivity [14,22]. Three reports presented speci-\nficity [12,19,20], one report [21] presented the propor-\ntion of false positives (1-specificity); and two reports\npresented sufficient data to allow calculation of the spe-\ncificity [14,22]. However, some of these reports treated\nitems describing different clinical actions as interchange-\nable within broad categories of behaviour [12,14,17]. No\nreports presented the positive or negative predictive\nvalues.\nFive reports presented agreement [8,11,15,18,21] based\non the percentage of recommended behaviours per-\nformed as measured by the direct and proxy measures.\nThree of these reports tested the null hypothesis that\nthese proportions were the same, using either analysis of\nvariance [8], Cochran\u2019s Q-test [15] or McNemar\u2019s test\n[18]. Both Cochran\u2019s Q-test and McNemar\u2019s test evalu-\nate the hypothesis that the proportions positive on the\ndirect measure and proxy are the same but, unlike\nMcNemar\u2019s test, Cochran\u2019s Q-test can be used for tables\nwith more than two methods of measuring behaviour\n[23]\nThree reports presented kappa-statistics [11,15,19] to\nsummarise agreement; two of these reports [11,15] also\nused them to test the null hypothesis that there was no\nmore agreement between the methods than would be\nexpected by chance.\nOne report presented disagreement [11] measured as:\nthe proportion of items recorded as performed by the\ndirect measure but not by the proxy measure; the pro-\nportion of items recorded as not performed by the\ndirect measure but recorded as performed by the proxy\nmeasure; and the total of these.\nTwo reports presented \u2018convergent validity\u2019 [10,14],\ndefined as the total number of items showing agreement\n(either present\/present or absent\/absent) on the two\nmeasures, as a proportion of the total number of items\nrecorded by either measure. Both reports treated items\ndescribing different clinical actions as interchangeable.\nOne report [10] calculated the convergent validity sepa-\nrately for each of 20 items in each consultation, assigned\nitems to one of five categories, and presented the med-\nian convergent validity within each category as well as\noverall; the other report [14] pooled items within five\ncategories and then calculated the convergent validity.\nDickinson et al. Implementation Science 2010, 5:20\nhttp:\/\/www.implementationscience.com\/content\/5\/1\/20\nPage 4 of 8\nInter-rater reliability was reported in six of the thir-\nteen studies that compared measures item-by-item\n[14,17-21]; it ranged from 0.39 to 1.0.\nComparisons of summary scores\nAll seven reports that compared summary scores used\nhypothesis testing. Three reports used analysis of var-\niance or t-tests to test the null hypothesis that the mean\nscores from direct and proxy measures were the same\n[12,16,18]; three reports used the Pearson correlation to\ntest the hypothesis that the scores were not correlated\n[13,14,17]; one report used both methods [15].\nNone of the reports plotted the data to compare direct\nand proxy measures or used any other method of show-\ning how the direct and proxy were related over the\nentire range of their values or the variability in their\nrelationship.\nInter-rater reliability was reported in four of the seven\nstudies that compared summary scores [13,14,17,18] it\nranged from 0.76 to 1.0.\nDiscussion\nBased on a companion systematic review of proxy mea-\nsures of clinical behaviour [2], we further reviewed the\nwide range of statistical methods used in the included\nstudies to compare proxy and direct measures of beha-\nviour. We now discuss these statistical methods and\nthen go on to make recommendations. Although our\nreview was not, in principle, limited to measures based\non binary (yes\/no) items, all included papers used this\napproach. Because some papers compared items directly,\nand others compared scores based on combining item\nresponses, we structure our discussion to reflect these\ntwo approaches.\nItem-by-item comparisons\nIn the current context, sensitivity answers the question:\nWhat proportion of actions that were actually per-\nformed and recorded by direct observation were identi-\nfied by the proxy? The positive predictive value answers\nthe question: What proportion of actions that were\nflagged by the proxy as having been performed were\nrecorded by direct observation as performed? Specificity\nand negative predictive values address similar questions,\nbut about actions that were not performed.\nFor single item comparisons, reporting of sensitivity\nand specificity is an appropriate way to assess the per-\nformance of a proxy [9,19-22], although thought needs\nto be given to which of these measures is most relevant\nto the clinical context and the research question, or\nwhether both measures are required, or whether the\npositive (and\/or negative) predictive value may be more\ninformative. The positive and negative predictive values\nhave the disadvantage that they vary with the prevalence\nof actual behaviour and so will vary between populations\n[24].\nHowever, it is doubtful whether it is appropriate to\nestimate sensitivities and specificities based on a combi-\nnation of items describing different clinical actions\n[10,12,14,17]. For example, it seems questionable\nwhether it is valid to combine actions to review drugs\nand to discuss smoking cessation [10], or actions to ask\nthe patient about the radiation of pain and to ask their\noccupation [12], or actions to apply a sling and to refer\nto a physiotherapist [17]. Combining items assumes that\ntheir proxy measures have the same underlying sensitiv-\nity and specificity, which may not be true. The validity\nof this assumption could be assessed and items com-\nbined only if their sensitivities and specificities were\nsimilar.\nAssessment of \u2018agreement\u2019 by comparison of the pro-\nportion of items performed that were identified by the\ndirect measure and proxy [8,11,15,18,21] is inappropri-\nate because, unlike the sensitivity, it gives no indication\nof whether an item recorded as performed on the direct\nmeasure is likewise recorded as performed on the proxy.\nIt is possible to have perfect agreement even if the\ndirect and proxy measures record completely different\nitems as performed. For example, the percentages\nrecorded as performed by a direct measure and by the\nproxy can both be 50%, even if the sensitivity, specificity,\npositive and negative predictive value are all zero (e.g., if\na = d = 0 and b = c = 50; see Table 1). Furthermore,\nassessment of \u2018agreement\u2019 treats the direct and proxy\nmeasures as having equal validity, which may not neces-\nsarily be the case as either measure may pose validity\nproblems.\nSome reports [11,15,19] used kappa-statistics to quan-\ntify levels of agreement between direct and proxy\nmeasures. Although it is sometimes claimed that the\nkappa-statistic gives a \u2018chance-corrected\u2019 measure of\nagreement between two measures, it has been argued\nthat this is misleading because the measures are clearly\nnot independent [25]. Two of these reports [11,15] also\nused kappa-statistics to test the hypothesis that there is\nno more agreement between direct and proxy measures\nthan might occur by chance. This is not very informa-\ntive, since the measures are dependent by definition\nbecause they are rating the same behaviour. Kappa-sta-\ntistics also share the flaws of other measures of correla-\ntion (the Pearson correlation and the intra-class\ncorrelation) for assessing agreement between methods\nof measurement: they assume that the two methods to\nbe compared are interchangeable, whereas we usually\nregard the direct measure as being closer to the true\nvalue than the proxy; and their value is influenced by\nthe range of measurement, with a wider range giving a\nhigher correlation [26].\nDickinson et al. Implementation Science 2010, 5:20\nhttp:\/\/www.implementationscience.com\/content\/5\/1\/20\nPage 5 of 8\nThe same criticisms apply to assessment of \u2018disagree-\nment\u2019[11]. The \u2018convergent validity\u2019 [10] assumes that\nnot performing specific actions has the same importance\nas performing them, which may or may not be true\ndepending on the situation.\nNone of the reports allowed for clustering of items\nwithin clinicians, for example by using a multi-level\nmodel [27]. It is likely that there will be correlation of\nitems within clinicians as actions performed by one clin-\nician are likely to be more similar to each other than to\nactions performed by other clinicians. Failure to allow\nfor this lack of independence of items is likely to result\nin spuriously precise estimates of sensitivity, specificity,\nand other summary statistics. Unfortunately, none of\nthese reports presented confidence intervals on any of\nthe summary statistics.\nRecommended methods to compare direct and proxy\nmeasures item by item\nIndividual items may be assessed for face and content\nvalidity by a group of subject matter experts. Their relia-\nbility may be assessed using a random or systematic\nsample of clinicians selected from a regional or national\nsampling frame [2]. If the focus of interest is actions\nthat were performed, then the sensitivity and positive\npredictive value are appropriate statistics for comparing\ndirect and proxy measures item-by-item. The proxy\nmeasure should have a high sensitivity and a high posi-\ntive predictive value, such that it detects most actions\nthat were performed and most actions that it flags as\nperformed were actually performed. If actions that were\nnot performed are also of interest, then the specificity\nand negative predictive value are also required. Items\nthat assess different actions should not be treated as if\nthey were interchangeable, unless they have been shown\nto have similar diagnostic properties.\nComparisons of summary scores\nIndividual items may function as either indicator vari-\nables or as causal variables [28,29]. Indicator variables\nare determined by an unobservable, underlying concept:\nfor example, the responses to items in an intelligence\ntest are assumed to be determined by an underlying\nlevel of ability, and so they are expected to be corre-\nlated. In contrast, causal variables jointly determine an\nunobserved construct. For example, socio-economic sta-\ntus may be determined jointly by education, income,\nneighbourhood, and occupational prestige; an increase\nin any of these might increase socio-economic status,\nbut we would not expect these indicators to be corre-\nlated. The methods used to combine items into scores\ndepend on whether the items are regarded as indicator\nvariables or causal variables. Item response theory,\nincluding Rasch models, may be applied to indicator\nvariables [30,31], but is inappropriate for causal vari-\nables, for which a range of methods have been proposed\n[28]. None of the included reports contained any discus-\nsion of whether the items were regarded as causal or\nindicator variables, although two reports [14,16]\nweighted items to reflect their importance.\nSeveral reports compared the means of summary\nscores [12,13,15,16,18], which is inadequate for assess-\nment of agreement. First, even if the means of the direct\nand proxy measures are similar, it cannot be assumed\nthat they agree for all values of the direct measure. Sec-\nond, the means do not give enough information to pre-\ndict the direct measure from a value of the proxy.\nThird, comparison of means does not tell us anything\nabout the variability of the proxy measure for any speci-\nfic value of the direct measure. Finally, it is possible for\nsummary scores to have the same value for direct mea-\nsure and proxy measures, even if the responses to the\nindividual items are very different.\nSome reports calculated summary scores for each con-\nsultation [12-16,18], whereas other reports averaged the\nconsultation scores for each clinician in order to obtain\na score for the clinician [17]. Simply averaging over con-\nsultations does not allow for the correlation of actions\nby the same clinician (discussed above): methods such\nas multi-level modelling are required [27]. However, one\nreport claimed, on the basis of analysis of variance, that\nthere was no significant effect of clustering within clini-\ncians [15].\nSeveral reports used methods based on a linear model-\nanalysis of variance [12,15,16], t-tests [18], or correlation\n[13-15,17]-to assess agreement. These methods assume\nthat the outcome of interest is continuous and normally\ndistributed. This is not strictly valid when the outcome\nis the proportion of items performed, as proportions\nhave discrete values and a binomial distribution,\nalthough in many cases, the inferences that are made\nmay still be valid.\nAnalysis of variance assesses how the mean value of a\nvariable is affected by the classification of the data [23].\nIt compares the variation between groups (in this case,\nmeasurements by direct and proxy methods) with the\nvariation within groups, in order to assess whether the\nmean values differ in different groups. Although this\nmethod has the advantage that it can allow for other\nfactors which might affect differences between methods,\ne.g., disease, case complexity, physician training level,\nand hospital sites, it is essentially a method of testing\nthe hypothesis that means are the same in different\ngroups, which is inappropriate for the reasons given\nbelow. T-tests are a special case of analysis of variance\nand share its disadvantages.\nThe Pearson correlation measures the strength of lin-\near association between two variables, and therefore\nDickinson et al. Implementation Science 2010, 5:20\nhttp:\/\/www.implementationscience.com\/content\/5\/1\/20\nPage 6 of 8\ngives a measure of the average variability in their rela-\ntionship [23]. If a scatter plot of the two variables shows\nthat all the points lie on a straight line, the Pearson cor-\nrelation has the value of one (or minus one); but if the\npoints show a lot of scatter, the Pearson correlation has\na value between zero and one (or minus one). However,\nit has the disadvantage that it does not assess bias: for\nexample, two measures can have perfect correlation\n(equal to one) even if one measure is consistently twice\nthe other measure [5,7]. Furthermore, the Pearson cor-\nrelation depends on the range of the variables: if there is\nindeed a linear relationship between the variables, then\na wider range of variation of behaviours will result in a\nhigher correlation coefficient [3,5,7].\nAll of the reports that compared summary scores used\nhypothesis testing. Assessment of agreement between\ntwo measures is a problem of estimation, not hypothesis\ntesting [3]. Estimation can predict the value that one\nmeasure (the direct measure) is likely to take, if the\nvalue of the other measure (the proxy) is known.\nHypothesis testing aims to aid decision-making about\nwhether the observed data provide evidence that a parti-\ncular hypothesis (e.g., that two values are the same) is\nunlikely to be true. Hypothesis testing and estimation\nmay lead to different conclusions: for example, if there\nis a wide range of variability in each measure, hypothesis\ntesting is likely to lead to a conclusion that the proxy\nand direct measure are similar, whereas estimation\nwould tend to indicate that the proxy may be a poor\npredictor of the direct measure.\nRecommended methods to compare proxy and direct\nmeasure summary scores\nMeasures that summarise several items should be reli-\nable, repeatable, capture a single underlying aspect of\nbehaviour, and measure that construct using a valid\nmeasurement scale. Once such direct and proxy mea-\nsures have been constructed, the relationship between\nthem should be evaluated over their entire range, first\nby a simple plot of one measure against the other [4,5].\nThe next step will depend on whether the direct mea-\nsure can be regarded as an error-free \u2018gold standard\u2019. In\nthe studies included in our review, inter-rater reliability\nwas good for direct measures based on simulated\npatients [14,17,18], suggesting that these measures had\nlittle error, but direct measures based on audio or video\nrecording were more prone to errors [2].\nIf we want to assess agreement between two methods\nof measurement, neither of which can be regarded as\nestimating the true value of the quantity measured,\nBland and Altman have recommended that the differ-\nence between two measures should then be plotted\nagainst their mean. This allows visual assessment of\nboth systematic bias and of variation [3-7].\nAlternatively, if one measure can be regarded as error-\nfree, and interest centres mainly on whether it shows a\nconsistent, predictable relationship with the proxy mea-\nsure, the problem is one of calibration rather than\nassessment of agreement [4,7]. This relationship can\nthen be captured by use of regression [6]: the regression\nline captures the average relationship between the mea-\nsures, and it is possible to construct a 95% prediction\ninterval that shows, for each value of the proxy measure,\nthe range within which the values of the direct measure\nfor an individual clinician (or consultation) are likely\nto lie.\nThis use of regression has some intrinsic weaknesses.\nFirst, as the proxy is inevitably measured with some\nerror, the relationship between the direct and proxy\nmeasures will almost certainly show regression to the\nmean [32,33], thus underestimating high values of the\ndirect measure and overestimating low values. Second,\nregression assumes that the amount of variation in the\nproxy scores does not depend on the value of direct\nmeasure, which was not true in the studies included in\nthis review. The summary scores used in included stu-\ndies had a limited range, e.g., 0 to100, so the variation in\nthe proxy score tended to be smaller if the direct scores\nwere closer to the extremes. This could lead to spurious\nprecision in estimates of the regression line and its pre-\ndiction interval. Such an effect would be more marked\nfor scores based on fewer items or with larger standard\ndeviations. Third, as noted above for analysis of variance\nand correlation, the assumption that the summary score\nis continuous and normally distributed is not valid.\nFinally, the relationship between direct and proxy mea-\nsures may not be linear over their entire range: non-lin-\nearity can be assessed by inspection of the plot or, more\nformally, by testing the effect of adding a quadratic term\nto the regression. Alternatives to a regression approach\ninclude item response theory [31] (if it is assumed that\nthe items are indicator variables) or multiplicative utility\nformulae or structural equation modelling (if it is\nassumed that the items are causal variables) [28,34].\nConclusions\nThe fifteen reports analysed in this review used a variety\nof methods to construct direct and proxy measures of\nclinical behaviour and to compare them. Four reports of\nfour studies that compared individual items [9,19-21]\nused appropriate statistical methods-sensitivity and spe-\ncificity-to do so. However, the reports that combined\nitems into summary scores focused on comparing\nmeans of these scores, whereas it would have been more\ninformative to describe the average relationship between\ndirect and proxy scores and the variability around that\naverage over the entire range of the scores. The paucity\nof this evidence and the heterogeneity of clinical\nDickinson et al. Implementation Science 2010, 5:20\nhttp:\/\/www.implementationscience.com\/content\/5\/1\/20\nPage 7 of 8\nbehaviours limit the conclusions that can be made about\nthe relationship between direct and proxy methods of\nassessing clinical behaviour.\nAcknowledgements\nWe thank Professor Eileen Kaner for help with reviewing articles and data\nabstraction.\nAuthor details\n1Institute of Health and Society, Newcastle University, 21 Claremont Place,\nNewcastle upon Tyne, NE2 4AA, UK. 2Health Services Research Unit,\nUniversity of Aberdeen, Health Sciences Building, Foresterhill, Aberdeen,\nAB25 2ZD, UK. 3Department of Psychology, University of Aberdeen, Health\nSciences Building, Foresterhill, Aberdeen, AB25 2ZD, UK.\nAuthors\u2019 contributions\nAll authors contributed to the conception and design of the study. HD\ndrafted the manuscript. All authors read and approved the submitted draft.\nHD, ME, JF, and SH reviewed the articles and abstracted the data.\nCompeting interests\nMPE is Co-Editor in Chief of Implementation Science. All editorial decisions\non this manuscript were made by Co-Editor in Chief Brian Mittman.\nReceived: 10 February 2009 Accepted: 26 February 2010\nPublished: 26 February 2010\nReferences\n1. Guyatt G, Cook D, Haynes B: Evidence based medicine has come a long\nway. [Editorial]. BMJ 2004, 329:990-991.\n2. Hrisos S, Eccles MP, Francis J, Dickinson HO, Kaner EF, Beyer F, Johnston M:\nAre there valid proxy measures of clinical behaviour? A systematic\nreview. Implementation Science 2009, 4:37.\n3. Altman DG: Section 14.2 Method comparison studies. Practical statistics\nfor medical research London: Chapman & Hall 1991, 396-403.\n4. Altman DG, Bland JM: Measurement in medicine: the analysis of method\ncomparison studies. The Statistician 1983, 32:307-317.\n5. Bland JM, Altman DG: Statistical methods for assessing agreement\nbetween two methods of clinical measurement. Lancet 1986, i:307-310.\n6. Bland JM, Altman DG: Measuring agreement in method comparison\nstudies. Statistical Methods in Medical Research 1999, 8:135-160.\n7. Bland JM, Altman DG: Applying the right statistics: analyses of\nmeasurement studies. Ultrasound in Obstetrics and Gynecology 2003,\n22:85-93.\n8. Dresselhaus TR, Peabody JW, Lee M, Wang MM, Luck J: Measuring\ncompliance with preventive care guidelines: standardized patients,\nclinical vignettes, and the medical record. Journal of General Internal\nMedicine 2000, 15:782-788.\n9. Flocke SA, Stange KC: Direct observation and patient recall of health\nbehavior advice. Prev Med 2004, 38:343-349.\n10. Gerbert B, Hargreaves WA: Measuring physician behavior. Medical Care\n1986, 24:838-847.\n11. Gerbert B, Stone G, Stulbarg M, Gullion DS, Greenfield S: Agreement\namong physician assessment methods. Searching for the truth among\nfallible methods. Medical Care 1988, 26:519-535.\n12. Luck J, Peabody JW, Dresselhaus TR, Lee M, Glassman P: How well does\nchart abstraction measure quality? A prospective comparison of\nstandardized patients with the medical record. American Journal of\nMedicine 2000, 108:642-649.\n13. O\u2019Boyle C, Henly S, Larson E: Understanding adherence to hand hygiene\nrecommendations: the theory of planned behavior. Am J Infect Control\n2001, 29:352-360.\n14. Page GG, Fielding DW: Performance on PMPs and performance in\npractice: are they related? J Med Educ 1980, 55:529-537.\n15. Pbert L, Adams A, Quirk M, Herbert JR, Ockene JK, Luippold RS: The patient\nexit interview as an assessment of physician-delivered smoking\nintervention: a validation study. Health Psychol 1999, 18:183-188.\n16. Peabody JW, Luck J, Glassman P, Dresselhaus TR, Lee M: Comparison of\nvignettes, standardized patients, and chart abstraction: a prospective\nvalidation study of 3 methods for measuring quality. JAMA 2000,\n283:1715-1722.\n17. Rethans JJ, Martin E, Metsemakers J: To what extent do clinical notes by\ngeneral practitioners reflect actual medical performance? A study using\nsimulated patients. British Journal of General Practice 1994, 44:153-156.\n18. Rethans JJ, van Boven CPA: Simulated patients in general practice: a\ndifferent look at the consultation. British Medical Journal 1987,\n294:809-812.\n19. Stange KC, Zyzanski SJ, Smith TF, Kelly R, Langa DM, Flocke SA, Jaen CR:\nHow valid are medical records and patient questionnaires for physician\nprofiling and health services research? A comparison with direct\nobservation of patients visits. Medical Care 1998, 36:851-867.\n20. Ward J, Sanson-Fisher R: Accuracy of patient recall of opportunistic\nsmoking cessation advice in general practice. Tobacco Control 1996,\n5:110-113.\n21. Wilson A: Comparison of patient questionnaire, medical record, and\naudio tape in assessment of health promotion in general practice\nconsultations. BMJ 1994, 309:1483-1485.\n22. Zuckerman ZE, Starfield B, Hochreiter C, Kovasznay B: Validating the\ncontent of pediatric outpatient medical records by means of tape-\nrecording doctor-patient encounters. Pediatrics 1975, 56:407-411.\n23. Armitage P, Berry G, Matthews JNS: Statistical methods in medical research\nOxford: Blackwell Science, 4 2002.\n24. Altman DG: Section 14.4 Diagnostic tests. Practical statistics for medical\nresearch London: Chapman & Hall 1991, 409-419.\n25. Uebersax JS: Diversity of decision-making models and the measurement\nof interrater agreement. Psychological Bulletin 1987, 101:140-146.\n26. Bland JM, Altman DG: A note on the use of the intraclass correlation\ncoefficient in the evaluation of agreement between two methods of\nmeasurement. Comput Biol Med 1990, 20:337-340.\n27. Goldstein H: Multilevel statistical models London: Arnold, 3 2003.\n28. Fayers P, Hand D: Causal variables, indicator variables and measurement\nscales: an example from quality of life. Journal of the Royal Statistical\nSociety Series A - Statistics in Society 2002, 165:233-253.\n29. Bollen K, Lennox R: Conventional wisdom on measurement - a structural\nequation perspective. Psychological Bulletin 1991, 110:305-314.\n30. Bond TG, Fox CM: Applying the Rasch Model: fundamental measurement in\nthe human sciences London: Lawrence Erlbaum Associates 2001.\n31. Skrondal A, Rabe-Hesketh S: Generalized latent variable modelling: multilevel,\nlongitudinal, and structural equation models London: Chapman & Hall 2004.\n32. Bland JM, Altman DG: Regression towards the mean. British Medical\nJournal 1994, 308:1499.\n33. Bland JM, Altman DG: Some examples of regression towards the mean.\nBritish Medical Journal 1994, 309:780.\n34. Torrance G, Feeny D, Furlong W, Barr R, Zhang Y, Wang Q: Multiattribute\nutility function for a comprehensive health status classification system -\nHealth Utilities Index Mark 2. Medical Care 1996, 34:702-722.\ndoi:10.1186\/1748-5908-5-20\nCite this article as: Dickinson et al.: Statistical considerations in a\nsystematic review of proxy measures of clinical behaviour.\nImplementation Science 2010 5:20.\nSubmit your next manuscript to BioMed Central\nand take full advantage of: \n\u2022 Convenient online submission\n\u2022 Thorough peer review\n\u2022 No space constraints or color figure charges\n\u2022 Immediate publication on acceptance\n\u2022 Inclusion in PubMed, CAS, Scopus and Google Scholar\n\u2022 Research which is freely available for redistribution\nSubmit your manuscript at \nwww.biomedcentral.com\/submit\nDickinson et al. Implementation Science 2010, 5:20\nhttp:\/\/www.implementationscience.com\/content\/5\/1\/20\nPage 8 of 8\n"}