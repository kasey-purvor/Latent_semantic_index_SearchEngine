{"doi":"10.1080\/0968776970050108","coreId":"14323","oai":"oai:generic.eprints.org:231\/core5","identifiers":["oai:generic.eprints.org:231\/core5","10.1080\/0968776970050108"],"title":"The impact of educational technology: A radical reappraisal of research methods","authors":["Mitchell, P. David"],"enrichments":{"references":[{"id":200556,"title":"Address to Convocation,","authors":[],"date":"1988","doi":null,"raw":"Beer, S. (1988), Address to Convocation, Concordia University, Montreal. Bradley, J.V. (1982), Distribution-Free Statistical Tests, Englewood Cliffs, NJ: PrenticeHall.","cites":null},{"id":200558,"title":"Falsification and the methodology of scientific research programmes'","authors":[],"date":"1978","doi":"10.1017\/cbo9780511621123.003","raw":"Lakatos, I. (1978), 'Falsification and the methodology of scientific research programmes' in Worrall, J. and Currie, G. (eds.), The Methodology of Scientific Research Programs, Philosophical Papers, vol. 1: Imre Lakatos, Cambridge: CUP.","cites":null},{"id":200563,"title":"Learning style: a critical analysis of the concept and its assessment'","authors":[],"date":"1994","doi":null,"raw":"Mitchell, P.D. (1994), 'Learning style: a critical analysis of the concept and its assessment' in Hoey, R. (ed.), Aspects of Educational Technology XXVII, London: Kogan Page.","cites":null},{"id":200557,"title":"Mitchell The impart of educational technology: a radical reappraisal of research methods","authors":[],"date":"1988","doi":null,"raw":"53P. David Mitchell The impart of educational technology: a radical reappraisal of research methods Krauth, J. (1988), Distribution-Free Statistics, Amsterdam: Elsevier.","cites":null},{"id":200564,"title":"On the theory of scales of measurement',","authors":[],"date":"1946","doi":"10.1126\/science.103.2684.677","raw":"Stevens, S.S. (1946), 'On the theory of scales of measurement', Science, 103, 677-80.","cites":null},{"id":200560,"title":"Statistical significance in psychological research',","authors":[],"date":"1968","doi":"10.1037\/h0026141","raw":"Lykken, D.T. (1968), 'Statistical significance in psychological research', Psychological Bulletin, 70, 151-9.","cites":null},{"id":200562,"title":"The origins and aims of epistemics',","authors":[],"date":"1972","doi":"10.1007\/bf00053968","raw":"Meredith, P. (1972), 'The origins and aims of epistemics', Instructional Science, 1 (1), 16.","cites":null},{"id":200561,"title":"Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology',","authors":[],"date":"1978","doi":"10.1037\/\/0022-006x.46.4.806","raw":"Meehl, P. (1978), 'Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology', Journal of Consulting and Clinical Psychology, 46 (4), 822.","cites":null},{"id":200559,"title":"Using a hypertext environment for teaching process writing: an evaluation study of three student groups',","authors":[],"date":"1995","doi":"10.1007\/bf02300471","raw":"Lohr, L., Ross, S.M. and Morrison, G.R. (1995), 'Using a hypertext environment for teaching process writing: an evaluation study of three student groups', Educational Technology Research and Development, 43 (2), 33-51.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"1997","abstract":"How can we decide whether some new tool or approach is valuable? Do published results of empirical research help? This paper challenges strongly entrenched beliefs and practices in educational research and evaluation. It urges practitioners and researchers to question both results and underlying paradigms. Much published research about education and the impact of technology is pseudo\u2010scientific; it draws unwarranted conclusions based on conceptual blunders, inadequate design, so\u2010called measuring instruments that do not measure, and\/or use of inappropriate statistical tests. An unacceptably high portion of empirical papers makes at least two of these errors, thus invalidating the reported conclusions","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14323.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/231\/1\/ALT_J_Vol5_No1_1997_The%20impact%20of%20educational%20tech.pdf","pdfHashValue":"27148b85e400eaea0af91c7424360af946becb65","publisher":"Universit of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:231<\/identifier><datestamp>\n      2011-04-04T09:22:11Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/231\/<\/dc:relation><dc:title>\n        The impact of educational technology: A radical reappraisal of research methods<\/dc:title><dc:creator>\n        Mitchell, P. David<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        How can we decide whether some new tool or approach is valuable? Do published results of empirical research help? This paper challenges strongly entrenched beliefs and practices in educational research and evaluation. It urges practitioners and researchers to question both results and underlying paradigms. Much published research about education and the impact of technology is pseudo\u2010scientific; it draws unwarranted conclusions based on conceptual blunders, inadequate design, so\u2010called measuring instruments that do not measure, and\/or use of inappropriate statistical tests. An unacceptably high portion of empirical papers makes at least two of these errors, thus invalidating the reported conclusions.<\/dc:description><dc:publisher>\n        Universit of Wales Press<\/dc:publisher><dc:date>\n        1997<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/231\/1\/ALT_J_Vol5_No1_1997_The%20impact%20of%20educational%20tech.pdf<\/dc:identifier><dc:identifier>\n          Mitchell, P. David  (1997) The impact of educational technology: A radical reappraisal of research methods.  Association for Learning Technology Journal, 5 (1).  pp. 48-54.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776970050108<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/231\/","10.1080\/0968776970050108"],"year":1997,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"The impact of educational technology:\na radical reappraisal of research methods\nR David Mitchell\nGraduate Programme in Educational Technology, Concordia University, Montreal,\nCanada\nHow can we decide whether some new tool or approach is valuable? Do published results of empirical\nresearch help? This paper challenges strongly entrenched beliefs and practices in educational research\nand evaluation. It urges practitioners and researchers to question both results and underlying paradigms.\nMuch published research about education and the impact of technology is pseudo-scientific; it draws\nunwarranted conclusions based on conceptual blunders, inadequate design, so-called measuring\ninstruments that do not measure, and\/or use of inappropriate statistical tests. An unacceptably high\nportion of empirical papers makes at least two of these errors, thus invalidating the reported conclusions.\nIntroduction\nThe practical problem which motivates this paper is that of deciding - on the basis of\npublished research - whether to adopt some new device, procedure or paradigm thought\nlikely to improve education. What models, methods or media are likely to be most useful?\nFrom the invention of the printing press to multimedia software, educators have adopted\nunproven aids and fads. Researchers usually claim each new device or procedure to be at\nleast as effective as its predecessor. How valid is all this research? How to decide? A typical\nview is: Design an experiment to observe the effects of your treatment. Any book on research\ndesign and statistics will show you how. But will it? What essential aspects of educational\nmeasurement and research must we consider?\nMeasurement or sorcery?\nSuppose we wish to conduct research on media-based learning as a function of different\nlearning styles. Let us assume that we decided operationally to define relevant styles with a\ncommonly used questionnaire (for a more complete discussion of learning styles and\nproblems of identifying them, see Mitchell, 1994). A typical questionnaire asks a series of\n48\nALTJ Volume 5 Number \/\nquestions that one answers on a scale of possible responses ranging from, for example,\nstrongly agree to strongly disagree. But let us examine a measurement issue first.\nMeasurement: neglected rules\nFrom mathematics, the theory of numbers and the theory of measurement provide the\nfoundation upon which educational measurement and statistics must rest if the latter are to\nbe more than superficial and deceptive. The axiom of identity requires that: each question\nbe equivalent to each of the others; two people with the same score must have comparable\nabilities; and equal differences between scores be equivalent. The result, if the assumption\nof equivalence is not violated, is similar to a thermometer; a one-degree difference is the\nsame unit regardless of the starting temperature. Such equal interval scales (see Stevens,\n1946) are common in science but not in education.\nInstruments presumed to measure some variable like an attitude, opinion or even\nknowledge, seldom have equivalent questions. Moreover such technical refinements as\nreliability, validity or internal consistency fail to satisfy this axiom of identity. There is no\nguarantee that identical scores represent students with identical answers; indeed, it is very\nunlikely. Yet researchers usually treat questionnaires dealing with linguistic concepts (for\nexample, comprehension or learning style) as if they were sharply defined interval scales.\nMost scales used in educational research actually are ordinal scales and therefore do not\nmeet the mathematical preconditions for the statistical manipulations commonly used\n(Liebetrau, 1983).\nConsider the questions on commonly used 'instruments' purported to measure learning\nstyles (there are over 100, but see Entwistle, 1981). Typically in such scales, response\ncategories are ranked in order of importance to the researcher. Ranking may begin with\ndefinitely disagree assigned a rank of 1, and so on to 5. The test creator usually considers\nthis rank to be a 'score' for each question so that he or she can perform statistical analyses\non the numbers. Another conceptual blunder is to add the so-called scores for several\nquestions to get a 'total' for that subscale which carries a label supposedly denoting a\nvariable (for example, a particular learning style). This occurs despite the items' appearing\nto violate the axiom of identity; thus they cannot be added even if each scale were interval.\nWith a wave of a magic wand (accompanied by the incantation, 'let us assume...') it seems\nthat we can represent a statement ('I disagree that . . . ' ) by a number which is not simply a\nsymbol or identifier of a position in a sequence but a quantity. But can we? Is it justified?\nMathematically, the difference between 4 and 3 is equivalent to 2 minus 1 or 3 minus 2, but\nis it correct to say that the difference between my saying that 'I definitely agree . . . ' and 'I\nagree with reservations...' is the same as between 'impossible to give a definite answer' and\n'I disagree with reservations...'? Logically, all we can assume with a ranking of categories\nis the sequence. Statistics deals with numbers, not what they represent. If numbers, as\ncollected and assigned, violate epistemological or mathematical requirements, the analysis\nwill produce mathematically correct results. But what do they mean?\nMeasurement or intellectual pollution?\nMany published 'measurement instruments' were generated by factor analysis. Surely this\nprocedure justifies the scale and its scoring? Space limitations permit no discussion here,\n49\nP. David Mitchell The impact of educational technology, a radical reappraisal of research methods\nbut this argument should be interpreted in the light of Patrick Meredith's pithy comments\nabout Spearman's contribution to the topic:\nWhat is disturbing is that Spearman's 'factorial' concept, whose epistemological basis is\nriddled with fallacies, not only took off but came to dominate the psychological and\neducational skies [. . .] Instructional Science has a decontamination job on its hands, to\ndisperse the intellectual pollution created by a whole profession reared on a contempt for real\ninformation and a superstitious worship of false quantification (Meredith, 1972, p. 16).\nIs it possible that educational researchers have a 'contempt for real information and a\nsuperstitious worship of false quantification'?\nInformation, numbers and statistics\nIn contrast to Stevens (1946) and his followers, I assert that measurement is not just the\nassignment of numbers to things according to specified operations. The purpose of\nmeasurement is to reduce the variety of some part of reality which we observe, whether\ndirectly or through some information-gathering activity, to yield summarizing information\nthat is accurate, precise and general. Usually our intention is to answer a question or to\nsupport a decision.\nPseudo-measurement\nWhat too frequently happens is that the 'score' produced by the 'scoring key' (by\nillegitimately summing ranks of ordinal measures) is treated as if it were quantitative\ninformation about that variable for each person. Textbooks and professors often claim that\nit is all right to treat Ordinal Scales as if they were Interval Scales because their test of\nsignificance is so robust that it is unlikely to lead to improper conclusions. How credible is\nthis? Note that 'robust' is contextual, not fixed, contrary to a common myth. And any\nviolation of a test's prerequisites alters its probabilities of Type I and II errors. Moreover,\nthe powers of some parametric tests have been shown to diminish to zero under violations\nof mathematical assumptions of the test (Bradley, 1982).\nIf we play games with epistemological underpinnings and mathematical prerequisites, the\nconsequences are unknown, and our analysis could be meaningless. Lakatos, a philosopher\nof science, dismissed our typical use of statistical techniques to produce 'phoney\ncorroborations and thereby a semblance of \"scientific progress\" where, in fact, there is\nnothing but an increase in pseudo-intellectual garbage' (Lakatos, 1978, p. 88).\nInsignificance of statistical significance\nConsider this quotation from a typical textbook:\nTests of statistical significance are used to help researchers to draw conclusions about\nthe validity of a knowledge claim. [...] If the null hypothesis is rejected, we conclude that\nthe knowledge claim (i.e. the research hypothesis) is true. If the null hypothesis is\naccepted, we conclude that the knowledge claim is false. (Meehl, 1978, p. 622).\nWe usually are told to reject the null hypothesis if the difference is 'significant' (i.e. p<0.05).\nBut consider Meehl's summarizing statement: '[. . .] if you have enough cases and your\nmeasures are not totally reliable, the null hypothesis will always be falsified, regardless of\n50\nAtrJ Volume 5 Number I\nthe truth of the substantive theory'. Despite its prevalence, null-hypothesis significance\ntesting has been criticized for half a century. Lykken's conclusion can guide us:\nFinding of statistical significance is perhaps the least important attribute of a good\nexperiment; it is never a sufficient condition for concluding that a theory has been\ncorroborated, that a useful empirical fact has been established with reasonable\nconfidence - or that an experimental report ought to be published. The value of any\nresearch can be determined, not from the statistical results, but only by skilled,\nsubjective evaluation of the coherence and reasonableness of the theory, the degree of\nexperimental control employed, the sophistication of the measuring techniques, the\nscientific or practical importance of the phenomena studied, and so on. (Lykken, 1968,\np. 159)\nInappropriate use of parametric statistics\nThe majority of reported studies reflect the parametric statistical techniques taught in\neducational research courses for decades. These 'assume' (i.e. have as a mathematical pre-\nrequisite) a normal distribution of the data in a population, even though educational\nresearchers seldom know much about the population beyond the small sample taken in\ntheir experiment. (Paradoxically, the more experimental control and the greater the\ntreatment effect, the less approximately normal the distribution will be, and the test's\npower is attenuated.)\nIn parametric statistical analysis, what we find is an answer to this question: 'IF the null\nhypothesis (of no difference between groups) is true, AND if both normality and\nhomogeneity of variance are true, what is the probability that we would find a difference at\nleast as large as that which we observed, if we had randomly drawn two samples from that\npopulation?' This is easy to compute, but it is only correct if the prerequisite assumptions\nare true. Alas, a major problem exists: we do not know if the null hypothesis is true, if the\ndata is normally distributed, if homogeneity of variance is true, or even if the data is\nrandom (though our design may reassure as about randomness). So what happens? The\nresearcher can only act as if this were the case by assuming it to be so. Much of the time, it\nis not.\nSimple observation of published means and standard deviations reveals many which clearly\ncannot be normal distributions (for example, relatively large standard deviations, often\nnearly as large as, and sometimes several times larger than, the mean; non-symmetrical\nshape). Lohr et al (1995) reported several differences in their 'comprehensive evaluation' of\na hypertext model for teaching, but their conclusions came from Analyses of Variance,\nsome of which were 'significant'. Although Meehl (1978) shows that one should not even\nmake such claims with multiple tests, let us examine the comparisons. The authors offer 21\nmeans and standard deviations. Given that a normal distribution is symmetrical and\nextends three standard deviations above and below the mean, it is noteworthy that only one\nof the 21 groups could be approximately normal. Eight involved standard.deviations as\nlarge as or larger than the mean. Clearly these do not describe normally distributed data,\nand a parametric test should not be used. Unlike those authors, we can conclude nothing\nexcept that the analysis was inappropriate. Over half the empirical papers in recent issues\nof several journals make the same mistake.\n51\nP. David Mitchell The impact of educational technology: a radical reappraisal of research methods\nAs Krauth showed:\nExamination of real data reveals that the assumption of a normal distribution is not\njustified in the majority of cases. Empirical distributions are seldom symmetrical, a\nnecessary assumption for normality. Furthermore, empirical distributions tend to have\nheavier tails [...] One argument often used to justify [parametric tests] is that these tests\nare quite robust [...] [an argument] based on some old studies [about which problems\nexist]. (Krauth, 1988, p. 15)\nBradley concurs:\nA fantastic folk-lore sprang up among research workers: [...] distribution-free tests were\nregarded as second-class statistics, hopelessly inferior to parametric statistics [whether\nor not they meet their assumptions]. The efficiency of distribution-free relative to\nclassical tests has been investigated under common non-parametric conditions, i.e. non-\nnormal populations and\/or heterogeneous variances, and the new statistics have often\nproven superior, sometimes infinitely so. (Bradley, 1982, p. 13).\nNote, too, that if you choose to use null hypotheses, those associated with distribution-free\nstatistics are more general and thus more realistic than those of classical statistics. So if you\nwonder: 'When should I use distribution-free statistical methods?', the answer is: whenever\npossible. . *\nWhere do we go from here?\nIt seems likely that few of the published papers in our field meet all or even most of the\nrequirements of scientific research (theory-oriented, conceptually clear, measurements\nconsistent with number theory and measurement theory, preconditions for parametric\nstatistics met or distribution-free statistics used, appropriate logic, replicative validity). As\nsupport for decisions or research, most published results and interpretations probably\nshould be discarded. This may be too extreme but it seems to be a good starting-point.\nEducational technology may be riding a wave - the wave of pseudo-science. This wave\nmoves out of universities and through our journals. It is generated by the use of textbooks\nthat perpetuate an unthinking, cookbook approach to scientistic rather than scientific\ninquiry. A radical shift in our own thinking is essential.\nThe challenge facing us is complex. I offer a few ideas merely as conversation starters. It\nstrikes me that the academic preparation of educational researchers should be reformed -\nradically. I suggest that the research student be expected to master cybernetic principles,\nsystems modelling, probability theory, epistemology and the philosophy of science before\nembarking on the study of research design and analysis. And the first stage in educational\nresearch should focus on epistemological, not statistical or even design, issues. Indeed, the\nfirst statistics course should focus on probability and non-parametric statistics with\nparametric statistics left for later. As for non-researchers, instead of wasting time studying\ncookbook-based parametric statistics, they might spend their time better studying the logic\nof scientific inference and the fundamentals of modelling and measurement so that they\nmay be able to detect and reject pseudo- and quasi-pseudo-scientific research when they\nencounter it in our periodicals. And if they study statistics, let them begin with non-\n52\nVolume 5 Number I\nparametric statistics which is so much easier to grasp and more generally applicable.\nBut this is not sufficient, journal editors and their referees need to modify their tendency to\naccept any quantitative article that contains a statistically significant outcome and focus\nmore on the quality of thinking, the validity of measurement and appropriateness of\nqualitative and quantitative methods. Even better, they should encourage replications.\nConceivably, a comparison of names of authors of pseudo-scientific articles with those on\nthe editorial boards of our journals might reveal another problem, and journal editors may\nhave to find new referees.\nConclusion\nMuch of our cherished research is pseudo-scientific with unwarranted conclusions. In\nconsidering the standard approach it is instructive to bear in mind Stafford Beer's\ncomment:\nA paradigm is a model that exhibits a closed logic and thus resists change [...] To create\nchange, you must challenge not only the models of unreality but the paradigms that\nunderwrite them. Dangerous work. (Beer, 1988)\nThe paradigm informing most empirical educational research is an input-process-output\nmodel which assumes a one-way direction of causality from independent to dependent\nvariables. This questionable model lies at the heart of t-tests, correlations, analysis of\nvariance, multiple regression, discriminant function analysis, etc. In many papers, the\nmathematical prerequisites of these statistical tests are not met, thus invalidating the\nresults.\nAnother fundamental flaw involves passing subjective judgements off as measurements (by\nattaching numbers to constructs for which none of the properties of measurable magnitude\nare met), then combining these, violating stringent mathematical requirements and\nproducing meaningless results. Finally, treating linguistic variables (for example,\ncomputer-aided learning, hypertext, multimedia, learner control) as if they are precise and\ncomparable may confuse rather than inform.\nI have tried to show that our research needs to begin with conceptual issues but that our\nbiggest problem is the way in which research is carried out. I have tried to stimulate\nthinking about and questioning the models and underlying paradigms that permeate our\nexciting field of study and practice. Whether or not you ever carry out a research\nprogramme, you are certain to encounter claims (in the mass media as well as in textbooks\nand journals) that are not justified or justifiable. Now you can reject them.\nReferences\nBeer, S. (1988), Address to Convocation, Concordia University, Montreal.\nBradley, J.V. (1982), Distribution-Free Statistical Tests, Englewood Cliffs, NJ: Prentice-\nHall.\nEntwistle, N.J. (1981), Styles of Learning and Teaching, Chichester: John Wiley.\n53\nP. David Mitchell The impart of educational technology: a radical reappraisal of research methods\nKrauth, J. (1988), Distribution-Free Statistics, Amsterdam: Elsevier.\nLakatos, I. (1978), 'Falsification and the methodology of scientific research programmes'\nin Worrall, J. and Currie, G. (eds.), The Methodology of Scientific Research Programs,\nPhilosophical Papers, vol. 1: Imre Lakatos, Cambridge: CUP.\nLiebetrau, A.M. (1983), Measures of Association, London: Sage.\nLohr, L., Ross, S.M. and Morrison, G.R. (1995), 'Using a hypertext environment for\nteaching process writing: an evaluation study of three student groups', Educational\nTechnology Research and Development, 43 (2), 33-51.\nLykken, D.T. (1968), 'Statistical significance in psychological research', Psychological\nBulletin, 70, 151-9.\nMeehl, P. (1978), 'Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the\nslow progress of soft psychology', Journal of Consulting and Clinical Psychology, 46 (4),\n822.\nMeredith, P. (1972), 'The origins and aims of epistemics', Instructional Science, 1 (1), 16.\nMitchell, P.D. (1994), 'Learning style: a critical analysis of the concept and its assessment'\nin Hoey, R. (ed.), Aspects of Educational Technology XXVII, London: Kogan Page.\nStevens, S.S. (1946), 'On the theory of scales of measurement', Science, 103, 677-80.\n54\n"}