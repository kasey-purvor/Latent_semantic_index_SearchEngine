{"doi":"10.1076\/sesi.14.3.321.15842","coreId":"66376","oai":"oai:dro.dur.ac.uk.OAI2:1683","identifiers":["oai:dro.dur.ac.uk.OAI2:1683","10.1076\/sesi.14.3.321.15842"],"title":"School performance feedback systems : conceptualisation, analysis, and reflection.","authors":["Visscher,  A. J.","Coe,  R."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2003-09-01","abstract":"Although there is an international trend to publish and feed back information to schools and teachers, the full complexity of improving schools through school performance feedback is not usually recognised. In this article, 'school performance feedback systems' (SPFSs) are first conceptualised, and the factors that have contributed to their popularity are analysed. Next, a framework is presented which includes the factors that are crucial for the use and impact of SPFSs. Finally, the balance sheet is drawn up for the evidence on the process, problems and impact of SPFSs, and strategies for improving schools through performance feedback are formulated","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66376.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/1683\/1\/1683.pdf","pdfHashValue":"67af5b870daae10bb54cf92ded2aba71272853f0","publisher":"Routledge","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:1683<\/identifier><datestamp>\n      2016-07-06T08:37:50Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        School performance feedback systems : conceptualisation, analysis, and reflection.<\/dc:title><dc:creator>\n        Visscher,  A. J.<\/dc:creator><dc:creator>\n        Coe,  R.<\/dc:creator><dc:description>\n        Although there is an international trend to publish and feed back information to schools and teachers, the full complexity of improving schools through school performance feedback is not usually recognised. In this article, 'school performance feedback systems' (SPFSs) are first conceptualised, and the factors that have contributed to their popularity are analysed. Next, a framework is presented which includes the factors that are crucial for the use and impact of SPFSs. Finally, the balance sheet is drawn up for the evidence on the process, problems and impact of SPFSs, and strategies for improving schools through performance feedback are formulated.<\/dc:description><dc:subject>\n        SPFS<\/dc:subject><dc:subject>\n         Evaluation<\/dc:subject><dc:subject>\n         Meta analysis<\/dc:subject><dc:subject>\n         Task.<\/dc:subject><dc:publisher>\n        Routledge<\/dc:publisher><dc:source>\n        School effectiveness and school improvement, 2003, Vol.14(3), pp.321-349 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2003-09-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:1683<\/dc:identifier><dc:identifier>\n        issn:0924-3453<\/dc:identifier><dc:identifier>\n        issn: 1744-5124<\/dc:identifier><dc:identifier>\n        doi:10.1076\/sesi.14.3.321.15842<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/1683\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1076\/sesi.14.3.321.15842<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/1683\/1\/1683.pdf<\/dc:identifier><dc:rights>\n        This is an electronic version of an article published in Visscher, A. J. and Coe, R. (2003) 'School performance feedback systems : conceptualisation, analysis, and reflection.', School effectiveness and school improvement., 14 (3). pp. 321-349. School effectiveness and school improvement is available online at: http:\/\/www.informaworld.com\/smpp\/content~db=all?content=10.1076\/sesi.14.3.321.15842.<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn: 1744-5124","issn:0924-3453","0924-3453"," 1744-5124"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2003,"topics":["SPFS","Evaluation","Meta analysis","Task."],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n24 February 2010\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nVisscher, A. J. and Coe, R. (2003) \u2019School performance feedback systems : conceptualisation, analysis, and\nreflection.\u2019, School effectiveness and school improvement., 14 (3). pp. 321-349.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1076\/sesi.14.3.321.15842\nPublisher\u2019s copyright statement:\nThis is an electronic version of an article published in Visscher, A. J. and Coe, R. (2003) \u2019School performance feedback\nsystems : conceptualisation, analysis, and reflection.\u2019, School effectiveness and school improvement., 14 (3). pp.\n321-349. School effectiveness and school improvement is available online at:\nhttp:\/\/www.informaworld.com\/smpp\/content db=all?content=10.1076\/sesi.14.3.321.15842.\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\n  \n \nDurham Research Online \n \nDeposited in DRO: \n24 February 2010 \n \nPeer-review status: \nPeer-reviewed \n \nPublication status: \nAccepted for publication version \n \nCitation for published item: \nVisscher, A. J. and Coe, R. (2003) 'School performance feedback systems : \nconceptualisation, analysis, and reflection.', School effectiveness and school improvement., \n14 (3). pp. 321-349. \n \nFurther information on publisher\u2019s website: \nhttp:\/\/dx.doi.org\/10.1076\/sesi.14.3.321.15842 \n \nPublisher\u2019s copyright statement: \nThis is an electronic version of an article published in Visscher, A. J. and Coe, R. (2003) \n'School performance feedback systems : conceptualisation, analysis, and reflection.', School \neffectiveness and school improvement., 14 (3). pp. 321-349. School effectiveness and \nschool improvement is available online at: \nhttp:\/\/www.informaworld.com\/smpp\/content~db=all?content=10.1076\/sesi.14.3.321.15842. \n \n \n \n \n \n \n \nUse policy \n \nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior \npermission or charge, for personal research or study, educational, or not-for-profit purposes provided that : \n \n\uf0a7 a full bibliographic reference is made to the original source \n\uf0a7 a link is made to the metadata record in DRO \n\uf0a7 the full-text is not changed in any way \n \nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders. \n \nPlease consult the full DRO policy for further details. \n \nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom \nTel : +44 (0)191 334 2975 | Fax : +44 (0)191 334 2971 \nhttp:\/\/dro.dur.ac.uk \n 1 \nSchool performance feedback systems:  \nconceptualisation, analysis and reflection \n \n \n \nAdrie J. Visscher* & Robert Coe** \n \n*   University of Twente, The Netherlands \n** University of Durham, England \n \n \n \nIntroduction  \nThere is an increasing trend internationally to publish and feed back information to \nschools and teachers on their performance. School improvement is often the main \nobjective for this, but accountability and the promotion of parental\/student school \nchoice also play important roles. However, practical initiatives and research in this \nfield often fail to recognise the full complexity of improving schools through school \nperformance feedback. Relevant issues are, for example: \n is the information fed back to schools valued and understood by school staff? \n does school performance feedback enable appropriate diagnosis and remediation \nto take place in schools? \n does the use of the feedback generate conflicts and tensions for teachers and \nadministrators? \n do the features of the implementation process and school organisation influence \nthe utilisation of school performance feedback systems? \n do schools actually improve as a result of performance feedback? \n do the feedback systems have unintended effects? \n \nWe have therefore attempted here to conceptualise \u201eschool performance feedback \nsystems\u201f, and to analyse the factors that have contributed to their international growth.  \nThereafter, the characteristics of a number of sophisticated school performance \nfeedback systems (SPFSs) that are being used around the world will be analysed, \ndrawing on two specific examples, followed by the presentation of a framework \nwhich sets out the factors influencing the utilisation of SPFS-information and its \neffects.  \nIn the final section the status of the field is discussed by portraying the evidence on \nthe process, problems and impact of school performance feedback from a variety of \ninternational contexts. In addition, strategies for school improvement through \nperformance feedback, and for research in this field, are formulated. \n \nSchool performance feedback systems  \nSchool performance feedback systems are defined here as information systems \nexternal to schools that provide them with confidential information on their \nperformance and functioning as a basis for school self-evaluation. Such systems have \nbecome widespread in education in many parts of the world. They share a goal of \nseeking to maintain and improve the quality of schools, and arise out of a belief in the \npower of feedback to learn, and to produce change, often accompanied by a sense of \ndisillusionment at the lack of impact of other models of school improvement. \n 2 \nThis definition excludes informal, self-generated feedback and separates SPFSs from \npublic systems of school performance accountability and support of school choice, \nwhich have rather different aims and contents. \nThe content of the information on the school\u201fs performance or functioning must be \ntaken broadly. School performance here is likely to mean some kind of contextualised \nmeasure for fair comparison, adjusted to take account of factors beyond the control of \nthe school (\u201evalue added\u201f). \u201ePerformance\u201f may also include absolute performance \nmeasures and may equally relate to non-academic outcomes of schooling (e.g. \nbehavioural and affective). Information on the functioning of schools relates to \norganisational and school-process measures like the resources spent, the subject \nmatter taught, the instructional methods used and the nature of school leadership. \nThat the feedback should provide a basis for self-evaluation implies that the feedback \nshould not simply be used for self-assessment, but that, once such judgements have \nbeen made, they ideally lead to some kind of action, such as the closer investigation of \nwhere and why the school under-performs and the development of a school \nimprovement policy. \n \nThe origins of school performance feedback systems \nA number of factors seem to have contributed to the growth of more formal school \nperformance feedback systems in many countries over the last twenty or so years. \nIn many western countries in the 1980s and 90s the rise of a political climate of public \nsector accountability can be observed. The pressure to evaluate and report on the \nperformance of publicly funded educational institutions may not have directly led to \nSPFSs, but helped to create a climate in which school performance feedback was seen \nas more salient than previously. \nRelated to this rise in accountability is the trend towards decentralisation in the \nadministration of educational systems. As a result, schools are more likely to seek \ninformation they can utilise for school quality control, i.e. some sort of SPFS. \nThere is, moreover, some evidence (e.g. Murdoch & Coe, 1997) that in some \ncountries, schools\u201f perceptions of the unfairness of the public judgements of their \neffectiveness were a factor in their choice to implement a confidential value added \nschool monitoring system (see Visscher, 2001, for an overview of the drawbacks of \npublic school performance indicators). The published school performance information \nincluded average raw achievement of a school\u201fs students, which did not adjust for \nrelevant features of the student intake (e.g. prior achievement levels). Schools wanted \nmore accurate and fairer data on their own performance in order to know whether \u2013 \nand precisely where \u2013 improvement was really needed. \nAlongside these pressures to develop systems for performance feedback, the progress \nmade in research in the twin fields of school effectiveness and school improvement \nhas meant that more sophisticated systems can be developed. The former line of \nresearch has resulted in a knowledge base (Scheerens & Bosker, 1997) that can be \nutilised in developing systems to monitor the quality of schools. An example of this is \nthe ZEBO feedback system, which will be described below.  \nResearch on school improvement may have influenced the development of SPFSs too, \nas scientific activity there showed that educational change initiatives imposed upon \nschools were often not very successful. Innovation and success are considered much \nmore likely if schools themselves are convinced that something needs to be changed \n(\u201eownership\u201f). Receiving information on how your school is doing in comparison with \nsimilar schools may be a powerful way to make you aware and determined that \nsomething needs to be changed in your organisation.  \n 3 \nDalin (1998), McLaughlin (1998) and Miles (1998) stress the local variability of \nschools, implying that general, centrally developed policies and reform strategies will \nnot lead to educational change in all schools. Schools are considered to differ so much \nwith respect to their performance levels (and the underlying reasons for them), their \ninnovation capacities and contextual characteristics, that change efforts should take \nmuch more account of what is called the \u201epower of site or place\u201f. Smith (1998) goes a \nstep further. He states that as practitioners know their educational practice best they \nshould state the goals and changes to be worked on and, after extensive training, try to \naccomplish those. Adaptation to the user-context can then be achieved. A SPFS may a \nvaluable tool within this perspective on school improvement, providing timely, high-\nquality information on how a school \u201eis doing\u201f as a basis for practitioner-led \nimprovement actions. That may help practitioners in finding problems in their schools \nas well as in solving them, before it is too late. An important additional effect may be \nthat practitioners gain a better insight into how their school works and which \ninterventions work best in their situation. \nRelated to the pessimism of the school improvement authors is the view of Glass \n(1979) who regards education as a very complex, highly uncertain and unpredictable \nsystem on which we possess only incomplete knowledge. We should not try to find \neternal truths about which of several things works well in particular circumstances, as \na basis for planning and manipulating education at a large distance from the teaching-\nlearning process in schools. Instead, we need diligent monitoring of the system in \nwhich the services are highly decentralised, and the actors are flexible and can choose \nfrom options what they consider best, rather than precisely implementing a universal \napproach that has been developed somewhere at a higher level. \nThe increase in feeding back information to schools has also been influenced by the \ndevelopment of multi-level and value-added data-analysis models, which enable the \ncomputation of more reliable and valid information on school functioning. The \navailability of computerised systems for information processing has made a \nsignificant contribution to the logistics of school performance feedback (Visscher, \nWild & Fung, 2001). \n \nEvidence about feedback effects \nGiven the prevalence \u2013 and growth \u2013 of performance feedback in schools, it might \nseem reasonable to suppose that there would be a substantial body of evidence to \nendorse its overall beneficial effects, and, more specifically, to identify the conditions \nunder which its impact would be optimal.  However, the evidence about feedback \neffects is mixed, complex and not well understood. Research results indicate that \nfeedback can be beneficial to future performance, but it can also do harm. Moreover, \nthe relative lack of evidence derived specifically from school contexts makes it hard \nto predict confidently what the effects will be in any particular case. This section sets \nout to review briefly what is known about feedback effects, drawing on empirical \nevidence and theoretical understandings from education, psychology and \norganisational behaviour.  \nSeveral theories have made predictions about feedback effects. The first and perhaps \nmost influential was formulated by Thorndike (1913). His \u201elaw of effect\u201f saw \nfeedback acting as either reinforcement or punishment, which determines whether \nsubsequent behaviour is encouraged or discouraged. This theory had substantial \ninfluence on feedback research, despite inconsistencies with empirical results (Kluger \nand DeNisi, 1996). \n 4 \nA later contribution to our understanding of how feedback may influence performance \ncomes from control theory (Podsakoff & Farh, 1989), which emphasises the \ndiscrepancy between a person\u201fs performance and their internal standards. Feedback \nthat suggests a discrepancy between these two exists (\u201enegative\u201f feedback) is likely to \nbe met with attempts to remove the discrepancy, either by increasing effort or \nreducing the standards, while following positive feedback, goals and effort are \nexpected to remain stable. \nRelated to this idea is a third theory, that of learned helplessness (Mikulincer, 1994), \nwhich also makes predictions about the effects of feedback. In particular, it predicts \nthat repeated negative feedback can lead to a reduction in performance below what \nwas previously achieved. \nA fourth theory that offers predictions about feedback effects is Bandura\u201fs (1991) \nsocial cognitive theory. According to this, individuals\u201f responses to feedback depend \non two regulators: a self-evaluative mechanism that compares performance to internal \nstandards or goals and a self-efficacy mechanism that judges their capacity for \nattaining those goals. Feedback contributes to both these mechanisms, informing the \ncomparison in the former and providing evidence on which to judge the latter. Social \ncognitive theory recognises an important tension between the effects of negative and \npositive feedback: although negative feedback is necessary to motivate the need for \nimprovement, without positive feedback, individuals are unlikely to believe \nthemselves capable of achieving it.  \nFinally, the theory of goal setting (Locke & Latham, 1990) offers an explanation of \nthe role of feedback in enhancing performance when challenging goals have been set. \nThe combination of specific and difficult goals with feedback makes satisfaction \ncontingent on high performance, so increasing task effort, focus and persistence. \nHowever, the most comprehensive synthesis of research on feedback effects is Kluger \nand DeNisi\u201fs (1996) meta-analysis and feedback intervention theory (FIT). Overall, \nthey found an effect size of 0.4, which they interpret as \u201esuggesting that, on average, \nfeedback intervention (FI) has a moderate positive effect on performance\u201f (p. 258). \nHowever, over a third of the effects in their analysis were negative and many were \nclose to zero. As they warn, \u201eF[eedback] I[ntervention]s do not always increase \nperformance and under certain conditions are detrimental to performance\u201f (p. 275). \nSimilar mixed results have been found in other recent reviews and meta-analyses \n(e.g., in Bangert-Drowns et al., 1991; Locke and Latham, 1990, Balcazar et al., 1985) \n\u2013 despite the resilience of the view that feedback universally enhances performance. \nAccording to Kluger and DeNisi\u201fs (1996) feedback intervention theory, feedback \neffects depend on three classes of variables: the cues in the feedback message, the \nnature of the task performed and on situational\/personality variables. In order to \npredict the impact of giving feedback it is important to understand the relationships \nbetween these variables and their effects; in order to optimise its impact it is necessary \nto know which variables can be altered and to manipulate them accordingly. \n \nCues in the feedback message \nCues in the feedback message may direct attention away from the task, for example \nby diverting it to the self, raising issues of self-efficacy or causing individuals to focus \non wider self-goals or issues of self-perception. The evidence from the meta-analysis \ngenerally supports this part of the theory, finding that when feedback was designed \neither to discourage or praise the recipient its effect was less.  Cues may also \ndetermine attributions for success or failure and so influence future performance. On \nthe other hand, feedback that directs attention to past performance or to learning \n 5 \nprocesses (for example, corrective feedback) may help to focus attention on task \ngoals. However, the level of specificity of feedback can have mixed effects, since if it \nis too specific it may direct attention below the level necessary for optimal \nimprovement, or if too detailed, may confuse and actually impair learning.  \nCoe (1998a) also examined evidence about a number of feedback characteristics \nnot considered in Kluger and DeNisi\u201fs (1996) meta-analysis. He suggests that \nfeedback should be made to seem credible and accurate, and perceived as providing \ninformation and supporting self-determination, rather than as surveillance or control. \nIt should seek to generate feelings of competence but not complacency. Moreover, it \nshould encourage recipients to attribute their level of performance to the effort they \nhave applied or to specific, alterable factors such as their choice of strategy, and so \nmake them feel they have control over the outcomes. Feedback should seek to make \npeople focus on the task or on their performance relative to their past achievements, \nand avoid directing attention to comparisons between their performance and that of \nothers. \nThe practical importance of these cues is clear, since they are explicitly \nmanipulable. Designers of feedback systems should therefore be able to use the \nresearch evidence to optimise the impact of their feedback. What is also clear, \nhowever, is that some of the recommendations from the research (such as the need to \navoid the perception of surveillance and not to focus on comparisons) are somewhat at \nodds with the design of some SPFSs. \n \nThe nature of the task \nThe relationship between task characteristics and feedback effects is also complex \nand not well understood, and is somewhat hampered by the lack of an adequate \ntaxonomy of the tasks in which the effects of feedback on performance have been \nevaluated.  \nThe main factor found by Kluger and DeNisi (1996) to be associated with \ndifferential effectiveness was task complexity. They report that feedback had \nsubstantial effects for simple tasks, but for complex tasks feedback had essentially no \neffect at all. This result is particularly interesting in view of the similarly modest \neffects of goal setting on complex tasks (Wood et al., 1986; DeShon and Alexander, \n1996) and the fact that precisely these tasks might be expected to be most relevant for \ndrawing inferences about school performance.  \n \nSituational\/personality variables \nKluger and DeNisi (1996) identify two main factors here. The first of these is the \nuse of a goal setting intervention. They acknowledge the weight of evidence \nsupporting the effectiveness of goal setting although the difference found in their \nstudy was relatively modest. Certainly, since goal setting is an independently \nmanipulable intervention, it would seem that feedback effects should be maximised \nby ensuring that individuals have clear, specific and challenging goals related to their \ntask performance against which the feedback can be used to measure performance. \nHowever, Neubert (1998) found an interesting interaction between task complexity \nand the role of feedback in goal setting. When Neubert\u201fs analysis was restricted to \ncomplex tasks (6 effects), the difference between combining feedback with goal \nsetting and the latter alone rose to 1.02. This suggests that although goal setting may \nbe less effective in general for complex tasks, the role of feedback in facilitating goal \nsetting effects may be particularly important for these tasks. However, this area is one \n 6 \nwhere findings seem somewhat mixed and hard to interpret; making confident \npredictions about feedback effects is likely to be even harder. \nThe second characteristic identified as significant in Kluger and DeNisi\u201fs FIT, and \nconfirmed by the meta-analysis was the extent to which self-esteem was perceived to \nbe at threat from the feedback message. In those studies where the threat was low the \neffects of feedback were much higher (effect size for studies in the bottom quartile of \nratings of threat to self-esteem = 0.47) than in those characterised as posing high \nthreat (effect size = 0.08). It seems likely that performance feedback given to teachers \nmight well be seen as extremely threatening and hence, on the basis of this result, be \nexpected to have very little positive effect on future performance. \nIn addition to these two main factors, a number of other situational\/personality \nvariables have been identified in the literature as significant. For example, the FIT \nalso predicts that feedback effects will be greatest when the cognitive demands of a \ntask are least. A review by Coe (1998a) points to further factors such as the existing \nlevel of task-motivation and the availability of other information or instruction that \nmay help to improve performance. Although maximising these factors is likely to be \nmost beneficial for performance, the specific effects of feedback seem to be greatest \nwhen each of these three factors is minimised. However, a review by Goodman \n(1998) emphasises the difference between improving performance while practising a \ntask and true learning. She suggests that, for tasks that provide little inherent \nfeedback, external feedback can actually interfere with feedback derived directly from \nthe task and so inhibit long-term learning. In the context of a SPFS, the nature of the \ntask is likely to be fixed, so this variable cannot be manipulated. \nPersonality variables are also important in moderating the effects of feedback on \nperformance. Individual differences in characteristics such as self-esteem, locus of \ncontrol and achievement-orientation have been shown to influence reactions to \nfeedback (Coe, 1998a). Kluger and DeNisi (1996) view these differences in terms of \nthe differences in the self-goals that are salient for different people (for example, \nthose low in self esteem are particularly anxious to avoid negative stimuli). According \nto their FIT, feedback that resonates with these salient goals is likely to divert \nattention away from the task and thus debilitate performance.  \n \nSpecific application to school performance feedback \nDespite the generally good fit between the data and their theory, Kluger and DeNisi \n(1996) are somewhat cautious in drawing implications for practice. They point out \nthat although feedback can substantially improve performance under certain \nconditions, if this is achieved through an increase in task-motivation, it may not be \nsustained after the feedback is removed. In some cases, the costs of providing \ncontinuous feedback might outweigh the benefits of improved performance. On the \nother hand, if the effect of the feedback is through task-learning processes, then \u201ethe \neffect may create only shallow learning and interfere with more elaborate learning\u201f (p. \n278). Further research and development of the theory are required to establish whether \neffects are lasting and efficient. \nOf most relevance to the current discussion, however, would be studies of the impact \nof performance feedback conducted in school contexts. Such studies include those by \nFuchs and Fuchs (1986), Cohen (1980), L\u201fHommedieu et al., (1988, 1990), Brandsma \nand Edelenbos (1992, 1998), Tymms (1995, 1997a, 1997b) and Coe (1998b). All \nthese studies have been reviewed in more detail by Coe (2002). However, it seems \nthat very few evaluation studies have directly addressed this issue; those that have \n 7 \nhave generally suffered from limitations that make their findings hard to interpret \nunequivocally.  \nGiven the complexity of the kinds of feedback that can be given to schools about their \nperformance, the varying contexts of school performance, and the range of ways \nfeedback can be provided, it is extremely difficult to make any kind of generalised \npredictions about its likely effects. The low level of our theoretical understandings of \nfeedback, the lack of evidence about many aspects of its effects and, most crucially, \nthe limitations of the evidence derived from school performance contexts make any \nsuch predictions extremely speculative. Even for a specific case in which the \nmoderating variables referred to above are known, it would often be hard to make an \nunequivocal prediction; different factors often work in opposite directions and the \nbalance of effects is usually uncertain. Moreover, such a well-described case would, \nin practice, be the exception rather than the rule. In short, we cannot confidently say \nwhat the benefits of giving schools performance feedback may be, or how those \npotential benefits may be maximised. \n \nThe factors that matter \nHaving examined the theoretical and empirical evidence underlying the use of school \nperformance feedback systems, we now turn to presenting a conceptual framework for \nanalysing the significant features of such systems. Since there is no generally accepted \nexisting framework, the groups of factors that are believed to matter have been \nidentified by reviewing the relevant literature in the fields of educational innovation, \neducational management, business administration, and computer science.  \nFigure 1 below presents the results of the theoretical analysis: a model depicting the \nassumed relationships between four groups of factors (Blocks A - D) on the one hand, \nand the use (Block E) and impact (Block F) of SPFSs on the other. The Figure shows \nthat the nature and intensity of SPFS use is supposed to be influenced by the SPFS \nfeatures, which in turn result from its design process. The nature of the \nimplementation process and the characteristics of schools are also expected to \ninfluence SPFS use. The implementation process can promote SPFS use directly (e.g. \nby supporting schools in accomplishing the innovation), or indirectly (e.g. via training \nschool staff in the required SPFS skills). Finally, the degree of SPFS use, and the way \nin which it is used, is expected to lead to both intended and unintended effects. \nIt is important to stress that Figure 1 is meant to clarify which factors influence SPFS \nuse and the resultant effects, so Blocks E and F are crucial. In other words, the Figure \nneither shows how all factors contribute to the effects in Block F nor how other blocks \nin the Figure are related. If the latter had been the case, arrows between other blocks \ncould also have been drawn. \nFigure 1 also indicates that the school environment plays a role. For example, the \nextent to which the school board, district and the community play an active role in \nrunning schools and demand high quality may influence to which degree schools use a \nSPFS to improve performance. If the quality of school functioning is a hot issue, for \ninstance where there are published league tables and \u201epunishments\u201f for under-\nperforming schools, then schools may be more inclined to improve than when external \nquality control is weak, and parents are unable to choose the school of their choice. \nThe educational system can also play a more supporting role by providing schools \nwith the resources required for change and improvement. \nEach of the Blocks in Figure 1 will now be discussed more in detail. It is however \nimpossible to provide much detail here on the reasons for selecting each factor. For \nmore details on the backgrounds of the factors the reader can refer to Visscher (2002). \n 8 \n \n \n \nFigure 1. The relationships between the factors influencing SPFS use and effects \n \n \nBlock A: The design process \nThe process of designing a SPFS can differ in three respects. \n \n1. The goal(s) of designing a SPFS  \nAlthough school improvement is the central goal for designing SPFSs, some SPFS \ndesigners may want to design systems that also serve accountability, and the support \nof parental school choice. Each of the three goals requires its own set of performance \nindicators. Whitford and Jones (1998) state that for school improvement the feedback \n \nA  Design process \nB  SPFS features \nC  Implementation process E  SPFS usage  \nF  Intended and \n unintended effects \nD  School org. features \n1. school performance level \n2. innovation attitude \nschool staff \n3. innovation capacity \n4. learning-enriched\/ \n-impoverished \n5. new skills \n6. high\/low reliability \nschools \n7. allocation extra \nresources \n1. instrumental use \n2. conceptual use \n3. symbolic use \n4. strategical use \n1. improved student  \nachievement \n2. intensified \nachievement \n3. improved teaching \n4. unintended effects \n1. tailored user-training \nand school support \n2. pressure to improve \n3. promote user \nparticipation \n4. monitor \nimplementation- \nconsistency and -effects \n5. extra change resources \n1. design goal(s) \n2. design strategy \n3. standardization- \nflexibility SPFS \n1. valid information \n2. reliable information \n3. up-to-date information \n4. relevant information \n5. absolute and\/or relative \nperformance \n6. variables, trends, \ninterrelationships, \ndifference scores \n7. standard or tailored \ninformation \n8. accessible information\/ \nsupport in usage \n9. school staff effort  \n10. innovation complexity \nand clarity \n11. problem solving \nsupport from SPFS \nEducational system \nBoard\/community\/district \norientation \n 9 \ninformation should be as detailed as the complexity of schooling. SPFS designers \nshould be clear which goal(s) they aim to serve. \n \n2. Design strategies \nMaslowski and Visscher (1999a) make a distinction between several design \napproaches and point to a number of relevant aspects of design strategies: \n how the problem analysis is carried out, and how objectives and means are \nformulated in SPFS design; \n the extent to which SPFS prototypes are developed and evaluated (formative \nevaluation is of great importance for design optimisation); \n the degree to which stakeholders communicate with each other and influence \ndecisions on the desired SPFS. If practitioners have more of a say, they may \ndevelop ownership, take its findings seriously and make more effort to apply the \nresults (cf. Huberman, 1987; Weiss, 1998); \n the creative, non-linear side of SPFS design activities. \n \n3. The standardisation-flexibility problem \nIdeally, a SPFS is so flexible that it satisfies requirements uniform to all schools as \nwell as varying information needs among schools. In practice it will be hard to fulfil \nboth goals completely, implying that a compromise between both is usually required. \n \nBlock B: SPFS features \nSPFSs can differ in the extent to which: \n1. the information fed back to schools is valid, e.g. value-added data versus raw data, \nbased on multi-level analysis, or aggregated data and the extent to which SPFS \ndata cover school quality (e.g. indicators on the performance of the overall school, \nschool divisions, departments, and teachers). Fitz-Gibbon (2002) provides a \ntypology of indicators that can support school improvement; \n2. information is reliable; \n3. information is up-to-date: \u201cTimeliness is a useful feature of a report dedicated to \nutility\u201d (Weiss, 1998); \n4. data is relevant for practitioners, fits with their needs and reduces their \nuncertainty; \n5. SPFS data indicates both relative and absolute school performance; \n6. data shows values for such factors as trends over time, relationships between data \nand differences between scores measured at two or more instances (the latter for \nexample to evaluate the effect of school policy); \n7. the SPFS provides standard information, and allows in-depth analysis and tailored \ninformation for users; \n8. data is presented in an accessible and appealing manner (e.g. quantitatively, \ngraphically) and users are supported in using performance data correctly, e.g. the \ncorrect interpretation of value-added data; \n9. the SPFS requires the investment of time and effort from school staff as a result of \ndata collection and feedback; \n10. innovation is complex yet clear, i.e. the difficulty of the required change and the \nsuccess of accomplishing it for those who need to make the change.  \n11. the SPFS provides user-support in problem-solving (e.g. via a manual, computer-\naided help or help line). \n \n 10 \nIn Block B the importance of sophisticated SPFSs \u201espitting out\u201f high quality \ninformation is stressed. In Blocks C and D other critical success factors are stressed \nsince SPFS quality is a necessary but insufficient precondition for the use and \nintended effects of SPFS. Features of schools and the change process itself are \nstrongly related to positive outcomes of school improvement efforts (McLaughlin, \n1990). \n \nBlock C: Implementation process features \nBased on our review of the educational innovation literature, the following \nimplementation process features appear to be relevant for successfully introducing \nSPFSs into schools: \n1. A lengthy, intensive tailored reform strategy and support, e.g. assisting in school \ndiagnosis, designing school change policies at class and school level. External \nchange facilitators combined with information exchange via school networks: \ngood examples can fulfil an important role here. The extent, method and content \nof user-training is very important: clarification of innovation goals and means, \nmotivating users for innovation, developing new organisational roles, values, \ninformation processing and school improvement skills. \n2. The pressure to improve via external targets and control, competition between \nschools and incentives. \n3. The encouragement of user participation and ownership in implementation. \n4. Monitoring the consistency of implementing SPFSs as well as the immediate \neffects of SPFS implementation on classrooms and student achievement. Often the \nimplementation process needs to be adapted to the local conditions. \n5. The provision of extra innovation resources e.g. for releasing school staff from \nroutine work.  \n \nBlock D: School organisational features \nThe following organisational features of schools are considered important for using a \nSPFS:  \n1. The level of performance of schools: relatively low levels combined with the \npressure strategy may motivate schools more to try to improve performance by \nusing a SPFS. \n2. The innovation attitude of school staff: receptive or not. \n3. The innovation capacity: being aware of the school\u201fs performance level, structure, \nculture, problems and the capacity to evaluate, to design reform goals and means, \ninterventions at school and classroom level, experimenting, evaluating, \nadaptations and improving. \n4. The degree to which schools promote organisational learning: encouragement and \nsupport via shared responsibilities for school goals\/outcomes, collaborative work \nstructures, and exchange of information, experimentation and innovation. \n5. New skills: interpretation of SPFS output. \n6. High\/low reliability schools: the degree to which classroom and other school \nactivities are co-ordinated. \n7. Allocation of school resources to innovation activities. \n \nBlock E: SPFS use \nWhat does SPFS use actually encompass? One element of use concerns the analysis \nand interpretation of the information received. This may not always be easy as some \nof the outcomes are the product of complex statistical techniques. Their correct \n 11 \ninterpretation requires some knowledge of statistical concepts like value-added scores, \ncorrelations, and confidence intervals. Ideally, users would have been trained in this \nrespect.  \nAnother aspect of SPFS use concerns the utilisation of the information schools receive \nfor improving their functioning, i.e. deciding to act to improve, and acting on it as \nmuch as possible. In the evaluation literature a distinction is made between three types \nof utilisation (Rossi & Freeman, 1993): \n1. direct or instrumental: the decision-maker analyses the information before taking \na decision, and bases decisions and actions on this; \n2. conceptual: less visible but also important is the extent to which the evaluative \ninformation influences the thinking of decision-makers and as such may have an \nimpact on their actions; \n3. convincing (symbolic): this type of use concerns using information in support of \nsomeone\u201fs own viewpoints in a discussion with others. Information is then used \nselectively to legitimise an opinion already held.  \nVisscher (2001) refers to Smith (1995) who presents a profound analysis of the \nunintended, strategical consequences of publishing performance data on public sector \ninstitutions. In the context of schools, the following strategic actions come to mind: \n- concentrating on those students where most \u201eprofit\u201f can be gained; \n- selective student admissions; \n- removing \u201edifficult\u201f students; \n- teaching to the test and so concentrating on the indicators to the exclusion of \nother qualifications; \n- consciously depressing baseline test scores to obtain high value-added scores. \n \nBlock F: The intended and unintended effects \nIn our view the ultimate goal of introducing SPFSs should be improving school \nperformance, e.g. higher, value-added school performance scores. Proving this type of \nprogress unequivocally will probably take a long time. In the meantime, it will be \ninteresting to investigate to what degree a number of important prerequisites for \nimproved school performance can be observed, such as: \n1. a stronger orientation of school staff to high student achievement; \n2. improved teaching; \n3. changes in school organisational processes and structures because the use of SPFS \noutput presupposes staff co-operation, communication and leadership. \nHowever, because of the potential strategical use of performance indicators (Smith, \n1995) it is important also to check for negative, unintended effects of introducing \nSPFSs.  \n \nThe validity of the framework  \nVisscher\u201fs framework in Figure 1 identifies 34 factors, which emerge from a literature \nreview as potentially influencing the success of SPFSs. The directors of five widely \nused SPFSs in the USA, Australia, the United Kingdom and The Netherlands were \nasked to use this framework to describe their projects (Teddlie et al., 2002; Hendriks \net al., 2002; Gray, 2002; Rowe et al., 2002; Tymms and Albone, 2002). Based on the \nresults of their activities, an analysis was made of the extent to which the structure \nand contents of Figure 1 seemed applicable to their context. \nThe comparison revealed that about three quarters of the factors in Figure 1 were \naddressed in the accounts of these SPFSs, implying that those involved explicitly \nnoted the importance of the majority of these factors based on their experience with \n 12 \ndeveloping and introducing SPFSs. This should not be too surprising since the \nframework is based on a review of the relevant literature, which reflects several \ndecades of experience and research. However, as this general literature has been \ntranslated to the specific context of SPFSs it is good to see that the new framework \napplies to this particular innovation.  \nNevertheless, some of the factors in the framework were not mentioned by the \ndirectors of the SPFS analysed, which is probably due to the fact that some activities \nwere not carried out in the projects (e.g. the factor \u201emonitoring implementation-\nconsistency and effects\u201f, or \u201econceptual use of SPFS-information\u201f, and \u201eunintended \neffects\u201f of SPFSs). As a result, no information is available on the extent to which \nthese factors influence SPFS success.  \nThe fact that so many of the factors from Visscher\u201fs framework were addressed in \nthe accounts of the experiences with international SPFSs suggests that these factors \ndefinitely play some role. Their precise role and the degree to which they influence \nthe success of introducing and using SPFSs are however unknown. More insight into \nthis can only be obtained by means of empirical research; the nature of this research is \nexplored below.  \n \nSPFS examples  \nIn this section, two specific examples of SPFSs are described briefly, in relation to the \nVisscher framework and the evidence on feedback effects. One, Performance \nIndicators in Primary Schools (PIPS), is from the UK, the other, Self-Evaluation in \nPrimary Education (ZEBO), from the Netherlands. \n \nPIPS: Performance Indicators in Primary Schools (Tymms and Albone, 2002) \nThe stated aims of the PIPS project are concerned primarily with school improvement, \ncentred on the belief that \u201ehigh quality information can help schools to find problems\u201f. \nA wider aim, though, involves promoting a more \u201escientific\u201f approach to educational \nprovision and attempting to \u201efoster a way of thinking that involves recourse to \ninvestigation and data rather than argument and opinion\u201f (p194). The assumption \nseems to have been made in the design of this project that the identification of \nweaknesses is a key step in the process of enabling schools to improve themselves.  \nPIPS uses a number of different outcomes at different levels, some of which are also \nthe key outcomes in the \u201eOfficial Accountability System\u201f (Tymms, 1998) of school \nleague tables, intended for accountability and to inform school choice. However, it is \ninteresting that a concern with the quality of these statutory assessments led the \nproject to create its own outcome measures. \nThe initial design of PIPS drew on an already existing model, the ALIS project, which \nat that time had been running successfully for nearly ten years (Fitz-Gibbon, 1996). \nThe development of PIPS has been very much an adaptive process, absorbing \ncontributions from schools, local education authorities, teacher and headteacher \nassociations and, of course, researchers. This constant incorporation of formative \nfeedback might well be seen as a validation of the expressed philosophy of the \nproject; that receiving feedback helps one to improve. One could also interpret the \nattempt to evaluate the impact of the project as evidence of the authenticity of its \ncreator\u201fs belief in \u201einvestigation and data\u201f. \nOne significant feature of PIPS is the fact that schools pay to join, either individually \nor as part of a group (e.g. an LEA). Schools who have paid for the information may \nfind it easier to argue for its confidentiality and against the data being used to hold \nthem accountable. They can choose what they do with it. Equally, the implicit \n 13 \nassumption in the project that simply giving schools feedback is enough to motivate \nthem to act on it may well be true for these volunteers, but arguably might not \ngeneralise to other schools who would not choose (and pay) to join. \nFlexibility is ensured by the variety of options available within the PIPS project. \nThese include choices of year groups, assessments to be used, baselines from which to \ncalculate value added, and levels of aggregation of the data. Some assessments are \nalso provided in computer-adaptive form and the project supplies software for schools \nto analyse and interpret their data. \nConcern with the technical adequacy of the measurements and models used has \nevidently been a key feature of PIPS. Great care has been taken with developing \nreliable and valid measurements. PIPS is unusual among SPFSs in not making use of \nmultilevel models in calculating the \u201evalue added\u201f that is fed back to schools \u2013 a \ndecision justified on the grounds of the greater simplicity and accessibility of ordinary \n(least squares) regression, and the repeated finding that residuals from the two \nmethods differ only trivially (Fitz-Gibbon, 1997). \nThe format of the feedback sent to schools makes extensive use of graphical \nrepresentations and though some of the information is quite complex, the PIPS staff \nhas sought to make it readily interpretable.  \nConsiderable evidence about the use of the data has been collected by the PIPS \nproject. Most teachers seem to use the data to determine the strengths and weaknesses \nof students and classes, and to inform other teachers about this. Around half of them \nuse the feedback to review the curriculum and to set targets. However, we do not \nknow to what extent the data are used beyond the individual-teacher level. This \nevidence about the extent of use of the PIPS data, and the fact that most schools who \nreceive it once continue to pay for it, testifies to its perceived relevance. Feedback is \nrequired to be \u2013 and is \u2013 returned quickly to schools and its relevance is explained by \nthe fact that its content is largely a direct response to consumer demand. \nPIPS has also made considerable efforts to evaluate rigorously its own impact on \nschool performance, and specifically on pupils\u201f achievements. Such efforts seem to be \nexceptional among SPFSs. It is somewhat disappointing, therefore, that the evidence \navailable does not confirm more emphatically the positive impact of PIPS on pupil \nachievement.  \nIn one randomised controlled trial, the invitation to join the project appeared to have \nno effect on achievement and even a slightly negative effect on attitudes. Actually \njoining was associated with slightly better achievement and more positive attitudes, \nthough differences of this size could easily have arisen by chance. In another \ncomparison, pupil gains were slightly greater in the schools that joined the project. \nHowever, none of these differences was statistically significant and the effect sizes are \nquite small (0.1 or less). \n \nZEBO: Self-Evaluation in Primary Education (Hendriks et al., 2002) \nZEBO arose from a strong tradition of evaluation and seems to set out to encompass \nboth improvement and accountability goals under this heading. In many ways \nevaluation emerges as an aim in its own right, underpinned by the belief that teachers \nneed to \u201eexperience a deficiency\u201f before change can turn into improvement. Self-\nevaluation is seen as including monitoring, analysis and diagnosis, as well as \nproviding the basis for informing other audiences about the school\u201fs performance. \nThe project itself was the subject of extensive development, building on existing \ninstruments in the Netherlands and going through several phases of piloting and \nimprovement. The choice of which process indicators to include was partly \n 14 \ndetermined by users (principals and teachers) after the developers were unable to find \nfull agreement within the school effectiveness research literature. \nThe ZEBO project was motivated in part by a concern about the technical adequacy of \nexisting school evaluation instruments, and the developers have provided a \nconsiderable level of detail in explaining the methodology to establish the reliability \nand validity of the instruments used. Clearly, a high priority for the development of \nthis SPFS was that it should meet demanding technical standards.  \nThe efforts made to justify the validity of the feedback are particularly notable. \nTeachers evidently found the feedback generally in line with their previous \nperceptions, but the information helped to provide new insights \u2013 a difficult balance to \nstrike for any SPFS. Its agreement with independent judgements made by the \ninspectorate or from peer observation is perhaps even more impressive. \nZEBO stresses the importance of information about school and classroom processes, \nand provides this mainly through the medium of quantitative data. Much of this is \npresented in the form of graphs comparing school values with those for the whole \npopulation. The project does not attempt to provide longitudinal analysis of trends but \nmay be seen as more of a snapshot of current functioning. The interpretation of the \ndata is largely left to the reader, supported by appropriate manuals and a telephone \nhelp-line. However, it is clear from teachers\u201f responses to the feedback that the \noverwhelming majority have found it easy to interpret. It is also clear that the \nfeedback was widely perceived as timely, relevant and complete. \nThe approach of collecting two perspectives on the same issue (for example, both \nteachers and pupils comment on classroom climate) is an interesting feature of this \nproject. Although this practice was initially justified in terms of establishing the \nconvergent validity of the constructs, it has also proved to be a valued feature of the \nfeedback for schools. The kinds of discussions that arose if, for example, the two \nperspectives appeared to be in conflict were themselves found to be useful to the \nschools in which they were stimulated. \nThe ZEBO feedback seems to have contributed to schools\u201f development plans, \nalthough plans of some kind were often in existence prior to their receiving the \nfeedback, and most of the areas mentioned for development were not directly \naddressed in the feedback. It generally seems to have stimulated discussion at various \nlevels within the school on matters such as policy and plans for improvement, and \nresults were also shared with the school board, parents and the school counselling \nservice. Schools that were inspected after receiving the feedback felt they were better \nprepared as a result and that it made the process more constructive. \n \nDrawing up the balance sheet  \nThe fact that thousands of schools around the world want these systems \u2013 and are \neven willing to pay for the support the SPFSs can offer \u2013 is rare for the outcomes of \neducational research and shows that school feedback systems are not just another \nresult of academic, ivory-tower work with little value for the practice of education. \nSPFSs are practical, thriving entities that are meeting a genuine need in many schools \nand this may be due to the fact that teachers and school managers work in quite \nuncertain contexts. Information is seen as the key to managing this uncertainty.  \nHowever, the fact that people perceive a need for SPFSs does not necessarily mean \nthat the \u201epros\u201f outweigh the \u201econs\u201f, much less that their current formulations are the \nbest they can be. Existing school feedback systems differ considerably in the domains \nmonitored, the units of analysis, and in the aspects of the domains monitored (input, \nprocess, output, outcomes). These differences may partly be due to deliberate \n 15 \ndecisions, for example, a theoretically based focus on school process indicators \nassociated with school effectiveness.  However, it is also possible that differences \nmight disappear \u2013 or be reduced \u2013 if SPFS developers had more information on the \nnature and content of other SPFSs. In other words, exchanging information among \nSPFS developers could lead to more informed development and better SPFSs. \nThere seems to be a general notion that the process of SPFS development consists of \ncreating something that is believed to be good for schools and, therefore, if introduced \nproperly will not only be used, but will improve the quality of school functioning. \nMany SPFS developers have not analysed thoroughly the prerequisites of SPFS use in \nterms of the change in attitudes and skills of school staff. Often enormous resources \nare invested to produce high quality SPFSs. However, the extent of any systematic \ncheck on how schools deal with the result of all that developmental work appears to \nbe negligible. As a consequence of the lack of evaluation of SPFS use, we also lack a \nbasis for improving SPFSs and the process of introducing them into schools.  \nThe previous discussion of the literature on feedback effects showed them to be \nextremely complex, not well understood and quite often even harmful. Given this \ncomplexity and the lack of high quality evaluations of interventions in schools \ninvolving feedback, it is very hard to say with confidence that we know what the \nlikely effects of any SPFS are.  \nWe offer, however, a number of somewhat tentative suggestions, based on the \navailable literature, for optimising the beneficial effects of performance feedback.  \nThe main requirement is that feedback should direct attention to an achievable gap \nbetween desired and actual performance. Two other aspects of the feedback also seem \nimportant, though the existing literature is less clear about these. Firstly, that it should \nfacilitate genuine task learning. This may be achieved by ensuring the feedback is \ntimely and relates to outcomes that are central to performance in the task, rather than \nsuperficial, but perhaps easily measurable aspects of it that might be improved \nwithout genuinely improving performance. Secondly, that it should be perceived as \ncredible, accurate and fair; otherwise it can simply be rejected and produce no change \nin performance. \nMost existing SPFSs do not seem to match very well with many of the clearest \nrecommendations from the literature. In most cases, the feedback is presented as \nneutral information, and hence does not contain any specific cues (or at least not any \nthat were intended). It seems likely, however, that the kinds of feedback presented in \nthe SPFSs could well have conveyed considerable threats to recipients\u201f self-esteem. If \nthe effect of the feedback were to make people think \u201eHow can I avoid looking bad \nhere?\u201f rather than \u201eHow can I do this better?\u201f then existing research suggests it would \nbe unlikely to lead to real improvements in performance. Systems in which the SPFS \nwas part of a wider system of accountability (e.g. Teddlie et al., 2002; Gray, 2002; \nRowe et al., 2002) may be the most prone to this threat. It is also these systems, \nhowever, in which the feedback may have been most closely related to clear, specific \nand challenging task goals. Hence, there may be two conflicting tendencies here, \nmaking the overall effect hard to predict. \nIn all SPFSs the main comparison offered to schools is with other schools. For \nthose doing worse than others this may have provided (at least implicitly) a clear goal \nof catching up, though again for these the potential threat may have been greater. For \nthose shown to be performing better than average, however, it may have been less \nclear what their goals should be. Within the Louisiana accountability system (Teddlie \net al., 2002) schools are also judged by, and given targets for, their improvement, \nwhich may have been a way of providing challenging but achievable targets for all \n 16 \nschools. Other systems (e.g. the English LEAs described by Gray, 2002) may well \nalso have had explicit individual school targets, though it is not clear that the feedback \nspecifically related to such targets. \nWith respect to the feedback\u201fs capacity for facilitating learning and its credibility, \nthe SPFSs we have analysed seem to have fared better. They were all at pains to \nprovide schools with genuine, relevant, timely information about important aspects of \ntheir functioning. For many the explicit intention was that schools should use this \ninformation to learn their own lessons about what is and is not working for them. \nSeveral of them also went to great lengths to ensure that the feedback not only was, \nbut was perceived to be, credible, accurate and fair. \nAlthough these variables have received less attention in the feedback literature, this \nmay be more a reflection of the field\u201fs general concern with theory development and \ntesting rather than evaluating practical applications, and the consequent lack of \necological validity of many of the contexts in which feedback effects have been \nevaluated (e.g. in laboratories). In practical terms, the credibility of the feedback and \nits focus on facilitating learning may be far more important than other features such as \nthreats to self esteem and attributional cues, though what the truth is we do not really \nknow at present. Certainly, we must evaluate the practical implications of these \nfeatures, as well as those that are suggested more strongly by the literature to be \nimportant. Meanwhile, the differences in approach between existing systems and \nresearch evidence may say as much about the limitations of the latter as of the former. \n \nDirections for future research \nDespite the clear good intentions and plausible justifications on the part of the creators \nof school feedback systems, and the positive perceptions of their receivers, we cannot \nbe confident that they are a positive benefit to the schools because they have not been \nevaluated adequately. A thorough and rigorous evaluation of the effects of varying \nSPFSs is urgently needed. Moreover, even if one were convinced that a SPFS had a \npositive impact this would not necessarily be sufficient reason to implement it. The \ncosts of implementation \u2013 in terms of resources and time \u2013 might mean that a similar \nbenefit could be achieved more easily. Thus, the question should not be simply \u201edoes \nit improve performance?\u201f but, just as importantly, \u201ehow much and does the \nimprovement justify the costs?\u201f.  \nNext to the key question of whether the introduction of SPFSs leads to better \nstudent achievement levels, other research questions are relevant and deserve serious \nattention too. First, the occurrence of the other intended and unintended effects in \nBlock F of Figure 1 need to be evaluated. Fulfilling the right conditions for SPFS-\nutilisation takes many years, which implies that improved student achievement levels \nmay only be shown in the (very) long run. In the meantime the prerequisites for \nraising student achievement need to be evaluated: an intensified student achievement \norientation, improved teaching strategies, and the execution of other individual \nactions and school policies meant to improve student achievement. \nIt is also of great importance to evaluate the extent to which unintended effects \noccur, e.g. do schools develop activities that show apparently higher student \nachievement levels which, however, may not be due to better teaching and student \ncounselling but to window dressing? \nIn addition, other evaluation work may improve the quality of SPFSs around the \nworld. As SPFSs differ considerably in their contents, in the support they offer and in \nwhat they demand from school staff, it would be valuable to have an independent \ngroup of evaluators evaluate the quality of alternative SPFSs in terms of the eleven \n 17 \nquality aspects in Block B of the Visscher framework. This may stimulate SPFS \ndevelopers to improve the quality of their SPFSs. \nAnother way of improving the quality of SPFSs empirically may be the study of \ntheir usability among user groups, e.g. by providing alternative types of information \n(e.g. indicators on absolute or relative information, information on school \nperformance trends over the years, and on associations between factors), alternative \nways of information presentation (e.g. numerical or graphical), and investigating to \nwhat extent each alternative is appreciated and provides support. The variation \nbetween SPFSs implies that the specific features of SPFSs always have to be \ndescribed explicitly when their use and impact are studied. When studying the extent \nand nature of SPFS use, the question should not simply be \u201eto what degree SPFSs are \nused, and with which effects?\u201f, but, \u201eWhich types of SPFSs are used mostly in the \ndesired ways, how have they been implemented, and in what types of schools are they \nused intensively?\u201f \nAlternative types of user support concern another valuable topic for research. User-\nsupport and training seem to be crucial prerequisites for successful SPFS \nimplementation. Little empirical evidence, however, is available regarding empirically \nproven methods of support. Again, the relationship between the type of SPFS on the \none hand and the type of user support offered on the other is important, as the two are \nprobably closely interrelated.  \n \nOverall, we could summarise the state of the art by saying that there is still a good \ndeal we do not yet know about the use and impact of SPFSs. We have detailed \naccounts of the features of a number of SPFSs themselves and some knowledge of the \nrationales behind their design. We also have good descriptions of the contexts in \nwhich they have been implemented and the actual process of their implementation. \nWe know little, however, about the nature and extent of their use by schools. \nNevertheless, the framework for analysing the implementation and impact of SPFSs \nas proposed by Visscher provides a basis for comparing their characteristics, and \nclearly points out the gaps in what has been investigated.  \nGiven our ignorance on the effects of implementing any of these SPFSs, we could \nhardly describe their use as being evidence-based. Enlightened policy-makers and \npractitioners will demand something rather more before changing policy or practice. \nNevertheless, there is a prima facie case for believing that SPFSs are a promising way \nforward. With this combination, the rational response must be to conduct more and \nbetter evaluations in order to produce a sounder evidence-base about the use and \neffectiveness of school performance feedback systems. \nOur overall hypothesis is that SPFSs will be used more intensively and effectively \nwhen they are more in accordance with the factors that are identified in the research as \nsignificant, summarized in the Visscher framework presented in this article. For \nexample, we expect that SPFS will be used more if: \n- school staff have developed more 'ownership' of the SPFSs introduced into their \n  school; \n- SPFSs are more flexible in meeting varying information needs among schools; \n- the information fed back is more valid, covers school quality better and allows more \n  in depth analysis of data; \n- the introduction of SPFSs is accompanied by comprehensive, tailored reform and \n  support strategies; \n- implementation of the SPFSs in schools is monitored more consistently; \n- the schools into which SPFSs are introduced promote organisational learning, and \n 18 \n  have a more developed innovation capacity. \n \n \nReferences \nBalcazar, F., Hopkins, B.L., & Suarez, Y. (1985). \u201eA critical, objective review of performance \nfeedback\u201f. Journal of Organizational Behavior Management, 7 (3\/4), 65-89. \nBandura, A. (1991) Social cognitive theory of self regulation. Organizational Behavior and Human \nDecision Processes, 50, 248-287. \nBangert-Drowns, R.L., Kulik, C.C., Kulik, J.A., & Morgan, M. (1991). \u201eThe instructional effect of \nfeedback in test-like events\u201f.  Review of Educational Research, 61, pp213-38. \nBrandsma, H.P., & Edelenbos, P. (1992). \u201eSchool improvement through systematic feedback of pupil \nlevel data at the school and classroom level: an experiment.\u201f Paper presented at the European \nConference on Educational Research, Enschede, The Netherlands, June 1992. \nBrandsma, H.P., & Edelenbos, P. (1998). \u201eThe effects of training programmes for principals and \nteachers in secondary education: a quasi-experiment based on educational effectiveness indicators.\u201f \nPaper presented at the International Congress of School Effectiveness and School Improvement, \nManchester, January 1998. \nBrophy, J. & Good, T. (1974). Teacher-pupil relationships. New York: Holt, Rinehart & Winston. \nButler, R, (1988). Enhancing and undermining intrinsic motivations: the effects of task-involving and \nego-involving evaluation on interest and performance.  British Journal of Educational Psychology, \n58, 1-14 \nCoe, R. (1998a). Can feedback improve teaching? A review of the social science literature with a view \nto identifying the conditions under which giving feedback to teachers will result in improved \nperformance. Research Papers in Education, 13 (1), 43-66. \nCoe, R. (1998b). \u201eFeedback, Value Added and Teachers\u201f Attitudes: Models, Theories and \nExperiments\u201f. Unpublished PhD thesis, University of Durham. \nCoe, R. (2002). \u201eEvidence on the role and impact of performance feedback in schools\u201f. In: Visscher, \nA.J. & Coe, R. (Eds.). School improvement through performance feedback. \nLisse\/Abingdon\/Exton\/Tokyo: Swets and Zeitlinger. \nCohen, P.A. (1980). \u201eEffectiveness of student-rating feedback for improving college instruction: a \nmeta-analysis of findings\u201f. Research in Higher Education, 13, 4, 321-341. \nDalin, P. (1998). Developing the twenty-first century school, a challenge to reformers. In A. \nHargreaves, A. Lieberman, M. Fullan & D. Hopkins (eds.), International Handbook of Educational \nChange (vol. 5, pp.1059-1073). Dordrecht\/Boston\/London: Kluwer Academic Publishers. \nDeShon, R.P., & Alexander, R.A. (1996). Goal setting effects on implicit and explicit learning of \ncomplex tasks. Organizational Behavior and Human Decision Processes, 65, 1, 18-36. \nFitz-Gibbon, C.T. (1996). Monitoring Education: Indicators, Quality and Effectiveness. London: \nCassell. \nFitz-Gibbon, C.T. (2002). A typology of Indicators. In: Visscher, A.J. & Coe, R. (Eds.). School \nimprovement through performance feedback. Lisse\/Abingdon\/Exton\/Tokyo: Swets and Zeitlinger. \nFuchs, L.S., & Fuchs, D. (1986). \u201eEffects of systematic formative evaluation: a meta-analysis.\u201f \nExceptional Children, 53, 199-208. \nGlass, G.V. (1979). Policy for the Unpredictable (Uncertainty Research and Policy). Educational \nResearcher, 8(9), 12-14. \nGoodman, J.S. (1998). The interactive effects of task and external feedback on practice performance \nand learning. Organizational Behavior and Human Decision Processes, 76 (3), 223-252. \nGray, J. (2002). Jolts and Reactions: Two decades of feeding back information on schools\u201f \nperformance. In: Visscher, A.J. & Coe, R. (Eds.) (forthcoming). School improvement through \nperformance feedback. Lisse\/Abingdon\/Exton\/Tokyo: Swets and Zeitlinger.  \nHuberman, M. (1987). Steps towards an integrated model of research utilization. Knowledge: Creation, \nDiffusion, Utilization, vol. 8(4), 586-611. \nKluger, A.N., & DeNisi, A. (1996). The effects of Feedback Interventions on performance: a historical \nreview, a meta-analysis, and a preliminary Feedback Intervention Theory. Psychological Bulletin, \n119, 2, 254-284. \nL\u201fHommedieu, R., Menges, R.J., & Brinko, K.T. (1988). The effects of student ratings feedback to \ncollege teachers: a meta-analysis and review of research. Unpublished manuscript, Northwestern \nUniversity, Centre for the teaching professions, Evanston, IL. \n 19 \nL\u201fHommedieu, R., Menges, R.J., & Brinko, K.T. (1990). \u201eMethodological explanations for the modest \neffects of feedback from student ratings\u201f. Journal of Educational Psychology, 82, 2, 232-241. \nLocke, E.A., & Latham, G.P. (1990). A Theory of Goal Setting and Task Performance. Englewood \nCliffs, NJ: Prentice Hall. \nMaslowski, R., & Visscher, A.J. (1999a). The potential of formative evaluation in program design \nmodels. In J. van den Akker, R. M. Branch, K. Gustafson N. Nieveen, & Tj. Plomp (eds.), Design \nMethodology and Development Research in Education and Training. Dordrecht: Kluwer Academic \nPublishers. \nMcColskey, W., & Leary, M.R. (1985). Differential effects of norm-referenced and self-referenced \nfeedback on performance expectancies, attributions and motivation. Contemporary Educational \nPsychology, 10, 275-284. \nMcLaughlin, M.W. (1998). Listening and learning from the field: tales of policy implementation and \nsituated practice. In A. Hargreaves, A. Lieberman, M. Fullan & D. Hopkins (eds.), International \nHandbook of Educational Change (vol. 5, pp.70-84). Dordrecht\/Boston\/London: Kluwer Academic \nPublishers. \nMcLaughlin, M.W. (1990). The Rand change agent study revisited; macro perspectives and micro \nrealities, Educational Researcher, 19 (9), 11-16. \nMcPherson, R.B., Crowson, R., & Pitner, N.J. (1986). Managing Uncertainty: administrative theory \nand practice in education. Columbus: C.E. Merril Publishing Company. \nMikulincer, M. (1994). Human learned helplessness: A coping perspective. New York: Plenum Press. \nMiles, M.B. (1998). Finding Keys to School Change: A 40-year Odyssey. In A. Hargreaves, A. \nLieberman, M. Fullan & D. Hopkins (eds.), International Handbook of Educational Change (vol. 5, \npp.37-39). Dordrecht\/Boston\/London: Kluwer Academic Publishers. \nMurdoch, K. and Coe, R. (1997). Working with ALIS: a study of how schools and colleges are using a \nvalue added and attitude indicator system. Durham: School of Education, University of Durham, \nUnited Kingdom. \nNeubert, M.J. (1998). The value of feedback and goal setting over goal setting alone and potential \nmoderators of this effect: a meta-analysis. Human Performance, 11 (4) 321-335. \nPodsakoff, P.M., & Farh, J.L. (1989). Effects of feedback sign and credibility on goal setting and task \nperformance. Organizational Behavior and Human Decision Processes, 44(1), 45-67. \nRossi, P.H., & Freeman, H.E. (1993). Evaluation; a systematic approach. Newbury Park\/London\/New \nDelhi: Sage. \nRowe, K.J., Turner R. & Lane, K. (2002). Performance feedback to schools of students\u201f year 12 \nassessments: The VCE data project. In: Visscher, A.J. & Coe, R. (Eds.) School improvement \nthrough performance feedback. Lisse\/Abingdon\/Exton\/Tokyo: Swets and Zeitlinger.  \nScheerens, J. & Bosker, R.J. (1997). The foundations of educational effectiveness. Oxford: Elsevier \nScience Ltd. \nSmith, P. (1995). On the Unintended Consequences of Publishing Performance Data in the Public \nSector. International Journal of Public Administration, 18 (2&3), 277 -310.  \nSmith, L.M. (1998). A kind of educational idealism: integrating realism and reform. In A. Hargreaves, \nA. Lieberman, M. Fullan & D. Hopkins (eds.), International Handbook of Educational Change \n(vol. 5, pp.100-120). Dordrecht\/Boston\/ London: Kluwer Academic Publishers. \nTeddlie, C., Kochan, S. & Taylor, D. (2002). The ABC+ model for school diagnosis, feedback, and \nimprovement. In : Visscher, A.J. & Coe, R. (Eds.) (forthcoming). School improvement through \nperformance feedback. Lisse\/Abingdon\/Exton\/Tokyo: Swets and Zeitlinger.  \nThorndike, E.L. (1913). Educational Psychology. Volume 1: The original nature of man. New York: \nColumbia University Teachers College. \nTymms P.B. (1995). \u201eInfluencing educational practice through performance indicators.\u201f School \nEffectiveness and School Improvement, 6, 2, 123-145. \nTymms, P. (1997a). \u201eResponses of headteachers to value added and the impact of feedback\u201f The Value \nAdded National Project, Technical Report: Primary 3. London: SCAA. \nTymms, P. (1997b). \u201eThe impact of indicators on primary schools.\u201f Paper presented at Evidence-Based \nPolicies and Indicator Systems Conference, University of Durham, July 1997. \nTymms, P.B. (1998). Accountability and Quality Assurance. In C. Richards & P. Taylor (Eds.), How \nshall we school our children? (pp. 171-181). London: Falmer Press. \nTymms, P. and Albone, S. (2002) \u201ePerformance Indicators in Primary Schools\u201f. In: Visscher, A.J. & \nCoe, R. (Eds.). School improvement through performance feedback. Lisse\/Abingdon\/Exton\/Tokyo: \nSwets and Zeitlinger. \n 20 \nVan Vilsteren, C.A. & Visscher, A.J. (1987). Schoolwerkplanning: mogelijk in schoolorganisaties? \n[School work planning: possible in school organisations?] In B. Creemers, J. Giesbers, C. van \nVilsteren & C. van der Perre (Eds.), Handboek Schoolorganisatie en onderwijsmanagement (pp. 6120-\n6124). Alphen aan den Rijn: Samson. \nVisscher, A.J. (2001). Public School Performance Indicators: problems and recommendations. Studies in \nEducational Evaluation, 27(3), 199-214. \nVisscher, A.J. (2002). A framework for studying school performance feedback systems. In: Visscher, \nA.J. & Coe, R. (Eds.) School improvement through performance feedback. \nLisse\/Abingdon\/Exton\/Tokyo: Swets and Zeitlinger.  \nVisscher, A.J. & Coe, R. (2002). Drawing up the balance sheet for school performance feedback \nsystems. In: Visscher, A.J. & Coe, R. (Eds.) School improvement through performance feedback. \nLisse\/Abingdon\/Exton\/Tokyo: Swets and Zeitlinger.  \nVisscher, A.J., Wild, P., & Fung, A. (eds.) (2001). Information Technology in Educational \nManagement; synthesis of experience, research and future perspectives on computer-assisted \nschool information systems. Dordrecht\/Boston\/London: Kluwer Academic Publishers. \nWeiss, C.H. (1998). Improving the use of evaluations: whose job is it anyway? In Reynolds, A.J. & \nWalberg, H.J. (eds.), Advances in Educational Productivity, volume 7, pp.263-276. \nGreenwich\/London: JAI Press. \nWhitford, B.L., & Jones, K. (1998). Assessment and accountability in Kentucky: how high stakes \naffects teaching and learning. In A. Hargreaves, A. Lieberman, M. Fullan & D. Hopkins (eds.), \nInternational Handbook of Educational Change (vol. 5, pp.1163-1178). Dordrecht\/ Boston\/London: \nKluwer Academic Publishers. \nWood, R.E., Mento, A.J., & Locke, E.A. (1987). Task complexity as a moderator of goal effects: a \nmeta-analysis. Journal of Applied Psychology, 72, 416-25. \n"}