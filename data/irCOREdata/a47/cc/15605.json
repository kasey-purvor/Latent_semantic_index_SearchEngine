{"doi":"10.2481\/dsj.6.s172","coreId":"15605","oai":"oai:eprints.erpanet.org:116","identifiers":["oai:eprints.erpanet.org:116","10.2481\/dsj.6.s172"],"title":"Detecting Family Resemblance: Automated Genre Classification.","authors":["Kim, Dr Yunhyong","Ross, Seamus"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2006-01-01","abstract":"This paper presents results in automated genre classification of digital documents in PDF format. It describes genre classification as an important ingredient in contextualising scientific data and in retrieving targetted material for improving research. The current paper compares the role of  visual layout, stylistic features and language model features in clustering documents and presents results in retrieving five selected genres (Scientific Article, Thesis, Periodicals, Business Report, and Form) from a pool of materials populated with documents of the nineteen most popular genres found in our experimental data set.","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/15605.pdf","fullTextIdentifier":"http:\/\/eprints.erpanet.org\/116\/01\/codata2006GCvFinal.pdf","pdfHashValue":"2a886417ba669c41be54f99028b4c21ff49a7f7d","publisher":null,"rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:eprints.erpanet.org:116<\/identifier><datestamp>\n      2007-04-25<\/datestamp><setSpec>\n      7374617475733D756E707562<\/setSpec><setSpec>\n      7375626A656374733D5265736F7572636520446973636F76657279<\/setSpec><setSpec>\n      7375626A656374733D45:4541<\/setSpec><\/header><metadata><oai_dc:dc xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Detecting Family Resemblance: Automated Genre Classification.<\/dc:title><dc:creator>\n        Kim, Dr Yunhyong<\/dc:creator><dc:creator>\n        Ross, Seamus<\/dc:creator><dc:subject>\n        M Resource Discovery<\/dc:subject><dc:subject>\n        EA Metadata<\/dc:subject><dc:description>\n        This paper presents results in automated genre classification of digital documents in PDF format. It describes genre classification as an important ingredient in contextualising scientific data and in retrieving targetted material for improving research. The current paper compares the role of  visual layout, stylistic features and language model features in clustering documents and presents results in retrieving five selected genres (Scientific Article, Thesis, Periodicals, Business Report, and Form) from a pool of materials populated with documents of the nineteen most popular genres found in our experimental data set. <\/dc:description><dc:date>\n        2006-01-01<\/dc:date><dc:type>\n        Preprint<\/dc:type><dc:identifier>\n        http:\/\/eprints.erpanet.org\/116\/<\/dc:identifier><dc:format>\n        pdf http:\/\/eprints.erpanet.org\/116\/01\/codata2006GCvFinal.pdf<\/dc:format><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2006,"topics":["M Resource Discovery","EA Metadata"],"subject":["Preprint"],"fullText":"Detecting\u00a0Family\u00a0Resemblance:\u00a0Automated\u00a0Genre\u00a0Classification\nYunhyong\u00a0Kim\u00a0and\u00a0Seamus\u00a0Ross\nDigital\u00a0Curation\u00a0Centre\u00a0(DCC)\u00a0\u00a0&\u00a0\u00a0Humanities\u00a0Advanced\u00a0Technology\u00a0Information\u00a0Institute\u00a0(HATII),\u00a0University\u00a0\nof\u00a0Glasgow,\u00a0Glasgow,\u00a0UK\nEmail:\u00a0{y.kim,\u00a0s.ross}@hatii.arts.gla.ac.uk\nABSTRACT\nThis\u00a0paper\u00a0presents\u00a0results\u00a0in\u00a0automated\u00a0genre\u00a0classification\u00a0of\u00a0digital\u00a0documents\u00a0in\u00a0PDF\u00a0format.\u00a0It\u00a0describes\u00a0\ngenre\u00a0classification\u00a0as\u00a0an\u00a0important\u00a0ingredient\u00a0in\u00a0contextualising\u00a0scientific\u00a0data\u00a0and\u00a0in\u00a0retrieving\u00a0targetted\u00a0\nmaterial\u00a0for\u00a0improving\u00a0research.\u00a0The\u00a0current\u00a0paper\u00a0compares\u00a0the\u00a0role\u00a0of\u00a0\u00a0visual\u00a0layout,\u00a0stylistic\u00a0features\u00a0and\u00a0\nlanguage\u00a0model\u00a0features\u00a0in\u00a0clustering\u00a0documents\u00a0and\u00a0presents\u00a0results\u00a0in\u00a0retrieving\u00a0five\u00a0selected\u00a0genres\u00a0\n(Scientific\u00a0Article,\u00a0Thesis,\u00a0Periodicals,\u00a0Business\u00a0Report,\u00a0and\u00a0Form)\u00a0from\u00a0a\u00a0pool\u00a0of\u00a0materials\u00a0populated\u00a0with\u00a0\ndocuments\u00a0of\u00a0the\u00a0nineteen\u00a0most\u00a0popular\u00a0genres\u00a0found\u00a0in\u00a0our\u00a0experimental\u00a0data\u00a0set.\u00a0\nkeywords:\u00a0automated\u00a0genre\u00a0classification,\u00a0metadata,\u00a0scientific\u00a0information,\u00a0information\u00a0management,\u00a0\ninformation\u00a0extraction\n1\u00a0\u00a0INTRODUCTION\nScientific\u00a0information\u00a0is\u00a0currently\u00a0being\u00a0created\u00a0at\u00a0an\u00a0exponential\u00a0rate\u00a0and\u00a0in\u00a0many\u00a0different\u00a0forms\u00a0(e.g.\u00a0formally\u00a0\nas\u00a0scientific\u00a0papers,\u00a0raw\u00a0data,\u00a0as\u00a0laboratory\u00a0notes,\u00a0or\u00a0technical\u00a0reports,\u00a0and\u00a0informally\u00a0as\u00a0emails,\u00a0or\u00a0letters).\u00a0Even\u00a0\nwhen\u00a0a\u00a0document\u00a0does\u00a0not\u00a0give\u00a0a\u00a0direct\u00a0description\u00a0of\u00a0scientific\u00a0data\u00a0or\u00a0result\u00a0it\u00a0may\u00a0provide\u00a0the\u00a0context\u00a0\nessential\u00a0for\u00a0interpreting\u00a0the\u00a0scientific\u00a0information:\u00a0the\u00a0context\u00a0for\u00a0understanding\u00a0databases\u00a0can\u00a0often\u00a0be\u00a0found\u00a0\nwithin\u00a0scientific\u00a0papers,\u00a0and,\u00a0in\u00a0turn,\u00a0the\u00a0context\u00a0for\u00a0the\u00a0results\u00a0decribed\u00a0in\u00a0scientific\u00a0papers\u00a0can\u00a0be\u00a0found\u00a0in\u00a0\ninformal\u00a0discussions\u00a0and\u00a0logs\u00a0in\u00a0the\u00a0form\u00a0of\u00a0emails,\u00a0letters,\u00a0laboratory\u00a0notes\u00a0or\u00a0technical\u00a0reports.\u00a0It\u00a0is\u00a0only\u00a0\npossible\u00a0to\u00a0keep\u00a0track\u00a0of\u00a0these\u00a0different\u00a0information\u00a0sources\u00a0and\u00a0relationships\u00a0between\u00a0data\u00a0with\u00a0metadata\u00a0\ndescribing\u00a0the\u00a0content\u00a0of\u00a0the\u00a0object.\u00a0Manually\u00a0acquiring\u00a0such\u00a0metadata\u00a0is\u00a0labour\u00a0intensive\u00a0and\u00a0consequently\u00a0\nexpensive.\u00a0The\u00a0research\u00a0in\u00a0this\u00a0paper\u00a0reflects\u00a0a\u00a0motivation\u00a0to\u00a0automate\u00a0the\u00a0extraction\u00a0of\u00a0metadata\u00a0from\u00a0digital\u00a0\ndocuments.\u00a0Previous\u00a0work\u00a0exists\u00a0on\u00a0the\u00a0extraction\u00a0of\u00a0descriptive\u00a0metadata\u00a0extraction\u00a0within\u00a0specific\u00a0domains\u00a0or\u00a0\ngenres\u00a0(e.g.\u00a0MetadataExtractor,\u00a0DC\u00addot,\u00a0Automatic\u00a0Metadata\u00a0Generation,\u00a0Thoma\u00a0(2001),\u00a0Giuffrida,\u00a0Shek\u00a0&\u00a0\nYang\u00a0(2000),\u00a0Han,\u00a0Giles,\u00a0Manavoglu,\u00a0Zha,\u00a0Zhang\u00a0&\u00a0Fox\u00a0(2000),\u00a0Bekkerman,\u00a0McCallum\u00a0&\u00a0Huang\u00a0(2004),\u00a0Ke,\u00a0\nBowerman\u00a0&\u00a0Oakes\u00a0(2006),\u00a0Sebastiani\u00a0(2002)\u00a0and\u00a0Witte,\u00a0Krestel\u00a0&\u00a0Bergler\u00a0(2005)).\u00a0However,\u00a0a\u00a0general\u00a0tool\u00a0has\u00a0\nyet\u00a0to\u00a0be\u00a0developed\u00a0to\u00a0extract\u00a0metadata\u00a0from\u00a0documents\u00a0of\u00a0varied\u00a0forms\u00a0and\u00a0subjects.\u00a0This\u00a0paper\u00a0is\u00a0a\u00a0\ncontinuation\u00a0of\u00a0the\u00a0work\u00a0in\u00a0Kim\u00a0&\u00a0Ross\u00a0(2006a),\u00a0Kim\u00a0&\u00a0Ross\u00a0(2006b)\u00a0and\u00a0Kim\u00a0&\u00a0Ross\u00a0(2006c)\u00a0to\u00a0develop\u00a0\ngenre\u00a0classification;\u00a0the\u00a0automatic\u00a0detection\u00a0of\u00a0document\u00a0types,\u00a0followed\u00a0by\u00a0deeper\u00a0metadata\u00a0extraction\u00a0from\u00a0\nsingle\u00a0document\u00a0types\u00a0using\u00a0domain\u00adspecific\u00a0methods,\u00a0as\u00a0a\u00a0means\u00a0of\u00a0creating\u00a0an\u00a0over\u00adarching\u00a0tool\u00a0which\u00a0can\u00a0\nextract \u00a0metadata\u00a0across\u00a0 many\u00a0 domains\u00a0 at \u00a0different\u00a0semantic\u00a0levels. \u00a0The\u00a0 focus\u00a0 on\u00a0 genre\u00a0classification\u00a0\nacknowledges\u00a0the\u00a0fact\u00a0that\u00a0different\u00a0communities\u00a0focus\u00a0on\u00a0scientific\u00a0materials\u00a0in\u00a0different\u00a0genres:\u00a0genre\u00a0\nclassification\u00a0will\u00a0support\u00a0automating\u00a0the\u00a0identification,\u00a0selection,\u00a0and\u00a0acquisition\u00a0of\u00a0materials\u00a0in\u00a0keeping\u00a0with\u00a0\nthe\u00a0goals\u00a0of\u00a0different\u00a0scientific\u00a0communities.\nAs\u00a0we\u00a0discussed\u00a0earlier\u00a0(Kim\u00a0&\u00a0Ross\u00a0(2006a))\u00a0there\u00a0is,\u00a0however,\u00a0a\u00a0lack\u00a0of\u00a0consensus\u00a0with\u00a0regard\u00a0to\u00a0the\u00a0\ndefinition\u00a0of\u00a0genre:\u00a0Biber\u2019s\u00a0analysis\u00a0(Biber\u00a0(1995))\u00a0of\u00a0document\u00a0genres\u00a0employed\u00a0five\u00a0dimensions\u00a0(information,\u00a0\nnarration,\u00a0elaboration,\u00a0persuasion,\u00a0abstraction)\u00a0to\u00a0characterise\u00a0text\u00a0while\u00a0others\u00a0(Karlgren\u00a0&\u00a0Cutting\u00a0(1994),\u00a0\nBoese\u00a0(2005))\u00a0examined\u00a0popularly\u00a0recognised\u00a0genre\u00a0classes\u00a0such\u00a0as\u00a0FAQ,\u00a0Job\u00a0Description,\u00a0Editorial\u00a0or\u00a0\nReportage.\u00a0There\u00a0were\u00a0attempts\u00a0(Kessler,\u00a0Nunberg\u00a0&\u00a0Schuetze\u00a0(1997),\u00a0Finn\u00a0&\u00a0Kushmerick\u00a0(2006))\u00a0to\u00a0\nautomatically\u00a0detect\u00a0limited\u00a0facets\u00a0(narravative,\u00a0objectivity,\u00a0intended\u00a0level\u00a0of\u00a0audience,\u00a0positive\u00a0or\u00a0negative\u00a0\nopinion)\u00a0and\u00a0an\u00a0attempt\u00a0(Bagdanov\u00a0&\u00a0Worring\u00a0(2001))\u00a0to\u00a0distinguish\u00a0specific\u00a0journals\u00a0and\u00a0brochures\u00a0from\u00a0one\u00a0\nanother.\u00a0Others\u00a0(Rauber\u00a0&\u00a0Mueller\u00adKoegler\u00a0(2001),\u00a0Barbu,\u00a0Heroux,\u00a0Adam\u00a0&\u00a0Turpin\u00a0(2005))\u00a0have\u00a0clustered\u00a0\ndocuments\u00a0into\u00a0similar\u00a0feature\u00a0groups\u00a0without\u00a0delving\u00a0into\u00a0genre\u00a0facets\u00a0or\u00a0classes.\u00a0An\u00a0overview\u00a0of\u00a0the\u00a0various\u00a0\nefforts\u00a0in\u00a0genre\u00a0analysis\u00a0can\u00a0be\u00a0found\u00a0in\u00a0a\u00a0technical\u00a0report\u00a0by\u00a0Santini\u00a0(2004a)\u00a0and\u00a0Santini\u00a0(2004b)\u00a0and\u00a0a\u00a0report\u00a0on\u00a0metadata\u00a0extraction\u00a0by\u00a0Dobreva,\u00a0Kim\u00a0&\u00a0Ross\n1\u00a0.\u00a0The\u00a0definition\u00a0of\u00a0genre\u00a0adopted\u00a0by\u00a0these\u00a0researchers\u00a0all\u00a0rely\u00a0\non\u00a0a\u00a0combination\u00a0of\u00a0two\u00a0notions:\u00a0one\u00a0of\u00a0structure\u00a0and\u00a0one\u00a0of\u00a0function.\u00a0Structure\u00a0is\u00a0defined\u00a0by\u00a0factors\u00a0which\u00a0are\u00a0\nreflected\u00a0in\u00a0the\u00a0visual\u00a0layout\u00a0of\u00a0the\u00a0document\u00a0while \u00a0function\u00a0 is\u00a0defined\u00a0by\u00a0the\u00a0intended\u00a0purpose\u00a0of\u00a0the\u00a0\ndocument.\u00a0The\u00a0two\u00a0notions\u00a0are\u00a0closely\u00a0related:\u00a0the\u00a0structure\u00a0of\u00a0the\u00a0document\u00a0is\u00a0formed\u00a0to\u00a0optimise\u00a0the\u00a0function\u00a0\nof\u00a0the\u00a0document\u00a0within\u00a0an\u00a0environment,\u00a0such\u00a0as\u00a0within\u00a0the\u00a0context\u00a0of\u00a0the\u00a0community\u00a0or\u00a0event,\u00a0in\u00a0which\u00a0the\u00a0\ndocument\u00a0is\u00a0created.\u00a0\n \nFigure 1: Evolution: document as a dynamic entity\nThe\u00a0situation\u00a0can\u00a0be\u00a0compared\u00a0to\u00a0the\u00a0process\u00a0of\u00a0natural\u00a0selection\u00a0in\u00a0the\u00a0theory\u00a0of\u00a0evolution\u00a0(Figure\u00a01).\u00a0There\u00a0are\u00a0\nbasic\u00a0functions\u00a0required\u00a0for\u00a0an\u00a0organism\u00a0to\u00a0survive\u00a0within\u00a0the\u00a0environment.\u00a0The\u00a0structures\u00a0with\u00a0properties\u00a0to\u00a0\noptimise\u00a0the\u00a0survival\u00a0functions\u00a0are\u00a0most\u00a0likely\u00a0to\u00a0survive.\u00a0These\u00a0structures\u00a0are\u00a0a\u00a0result\u00a0of\u00a0the\u00a0expression\u00a0of\u00a0the\u00a0\ngenes\u00a0in\u00a0the\u00a0DNA\u00a0sequence\u00a0which\u00a0represents\u00a0an\u00a0organism:\u00a0the\u00a0entities\u00a0within\u00a0a\u00a0species\u00a0with\u00a0genes\u00a0which\u00a0\naccompany\u00a0the\u00a0best\u00a0structural\u00a0properties\u00a0will\u00a0prosper.\u00a0The\u00a0question\u00a0lies\u00a0in\u00a0determining\u00a0the\u00a0representation\u00a0of\u00a0a\u00a0\ndocument\u00a0which\u00a0constitutes\u00a0a\u00a0DNA\u00a0sequence,\u00a0and\u00a0further,\u00a0to\u00a0determine\u00a0how\u00a0genetic\u00a0information\u00a0of\u00a0documents\u00a0\nis\u00a0encoded\u00a0in\u00a0the\u00a0representation\u00a0to\u00a0characterise\u00a0the\u00a0document\u00a0type.\u00a0As\u00a0the\u00a0question\u00a0mark\u00a0in\u00a0Figure\u00a01\u00a0indicates,\u00a0\nwe\u00a0feel\u00a0that\u00a0there\u00a0is\u00a0yet\u00a0no\u00a0proper\u00a0understanding\u00a0of\u00a0what\u00a0the\u00a0DNA\u00a0sequence\u00a0of\u00a0a\u00a0document\u00a0should\u00a0be.\u00a0\nIn\u00a0this\u00a0paper,\u00a0we\u00a0propose\u00a0five\u00a0types\u00a0of\u00a0features\u00a0as\u00a0a\u00a0candidate\u00a0subsequence\u00a0of\u00a0DNA\u00a0for\u00a0documents:\u00a0image\u00a0\nfeatures,\u00a0syntactic\u00a0features,\u00a0stylistic\u00a0features,\u00a0semantic\u00a0structure,\u00a0and\u00a0domain\u00a0knowledge\u00a0features.\u00a0We\u00a0aim\u00a0to\u00a0\neventually\u00a0model\u00a0the\u00a0five\u00a0types\u00a0of\u00a0features,\u00a0and\u00a0additional\u00a0features,\u00a0if\u00a0necessary,\u00a0to\u00a0predict\u00a0the\u00a0genres\u00a0of\u00a0PDF\u00a0\ndocuments\u00a0from\u00a0a\u00a0working\u00a0schema\u00a0of\u00a0seventy\u00a0genres\u00a0which\u00a0was\u00a0described\u00a0in\u00a0Kim\u00a0&\u00a0Ross\u00a0(2006a).\u00a0In\u00a0this\u00a0paper,\u00a0\nwe\u00a0will\u00a0examine\u00a0the\u00a0nineteen\u00a0most\u00a0prolific\u00a0PDF\u00a0(Adobe\u00a0Acrobat\u00a0PDF)\u00a0genres\u00a0(Table\u00a01)\u00a0found\u00a0in\u00a0our\u00a0data\u00a0set.\u00a0\nThe\u00a0570\u00a0PDF\u00a0files\u00a0in\u00a0our\u00a0data\u00a0set\u00a0were\u00a0collected\u00a0over\u00a0the\u00a0Internet\u00a0by\u00a0choosing\u00a0a\u00a0random\u00a0word\u00a0from\u00a0a\u00a0dictionary\u00a0\nand\u00a0retrieving\u00a0a\u00a0random\u00a0PDF\u00a0from\u00a0the\u00a0list\u00a0returned\u00a0by\u00a0a\u00a0search\u00a0engine\u00a0(details\u00a0in\u00a0Kim\u00a0&\u00a0Ross\u00a0(2006a)).\u00a0These\u00a0\ngenres\u00a0are\u00a0expected\u00a0to\u00a0represent\u00a0the\u00a0most\u00a0popular\u00a0PDF\u00a0genres\u00a0in\u00a0the\u00a0public\u00a0domain.\u00a0The\u00a0main\u00a0reason\u00a0for\u00a0\nchoosing\u00a0to\u00a0work\u00a0with\u00a0PDF\u00a0files\u00a0is\u00a0the\u00a0fact\u00a0that\u00a0it\u00a0is\u00a0a\u00a0portable\u00a0widely\u00a0used\u00a0format\u00a0for\u00a0archived\u00a0digital\u00a0materials\u00a0\nin\u00a0scientific\u00a0repositories.\u00a0\nTable\u00a01:\u00a0\u00a0Reduced\u00a0Scope\u00a0of\u00a0Genres\nGroups Genres\nBook Academic\u00a0book,\u00a0Fiction,\u00a0Other\u00a0book\nArticle\nScientific\u00a0research\u00a0article,\u00a0Other\u00a0research\u00a0article,\u00a0\nMagazine\u00a0article\nSerial Peridocials\u00a0(Newpaper,\u00a0Magazine),\u00a0Newsletter\nTreatise Thesis,\u00a0Business\/Operational\u00a0report,\u00a0Technical\u00a0report\nInformation\u00a0Structure List,\u00a0Form\n1Funded\u00a0by\u00a0DELOS[11].\u00a0Expected\u00a0to\u00a0be\u00a0available\u00a0December,\u00a02006\nSelection Structure Structure\nDocument\u00a0Function\nOrganism\n? DNA Selection\nSurvival\u00a0Function\nDigital\u00a0Document\nEnvironmentEvidential\u00a0Document Minutes\nOther\u00a0Functional\u00a0Document\nGuideline,\u00a0Job\/Course\/Project\u00a0Description,\u00a0\nProduct\/Application\u00a0Description,\u00a0\nFact\u00a0sheet,\u00a0Slides\nThe\u00a0experiments\u00a0in\u00a0this\u00a0paper\u00a0are\u00a0motivated\u00a0by\u00a0the\u00a0conviction\u00a0that\u00a0a\u00a0comparison\u00a0of\u00a0classifiers\u00a0built\u00a0separately\u00a0on\u00a0\ndifferent\u00a0types\u00a0of\u00a0features\u00a0to\u00a0analyse\u00a0which\u00a0genre\u00a0is\u00a0best\u00a0distinguished\u00a0by\u00a0which\u00a0classifier\u00a0is\u00a0an\u00a0crucial\u00a0step\u00a0in\u00a0\nthe\u00a0construction\u00a0of\u00a0a\u00a0general\u00a0classifier.\u00a0Once\u00a0the\u00a0genres\u00a0are\u00a0better\u00a0ubderstood\u00a0in\u00a0terms\u00a0of\u00a0their\u00a0feature\u00a0strengths,\u00a0\nclassifiers\u00a0can\u00a0be\u00a0combined\u00a0in\u00a0an\u00a0intelligent\u00a0way\u00a0to\u00a0create\u00a0a\u00a0final\u00a0prototype.\u00a0Just\u00a0as\u00a0statistically\u00a0modeling\u00a0the\u00a0\nentire\u00a0DNA\u00a0sequences\u00a0for\u00a0several\u00a0species\u00a0does\u00a0not\u00a0produce\u00a0a\u00a0high\u00adlevel\u00a0performance\u00a0in\u00a0the\u00a0automatic\u00a0detection\u00a0\nof\u00a0family\u00a0resemblance,\u00a0it\u00a0does\u00a0not\u00a0seem\u00a0reasonable\u00a0to\u00a0believe\u00a0that\u00a0statistically\u00a0bundling\u00a0up\u00a0all\u00a0these\u00a0features\u00a0\nwould\u00a0result\u00a0in\u00a0an\u00a0optimal\u00a0automatic\u00a0classification\u00a0system\u00a0of\u00a0document\u00a0types.\u00a0If\u00a0all\u00a0features\u00a0are\u00a0processed\u00a0in\u00a0one\u00a0\nclassifier,\u00a0the\u00a0statistical\u00a0model\u00a0can\u00a0be\u00a0misled\u00a0by\u00a0non\u00addistinguishing\u00a0features.\u00a0If\u00a0we\u00a0were\u00a0to\u00a0train\u00a0on\u00a0sufficient\u00a0\ndata,\u00a0this\u00a0would\u00a0not\u00a0be\u00a0a\u00a0problem;\u00a0the\u00a0non\u00addistinguishing\u00a0features\u00a0will\u00a0be\u00a0filtered\u00a0out\u00a0as\u00a0noise.\u00a0It\u00a0is,\u00a0however,\u00a0\nvery\u00a0difficult\u00a0to\u00a0have\u00a0sufficient\u00a0data\u00a0when\u00a0constructing\u00a0a\u00a0tool\u00a0which\u00a0is\u00a0intended\u00a0to\u00a0have\u00a0dynamic\u00a0and\u00a0domain\u00ad\nindependent\u00a0properties.\u00a0In\u00a0Kim\u00a0(2004)\u00a0and\u00a0Kim\u00a0&\u00a0Webber\u00a0(2006),\u00a0the\u00a0CANDC\u00a0part\u00adof\u00adspeech\u00a0tagger\u00a0(Curran\u00a0\n&\u00a0Clark\u00a0(2003)),\u00a0reputed\u00a0to\u00a0have\u00a0performed\u00a0well\u00a0elsewhere,\u00a0was\u00a0employed\u00a0to\u00a0tag\u00a0words\u00a0in\u00a0Astronomy\u00a0research\u00a0\narticles.\u00a0In\u00a0Astronomy\u00a0there\u00a0is\u00a0frequent\u00a0usage\u00a0of\u00a0the\u00a0term\u00a0He\u00a0to\u00a0refer\u00a0to\u00a0the\u00a0chemical\u00a0element\u00a0Helium.\u00a0The\u00a0\ntagger,\u00a0which\u00a0was\u00a0trained\u00a0on\u00a0the\u00a0Wall\u00a0Street\u00a0Journal\u00a0articles,\u00a0tagged\u00a0He\u00a0to\u00a0be\u00a0a\u00a0pronoun\u00a0for\u00a0all\u00a0instances,\u00a0\npropagating\u00a0further\u00a0errors\u00a0on\u00a0subsequent\u00a0words.\u00a0Separating\u00a0features\u00a0into\u00a0smaller\u00a0groups\u00a0will\u00a0minimise\u00a0the\u00a0\nimpact\u00a0of\u00a0such\u00a0artefacts,\u00a0by\u00a0trying\u00a0to\u00a0exclude\u00a0the\u00a0noise\u00a0from\u00a0the\u00a0start,\u00a0making\u00a0the\u00a0most\u00a0of\u00a0the\u00a0differing\u00a0feature\u00a0\nstrengths\u00a0for\u00a0each\u00a0genre\u00a0type.\nThis\u00a0paper,\u00a0along\u00a0with\u00a0Kim\u00a0&\u00a0Ross\u00a0(2006a),\u00a0Kim\u00a0&\u00a0Ross\u00a0(2006b)\u00a0and\u00a0Kim\u00a0&\u00a0Ross\u00a0(2006c)\u00a0also\u00a0emphasises\u00a0\nthat\u00a0the\u00a0bottom\u00adup\u00a0approach\u00a0of\u00a0starting\u00a0from\u00a0genre\u00adspecific\u00a0extraction\u00a0may\u00a0result\u00a0in\u00a0several\u00a0tools\u00a0which\u00a0are\u00a0\noverly\u00a0dependent\u00a0on\u00a0the\u00a0structures\u00a0of\u00a0the\u00a0documents\u00a0within\u00a0a\u00a0specific\u00a0domain,\u00a0with\u00a0no\u00a0obvious\u00a0means\u00a0of\u00a0\ninteroperability.\u00a0The\u00a0top\u00addown\u00a0approach\u00a0of\u00a0creating\u00a0a\u00a0tool\u00a0which\u00a0stretches\u00a0across\u00a0genres,\u00a0to\u00a0be\u00a0refined\u00a0further\u00a0\nwithin\u00a0the\u00a0domain,\u00a0will\u00a0enable\u00a0us\u00a0to\u00a0avoid\u00a0this\u00a0problem.\n2\u00a0\u00a0CLASSIFIERS\nThe\u00a0experiments\u00a0described\u00a0in\u00a0this\u00a0paper\u00a0involve\u00a0the\u00a0use\u00a0of\u00a0three\u00a0classifiers:\nImage\u00a0classifier:this\u00a0classifier\u00a0depends\u00a0on\u00a0features\u00a0extracted\u00a0from\u00a0the\u00a0PDF\u00a0document\u00a0when\u00a0handled\u00a0as\u00a0an\u00a0\nimage.\u00a0It\u00a0uses\u00a0the\u00a0module\u00a0pdftoppm\u00a0from\u00a0XPDF\u00a0(Noonberg\u00a0(2006))\u00a0to\u00a0extract\u00a0the\u00a0first\u00a0page\u00a0of\u00a0the\u00a0document\u00a0as\u00a0\nan\u00a0image.\u00a0The\u00a0resulting\u00a0image\u00a0is\u00a0divided\u00a0into\u00a0a\u00a0sixty\u00adsix\u00a0by\u00a0sixty\u00adsix\u00a0grid\n2\u00a0.\u00a0Then\u00a0Python\u2019s\u00a0Image\u00a0Library\u00a0(PIL)\u00a0\nis\u00a0employed\u00a0to\u00a0extract\u00a0pixel\u00a0values\u00a0in\u00a0each\u00a0region.\u00a0Each\u00a0region\u00a0is\u00a0given\u00a0a\u00a0value\u00a0of\u00a00\u00a0or\u00a01\u00a0depending\u00a0on\u00a0whether\u00a0\nthere\u00a0is\u00a0more\u00a0than\u00a0one\u00a0pixel\u00a0darker\u00a0than\u00a0a\u00a0specified\u00a0value\u00a0of\u00a0245.\u00a0The\u00a0result\u00a0is\u00a0modeled\u00a0using\u00a0Na\u00efve\u00a0Bayes\u00a0as\u00a0\nimplemented\u00a0by\u00a0the\u00a0Weka\u00a0(Witten\u00a0&\u00a0Frank\u00a0(2005)\u00a0)\u00a0machine\u00a0learning\u00a0toolkit.\nLanguage\u00a0model\u00a0classifier:this\u00a0classifier\u00a0depends\u00a0on\u00a0an\u00a0N\u00adgram\u00a0model\u00a0on\u00a0the\u00a0level\u00a0of\u00a0words,\u00a0Part\u00adof\u00adSpeech\u00a0\ntags\u00a0and\u00a0Partial\u00a0Parsing\u00a0tags.\u00a0N\u00adgram\u00a0models\u00a0look\u00a0at\u00a0the\u00a0possibility\u00a0of\u00a0unit\u00a0W(N)\u00a0coming\u00a0after\u00a0a\u00a0string\u00a0of\u00a0units\u00a0\nW(1),W(2),...,w(N\u00ad1).\u00a0A\u00a0popular\u00a0model\u00a0is\u00a0the\u00a0case\u00a0when\u00a0N=3.\u00a0In\u00a0the\u00a0experiments\u00a0of\u00a0the\u00a0current\u00a0paper,\u00a0we\u00a0are\u00a0\nonly\u00a0working\u00a0with\u00a0the\u00a0model\u00a0where\u00a0W(i)\u00a0are\u00a0words.\u00a0This\u00a0can\u00a0be\u00a0modeled\u00a0for\u00a0other\u00a0syntactic\u00a0units\u00a0or\u00a0semantic\u00a0\nunits\u00a0to\u00a0capture\u00a0the\u00a0patterns\u00a0of\u00a0higher\u00a0level\u00a0structures.\u00a0This\u00a0has\u00a0been\u00a0modelled\u00a0by\u00a0the\u00a0BOW\u00a0toolkit\u00a0developed\u00a0by\u00a0\nAndrew\u00a0McCallum\u00a0(1998).\u00a0We\u00a0used\u00a0the\u00a0default\u00a0Na\u00efve\u00a0Bayes\u00a0model.\nStylo\u00admetric\u00a0classifier:this\u00a0classifier\u00a0looks\u00a0at\u00a0the\u00a0frequency\u00a0of\u00a0selected\u00a0words,\u00a0number\u00a0of\u00a0font\u00a0changes,\u00a0the\u00a0\ndifference\u00a0between\u00a0the\u00a0largest\u00a0font\u00a0size\u00a0and\u00a0smallest\u00a0font\u00a0size,\u00a0length\u00a0of\u00a0the\u00a0document\u00a0,\u00a0average\u00a0length\u00a0of\u00a0words,\u00a0\nand\u00a0number\u00a0of\u00a0words\u00a0in\u00a0the\u00a0front\u00a0page\u00a0of\u00a0the\u00a0document.\u00a0The\u00a0font\u00a0information\u00a0was\u00a0extracted\u00a0on\u00a0the\u00a0level\u00a0of\u00a0\nwords\u00a0using\u00a0a\u00a0modified\u00a0version\u00a0of\u00a0PDFTOHTML,\u00a0developed\u00a0as\u00a0part\u00a0of\u00a0the\u00a0DELOS\u00a0Digital\u00a0Preservation\u00a0Cluster\u00a0\n2\u00a0The\u00a0choice\u00a0of\u00a0the\u00a0dimension\u00a0reflects\u00a0the\u00a0fact\u00a0that\u00a0it\u00a0seemed\u00a0to\u00a0produce\u00a0the\u00a0best\u00a0results\u00a0at\u00a0the\u00a0time\u00a0but\u00a0further\u00a0\nanalysis\u00a0may\u00a0be\u00a0necessary.investigations\u00a0at\u00a0Historisch\u00adKulturwissenschaftliche\u00a0Informationsverarbeitung\u00a0(HKI),\u00a0University\u00a0of\u00a0Cologne.\u00a0\nThe\u00a0modified\u00a0version\u00a0converts\u00a0a\u00a0PDF\u00a0document\u00a0to\u00a0an\u00a0XML\u00a0file\u00a0with\u00a0the\u00a0font\u00a0size\u00a0and\u00a0style\u00a0information\u00a0for\u00a0each\u00a0\nword\u00a0in\u00a0the\u00a0document.\u00a0A\u00a0word\u00a0list\u00a0was\u00a0automatically\u00a0constructed\u00a0containing\u00a0all\u00a0words\u00a0which\u00a0appear\u00a0in\u00a0more\u00a0\nthan\u00a0half\u00a0of\u00a0the\u00a0files\u00a0in\u00a0any\u00a0one\u00a0genre.\u00a0For\u00a0each\u00a0file,\u00a0the\u00a0frequency\u00a0of\u00a0each\u00a0word\u00a0was\u00a0recorded\u00a0as\u00a0a\u00a0vector\u00a0then\u00a0\naugmented\u00a0by\u00a0length\u00a0and\u00a0font\u00a0information.\u00a0The\u00a0result\u00a0was\u00a0modeled\u00a0using\u00a0Na\u00efve\u00a0Bayes\u00a0in\u00a0the\u00a0Weka\u00a0(Witten\u00a0&\u00a0\nFrank\u00a0(2005))\u00a0machine\u00a0learning\u00a0toolkit.\nWe\u00a0have\u00a0chosen\u00a0to\u00a0use\u00a0Na\u00efve\u00a0Bayes\u00a0method\u00a0for\u00a0all\u00a0the\u00a0classifiers.\u00a0This\u00a0is\u00a0because\u00a0the\u00a0experiments\u00a0are\u00a0intended\u00a0\nto\u00a0compare\u00a0different\u00a0feature\u00a0sets\u00a0given\u00a0similar\u00a0conditions.\u00a0Further\u00a0analysis\u00a0 \u00a0will\u00a0be\u00a0required\u00a0to\u00a0make\u00a0a\u00a0\ncomparison\u00a0of\u00a0the\u00a0effects\u00a0of\u00a0different\u00a0statistical\u00a0models.\u00a0Naive\u00a0Bayes\u00a0was\u00a0chosen\u00a0for\u00a0the\u00a0experiments\u00a0in\u00a0this\u00a0\npaper\u00a0because\u00a0it\u00a0gave\u00a0the\u00a0best\u00a0overall\u00a0results\u00a0for\u00a0all\u00a0the\u00a0classifiers.\nThe\u00a0view\u00a0in\u00a0this\u00a0paper\u00a0is\u00a0that\u00a0the\u00a0image\u00a0along\u00a0with\u00a0the\u00a0stylistic\u00a0features\u00a0will\u00a0capture\u00a0the\u00a0structural\u00a0elements\u00a0of\u00a0\ngenres\u00a0while\u00a0the\u00a0language\u00a0model\u00a0combined\u00a0with\u00a0the\u00a0stylistic\u00a0and\u00a0semantic\u00a0features\u00a0will\u00a0help\u00a0to\u00a0separate\u00a0\ndocuments\u00a0of\u00a0distinct\u00a0functional\u00a0categories.\u00a0\nInvolving\u00a0the\u00a0image\u00a0of\u00a0a\u00a0document\u00a0in\u00a0the\u00a0process\u00a0may\u00a0enable\u00a0genre\u00a0classification\u00a0of\u00a0documents\u00a0without\u00a0\nviolating\u00a0pasword\u00a0protection\u00a0or\u00a0copyright,\u00a0will\u00a0maximise\u00a0the\u00a0viability\u00a0of\u00a0a\u00a0language\u00a0independent\u00a0tool\u00a0and\u00a0free\u00a0\nthe\u00a0process\u00a0from\u00a0being\u00a0solely\u00a0dependent\u00a0on\u00a0text\u00a0processing\u00a0tools\u00a0with\u00a0encoding\u00a0requirements\u00a0and\u00a0problems\u00a0\nrelating\u00a0to\u00a0special\u00a0characters\n3.\u00a0It\u00a0also\u00a0makes\u00a0part\u00a0of\u00a0the\u00a0process\u00a0immediately\u00a0applicable\u00a0to\u00a0paper\u00a0documents\u00a0\ndigitally\u00a0imaged\u00a0(i.e.\u00a0scanned).\u00a0\n3\u00a0\u00a0EXPERIMENTS\nTwo\u00a0main\u00a0experiments\u00a0are\u00a0described\u00a0in\u00a0this\u00a0paper:\u00a0\u00a0\nClustering\u00a0experiment:in\u00a0this\u00a0experiment\u00a0we\u00a0compared\u00a0the\u00a0cluster\u00a0resolution\u00a0for\u00a0three\u00a0sets\u00a0of\u00a0features:\u00a0the\u00a0\nimage\u00a0features,\u00a0the\u00a0stylo\u00admetric\u00a0features,\u00a0and\u00a0the\u00a0case\u00a0where\u00a0the\u00a0two\u00a0features\u00a0were\u00a0combined.\u00a0We\u00a0grouped\u00a0the\u00a0\ndata\u00a0in\u00a0nineteen\u00a0genres\u00a0into\u00a0two\u00a0clusters\u00a0using\u00a0the\u00a0Weka\u00a0Machine\u00a0Learning\u00a0Toolkit\u2019s\u00a0(Witten\u00a0&\u00a0Frank\u00a0(2005))\u00a0\nEstimation\u00adMaximisation\u00a0algorithm.\u00a0The\u00a0purpose\u00a0was\u00a0to\u00a0see\u00a0how\u00a0well\u00a0the\u00a0files\u00a0in\u00a0each\u00a0genre\u00a0group\u00a0into\u00a0one\u00a0\ncluster.\u00a0The\u00a0result\u00a0is\u00a0expressed\u00a0in\u00a0terms\u00a0of\u00a0the\u00a0percentage\u00a0of\u00a0files\u00a0within\u00a0each\u00a0genre\u00a0which\u00a0have\u00a0been\u00a0grouped\u00a0\ninto\u00a0one\u00a0cluster.\nA\u00a0Comparison\u00a0of\u00a0Binary\u00a0Predictions:In\u00a0this\u00a0experiment\u00a0we\u00a0will\u00a0compare\u00a0the\u00a010\u00adfold\u00a0cross\u00a0validation\u00a0test\u00a0of\u00a0\nthe\u00a0three\u00a0classifiers\u00a0described\u00a0in\u00a0Section\u00a02\u00a0in\u00a0the\u00a0following\u00a0binary\u00a0predictions.\n\u2022 \u00a0Periodicals\u00a0versus\u00a0Other\u00a0Genre:\u00a0 in\u00a0Kim\u00a0&\u00a0Ross\u00a0(2006c)\u00a0we\u00a0presented\u00a0the\u00a0results\u00a0of\u00a0distinguishing\u00a0\nPeriodicals\u00a0from\u00a0Thesis\u00a0and\u00a0Periodicals\u00a0from\u00a0Non\u00adperiodicals\u00a0consisting\u00a0of\u00a0Thesis,\u00a0Business\u00a0Reports,\u00a0Minutes,\u00a0\nAcademic\u00a0Book,\u00a0and\u00a0Fictional\u00a0Book.\u00a0In\u00a0this\u00a0experiment,\u00a0we\u00a0have\u00a0expanded\u00a0the\u00a0class\u00a0of\u00a0non\u00adperiodicals\u00a0to\u00a0Other\u00a0\nGenre\u00a0to\u00a0include\u00a0all\u00a0of\u00a0the\u00a0genres\u00a0in\u00a0Table\u00a01\u00a0apart\u00a0from\u00a0periodicals.\u00a0\n\u2022\u00a0Scientific\u00a0Research\u00a0Article\u00a0versus\u00a0Other\u00a0Genre:\u00a0in\u00a0this\u00a0experiment,\u00a0we\u00a0test\u00a0the\u00a0binary\u00a0classification\u00a0of\u00a0\ngenres\u00a0in\u00a0Table\u00a01\u00a0into\u00a0Scientific\u00a0Research\u00a0Article\u00a0and\u00a0Other\u00a0Genre.\u00a0\n\u2022\u00a0Thesis\u00a0versus\u00a0Other\u00a0Genres\u00a0:\u00a0in\u00a0this\u00a0experiment,\u00a0we\u00a0test\u00a0the\u00a0binary\u00a0classification\u00a0of\u00a0genres\u00a0in\u00a0Table\u00a01\u00a0into\u00a0\nThesis\u00a0and\u00a0Other\u00a0Genre.\u00a0\nWe\u00a0also\u00a0compared\u00a0the\u00a0image\u00a0classifier\u00a0to\u00a0the\u00a0stylo\u00admetric\u00a0classifier\u00a0in\u00a0the\u00a0following\u00a0binary\u00a0predictions:\u00a0\n\u2022\u00a0Business\u00a0Report\u00a0versus\u00a0Other\u00a0Genres:\u00a0in\u00a0this\u00a0experiment,\u00a0we\u00a0test\u00a0the\u00a0binary\u00a0classification\u00a0of\u00a0genres\u00a0in\u00a0\nTable\u00a01\u00a0into\u00a0Thesis\u00a0and\u00a0Other\u00a0Genre.\u00a0\n\u2022\u00a0Forms\u00a0versus\u00a0Other\u00a0Genres\u00a0:\u00a0in\u00a0this\u00a0experiment,\u00a0we\u00a0test\u00a0the\u00a0binary\u00a0classification\u00a0of\u00a0genres\u00a0in\u00a0Table\u00a01\u00a0into\u00a0\nThesis\u00a0and\u00a0Other\u00a0Genre.\u00a0\nFinally,\u00a0we\u00a0also\u00a0present\u00a0the\u00a0results\u00a0for\u00a0detecting\u00a0Forms,\u00a0using\u00a0the\u00a0stylo\u00admetric\u00a0classifier,\u00a0within\u00a0a\u00a0group\u00a0of\u00a0files\u00a0\npopulated \u00a0 with \u00a0 a \u00a0 smaller \u00a0 variety \u00a0 of \u00a0 genres \u00a0 including \u00a0 Fact \u00a0 Sheet, \u00a0 Forms, \u00a0 Instruction\/Guideline,\u00a0\nJob\/Course\/Project\u00a0Description,\u00a0Minutes,\u00a0Newsletter,\u00a0Scientific\u00a0Reserch\u00a0Article\u00a0and\u00a0Other\u00a0research\u00a0Article.\u00a0\n3\u00a0pdftohtml\u00a0failed\u00a0to\u00a0extract\u00a0information\u00a0from\u00a0seventeen\u00a0percent\u00a0of\u00a0the\u00a0documents.\u00a0The\u00a0image\u00a0processing\u00a0did\u00a0not\u00a0\nfail\u00a0on\u00a0any\u00a0documents.4\u00a0\u00a0RESULTS\nTable\u00a02\u00a0shows\u00a0the\u00a0results\u00a0of\u00a0the\u00a0clustering\u00a0experiment.\u00a0The\u00a0percentages\u00a0for\u00a0the\u00a0visual\u00a0clusters\u00a0are\u00a0different\u00a0from\u00a0\nthe\u00a0previously\u00a0reported\u00a0clusters\u00a0(Kim\u00a0&\u00a0Ross\u00a0(2006a))\u00a0because\u00a0we\u00a0have\u00a0excluded\u00a0the\u00a0files\u00a0for\u00a0which\u00a0pdftohtml\u00a0\nfailed\u00a0to\u00a0extract\u00a0the\u00a0correct\u00a0information.\u00a0The\u00a0results\u00a0of\u00a0this\u00a0experiment\u00a0encourages\u00a0two\u00a0conclusions:\u00a0\n1.\u00a0The\u00a0genres\u00a0for\u00a0which\u00a0image\u00a0features\u00a0fail\u00a0to\u00a0show\u00a0a\u00a0sharp\u00a0clustering\u00a0tendency\u00a0(>\u00a080)\u00a0tend\u00a0to\u00a0be\u00a0the\u00a0genres\u00a0for\u00a0\nwhich\u00a0stylo\u00admetric\u00a0features\u00a0cluster\u00a0very\u00a0well\u00a0and\u00a0vice\u00a0versa.\u00a0In\u00a0the\u00a0case\u00a0of\u00a0only\u00a0three\u00a0genres\u00a0(Forms,\u00a0Job\u00a0\nDescription,\u00a0and\u00a0Product\u00a0Description)\u00a0this\u00a0does\u00a0not\u00a0hold.\u00a0For\u00a0instance,\u00a0note\u00a0that,\u00a0stylistic\u00a0features\u00a0only\u00a0group\u00a0\n62.5\u00a0percent\u00a0of\u00a0files\u00a0in\u00a0the\u00a0class\u00a0Periodicals\u00a0into\u00a0one\u00a0cluster\u00a0whereas\u00a0the\u00a0visual\u00a0features\u00a0group\u00a0one\u00a0hundred\u00a0\npercent\u00a0of\u00a0the\u00a0files\u00a0into\u00a0one\u00a0cluster.\u00a0\n2.\u00a0Clustering\u00a0based\u00a0on\u00a0a\u00a0combination\u00a0of\u00a0both\u00a0types\u00a0of\u00a0features\u00a0at\u00a0the\u00a0same\u00a0time\u00a0does\u00a0not\u00a0improve\u00a0upon\u00a0\nclustering\u00a0based\u00a0on\u00a0features\u00a0of\u00a0one\u00a0type.\u00a0In\u00a0fact\u00a0the\u00a0combined\u00a0system,\u00a0as\u00a0the\u00a0third\u00a0column\u00a0of\u00a0Table\u00a02\u00a0shows,\u00a0\nresults\u00a0in\u00a0blurring\u00a0the\u00a0division\u00a0for\u00a0most\u00a0genres.\u00a0And,\u00a0in\u00a0the\u00a0the\u00a0case\u00a0of\u00a0Forms\u00a0and\u00a0Scientific\u00a0Articles,\u00a0the\u00a0\ncombined\u00a0features\u00a0show\u00a0poorer\u00a0clustering\u00a0results\u00a0than\u00a0either\u00a0of\u00a0the\u00a0clustering\u00a0results\u00a0in\u00a0column\u00a0one\u00a0and\u00a0two.\u00a0\nTable\u00a02:\u00a0\u00a0A\u00a0Comparison\u00a0of\u00a0Visual\u00a0and\u00a0Stylo\u00admetric\u00a0Clusters\u00a0(percentage\u00a0of\u00a0files\u00a0in\u00a0one\u00a0cluster)\nGroups Genres visual stylistic combi\nBook\nAcademic\nFiction\nOther\u00a0Book\n100%\n92.8%\n70.6%\n60%\u00a0\n83.3%\n82.4%\n100%\n75%\n70.6%\nArticle\nSci.\u00a0Research\u00a0Article\nOther\u00a0Research\u00a0Article\nMagazine\u00a0Article\n76%\n94.7%\n61.5%\n92%\n73.7%\n84.6%\n64%\n84.2%\n61.5%\nSerial\nPeriodicals\u00a0(Newspapers,\u00a0Magazine)\nNewsletter\n100%\n54.2%\n63%\n83.3%\n88%\n58.3%\nTreatise\nThesis\nBusiness\/Operational\u00a0Report\nTechnical\u00a0Report\n100%\n81.8%\n88.9%\n90%\n90.9%\n72.2%\n90%\n81.8%\n83.3%\nInformation\u00a0Structure List\nForms\n71%\n61.5%\n86%\n69.2%\n71%\n53.8%\nEvidential\u00a0Document Minutes 100% 77% 100%\nOther\u00a0Functional\u00a0Documents\nInstruction\/Guideline\nJob\/Course\/Project\u00a0Description\nProduct\/Application\u00a0Description\nFactsheet\nSlides\n95%\n73.3%\n62.5%\n72.4%\n61.5%\n79%\n50%\n66.7%\n85.7%\n91.7%\n90%\n73.3%\n56.3%\n64.3%\n61.5%\nThe\u00a0results\u00a0described\u00a0in\u00a0Tables\u00a03,\u00a04,\u00a05,\u00a06,\u00a07\u00a0and\u00a08\u00a0use\u00a0three\u00a0standard\u00a0indices\u00a0in\u00a0classification\u00a0tasks:\u00a0accuracy,\u00a0\nprecision\u00a0and\u00a0recall.\u00a0Let\u00a0N\u00a0be\u00a0the\u00a0total\u00a0number\u00a0of\u00a0documents\u00a0in\u00a0the\u00a0data,\u00a0\u00a0NC\u00a0the\u00a0number\u00a0of\u00a0documents\u00a0in\u00a0the\u00a0\ndata\u00a0set\u00a0which\u00a0are\u00a0in\u00a0class\u00a0C,\u00a0T\u00a0the\u00a0total\u00a0number\u00a0of\u00a0correctly\u00a0labelled\u00a0documents\u00a0in\u00a0the\u00a0data\u00a0set\u00a0independent\u00a0of\u00a0\nthe\u00a0class,\u00a0\u00a0TC\u00a0the\u00a0number\u00a0of\u00a0true\u00a0positives\u00a0for\u00a0class\u00a0C,\u00a0and\u00a0\u00a0FC\u00a0the\u00a0number\u00a0of\u00a0false\u00a0positives\u00a0for\u00a0class\u00a0C.\u00a0\nAccuracy\u00a0is\u00a0defined\u00a0to\u00a0be\u00a0T\/N;\u00a0precision\u00a0and\u00a0recall\u00a0for\u00a0class\u00a0C\u00a0is\u00a0defined\u00a0to\u00a0be\u00a0\u00a0TC\/(TC+FC)\u00a0and\u00a0TC\/NC,\u00a0\nrespectively.\u00a0\nIn\u00a0[22]\u00a0the\u00a0image\u00a0classifier\u00a0was\u00a0compared\u00a0against\u00a0the\u00a0stylo\u00admetric\u00a0classifier\u00a0to\u00a0show\u00a0that\u00a0the\u00a0image\u00a0classifier\u00a0\nperformed\u00a0much\u00a0better\u00a0in\u00a0distinguishing\u00a0Periodicals\u00a0from\u00a0the\u00a0group\u00a0of\u00a0Non\u00adperiodicals\u00a0(consisting\u00a0of\u00a0Thesis,\u00a0\nBusiness\u00a0Report,\u00a0Minutes,\u00a0Fictional\u00a0Book\u00a0and\u00a0Academic\u00a0Book).\u00a0In\u00a0Table3,\u00a0we\u00a0have\u00a0presented\u00a0the\u00a0results\u00a0of\u00a0a\u00a0\n10\u00adfold\u00a0cross\u00a0validation\u00a0test\u00a0using\u00a0the\u00a0image,\u00a0stylo\u00admetric\u00a0and\u00a0language\u00a0model\u00a0classifiers\u00a0(in\u00a0respective\u00a0order\u00a0\nfrom\u00a0top\u00a0to\u00a0bottom)\u00a0on\u00a0detecting\u00a0Periodicals.\u00a0If\u00a0we\u00a0put\u00a0only\u00a0the\u00a0overall\u00a0accuracy\u00a0into\u00a0consideration,\u00a0the\u00a0language\u00a0model\u00a0seems\u00a0to\u00a0be\u00a0the\u00a0best\u00a0classifier,\u00a0but,\u00a0the\u00a0recall\u00a0rates\u00a0show\u00a0that\u00a0the\u00a0language\u00a0model\u00a0classifier\u00a0failed\u00a0to\u00a0label\u00a0\neven\u00a0a\u00a0single\u00a0periodical\u00a0correctly.\u00a0The\u00a0recall\u00a0rate\u00a0for\u00a0the\u00a0stylo\u00admetric\u00a0classifier\u00a0fares\u00a0better\u00a0but\u00a0it\u00a0is\u00a0nowhere\u00a0near\u00a0\nthat\u00a0of\u00a0the\u00a0image\u00a0classifier.\u00a0The\u00a0language\u00a0model\u00a0classifier\u00a0and\u00a0the\u00a0stylo\u00admetric\u00a0classifier\u00a0are\u00a0labelling\u00a0every\u00a0item\u00a0\nor\u00a0almost\u00a0every\u00a0item\u00a0as\u00a0Other\u00a0Genre,\u00a0thereby\u00a0making\u00a0no\u00a0distinction\u00a0between\u00a0Periodicals\u00a0and\u00a0Non\u00adperiodicals;\u00a0\nthe\u00a0higher\u00a0percentage\u00a0of\u00a0Other\u00a0Genre\u00a0in\u00a0experimental\u00a0data\u00a0set\u00a0ensures\u00a0a\u00a0high\u00a0overall\u00a0accuracy\u00a0rate\u00a0for\u00a0a\u00a0blind\u00a0\nsystem\u00a0which\u00a0labels\u00a0all\u00a0files\u00a0as\u00a0Other\u00a0Genre.\u00a0The\u00a0precision\u00a0of\u00a0the\u00a0image\u00a0classifier\u00a0for\u00a0Periodicals\u00a0may\u00a0seem\u00a0\nrather\u00a0low,\u00a0but\u00a0it\u00a0is\u00a0important\u00a0to\u00a0keep\u00a0in\u00a0mind\u00a0that\u00a0the\u00a0number\u00a0of\u00a0Periodicals\u00a0in\u00a0the\u00a0whole\u00a0dataset\u00a0is\u00a0only\u00a0five\u00a0\npercent\u00a0of\u00a0the\u00a0number\u00a0of\u00a0files\u00a0in\u00a0Other\u00a0Genre;\u00a0only\u00a0a\u00a0small\u00a0percentage\u00a0of\u00a0mislabelling\u00a0in\u00a0the\u00a0Other\u00a0Genre\u00a0would\u00a0\nresult\u00a0in\u00a0a\u00a0sizable\u00a0drop\u00a0of\u00a0precision\u00a0for\u00a0Periodicals.\u00a0The\u00a0overall\u00a0accuracy\u00a0of\u00a0the\u00a0image\u00a0classifier\u00a0shows\u00a0a\u00a0slight\u00a0\ndecrease\u00a0when\u00a0compared\u00a0to\u00a0the\u00a0previous\u00a0experiment\u00a0(Kim\u00a0&\u00a0Ross(2006c))\u00a0when\u00a0the\u00a0variety\u00a0of\u00a0non\u00adperiodical\u00a0\ngenres\u00a0was\u00a0smaller\u00a0but\u00a0the\u00a0results\u00a0in\u00a0the\u00a0two\u00a0cases\u00a0seem\u00a0comparable.\u00a0The\u00a0result\u00a0suggests\u00a0the\u00a0image\u00a0features\u00a0as\u00a0a\u00a0\ndistinguishing\u00a0factor\u00a0for\u00a0Periodicals.\u00a0\nTable\u00a03:\u00a0\u00a0Distinguishing\u00a0Periodicals\u00a0from\u00a0Other\u00a0Genre\u00a0using\u00a0image\u00a0(top)\u00a0stylo\u00admetric\u00a0(middle)\u00a0and\u00a0language\u00a0\nmodel\u00a0(bottom)\n10\u00a0fold\u00a0Cross\u00a0Validation\u00a0with\u00a0the\u00a0Image\u00a0classifier,\u00a0Overall\u00a0accuracy:\u00a088.6%\nGenre Precision(%) Recall(%)\nPeriodicals\u00a0(16\u00a0items) 29.8 87.5\nOther\u00a0Genre\u00a0(291\u00a0items) 99.2 88.7\n10\u00a0fold\u00a0Cross\u00a0Validation\u00a0with\u00a0the\u00a0stylo\u00admetric\u00a0classifier,\u00a0Overall\u00a0accuracy:\u00a088.52%\nGenre Precision(%) Recall(%)\nPeriodicals\u00a0(16\u00a0items) 14.8 25\nOther\u00a0Genre\u00a0(291\u00a0items) 92 93.8\n10\u00a0fold\u00a0Cross\u00a0Validation\u00a0with\u00a0the\u00a0language\u00a0model\u00a0classifier,\u00a0Overall\u00a0accuracy:\u00a094.79%\nGenre Precision(%) Recall(%)\nPeriodicals\u00a0(16\u00a0items) 0 0\nOther\u00a0Genre\u00a0(291\u00a0items) 96.9 100\nTable\u00a04\u00a0shows\u00a0the\u00a010\u00adfold\u00a0cross\u00a0validation\u00a0results\u00a0of\u00a0the\u00a0three\u00a0classifiers\u00a0on\u00a0distinguishing\u00a0Scientific\u00a0Research\u00a0\nArticles\u00a0from\u00a0Other\u00a0Genre.\u00a0As\u00a0in\u00a0the\u00a0case\u00a0of\u00a0Periodicals,\u00a0the\u00a0overall\u00a0accuracy\u00a0is\u00a0highest\u00a0for\u00a0the\u00a0language\u00a0model\u00a0\nclassifier\u00a0but\u00a0careful\u00a0examination\u00a0shows\u00a0that\u00a0the\u00a0language\u00a0classifier\u00a0labels\u00a0only\u00a0fifteen\u00a0percent\u00a0of\u00a0the\u00a0Scientific\u00a0\nResearch\u00a0Articles\u00a0correctly.\u00a0The\u00a0best\u00a0recall\u00a0rate\u00a0for\u00a0Scientific\u00a0Research\u00a0Articles\u00a0is\u00a0found\u00a0in\u00a0the\u00a0image\u00a0classifier.\u00a0\nHowever,\u00a0the\u00a0recall\u00a0rate\u00a0for\u00a0Scientific\u00a0Research\u00a0Articles\u00a0using\u00a0the\u00a0image\u00a0classifier\u00a0is\u00a0only\u00a0four\u00a0percent\u00a0better\u00a0\nthan\u00a0the\u00a0stylo\u00admetric\u00a0classifier\u00a0while\u00a0the\u00a0precision\u00a0falls\u00a0more\u00a0than\u00a0twenty\u00a0eight\u00a0percent\u00a0below\u00a0that\u00a0of\u00a0the\u00a0stylo\u00ad\nmetric\u00a0classifier.\u00a0The\u00a0overall\u00a0performance\u00a0seems\u00a0to\u00a0be\u00a0best\u00a0with\u00a0the\u00a0stylo\u00admetric\u00a0classifier.\u00a0\nTable\u00a04:\u00a0\u00a0Distinguishing\u00a0Scientific\u00a0Research\u00a0Articles\u00a0from\u00a0Other\u00a0Genre\u00a0using\u00a0image\u00a0(top),\u00a0stylo\u00admetrics\u00a0\n(middle)\u00a0and\u00a0language\u00a0model\u00a0(bottom)\n10\u00a0fold\u00a0Cross\u00a0Validation\u00a0with\u00a0the\u00a0Image\u00a0classifier,\u00a0Overall\u00a0accuracy:\u00a073.94\u00a0%\nGenres Precision(%) Recall(%)\nScientific\u00a0Research\u00a0Articles\u00a0(25\u00a0items) 21.11 80\nOtherGenre\u00a0(280\u00a0items) 97.6 73.410\u00a0fold\u00a0Cross\u00a0Validation\u00a0with\u00a0the\u00a0stylo\u00admetric\u00a0classifier,\u00a0Overall\u00a0accuracy:\u00a091.80\u00a0%\nGenres Precision(%) Recall(%)\nScientific\u00a0Research\u00a0Articles\u00a0(25\u00a0items) 50 76\nOtherGenre\u00a0(280\u00a0items) 97.8 93.2\n10\u00a0fold\u00a0Cross\u00a0Validation\u00a0with\u00a0the\u00a0language\u00a0model\u00a0classifier,\u00a0Overall\u00a0accuracy:\u00a094.68\u00a0%\nGenres Precision(%) Recall(%)\nScientific\u00a0Research\u00a0Articles\u00a0(25\u00a0items) 100 15\nOtherGenre\u00a0(280\u00a0items) 94.6 100\nSimilar\u00a0analysis\u00a0of\u00a0results\u00a0for\u00a0Thesis\u00a0(Tables\u00a05)\u00a0seem\u00a0to\u00a0indicate\u00a0the\u00a0image\u00a0features\u00a0as\u00a0the\u00a0best\u00a0distinguishing\u00a0\nfactor\u00a0among\u00a0the\u00a0three\u00a0types\u00a0of\u00a0feature\u00a0sets\u00a0for\u00a0the\u00a0category\u00a0Thesis.\u00a0\nTable\u00a05:\u00a0\u00a0Distinguishing\u00a0Thesis\u00a0from\u00a0Other\u00a0Genre\u00a0using\u00a0image(top)\u00a0stylo\u00admetrics\u00a0(middle)\u00a0and\u00a0language\u00a0model\u00a0\n(bottom)\n10\u00a0fold\u00a0Cross\u00a0Validation\u00a0with\u00a0the\u00a0Image\u00a0classifier,\u00a0Overall\u00a0accuracy:\u00a082.74\u00a0%\nGenres Precision(%) Recall(%)\nThesis\u00a0(10\u00a0items) 13.6 80\nOtherGenre\u00a0(280\u00a0items) 99.2 90.3\n10\u00a0fold\u00a0Cross\u00a0Validation\u00a0with\u00a0the\u00a0stylo\u00admetric\u00a0classifier,\u00a0Overall\u00a0accuracy:\u00a075.40\u00a0%\nGenres Precision(%) Recall(%)\nThesis\u00a0(10\u00a0items) 7 60\nOtherGenre\u00a0(280\u00a0items) 98.2 75.9\n10\u00a0fold\u00a0Cross\u00a0Validation\u00a0with\u00a0the\u00a0language\u00a0model\u00a0classifier,\u00a0Overall\u00a0accuracy:\u00a093.87\u00a0%\nGenres Precision(%) Recall(%)\nThesis\u00a0(10\u00a0items) 40 17.4\nOtherGenre\u00a0(280\u00a0items) 98 95.67\nFor\u00a0Business\u00a0Report\u00a0and\u00a0Forms\u00a0(Tables\u00a06,\u00a07)\u00a0seem\u00a0to\u00a0indicate\u00a0stylo\u00admetric\u00a0features\u00a0as\u00a0a\u00a0better\u00a0distinguishing\u00a0\nfactor\u00a0for\u00a0detecting\u00a0these\u00a0genres.\u00a0In\u00a0fact,\u00a0Table\u00a08\u00a0shows\u00a0that,\u00a0in\u00a0a\u00a010\u00adfold\u00a0cross\u00a0validation\u00a0classification\u00a0\nexperiment\u00a0using\u00a0the\u00a0stylo\u00admetric\u00a0classifier\u00a0of\u00a0files\u00a0belonging\u00a0to\u00a0one\u00a0of\u00a0the\u00a0classes\u00a0Fact\u00a0Sheet,\u00a0Forms,\u00a0\nInstruction\u00a0Guidelines,\u00a0Job\/Course\/Project\u00a0Description,\u00a0Minutes,\u00a0Newsletter,\u00a0Scientific\u00a0Research\u00a0Article\u00a0and\u00a0\nOther\u00a0Research\u00a0Article,\u00a0Forms\u00a0achieves\u00a0a\u00a0Recall\u00a0of\u00a092.3%\u00a0and\u00a0a\u00a0Precision\u00a0of\u00a070.6%.\u00a0\nTable\u00a06:\u00a0\u00a0Distinguishing\u00a0Business\u00a0Report\u00a0from\u00a0Other\u00a0Genre\u00a0using\u00a0image(top)\u00a0and\u00a0stylo\u00admetrics\u00a0(bottom)\n10\u00a0fold\u00a0Cross\u00a0Validation\u00a0with\u00a0the\u00a0Image\u00a0classifier,\u00a0Overall\u00a0accuracy:\u00a060.72\u00a0%\nGenres Precision(%) Recall(%)Business\u00a0Report\u00a0(10\u00a0items) 5.6 63.6\nOtherGenre\u00a0(280\u00a0items) 97.8 60.1\n10\u00a0fold\u00a0Cross\u00a0Validation\u00a0with\u00a0the\u00a0stylo\u00admetric\u00a0classifier,\u00a0Overall\u00a0accuracy:\u00a072.79\u00a0%\nGenres Precision(%) Recall(%)\nBusiness\u00a0Report\u00a0(10\u00a0items) 9.1 72.7\nOtherGenre\u00a0(280\u00a0items) 98.6 72.8\nTable\u00a07:\u00a0\u00a0Distinguishing\u00a0Forms\u00a0from\u00a0Other\u00a0Genre\u00a0using\u00a0image(top)\u00a0stylo\u00admetric\u00a0(bottom)\n10\u00a0fold\u00a0Cross\u00a0Validation\u00a0with\u00a0the\u00a0Image\u00a0classifier,\u00a0Overall\u00a0accuracy:\u00a076.55\u00a0%\nGenres Precision(%) Recall(%)\nForms\u00a0(10\u00a0items) 7.2 38.5\nOtherGenre\u00a0(280\u00a0items) 96.6 78.2\n10\u00a0fold\u00a0Cross\u00a0Validation\u00a0with\u00a0the\u00a0stylo\u00admetric\u00a0classifier,\u00a0Overall\u00a0accuracy:\u00a071.48\u00a0%\nGenres Precision(%) Recall(%)\nForms\u00a0(10\u00a0items) 10.6 76.9\nOtherGenre\u00a0(280\u00a0items) 98.6 71.2\nTable\u00a08:\u00a0\u00a0Classification\u00a0of\u00a0files\u00a0into\u00a0eight\u00a0classes\n10\u00a0fold\u00a0Cross\u00a0Validation\u00a0with\u00a0the\u00a0stylo\u00admetric\u00a0classifier,\u00a0Overall\u00a0accuracy:\u00a088.11\u00a0%\nGenres Precision(%) Recall(%)\nFact\u00a0Sheet\u00a0(14\u00a0items) 27.3 21.4\nForms\u00a0(10\u00a0items) 41.7 76.9\nInstruction\u00a0(20\u00a0items) 53.8 35\nJob\/Course\/Proj.Desc.\u00a0(15\u00a0items) 22.2 26.7\nMinutes\u00a0(13\u00a0items) 75 69.2\nNewsletter\u00a0(24\u00a0items) 48.4 62.5\nSci.Res.Article\u00a0(25\u00a0items) 62.1 72\nOther\u00a0Res.Article\u00a0(19\u00a0items) 50 31.6Finally,\u00a0Table\u00a09\u00a0shows\u00a0the\u00a0results\u00a0of\u00a0using\u00a0the\u00a0language\u00a0model\u00a0classifier\u00a0to\u00a0predict\u00a0the\u00a0classes\u00a0Thesis,\u00a0Fictional\u00a0\nBook,\u00a0Academic\u00a0Book,\u00a0Minutes,\u00a0and\u00a0Business\u00a0Report.\u00a0The\u00a0results\u00a0show\u00a0that,\u00a0when\u00a0the\u00a0variety\u00a0of\u00a0genres\u00a0is\u00a0\nlimited,\u00a0the\u00a0language\u00a0model\u00a0classifier\u00a0can\u00a0do\u00a0very\u00a0well\u00a0on\u00a0Business\u00a0Report\u00a0and\u00a0Fictional\u00a0Book\u00a0and\u00a0Minutes,\u00a0\nwhich\u00a0leads\u00a0us\u00a0to\u00a0suggest,\u00a0along\u00a0with\u00a0the\u00a0results\u00a0in\u00a0Tables\u00a06\u00a0and\u00a08,\u00a0that\u00a0combining\u00a0the\u00a0stylo\u00admetric\u00a0classifier\u00a0\nwith\u00a0the\u00a0language\u00a0model\u00a0classifier\u00a0may\u00a0result\u00a0in\u00a0a\u00a0classifier\u00a0able\u00a0to\u00a0detect\u00a0these\u00a0classes.\u00a0\nTable\u00a09:\u00a0\u00a0Classifying\u00a0five\u00a0types\u00a0of\u00a0genres\u00a0using\u00a0language\u00a0model\n10\u00a0fold\u00a0Cross\u00a0Validation\u00a0with\u00a0language\u00a0model\u00a0classifier,\u00a0Overall\u00a0accuracy\u00a0on\u00a05\u00a0classes:\u00a082.4%\nGenre Precision(%) Recall(%)\nAcademic\u00a0Book\u00a0(5\u00a0items) 42.9 60\nBusiness\u00a0Report\u00a0(11\u00a0items) 90 81.8\nFictional\u00a0Book\u00a0(14\u00a0items) 100 100\nMinutes\u00a0(13\u00a0items) 86.7 92.9\nThesis\u00a0(10\u00a0items) 75 60\n5\u00a0\u00a0CONCLUSION\nThe\u00a0results\u00a0in\u00a0Kim\u00a0&\u00a0Ross\u00a0(2006a),\u00a0Kim\u00a0&\u00a0Ross\u00a0(2006c),\u00a0and\u00a0this\u00a0paper\u00a0indicate\u00a0the\u00a0promise\u00a0of\u00a0focusing\u00a0\nsimilar\u00a0feature\u00a0types\u00a0to\u00a0extract\u00a0documents\u00a0of\u00a0selected\u00a0genre\u00a0classes.\u00a0The\u00a0results\u00a0in\u00a0Table\u00a02\u00a0illustrate\u00a0definite\u00a0\ndivisions\u00a0between\u00a0genres\u00a0which\u00a0have\u00a0strong\u00a0image\u00a0features\u00a0and\u00a0genres\u00a0that\u00a0have\u00a0strong\u00a0stylistic\u00a0features.\u00a0They\u00a0\nalso\u00a0show\u00a0that\u00a0combining\u00a0features\u00a0into\u00a0one\u00a0boiling\u00a0pot\u00a0does\u00a0not\u00a0necessarily\u00a0improve\u00a0the\u00a0clusterer.\u00a0The\u00a0results\u00a0in\u00a0\nTables\u00a03\u00a0and\u00a05\u00a0indicate\u00a0that\u00a0the\u00a0classes\u00a0Periodicals\u00a0and\u00a0Thesis\u00a0have\u00a0more\u00a0clearly\u00a0distinguishing\u00a0image\u00a0features\u00a0\nthan\u00a0stylo\u00admetric\u00a0features\u00a0or\u00a0language\u00a0model\u00a0features.\u00a0The\u00a0figures\u00a0in\u00a0Table\u00a04\u00a0shows\u00a0that\u00a0the\u00a0tendency\u00a0is\u00a0less\u00a0\nclear\u00a0in\u00a0the\u00a0case\u00a0of\u00a0the\u00a0class\u00a0Scientific\u00a0Research\u00a0Article.\u00a0Forms\u00a0and\u00a0Business\u00a0Reports\u00a0seem\u00a0to\u00a0have\u00a0stronger\u00a0\nstylo\u00admetric\u00a0features\u00a0(Tables\u00a07,\u00a08\u00a0and\u00a06)\u00a0and\u00a0extracting\u00a0some\u00a0genres\u00a0may\u00a0become\u00a0more\u00a0viable\u00a0by\u00a0incorporating\u00a0\nthe\u00a0language\u00a0model\u00a0classifier\u00a0(Table\u00a09).\u00a0There\u00a0is\u00a0still\u00a0more\u00a0work\u00a0to\u00a0do\u00a0before\u00a0we\u00a0arrive\u00a0at\u00a0a\u00a0tool\u00a0which\u00a0can\u00a0\nclassify\u00a0files\u00a0into\u00a0nineteen\u00a0genres\u00a0or\u00a0more.\u00a0However,\u00a0the\u00a0results\u00a0in\u00a0this\u00a0paper\u00a0indicate\u00a0that,\u00a0by\u00a0targetting\u00a0selected\u00a0\ngenres\u00a0and\u00a0determining\u00a0their\u00a0strong\u00a0feature\u00a0types,\u00a0we\u00a0can\u00a0narrow\u00a0down\u00a0the\u00a0search\u00a0for\u00a0files\u00a0in\u00a0the\u00a0selected\u00a0genres\u00a0\nwithin\u00a0a\u00a0pool\u00a0of\u00a0other\u00a0documents;\u00a0perhaps\u00a0a\u00a0more\u00a0promising\u00a0approach\u00a0than\u00a0to\u00a0create\u00a0a\u00a0general\u00a0tool\u00a0which\u00a0\nclassifies\u00a0files\u00a0within\u00a0a\u00a0range\u00a0of\u00a0genre\u00a0classes\u00a0that\u00a0keep\u00a0changing\u00a0in\u00a0size\u00a0and\u00a0variety.\u00a0\nFurther\u00a0improvement\u00a0can\u00a0also\u00a0be\u00a0envisioned\u00a0by\u00a0integrating\u00a0more\u00a0classifiers\u00a0into\u00a0the\u00a0decision\u00a0making\u00a0process.\u00a0In\u00a0\nKim\u00a0\u00a0&\u00a0Ross\u00a0(2006a)\u00a0we\u00a0suggested\u00a0the\u00a0following\u00a0classifiers:\u00a0We\u00a0could\u00a0consider\u00a0an\u00a0Extended\u00a0image\u00a0classifier\u00a0\nwhich\u00a0looks\u00a0at\u00a0more\u00a0than\u00a0the\u00a0first\u00a0page\u00a0of\u00a0the\u00a0document.\u00a0This\u00a0would\u00a0involve\u00a0decisions\u00a0on\u00a0the\u00a0optimal\u00a0number\u00a0of\u00a0\npages\u00a0to\u00a0be\u00a0used\u00a0and\u00a0the\u00a0best\u00a0way\u00a0to\u00a0statistically\u00a0combine\u00a0the\u00a0information\u00a0from\u00a0different\u00a0pages.\u00a0The\u00a0Language\u00a0\nmodel\u00a0classifier\u00a0could\u00a0be\u00a0built\u00a0on\u00a0the\u00a0level\u00a0of\u00a0part\u00adof\u00adspeech\u00a0tags\u00a0(tags\u00a0which\u00a0denote\u00a0whether\u00a0a\u00a0word\u00a0is\u00a0a\u00a0verb,\u00a0\nnoun\u00a0or\u00a0preposition)\u00a0or\u00a0partial\u00a0chunk\u00a0tags\u00a0(tags\u00a0indicating\u00a0noun\u00a0phrases,\u00a0verb\u00a0phrases\u00a0or\u00a0prepositional\u00a0phrases).\u00a0\nA\u00a0Semantic\u00a0classifier\u00a0could\u00a0be\u00a0employed\u00a0to\u00a0model\u00a0subjective\u00a0or\u00a0objective\u00a0noun\u00a0phrases\u00a0(e.g.\u00a0using\u00a0Riloff,\u00a0\nWiebe\u00a0&\u00a0Wilson\u00a0(2003))\u00a0and\u00a0latent\u00a0semantic\u00a0analysis.\u00a0A\u00a0Contextual\u00a0Classifier\u00a0built\u00a0on\u00a0source\u00a0information\u00a0of\u00a0\nthe\u00a0document\u00a0such\u00a0as\u00a0the\u00a0name\u00a0of\u00a0the\u00a0journal\u00a0or\u00a0address\u00a0of\u00a0the\u00a0web\u00a0page,\u00a0anchor\u00a0text\u00a0or\u00a0domain\u00a0subject\u00a0\ninformation,\u00a0along\u00a0with\u00a0administrative\u00a0organisational\u00a0context\u00a0when\u00a0available,\u00a0would\u00a0be\u00a0a\u00a0useful\u00a0addition.\u00a0\nThere\u00a0are\u00a0two\u00a0obvious\u00a0ways\u00a0of\u00a0gauging\u00a0the\u00a0performance\u00a0of\u00a0a\u00a0classifier:\u00a0comparing\u00a0against\u00a0human\u00a0performance\u00a0\nand\u00a0measuring\u00a0the\u00a0stability\u00a0of\u00a0the\u00a0performance\u00a0as\u00a0you\u00a0transfer\u00a0it\u00a0across\u00a0domains.\u00a0We\u00a0are\u00a0undertaking\u00a0an\u00a0\nexperiment\u00a0to\u00a0examine\u00a0human\u00a0performance.\u00a0A\u00a0significant\u00a0amount\u00a0of\u00a0disagreement\u00a0is\u00a0expected\u00a0in\u00a0labelling\u00a0genres\u00a0even\u00a0between\u00a0human\u00a0labellers;\u00a0we\u00a0intend\u00a0to\u00a0cross\u00a0check\u00a0the\u00a0labelled\u00a0data\u00a0in\u00a0two\u00a0ways:\u00a0\n1.\u00a0Document\u00a0Retrieval\u00a0Exercise\u00a0(DRE):\u00a0we\u00a0are\u00a0employing\u00a0a\u00a0cohort\u00a0of\u00a0postgraduates\u00a0in\u00a0information\u00a0science\u00a0\nwho\u00a0will\u00a0be\u00a0assigned\u00a0genres\u00a0from\u00a0Table\u00a01\u00a0in\u00a0Kim\u00a0&\u00a0Ross\u00a0(2006c).\u00a0They\u00a0will\u00a0retrieve\u00a0one\u00a0hundred\u00a0PDF\u00a0\ndocuments\u00a0for\u00a0each\u00a0of\u00a0the\u00a0genres\u00a0they\u00a0have\u00a0been\u00a0assigned,\u00a0and\u00a0give\u00a0a\u00a0brief\u00a0description\u00a0of\u00a0the\u00a0source\u00a0of\u00a0the\u00a0\ndocument\u00a0and\u00a0the\u00a0reasons\u00a0for\u00a0including\u00a0the\u00a0document\u00a0in\u00a0their\u00a0collection.\u00a0\n2.\u00a0Re\u00adlabelling\u00a0Experiment:\u00a0we\u00a0will\u00a0anonymise\u00a0the\u00a0file\u00a0names\u00a0of\u00a0the\u00a0documents\u00a0collected\u00a0in\u00a0the\u00a0DRE\u00a0and\u00a0\nrandomise\u00a0the\u00a0document\u00a0sequence.\u00a0This\u00a0corpus\u00a0will\u00a0be\u00a0presented\u00a0to\u00a0two\u00a0new\u00a0groups\u00a0of\u00a0labellers\u00a0drawn\u00a0from\u00a0\ndifferent\u00a0backgrounds\u00a0for\u00a0re\u00adclassifying.\u00a0They\u00a0will\u00a0not\u00a0have\u00a0access\u00a0to\u00a0the\u00a0initial\u00a0genre\u00a0classification\u00a0information.\u00a0\nThe\u00a0first\u00a0experiment\u00a0will\u00a0create\u00a0a\u00a0pool\u00a0of\u00a0PDF\u00a0files\u00a0which\u00a0have\u00a0already\u00a0been\u00a0classified\u00a0into\u00a0genres\u00a0by\u00a0\nestablished\u00a0organisations\u00a0and\u00a0users;\u00a0this\u00a0will\u00a0serve\u00a0as\u00a0a\u00a0reference\u00a0point,\u00a0and\u00a0help\u00a0us\u00a0to\u00a0index\u00a0the\u00a0performance\u00a0on\u00a0\nwell\u00addesigned\u00a0classification\u00a0standards.\u00a0The\u00a0re\u00adlabelling\u00a0experiment\u00a0will\u00a0enable\u00a0us\u00a0to\u00a0compare\u00a0the\u00a0disagreement\u00a0\nof\u00a0the\u00a0three\u00a0classes\u00a0of\u00a0labellers\u00a0over\u00a0the\u00a0same\u00a0data\u00a0set:\u00a0this\u00a0will\u00a0help\u00a0to\u00a0determine\u00a0the\u00a0maximum\u00a0level\u00a0of\u00a0\naccuracy\u00a0at\u00a0which\u00a0the\u00a0automated\u00a0system\u00a0can\u00a0be\u00a0expected\u00a0to\u00a0perform\u00a0and\u00a0determine\u00a0which\u00a0genres\u00a0are\u00a0better\u00a0\ndefined\u00a0by\u00a0looking\u00a0at\u00a0percentage\u00a0of\u00a0files\u00a0in\u00a0agreement\u00a0within\u00a0each\u00a0genre.\u00a0\nThe\u00a0longer\u00a0term\u00a0aim,\u00a0once\u00a0a\u00a0genre\u00a0classifier\u00a0or\u00a0identifier\u00a0with\u00a0performance\u00a0comparable\u00a0to\u00a0an\u00a0average\u00a0human\u00a0\nlabeller\u00a0has\u00a0been\u00a0developed,\u00a0will\u00a0be\u00a0to\u00a0integrate\u00a0the\u00a0method\u00a0with\u00a0other\u00a0tools\u00a0which\u00a0extract\u00a0author,\u00a0title,\u00a0date,\u00a0\nidentifier,\u00a0keywords,\u00a0topic,\u00a0language,\u00a0summarisations\u00a0and\u00a0other\u00a0compositional\u00a0properties\u00a0and\u00a0objects\u00a0(tables,\u00a0\nlinks,\u00a0figures)\u00a0of\u00a0files\u00a0within\u00a0a\u00a0single\u00a0genre.\u00a0As\u00a0we\u00a0mentioned\u00a0in\u00a0Section\u00a01,\u00a0this\u00a0will\u00a0help\u00a0to\u00a0create\u00a0the\u00a0context\u00a0\nnecessary\u00a0for\u00a0understanding\u00a0scientific\u00a0data.\u00a0The\u00a0construction\u00a0of\u00a0a\u00a0genre\u00a0classification\u00a0or\u00a0detection\u00a0tool,\u00a0even\u00a0\nwithout\u00a0further\u00a0extraction\u00a0of\u00a0metadata\u00a0would\u00a0be\u00a0useful\u00a0already\u00a0in\u00a0quickly\u00a0and\u00a0efficiently\u00a0searching\u00a0for\u00a0and\u00a0\nretrieving\u00a0the\u00a0scientific\u00a0materials\u00a0necessary\u00a0for\u00a0interpreting\u00a0and\u00a0carrying\u00a0out\u00a0scientific\u00a0research.\n6\u00a0ACKNOWLEDGEMENTS\nThis\u00a0research\u00a0is\u00a0a\u00a0part\u00a0of\u00a0The\u00a0Digital\u00a0Curation\u00a0Centre\u2019s\u00a0(DCC)\u00a0research\u00a0programme.\u00a0The\u00a0DCC\u00a0is\u00a0supported\u00a0by\u00a0a\u00a0\ngrant\u00a0from\u00a0the\u00a0United\u00a0Kingdom\u2019s\u00a0Joint\u00a0Information\u00a0Systems\u00a0Committee\u00a0(JISC)\u00a0and\u00a0the\u00a0e\u00adScience\u00a0Core\u00a0\nProgramme\u00a0of\u00a0the\u00a0Engineering\u00a0and\u00a0Physical\u00a0Sciences\u00a0Research\u00a0Council\u00a0(EPSRC)\u00a0(grant\u00a0GR\/T07374\/01)\u00a0\nprovides\u00a0the\u00a0support\u00a0for\u00a0the\u00a0research\u00a0programme.\u00a0Additional\u00a0support\u00a0comes\u00a0from\u00a0the\u00a0DELOS:\u00a0Network\u00a0of\u00a0\nExcellence\u00a0on\u00a0Digital\u00a0Libraries\u00a0(G038\u00ad507618)\u00a0funded\u00a0under\u00a0the\u00a0European\u00a0Commission\u2019s\u00a0IST\u00a06th\u00a0Framework\u00a0\nProgramme.\u00a0We\u00a0also\u00a0extend\u00a0our\u00a0thanks\u00a0to\u00a0Volker\u00a0Heydegger\u00a0at\u00a0the\u00a0Historisch\u00adKulturwissenschaftliche\u00a0\nInformationsverarbeitung\u00a0(HKI),\u00a0University\u00a0of\u00a0Cologne,\u00a0for\u00a0his\u00a0programming\u00a0expertise;\u00a0HKI\u00a0participates\u00a0in\u00a0the\u00a0\nDELOS\u00a0Digital\u00a0Preservation\u00a0Cluster\u00a0led\u00a0by\u00a0the\u00a0University\u00a0of\u00a0Glasgow.\u00a0\u00a0\nNote\u00a0on\u00a0website\u00a0citations:All\u00a0citations\u00a0of\u00a0websites\u00a0were\u00a0validated\u00a0on\u00a022\u00a0November\u00a02006.\n5\u00a0REFERENCES\nAutomatic\u00a0Metadata\u00a0Generation:\u00a0http:\/\/www.cs.kuleuven.ac.be\/hmdb\/amg\u00a0\nBagdanov,\u00a0A.\u00a0D.\u00a0&\u00a0Worring,\u00a0M.\u00a0(2001)\u00a0\u00a0Fine\u00adGrained\u00a0Document\u00a0Genre\u00a0Classification\u00a0Using\u00a0First\u00a0Order\u00a0\nRandom\u00a0Graphs.\u00a0Intnl.\u00a0Conf.\u00a0Document\u00a0Analysis\u00a0and\u00a0Recognition,\u00a079.\u00a0\nBarbu,\u00a0E.,\u00a0Heroux,\u00a0P.,\u00a0Adam,\u00a0S.\u00a0&\u00a0Trupin,\u00a0E.\u00a0(2005)\u00a0Clustering\u00a0Document\u00a0Images\u00a0Using\u00a0a\u00a0Bag\u00a0of\u00a0Symbols\u00a0\nRepresentation.\u00a0Intnl.\u00a0Conference\u00a0on\u00a0Document\u00a0Analysis\u00a0and\u00a0Recognition,\u00a01216\u20131220.\u00a0\nBekkerman,\u00a0R.,\u00a0McCallum,\u00a0A.\u00a0&\u00a0Huang,\u00a0G.\u00a0(2004)\u00a0Automatic\u00a0Categorization\u00a0of\u00a0Email\u00a0into\u00a0Folders.\u00a0Benchmark\u00a0\nExperiments\u00a0on\u00a0Enron\u00a0and\u00a0SRI\u00a0Corpora\u2019,\u00a0CIIR\u00a0Tech.\u00a0Report,\u00a0IR\u00ad418.\u00a0\nBiber,\u00a0D.\u00a0(1995)\u00a0Dimensions\u00a0of\u00a0Register\u00a0Variation:a\u00a0Cross\u00adLinguistic\u00a0Comparison.\u00a0Cambridge\u00a0University.\u00a0Boese,\u00a0E.\u00a0S.(2005)\u00a0Stereotyping\u00a0the\u00a0web:\u00a0genre\u00a0classification\u00a0of\u00a0web\u00a0documents.\u00a0Master\u2019s\u00a0thesis\u00a0Colorado\u00a0State\u00a0\nUniversity.\u00a0\nCurran,\u00a0J.\u00a0&\u00a0Clark,\u00a0S.\u00a0(2003)\u00a0Investigating\u00a0GIS\u00a0and\u00a0Smoothing\u00a0for\u00a0Maximum\u00a0Entropy\u00a0Taggers.\u00a0Proceedings,\u00a0\nAunnual\u00a0Meeting\u00a0European\u00a0Chapter\u00a0of\u00a0the\u00a0Assoc.\u00a0of\u00a0Computational\u00a0Linguistics,\u00a091\u00ad98.\u00a0\nDigital\u00a0Curation\u00a0Centre:\u00a0http:\/\/www.dcc.ac.uk\u00a0\nDC\u00addot,\u00a0UKOLN\u00a0Dublin\u00a0Core\u00a0metadata\u00a0editor:\u00a0http:\/\/www.ukoln.ac.uk\/metadata\/dcdot\/\u00a0\nDELOS\u00a0Network\u00a0of\u00a0Excellence\u00a0on\u00a0Digital\u00a0Libraries:\u00a0http:\/\/www.delos.info\/\u00a0\nEngineering\u00a0and\u00a0Physical\u00a0Sciences\u00a0Research\u00a0Council\u00a0(EPSRC):\u00a0http:\/\/www.epsrc.ac.uk\/\u00a0\nFinn,\u00a0A.\u00a0&\u00a0Kushmerick,\u00a0N.\u00a0(2006)\u00a0Learning\u00a0to\u00a0Classify\u00a0Documents\u00a0According\u00a0to\u00a0Genre.\u00a0Journal\u00a0of\u00a0American\u00a0\nSociety\u00a0for\u00a0Information\u00a0Science\u00a0and\u00a0Technology,\u00a057\u00a0(11),\u00a01506\u00ad1518.\u00a0\nGiuffrida,\u00a0G.,\u00a0Shek,\u00a0E.\u00a0\u00a0&\u00a0Yang,\u00a0J.\u00a0(2000)\u00a0Knowledge\u00adbased\u00a0Metadata\u00a0Extraction\u00a0from\u00a0PostScript\u00a0File.\u00a05th\u00a0ACM\u00a0\nIntnl.\u00a0Conf.\u00a0Digital\u00a0Libraries,\u00a077\u201384.\u00a0\nHan,\u00a0H.,\u00a0Giles,\u00a0L.,\u00a0Manavoglu,\u00a0E.,\u00a0Zha,\u00a0H.,\u00a0Zhang,\u00a0Z.\u00a0&\u00a0Fox,\u00a0E.\u00a0A.\u00a0(2000)\u00a0Automatic\u00a0Document\u00a0Metadata\u00a0\nExtraction\u00a0using\u00a0Support\u00a0Vector\u00a0Machines.\u00a03rd\u00a0ACM\/IEEE\u00adCS\u00a0Conf.\u00a0Digital\u00a0libraries\u00a037\u201348.\u00a0\nHistorisch\u00adKulturwissenschaftliche\u00a0Informationsverarbeitung\u00a0(HKI),\u00a0University\u00a0of\u00a0Koeln:\u00a0http:\/\/www.hki.uni\u00ad\nkoeln.de\/\u00a0\nJoint\u00a0Information\u00a0Systems\u00a0Committee:\u00a0http:\/\/www.jisc.ac.uk\/\u00a0\nKarlgren,\u00a0J.\u00a0&\u00a0Cutting,\u00a0D.\u00a0(1994)\u00a0Recognizing\u00a0Text\u00a0Genres\u00a0with\u00a0Simple\u00a0Metric\u00a0using\u00a0Discriminant\u00a0Analysis.\u00a0\n15th\u00a0Conf.\u00a0Comp.\u00a0Ling.\u00a0Vol\u00a02\u00a01071\u20131075.\u00a0\nKe,\u00a0S.\u00a0W.,\u00a0Bowerman,\u00a0C.\u00a0&\u00a0Oakes,\u00a0M.\u00a0(2006)\u00a0PERC:\u00a0A\u00a0Personal\u00a0Email\u00a0Classifier.\u00a028th\u00a0European\u00a0Conf.\u00a0\nInformation\u00a0Retrieval\u00a0(ECIR\u00a02006),\u00a0460\u2013463.\u00a0\nKessler,\u00a0B.,\u00a0Nunberg,\u00a0G.\u00a0&\u00a0Schuetze,\u00a0H.\u00a0(1997)\u00a0Automatic\u00a0Detection\u00a0of\u00a0Text\u00a0Genre.\u00a035th\u00a0Ann.\u00a0Meeting\u00a0ACL\u00a032\u2013\n38.\u00a0\nKim,\u00a0Y.\u00a0(2004)\u00a0Anaphora\u00a0Resolution\u00a0for\u00a0Automatic\u00a0Citation\u00a0Linking.\u00a0Masters\u00a0Thesis\u00a0Speech\u00a0and\u00a0Language\u00a0\nProcessing,\u00a0University\u00a0of\u00a0Edinburgh.\u00a0\nKim,\u00a0Y.\u00a0&\u00a0Ross,\u00a0S.\u00a0(2006a)\u00a0Genre\u00a0Classification\u00a0in\u00a0Automated\u00a0Ingest\u00a0and\u00a0Appraisal\u00a0Metadata.\u00a0J.\u00a0Gonzalo\u00a0et\u00a0\nal.\u00a0(eds.):\u00a0ECDL\u00a02006,\u00a0LNCS\u00a04172,\u00a063\u201374.\nKim,\u00a0Y.\u00a0&\u00a0Ross,\u00a0S.\u00a0(2006b)\u00a0Automating\u00a0Metadata\u00a0Extraction:\u00a0Genre\u00a0Classification\u00a0Poster\u00a0at\u00a0the\u00a0UK\u00a0e\u00adScience\u00a0\nAll\u00a0Hands\u00a0Meeting,\u00a0Nottingham,\u00a0UK.\u00a0http:\/\/www.allhands.org.uk\/2006\/proceedings\/papers\/663.pdf\u00a0Kim,\u00a0Y.\u00a0&\u00a0Ross,S.\u00a0(2006c)\u00a0\u201cThe\u00a0Naming\u00a0of\u00a0Cats\u201d:\u00a0Automated\u00a0Genre\u00a0Classification.\u00a0Proc.\u00a02nd\u00a0International\u00a0\nDigital\u00a0Curation\u00a0Conference,\u00a021\u00ad22\u00a0November,\u00a02006,\u00a0Glasgow,\u00a0UK.\nKim,\u00a0Y.\u00a0&\u00a0Webber,\u00a0B.(2006)\u00a0Implicit\u00a0Reference\u00a0to\u00a0Citations:\u00a0A\u00a0study\u00a0of\u00a0astronomy\u00a0papers.\u00a0Preprint\u00a0at\u00a0\nERPAePRINTS,\u00a0ID\u00a0Code\u00a0115,\u00a0http:\/\/eprints.erpanet.org\nMcCallum,\u00a0A.\u00a0(1998)\u00a0Bow:\u00a0A\u00a0Toolkit\u00a0for\u00a0Statistical\u00a0Language\u00a0Modeling,\u00a0Text\u00a0Retrieval,\u00a0Classification\u00a0and\u00a0\nClustering.\u00a0\nMetadataExtractor:\u00a0http:\/\/pami.uwaterloo.ca\/\u00a0(follow\u00a0the\u00a0link\u00a0for\u00a0Text\u00a0Mining)\u00a0\nNoonberg,\u00a0D.,\u00a0B.\u00a0(2006)\u00a0XPDF\u00a0PDF\u00a0document\u00a0viewer.\u00a0http:\/\/www.foolabs.com\/xpdf\/\u00a0\nPDF,\u00a0Adobe\u00a0Acrobat\u00a0specification:\u00a0http:\/\/partners.adobe.com\/public\/developer\/pdf\/index_reference.html\u00a0\nPDFTOHTML,\u00a0PDF\u00a0to\u00a0HTML\u00a0converter.\u00a0http:\/\/pdftohtml.sourceforge.net\/\u00a0\nPREMIS\u00a0(PREservation\u00a0Metadata:\u00a0Implementation\u00a0Strategy)\u00a0Working\u00a0Group:\u00a0\nhttp:\/\/www.oclc.org\/research\/projects\/pmwg\/\u00a0\nPython:\u00a0http:\/\/www.python.org\u00a0\nPython\u00a0Imaging\u00a0Library:\u00a0http:\/\/www.pythonware.com\/products\/pil\/\u00a0\nRauber,\u00a0A.\u00a0&\u00a0M\u00fcller\u00adK\u00f6gler,\u00a0A.\u00a0(2001)\u00a0\u00a0Integrating\u00a0Automatic\u00a0Genre\u00a0Analysis\u00a0into\u00a0Digital\u00a0Libraries.\u00a0\nACM\/IEEE\u00a0Joint\u00a0Conf.\u00a0Digital\u00a0Libraries,\u00a0Roanoke,\u00a0VA.\u00a0\nRiloff,\u00a0E.,Wiebe,\u00a0J.,\u00a0&\u00a0Wilson,\u00a0T.\u00a0(2003)\u00a0Learning\u00a0Subjective\u00a0Nouns\u00a0using\u00a0Extraction\u00a0Pattern\u00a0Bootstrapping.\u00a0\n7th\u00a0CoNLL,\u00a025\u201332.\u00a0\nSantini,\u00a0M.\u00a0(2004a)\u00a0A\u00a0Shallow\u00a0Approach\u00a0To\u00a0Syntactic\u00a0Feature\u00a0Extraction\u00a0For\u00a0Genre\u00a0Classification.\u00a07th\u00a0Ann.\u00a0\nColloq.\u00a0UK\u00a0Special\u00a0Interest\u00a0Group\u00a0for\u00a0Comp.\u00a0Ling.\u00a0\nSantini,\u00a0M.\u00a0(2004b)\u00a0State\u00adof\u00adthe\u00adart\u00a0on\u00a0Automatic\u00a0Genre\u00a0Identification.\u00a0Tech.\u00a0Report\u00a0ITRI\u00ad04\u00ad03\u00a0ITRI\u00a0\nUniversity\u00a0of\u00a0Brighton,\u00a0UK.\u00a0\nSebastiani,\u00a0F.\u00a0(2002)\u00a0Machine\u00a0Learning\u00a0in\u00a0Automated\u00a0Text\u00a0Categorization.\u00a0ACM\u00a0Computing\u00a0Surveys,\u00a0Vol.\u00a034\u00a0\n(2002)\u00a01\u00ad47\u00a0\nThoma,G.\u00a0(2001)\u00a0Automating\u00a0the\u00a0production\u00a0of\u00a0bibliographic\u00a0records.\u00a0R&D\u00a0report\u00a0of\u00a0the\u00a0Communications\u00a0\nEngineering\u00a0Branch,\u00a0Lister\u00a0Hill\u00a0National\u00a0Center\u00a0for\u00a0Biomedical\u00a0Communications,\u00a0National\u00a0Library\u00a0of\u00a0\nMedicine.\u00a0\nWitte,\u00a0R.,\u00a0Krestel,\u00a0R.\u00a0&\u00a0Bergler,\u00a0S.\u00a0(2005)\u00a0ERSS\u00a02005:Coreference\u00adbased\u00a0Summarization\u00a0Reloaded.\u00a0DUC2005\u00a0\nDocument\u00a0Understanding\u00a0Workshop,\u00a0Canada\u00a0Witten,\u00a0I.,\u00a0H.\u00a0&\u00a0Frank,\u00a0E.\u00a0(2005)\u00a0Data\u00a0Mining:\u00a0Practical\u00a0machine\u00a0Learning\u00a0tools\u00a0and\u00a0techniques.\u00a02nd\u00a0Edition,\u00a0\nMorgan\u00a0Kaufmann,\u00a0San\u00a0Francisco,\u00a0USA.\u00a0"}