{"doi":"10.1080\/0968776950030203","coreId":"6956","oai":"oai:generic.eprints.org:186\/core5","identifiers":["oai:generic.eprints.org:186\/core5","10.1080\/0968776950030203"],"title":"Iterative student\u2010based testing of automated information\u2010handling exercises","authors":["Ramaiah, C.K.","Sulaiman, Mubarak","Meadows, A.J."],"enrichments":{"references":[{"id":193438,"title":"Design rules based on analysis of human error',","authors":[],"date":"1983","doi":"10.1145\/2163.358092","raw":"Norman, D.A. (1983), 'Design rules based on analysis of human error', Communications of the ACM, 26, 254-8.","cites":null},{"id":193436,"title":"Models of Teaching, Englewood Cliffs NJ,","authors":[],"date":"1980","doi":"10.1177\/002248718703800611","raw":"Joyce, B.R. and Weil, M. (1980), Models of Teaching, Englewood Cliffs NJ, PrenticeHall.","cites":null},{"id":193439,"title":"Sparks of Innovation in Human-computer Interaction,","authors":[],"date":"1993","doi":null,"raw":"Shneiderman, B. (ed) (1993), Sparks of Innovation in Human-computer Interaction, Norwood NJ, Ablex.","cites":null},{"id":193437,"title":"The command language grammar: a representative computing system',","authors":[],"date":"1981","doi":null,"raw":"Moran, T.P. (1981), 'The command language grammar: a representative computing system', International Journal of Man-Machine Studies, 15, 3-50.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"1995","abstract":"The continuing rapid changes in how information is handled via computers means that training in computer\u2010based information handling must itself undergo continuous modification. The investigations reported here examine the use of student feedback to improve presentation of hands\u2010on exercises. Student responses appear to be reasonably consistent across both time and different student backgrounds. For the groups examined here, such factors as age and sex appear to have little effect on responses. The only significant factor is duration and extent of prior computer experience. Problems in hands\u2010on exercises noted by the students therefore tend to have a continuing applicability, which suggests the value of iterative feedback in the provision of such exercises","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/6956.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/186\/1\/ALT_J_Vol3_No2_1995_Iterative_student_based_testin.pdf","pdfHashValue":"ad7efcac4d5ea7515390648ede3ec5a9faf45c1a","publisher":"Universit of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:186<\/identifier><datestamp>\n      2011-04-04T09:24:51Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/186\/<\/dc:relation><dc:title>\n        Iterative student\u2010based testing of automated information\u2010handling exercises<\/dc:title><dc:creator>\n        Ramaiah, C.K.<\/dc:creator><dc:creator>\n        Sulaiman, Mubarak<\/dc:creator><dc:creator>\n        Meadows, A.J.<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        The continuing rapid changes in how information is handled via computers means that training in computer\u2010based information handling must itself undergo continuous modification. The investigations reported here examine the use of student feedback to improve presentation of hands\u2010on exercises. Student responses appear to be reasonably consistent across both time and different student backgrounds. For the groups examined here, such factors as age and sex appear to have little effect on responses. The only significant factor is duration and extent of prior computer experience. Problems in hands\u2010on exercises noted by the students therefore tend to have a continuing applicability, which suggests the value of iterative feedback in the provision of such exercises.<\/dc:description><dc:publisher>\n        Universit of Wales Press<\/dc:publisher><dc:date>\n        1995<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/186\/1\/ALT_J_Vol3_No2_1995_Iterative_student_based_testin.pdf<\/dc:identifier><dc:identifier>\n          Ramaiah, C.K. and Sulaiman, Mubarak and Meadows, A.J.  (1995) Iterative student\u2010based testing of automated information\u2010handling exercises.  Association for Learning Technology Journal, 3 (2).  pp. 35-41.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776950030203<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/186\/","10.1080\/0968776950030203"],"year":1995,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"Iterative student-based testing\nof automated information-handling exercises\nC.K. Ramaiah*. Mubarak Sulaiman** and A.J. Meadows***\n*DESIDOC, Metcalfe House, Delhi, India\n** Department of Library and Information Science, King Saud University, Riyadh,\nSaudi Arabia\n***Department of Information and Ubrary Studies, Loughborough University\nof Technology\nThe continuing rapid changes in how information is handled via computers means that training in\ncomputer-based information handling must itself undergo continuous modification. The investigations\nreported here examine the use of student feedback to improve presentation of hands-on exercises.\nStudent responses appear to be reasonably consistent across both time and different student\nbackgrounds. For the groups examined here, such factors as age and sex appear to have little effect on\nresponses. The only significant factor is duration and extent of prior computer experience. Problems in\nhands-on exercises noted by the students therefore tend to have a continuing applicability, which\nsuggests the value of iterative feedback in the provision of such exercises.\nIntroduction\nMuch laboratory teaching of information-handling involves students in evaluating\ninformation provided either online or via a computer package. A lecturer can help\nstudents carry out these tasks in a variety of ways. In particular, it is customary to\nprovide students with hand-outs, and there is good evidence that such hand-outs are a\nvaluable resource, especially for lower-ability students (see, for example Saloman, 1979).\nIn many of these exercises, students are passive receivers of information, in the sense that\nthey assess the information but do not change it. However, it is sometimes possible to use\nstudent feedback to change the original input. In this case, the users' mental models of the\nsystem can be employed to modify the user-interface set up by the original designer (see\nMoran, 1981). A number of experiments have been carried out in the Department of\n35\nCK. Ramaiah et al Iterative student-based testing of automated information-handling exercises\nInformation and Library Studies at Loughborough University to examine how computer\ninterfaces and instruction sheets used in teaching can be improved by student feedback.\nThe present paper discusses examples of this work to help suggest both the factors to be\ntaken into account and the sorts of changes involved. Our approach has been based on\nthe concept of 'iterative usability testing', the value of which has recently been\nemphasized by Shneiderman (1993).\nThe teaching\/learning model envisaged here is essentially teacher-focused, and supposes a\nteaching environment in which both hand-outs and practical assistance are provided\n(Joyce and Weil, 1980). However, part of our experiments involves student groups of\ndiffering sizes, so that the results also have some implication for group-focused teaching.\nThe experimental approach is task-based, depending on the provision of well-structured\ntasks for students to perform at three levels. These levels are (1) simple (finding factual\ninformation from a hypertext database); (2) complex (evaluating the presentation of\ninformation via an electronic bulletin board); (3) design (setting up a HyperCard stack).\nFor this work, teaching sessions with second-year students of information and library\nstudies were employed. Experiments in categories (1) and (3) were carried out with the\nsame groups of students, and are discussed first below.\nHypertext experiment\nThe exercises here were based on HyperCard. The first required the retrieval of factual\ninformation from a disk produced at Drexel University in the USA, which described the\nuniversity and its facilities. The second involved a simple customization exercise to\nproduce a daily diary. An initial pilot study was carried out with six students to give some\nestimate of the likely minimum teaching time for the exercise, and how it should be\norganized.\nFirst study\nThe first full study involved 41 students who had no previous experience of hypertext.\nTwo-thirds were 20 years old or less, four-fifths were female, and most had at least three\nyears of computing experience. For teaching purposes, the students were randomly\nassigned to one of 10 groups, each consisting of three to four students. (The number per\ngroup was determined by the number of machines available in the laboratory. However,\nsince more machines were on order for the following year, this offered the chance of\nexperimenting with different-sized groups.) The student in the group who did the\ninputting was changed at specified intervals, and the groups were instructed to discuss\neach step. The groups were separated into two sets, each of five groups. The first set was\ngiven a short introduction to hypertext (specifically HyperCard), and then went hands-on\nimmediately. The second set was exposed to a much more extensive demonstration and\ndiscussion of hypertext and HyperCard before going hands-on. All groups were given the\nsame hand-outs, covering the background to hypertext, to the specifics of HyperCard,\nand to the exercises.\nIn the Drexel disk experiment, note was taken of the time required to retrieve the\ninformation and the number of false steps involved. Students were also asked to complete\nquestionnaires regarding any problems they had encountered. Similarly, in the diary\n36\nAIT-J Volume 3 Number 2\ncustomization exercise, note was taken of how far each group had progressed in the time\navailable, and students were again asked to record any problems encountered.\nNo significant differences were found between the two sets in terms of time required. The\nsame was true of most on-screen problems (for example, interpretation of menu options,\nor of icons). However, there were significant differences in terms of moving between\nscreens, or (in the second exercise) of transferring items. For the Drexel disk, the less well-\nprepared set recorded twice as many problems in this category, and for the diary exercise,\n50% more as compared with the better-prepared set. For students who had not\nencountered a mouse before, the physical handling of a mouse proved to be a major\n(though rapidly surmountable) problem, regardless of the set.\nSecond study\nChanges were next made in the organization of the teaching and the hand-out materials in\nthe light of student feedback from the first study. The customization exercise was\nsimplified; the questions on information retrieval, which had previously been in random\norder, were re-ordered in terms of hardness (as perceived by the students); and a hand-out\nwas prepared regarding use of the mouse. Students had indicated that three to four\nparticipants per group was too many, and that differences in computing experience were\nparticularly irritating when discussing responses. For the repeat study, two thirds of the\nstudents were grouped into pairs with similar computing backgrounds; the remainder\nworked on their own. There had been adverse comments on the duration of both the long\nand short introductions to hypertext and HyperCard; in the revised exercise, all students\nwere given the same intermediate-length introduction before going hands-on.\nTable I: Groupings used in\nthe second study\nSet Characteristics\nA Female; similar ages; familiar with Macintosh\nB Female; range of ages; not familiar with Macintosh\nC Male; similar ages; not familiar with Macintosh\nD Male and female; range of ages; familiar with Macintosh\nThe teaching exercise was now repeated with the next cohort of second-year students - 45\naltogether - who had again not had prior exposure to hypertext. In this case, a half were 20\nyears old or less and two-thirds were female - rather different from the first group - but,\nagain, most had three years or more of computing experience. This time round, the students\nwere divided into four sets. In contrast to the first study, these sets were selected to have\ncertain group characteristics in common, as listed in Table I. The first study had suggested\nthat computing experience, particularly in the use of Apple Macintoshes, was the most\nimportant factor relating to the accuracy and speed with which the HyperCard exercises were\ncarried out. Membership of these four sets was therefore determined on the basis of familiarity\nwith Macintoshes, in order to test how significant this effect was. It was decided to investigate,\nat the same time, how useful the hand-outs were. Sets A and D were provided with detailed\nhand-outs, but were expected to work in the laboratory mainly on their own. Sets B and C\nwere given condensed hand-outs, but could readily call on oral advice in the laboratory.\n37\nCK. Ramaiah et a\\ Iterative student-based testing of automated information-handling exercises\nVery useful\nFairly useful\nNot very useful\nAssessment of\nhand-outs\nMedium-length\nhand-outs\n20%\n66%\n14%\nFirst Study\nDetailed\nhand-outs\n35%\n57%\n9%\nSecond Study\nCondensed\nhand-outs\n9%\n59%\n32%\nTable 2: Evaluation of band-out\nmaten'al\nIn general, respondents in the second study indicated an appreciably higher level of\nsatisfaction with the exercises than those in the first study. In terms of ability to cope with\nthe exercises, there were significant differences between sets A and D, on the one hand,\nand sets B and C, on the other. Though, overall, problems in handling a mouse were\nreduced compared with the first study, the level was significantly higher for sets B and C\nthan for A and D. There was a smaller, but still clear-cut difference between these sets in\nterms of problems with interpreting icons: sets B and C also found this more difficult. In\naddition, sets B and C encountered more problems in moving between screens and in\nusing paint tools. The level of difference can be illustrated by the time taken by students\nto complete the customization exercise. They were allotted 90 minutes of class time for\nthis (otherwise the project had to be completed in their own time). Almost all the students\nin set D (which contained those with most computer experience) completed the exercise in\nthis time, as did over a third of the students in set A. Only a quarter of the students in sets\nB and C finished within the time limit. By way of contrast, the responses from all sets\nindicated that working in pairs was generally preferred to working alone.\nStudents were asked to evaluate the hand-out material they were given. Their responses to\nthe question of content are given in Table 2.\nDiscussion of these responses with the students indicated that the provision of detailed\nhand-outs was regarded as desirable by all groups, regardless of computer experience or of\nthe amount of other assistance that was available. The revised introduction to the course\nreceived a higher rating than either the long or short introductions used in the first study.\nElectronic bulletin board experiment\nAn initial study was also made here in order to obtain a general view of how students\nreacted to different kinds of electronic bulletin board, and what problems they found in\nusing them. To this end, 47 students were asked to access and report on four readily\navailable bulletin boards. The method was to ask them to respond to a series of questions\nwhich could be answered via the information on each bulletin board. One of the bulletin\nboards was BUBL (the Bulletin Board for Librarians). It was decided to concentrate on\nthis, paying particular attention to the section that contained practical exercises designed\nto provide training in the use of online services available over JANET. The question at\nissue was how well the students felt that this section of the bulletin board had been laid\nout for training purposes. In studying this question, the students obtained practice in the\n38\nALT-) Volume 3 Number 2\nevaluation of bulletin boards while experimenting with online training. The BUBL\nadministrators had agreed to try and modify this section in the light of student feedback.\nTwo successive studies were therefore planned. The first cohort of students would carry\nout their assessment of the bulletin board. The section concerned would be modified on\nthe basis of their comments, and then reassessed by a second cohort of students.\nFirst stud\/\nThe first cohort consisted of 41 students, of whom some two-thirds were female and three-\nquarters under 25; a half had more than three years of computing experience, and a half\nhad accessed electronic bulletin boards before. The students were divided into four sets,\ndepending on their previous computing experience. Each set had four sessions devoted\nto bulletin boards. In the first two, the students worked in pairs and received a general\nintroduction to electronic bulletin boards. In the next two, they had the opportunity to\nwork individually on BUBL. During the latter sessions, the students answered (by\nelectronic mail to the coordinator) a series of questions relating to the layout and design of\nthe interface, ease of absorbing the contents, and ease of navigation. The students'\nresponses indicated that they encountered most problems in the area of layout and design.\nFor example, they noted a need for better spacing on-screen between instructions and\nother types of text, a need to use highlighting (or some similar method) for distinguishing\nbetween different types of heading, and a need to differentiate between levels of difficulty\nin the practical exercises (for example, basic, intermediate and advanced).\nSecond study\nChanges were made to BUBL during the intervening summer vacation. Then the material\nwas re-evaluated by a new cohort of 54 students. Their characteristics were very similar to\nthose of the first group except that, owing to a change in the pattern of teaching, only\nsome 15% had previously accessed bulletin boards. The teaching methods and questions\nasked were the same as in the previous year. A comparison of the comments from the first\nand second studies indicates that both cohorts were looking for similar factors when using\nthe bulletin board. However, the respondents in the second study clearly preferred the\nrevised layout and design of the interface. For example, the proportion of students who\nsuggested the need for changes in the training section to make it more user-friendly fell\nfrom 84% to 27%. At the same time, student responses on the second time round also\nimplied that the overall ease of use of the system had decreased. Further feedback was\nsought on this apparently contradictory result: it was found that the problem lay in a\nmajor increase in the amount of information available via BUBL from the first to the\nsecond year (this was reflected in the fact that the main menu contained 15 items in the\nfirst year and 25 in the second). In consequence, though the material was better presented\non-screen in the second year, it often took more time and effort to track down any\nparticular item of information. There were no systematic differences, in this case, relating\nto the students' computer experience.\nConclusions\nThe participants in these experiments formed a homogeneous group in the sense that they\nwere second-year students in the same department. However, they had both a range of\ncomputing backgrounds and widely differing levels of interest in computer-based systems.\nFrom this viewpoint, they can be seen as a reasonably typical student group. The fact that\n39\nCK. Ramaiah et al Iterative student-based testing of automated information-handling exercises\nthe types of problem they encountered in these experiments appeared to be common to all\nsuggests that, regardless of experience and motivation, the whole group had a similar\nmental model of what they were trying to do. Indeed, from their responses, it was clear\nthat they envisaged the computer as primarily an information provider. In terms of\npersonal characteristics, there was no correlation between student responses and sex or\nage. There was a slight suggestion in the first investigation that students with a science\nbackground experienced rather fewer problems than those with a humanities background,\nbut this was submerged by differences in computer background. Experience in using a\nMacintosh was clearly a factor, to the extent that it made sense to teach HyperCard to\nMacintosh users and non-Macintosh users separately. By way of contrast, using and\nevaluating the training section of BUBL was not significantly influenced by computer\nexperience. The reason appears to be that, although some students had had prior\nexperience with bulletin boards, none had previously been involved with online training.\nPrior computer experience in the first investigation actually did not affect the range of\nproblems that a student encountered: it did affect the number of errors made in carrying\nout the exercises. It is useful here to draw a distinction between two types of error:\n'mistakes' and 'slips' (Norman, 1983). Computer users begin with some kind of intention\nwhen they start using a system. A 'mistake' occurs when there is some error or deficiency\nin this intention. A different type of error occurs when the intention is correctly\nformulated, but something goes wrong in the attempt to implement it. Such an error can\nbe referred to as a 'slip'. In the studies reported here, where conditions are fairly well\ndefined, novice and experienced computer users tend to make similar 'mistakes', but the\nformer make considerably more 'slips'. In working on these exercises, the student\npreference was clearly for operating in pairs, with both members having the same\ncomputer background. Whatever their level of computer experience, students preferred to\nhave their hand-outs as detailed as possible. At the same time, they preferred the\nintroductory lectures and demonstrations to be of moderate length, simply providing a\nframework which would allow them to appreciate the overall nature of an activity before\nthey went hands-on.\nStudents who participated in the second experiments were not told what modifications\nhad been introduced as a result of feedback from the first experiments. Their responses\nindicate that the modifications generally improved the acceptability of the teaching\/\nlearning interface involved. So far as the present type of information-acquiring and\ninformation-evaluating activities are concerned, it appears that iterative feedback can\nimprove computer-based teaching exercises. This result seems to apply to all three levels\nof task distinguished in our introduction: simple, complex and design.\nReferences\nJoyce, B.R. and Weil, M. (1980), Models of Teaching, Englewood Cliffs NJ, Prentice-\nHall.\nMoran, T.P. (1981), 'The command language grammar: a representative computing\nsystem', International Journal of Man-Machine Studies, 15, 3-50.\nNorman, D.A. (1983), 'Design rules based on analysis of human error', Communications\nof the ACM, 26, 254-8.\n40\nALT-J Volume 3 Number 2\nSaloman, G. (1979), Intuition of Media, Cognition and Learning, San Francisco, Jossey-\nBass.\nShneiderman, B. (ed) (1993), Sparks of Innovation in Human-computer Interaction,\nNorwood NJ, Ablex.\n41\n"}