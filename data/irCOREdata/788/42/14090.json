{"doi":"10.1080\/09687760108656773","coreId":"14090","oai":"oai:generic.eprints.org:761\/core5","identifiers":["oai:generic.eprints.org:761\/core5","10.1080\/09687760108656773"],"title":"Towards electronically assisted peer assessment: a case study","authors":["Bhalerao, Abhir","Ward, Ashley"],"enrichments":{"references":[{"id":446295,"title":"A unique style of computer-assisted assessment',","authors":[],"date":"1998","doi":"10.1080\/0968776980060205","raw":"Thelwall, M. (1998), 'A unique style of computer-assisted assessment', ALT-J, 6 (2), 49-57.","cites":null},{"id":194620,"title":"Best practice in non-subjective assessment',","authors":[],"date":"1998","doi":null,"raw":"Farthing, D. W  (1998),  'Best practice in non-subjective assessment', Monitor (Journal of CTI Centre of Computing),  24-6.","cites":null},{"id":194621,"title":"Building Database Applications on the Web Using PHP3,","authors":[],"date":"1999","doi":null,"raw":"Hilton, C.,  Willis,  J., and Borud, B.  (1999),  Building Database Applications  on the  Web Using PHP3, New York: Addison-Wesley Longman Inc., USA.","cites":null},{"id":194617,"title":"Developing Student Autonomy","authors":[],"date":"1988","doi":"10.1177\/135050768201300108","raw":"Boud, D. (1988), Developing Student Autonomy in Learning, London: Kogan Page.","cites":null},{"id":446293,"title":"Different worlds in the same classroom',","authors":[],"date":"1988","doi":null,"raw":"Perry, G. (1988), 'Different worlds in the same classroom', in P. Ramsden (ed.), improving Learning: New Perspectives, London: Kogan Page, 145-61.","cites":null},{"id":446294,"title":"Electronically-assisted p er review',","authors":[],"date":"1999","doi":null,"raw":null,"cites":null},{"id":1877135,"title":"Electronically-assisted peer review',","authors":[],"date":"1999","doi":null,"raw":"Robinson, J. (1999), 'Electronically-assisted peer review', in S. Brown, P. Race and J. Bull (eds.),  Computer-Assisted  Assessment  in  Higher  Education,  Staff  and  Educational Development Series, SEDA.","cites":null},{"id":194624,"title":"Experiential Learning: Experience as the Source of Learning and Development, Englewood Cliffs, N J\/London:","authors":[],"date":"1984","doi":null,"raw":"Kolb,  D.  A.  (1984),  Experiential Learning:  Experience  as  the  Source  of Learning and Development, Englewood Cliffs, N J\/London: Prentice Hall.","cites":null},{"id":1877124,"title":"Issues of partial credit in mathematical assessment by computer',","authors":[],"date":"1999","doi":"10.3402\/rlt.v7i1.11236","raw":"Beevers, C. E., Youngson, M. A., McGuire, G. R., Wild, D. G., and Fiddes, D. J. (1999), ~Issues of partial credit in mathematical assessment by computer', ALT-J, 7 (1), 26-32.","cites":null},{"id":194616,"title":"Issues of partial credit in mathematical assessment bycomputer',","authors":[],"date":"1999","doi":"10.3402\/rlt.v7i1.11236","raw":null,"cites":null},{"id":446296,"title":"MySQL andmSQE, O'Reilly and Associates.","authors":[],"date":"1999","doi":null,"raw":"Yarger, R. J., Reese, G., and King, T. (1999), MySQL andmSQE, O'Reilly and Associates.","cites":null},{"id":194625,"title":"PHP3: Programming Browser-Based Applications,","authors":[],"date":"1999","doi":null,"raw":"Medinets, D. (1999), PHP3: Programming Browser-Based Applications,  McGraw-Hill.","cites":null},{"id":194623,"title":"Plagiarism in programming assignments',","authors":[],"date":"1999","doi":"10.1109\/13.762946","raw":"Joy,  M.  S.  and  Luck,  M.  (1999),  'Plagiarism  in  programming  assignments',  IEEE Transaction on Education, 42 (2), 129-33.","cites":null},{"id":194618,"title":"Promoting peer assisted learning amongst students in higher and further ducation',","authors":[],"date":"1996","doi":null,"raw":null,"cites":null},{"id":1877126,"title":"Promoting peer assisted learning amongst students in higher and further education',","authors":[],"date":"1996","doi":null,"raw":"Donaldson, A. J. M. and Topping, K. J. (1996), 'Promoting peer assisted learning amongst students in higher and further education', Staff and Educational Development Association, SEDA Annual Conference, Paper 96.","cites":null},{"id":194619,"title":"Styles of Learning and Teaching.&quot; An Integrated Outline of Educational Psychology,","authors":[],"date":"1988","doi":null,"raw":"Entwistle,  N.  J.  (1988),  Styles  of  Learning  and  Teaching.&quot; An  Integrated  Outline  of Educational Psychology, Fulton.","cites":null},{"id":194622,"title":"The BOSS system for online submission and assessment monitor',","authors":[],"date":"1998","doi":null,"raw":"Joy, M. S. and Luck, M. (1998), 'The BOSS system for online submission and assessment monitor', Journal of the CTI Centre for Computing,  10, 27-9.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2001","abstract":"One of the primary goals of formative assessment is to give informative feedback to the learner on their progress and attainment of the learning objectives. However, when the student\/tutor ratios are large, effective and timely feedback is hard to achieve. Many testing systems have been developed that use multiple choice questions (MCQ), which are easy to mark automatically. MCQ tests are simple to develop and administer through Web-based technologies (browsers, Internet and Web servers). One of the principal drawbacks of current systems is that the testing format is limited to MCQ and general questions requiring free responses are not included because marking cannot be easily automated. Consequently, many learning tasks, such as the correctness and style of solutions to programming problems, cannot be assessed automatically. Our approach is a hybrid system combining MCQ testing with free response questions. Our system, OASYS, marks MCQs automatically and then controls the anonymous distribution of completed scripts amongst learners for peer assessment of free response answers. We briefly describe the design and implementation of OASYS, which is built on freely available technologies. We present and discuss findings from a case study which used OASYS for 240 students taking a programming class involving four assessed programming laboratories in groups of approximately forty student","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14090.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/761\/1\/ALT_J%2DVol09_No1_2001_Towards_electronically_assiste.pdf","pdfHashValue":"3a1a3a5469dcd8ca757341441ec985e1267990ff","publisher":"Taylor and Francis Ltd","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:761<\/identifier><datestamp>\n      2011-04-04T08:59:22Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/761\/<\/dc:relation><dc:title>\n        Towards electronically assisted peer assessment: a case study<\/dc:title><dc:creator>\n        Bhalerao, Abhir<\/dc:creator><dc:creator>\n        Ward, Ashley<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        One of the primary goals of formative assessment is to give informative feedback to the learner on their progress and attainment of the learning objectives. However, when the student\/tutor ratios are large, effective and timely feedback is hard to achieve. Many testing systems have been developed that use multiple choice questions (MCQ), which are easy to mark automatically. MCQ tests are simple to develop and administer through Web-based technologies (browsers, Internet and Web servers). One of the principal drawbacks of current systems is that the testing format is limited to MCQ and general questions requiring free responses are not included because marking cannot be easily automated. Consequently, many learning tasks, such as the correctness and style of solutions to programming problems, cannot be assessed automatically. Our approach is a hybrid system combining MCQ testing with free response questions. Our system, OASYS, marks MCQs automatically and then controls the anonymous distribution of completed scripts amongst learners for peer assessment of free response answers. We briefly describe the design and implementation of OASYS, which is built on freely available technologies. We present and discuss findings from a case study which used OASYS for 240 students taking a programming class involving four assessed programming laboratories in groups of approximately forty students<\/dc:description><dc:publisher>\n        Taylor and Francis Ltd<\/dc:publisher><dc:date>\n        2001<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/761\/1\/ALT_J-Vol09_No1_2001_Towards_electronically_assiste.pdf<\/dc:identifier><dc:identifier>\n          Bhalerao, Abhir and Ward, Ashley  (2001) Towards electronically assisted peer assessment: a case study.  Association for Learning Technology Journal, 9 (1).  pp. 26-37.  ISSN 0968-7769 (print)\/1741-1629 (online)     <\/dc:identifier><dc:relation>\n        10.1080\/09687760108656773<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/761\/","10.1080\/09687760108656773"],"year":2001,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"Towards electronically assisted \na case study \npeer assessment: \nAbhir Bhalerao and Ashley Ward \nDepartment of Computer Science, University of Warwick \nemail: abh irr~dcs, warwicl< ac.uk \nOne of the primary goals of formative assessment is to give in formative feedback to the \nlearner on their progress and attainment of the learning objectives. However, when the \nstudent~tutor ratios are large, effective and timely feedback is hard to achieve. Many \ntesting systems have been developed that use multiple choice questions (MCQ), which \nare easy to mark automatically. MCQ tests are simple to develop and administer through \nWeb-based technologies (browsers, Internet and Web servers). One of the principal \ndrawbacks of current systems i that the testing format is limited to MCQ and general \nquestions requiring free responses are not included because marking cannot be easily \nautomated Consequently, many learning tasks, such as the correctness and style of \nsolutions to programming problems, cannot be assessed automatically. Our approach is a \nhybrid system combining MCQ testing with free response questions. Our system, \nOASYS, marks MCQs automatically and then controls the anonymous distribution of \ncompleted scripts amongst learners for peer assessment of free response answers. We \nbriefly describe the design and implementation of OASYS, which is built on freely \navailable technologies. We present and discuss findings from a case study which used \nOASYS for 240 students taking a programming class involving four assessed \nprogramming laboratories in groups of approximately forty students. \nIntroduction \nFormative assessment plays an important role in teaching by motivating learners and \nproviding feedback on the achievement of learning objectives to both students and tutors. \nHowever, formative assessment is confounded by large student\/tutor ratios, which is an \ninevitable consequence of resource constraints in publicly funded higher education today. \nIndeed, for formative assessment tobe effective, the feedback to the learner must be timely, \nspecific to the individual, and discursive. \n26 \nALT-] Volume 9 Number I \nFor example, returning the marked scripts of a class test late and then giving only a \nsummative grade completely defeats these aims. However, to be able to give individualized \ndiscursive' comments on a script and return it back to the learner in a reasonable ngth of \ntime is only possible if the student\/tutor ratio is low. Ideally, if resources are available and \nscripts are multiply marked, this would give students greater confidence in the validity of \nthe grade and comments. \nFormative assessment is a critical part of science-based curricula such as Engineering, \nPhysics, Biological Sciences and Psychology where practical laboratory sessions are \nintegral to the teaching and learning. Such sessions are essential to give students a 'hands- \non' experience of otherwise theoretical concepts (experiential learning such as Kolb, 1984). \nAnother laudable aspect of labs is that learning is naturally student-centred and promotes \nautonomy (Boud, 1988). Depending on the discipline, the learning objectives will range \nfrom knowing laboratory safety procedures, to learning to conduct and record \nexperimental observations, problem-solving, or devising or applying abstract principles. \nOften these sessions are graded by spot-tests or marking of lab books (sometimes as an \nincentive for attendance), to make sure that safety procedures are known, and to provide \nlearning and progress feedback. \nIn Computer Science, we are increasingly using supervised practical programming sessions \nrather than seminars to reinforce problem-solving. Our first-year undergraduate \nprogrammes currently have 240 students, so having, say, four assessed labs on any course \n(which is not untypical) results in about 1,000 scripts to be marked by four tutors. To be \neffective, these scripts have to be graded and commented on within the week, ideally before \nthe next lab session. One of the objectives of the programming-based courses is to teach \nstudents how to program, and so, for the assessment to match this objective, we must assess \nprograms (or at least snippets of programs) constructed by the learners. This further \nincreases the marking burden since correct program solutions are seldom unique. \nOne means to address the resource issue caused by larger student\/tutor ratios is by some \ndegree of computer automation, the obvious being computer-based testing. Another \napproach is to incorporate peer assessment. We have therefore proposed a hybrid system \nwhich exploits the efficiency of electronic document-handling whilst achieving the quality \nof feedback that can only be given by humans. Computer-based assessment is largely \ncentred on the presentation and automated marking of multiple choice questions (MCQs). \nExamples of such online assessment systems are the CASTLE system (www. \nle.ac.uk\/castle), COSE (www.staffs.ac.uk\/COSE) and TRIADS (www.derby.ac.uk\/assess\/ \nnewdemo\/mainmenu.html), amongst others. The need to automate the marking currently \nlimits the answers to highly structured responses, hence the use of MCQs. Extensions of \nthe single-stem (single) answer (permutation MCQ or PMCQ) have been proposed to \nincrease the scope and reliability of the testing (Farthing, 1998). The goal of automated \nmarking of open-ended questions, perhaps using natural anguage-processing, is still some \nway in the future. In our software labs, with answers consisting of program solutions \nwithout a highly constrained specification, an automated process cannot even perform a \nsimple test of correctness (Joy and Luck, 1998). To build a system that can provide \nmeaningful feedback about the construction of the program is even harder (Beevers, \nYoungson, McGuire, Wild, and Fiddes, 1999; Thelwal, 1998). As Joy and Luck have shown \n(1999), simple measurements about the code, such as the number of comments, can be \n27 \nAbhir Bhalerao and Ashley Ward Towards electronically assisted peer assessment: a case study \nmade but an automated process cannot perceive the more subtle aspects: choice of variable \nnames; elegance of solution; why the code does not give a desired result. In short, the \nremoval of the human element from the code assessment processes reduces the quality, and \nconsequently, the validity of the assessment. \nAlthough computers cannot make effective judgements of scripts they can certainly \nsimplify the document management problem. The current Web and database architectures \nnow offer a great deal of flexibility and portability in the development of computer-based \nassessment systems. Our solution to the problem of meaningfully assessing solutions to \nopen-ended questions or answers which require problem-solving was to engage the group \nof learners themselves in the assessment process, namely to use peer assessment (Brown, \n1998; Topping and Ehly, 1998). \nElectronically mediated peer assessment \nPeer assessment immediately raises the spectre of 'the blind leading the blind' - how can \nlearners help each other when they themselves do not fully understand the material? To \nalleviate this concern we chose to use two aspects of our automated system: that it can \neasily anonymize and distribute multiple copies of scripts between learners, and that \nlearners can be quickly graded on the response to 'closed' MCQ questions. The automated \nmarking process works as follows. Each learner takes a test consisting of a number of \nMCQ and open-ended questions. After completing a test, each learner than becomes an \nassessor and is required to mark three scripts. The system ranks the learners into three \ngroups by the total of their correct MCQ answers. This can be done immediately the script \nis complete. The distribution of scripts is controlled such that peers receive approximately \none script from each of the good, intermediate and poor MCQ results. If necessar3; the \nperceived ability of an assessor can be further augmented by the overall marks from a \nprevious test or some other a priori information. After this peer marking is complete, each \nscript will have been marked several times, increasing the validity of the marks. \nFurthermore, tutors can instantly view the variability of the marks given to each script. If \nthe variance is high because of disagreement between the assessors, the script is highlighted \nfor moderation by a tutor. \nAs well as the statistical safeguards in our system, the worries surrounding peer assessment \ncan be countered by other educational benefits that a large-scale, learner-centred system \naffords. Feedback is given in triplicate and it is individualized and discursive. Asking \nlearners to evaluate work is also justifiable: they are asked to read as well as write code, \nwhich is a vital skill since few programs are written entirely from scratch. It is often quoted \nthat the best way to learn is to teach and here, in writing meaningful feedback, is an \nopportunity for the learner to take on the role of teacher (sometimes termed peer \ntutoring). Also, evaluation is an active process, encouraging reflection upon and discussion \nof one's own answers, thus fostering a deeper approach to learning (Donaldson and \nTopping, 1996). The experience of seeing multiple opinions on a piece of work (peer review \nas it were (Robinson, 1999)) further promotes what Perry termed a 'relativist' as opposed \nto a 'dualist' approach to learning (Perry, 1988). Finally, the system alleviates a greater part \nof the marking burden, thus allowing tutors to concentrate more on the teaching material \nand the moderation process. \n28 \nALT-] Volume 9Number I\nDesign of online assessment system (OASYS) \nAt the outset, we identified the following requirements forour computer-assisted system. It \nshould: \n\u2022 provide anonymity to all learners; \n\u2022 be distributed and cross-platform; \n\u2022 respond in real time; \n\u2022 present learners with test, mark and view-results interfaces; \n\u2022 provide tutors with authoring, moderation and administration interfaces. \nAs well as considering these requirements, wedecided to use open-source (free) software in \nits design to overcome the need for licensing costs and to provide greater control of \nimplementation choice. The architecture of the resulting system is illustrated in Figure 1. It \nis not surprising that a Web-server-based solution satisfies most of our requirements: \naccess control and registration, distributed, cross-platform and the means to give learners \nand tutors appropriate interfaces by static and dynamic HTML pages. \nr Netscape ] I Netscape I [Netscape I I Netscape] \nApache web server \n. . . . . .  ! i terpreter  1l FP~P3 script \n[mysqi database I [~i!tsr~~ \n... x60  \nFigure I: Technical architecture of OASYS. Standard, static Web pages are served up as well as pages \ngenerated on-the-fly by scripts written in PHP3. Test data, responses and marks are kept by a mysql \nrelational database. The system is built entirely from open-source software. \n29 \nAbhir Bhalerao and Ashley Ward Towards electronically assisted peer assessment: a case study \nA relational database, mysql (Yarger, Reese, and King, 1999; DuBois, 1999), was used at \nthe backend to store the test data and record all responses. As well as providing structured \nstorage and persistenbe, by means of simple queries, this enables rapid analyses of the \nresults for script distribution, online test monitoring and moderation. The response times \nwere minimized by using a Web-server-optimized HTML pre-processor, PHP3 (Medinets, \n1999; Hilton, Willis, and Borud, 1999), as opposed to slower CGI scripting technologies. In\nfact, we were able to achieve near instantaneous response times for up to 100 students \nsimultaneously being presented with test questions and the system recording their answers. \nThe need for anonymizing cannot be overemphasized. As well as being a statutory \nrequirement under the regulations of the university regarding student esting, it is a \nrequirement to store marks securely in this form under the Data Protection Act. \nFurthermore, part of the success of peer assessment rests on learners being free to \ncomment on each other's work without reprisal. To use OASYS, students first register \nusing their university ID and are given a unique password. At this point, the system creates \na random internal identifier, which is subsequently used as an anonymized primary-key for \nall responses to and from the user. \nii . . . . . . . . . . . .  o \n~!!!!!t ........................................ \nFigure 2: Left - the question navigator is shown at the top. The current question has a larger button, \nand the colour of the question number indicates whether the question has been answered The test \nneed not be done in linear sequence: this learner has only answered questions I, 3 and 4. Right - \nPermutational Multiple Choice Questions (PMCQs) are another question type. Potential answers to \nboth MCQ and PMCQs are jumbled up randomly (akthough deterministicatly - this learner wilt always \nsee this question this way) so that answers cannot be copied from a nearby learner. \n30 \nAL~-] Volume 9 Number I \nFigure 3: Entirely free answers \nare a major feature of OASYS \n- free responses are marked \nby other learners. Also note \nhere the use of HTML links to \nsupporting material, and the \nuse of images in the question \nand potential answers in \nFigure 2, \nQuestion 2: \ns.pu~h(\"~\" , \n\u2022 ~ d0\"~ ~, su,~le ~-~t~ ~I: to display the steele of s \n2. Howwould you :eeversedxe~ ou~ut w~'Aou\u00a2 chatting s ,c~.  ~os~:~O but eh'e~g~e p~.O s.~d \nanswer: \n\/ \/  3 a $~gle $%atement to Rz\/n~ tile stat~ of 3 \nsystem, \u00a2~t. pr~t\/1%<$); \n\/ \/  2, gave a wap to z~r~e the printed out~t  \n\/\/ (look at ~hat 3inglpLL~kedLi~t ~etho~ pou could equivalentlp \n\/ \/  use to impl~B%ent push<) and pop()) i \ni \nAn illustrative tour of OASYS is presented in Figures 2-7. These figures show the testing \ninterface for MCQs (Figure 2) and open-ended questions during a given test (Figure 3); the \nassociated marking interface when performing peer assessment (Figure 4); and \nfinally the see-my-marks interface, wh re a learner can view comments from markers \n(Figure 5). Each figure caption gives more details on these interfaces and how they are \nused. \nIn designing the look-and-feel of the interfaces, we were particularly keen to mimic the \npaper system where possible. For example, the question avigation bar quickly allows the \nlearner to skip between questions as questions are rarely answered in sequential order. \nRed- and green-coloured labels are used on these buttons to indicate whether a question \nhas been attempted. \nThere is also a consistency of interface design between the testing, marking and mark- \nviewing interfaces. As we had several tests in the course, we employed a distinctive (but \nWeb-safe) colour scheme for the screen backgrounds. This was useful to see which students \nwere taking tests out of sequence because of previous absences, and so on. A distinctive \nend test screen was also used to show tutors those students who have signalled the \ncompletion of their test to the system. \nAfter taking a test, students were required to mark at least three other scripts in their own \ntime before the next lab session. During marking, the marker requests a script to be \n31 \nAbhir Bhalerao and Ashley Ward Towards electronically assisted peer assessment: a case study \nOASYS: Mark \nMr A Ward (cssbz) is marking lab1 script on Mon Feb 28 \n19;31 ;29 2000 \nQuestion 8 was: \nThings to consider: \nt \nSJ \nScript to mark (Id: 900121, ;abl ): \n} \nX \ni \nt \nMy marks', \n................................................................... ~C ;,~2;S; ..................... \nFigure 4: Leamers are required to mark three scripts in order to gain full credit. The marking page \nshows the original question; advice on the correct use of the mark scheme for this question; the answer \nto be marked (which was previously entered by another learner) and finally the marking interface. \nNotice that answers to the MCQs (in this test, questions I -7) are automatically marked and so \ncannot be selected in the question navigator. \nOASVS:  See my marks \nMarks for  ~ ~:~ ~ test tab1 on M n Feb  28 19:45:06 \n20OO \nQuest ion I was: \nDf the f~o~ ~tacero~ ,le.cl~e ~u m~e~e~ ~'~ay to ~okl t0 v~ue~? \nAnswer: \n0 ', . . . . . . .  \ni o . . . . . . . . .  i \nYour mark for  this quest ion:  \nI out of i \nFigure 5: 'See my marks' is where \nleamers obtain feedback on their \nscripts, and can be viewed at any \ntime from anywhere on the \nIntemet. Marking can also be done \nin learners' own time. The MCQ is \njumbled the same way as the \nlearner originally saw it. \n32 \nALT-J Volume 9 Number \nOASY$: !~e my marks \nMarks  fo r  \". ..... . . , : :  test l ab3  on  Non Feb 28 \n2o:0~.~00o \nSc~ipt mark sumn~y tab~ \n2 ~ \" i\"  \nB t I \n1 \nle  ~o a \n.- 11 ~ 2 \nYour  c t~rent  o~rMI  mark~of  tes t  |ab3  \n~t ag \nBroken down by CritNa: \n9 9 ~0 \n4 Io ~ 10 \nFigure 6: All the marking information about a script ~s collated for the learner in the summary sheet. \nThis particular learner has quite a good overall result, but the overall spread of marks given by other \nlearners (shown in the breakdown table) seems to be fairly wide, despite the lack of variance in \nmarking by one learner who seems to have mainly marked in the middle. \n[ m ~ %  ~ItTeT~I takes ~ St }g IN\u00a2 a $mgjylagke~Listi~le~a~t m de !ue~e, \nTest lab4 MCQ marks: \nTest lab4 Free Response Grades: \n!~iT 7~:;;-'v f~-~:ITZT:7 ~N 5;=i' i :~  i2'; ? iiZ ?~= \ni:i;T ! t;.'~ :5-2 ~ i iTN~=!; ~i 717\u00a3'i N~ ;-i?'.~!~ =; ii~  ;Z'Z ; - i2 ; :  \n:~: :: ;i7!7 ~ iT  7 : i i~ ;17:\" \ni-~-US~i 5:iN \nFigure 7: Left: administrators can browse feedback comments entered during marking. Right: statistics \ncan be generated in real time from the database. Question 4 (a MCQ) seems to have misled the \nmajority of learners who have done this test so far (there are approximately another 80 learner's still \nto complete this test at this point in time). \nmarked which the system then anonymizes and distributes from the \"pile' of all completed \nscripts within the database. Each script is presented with associated 'model' answers, hints \nand tips for marking and a grading scheme. Since we wanted the learners to be critical of \nprograms we allowed them to grade each question on the readability, correctness and style \n33 \nAbhir Bhalerao andAshley Ward Towards electronically assisted peer assessment: a case study \nof the answer and possible suggestions on where they thought the learner had gone wrong \n(Figure 4). The marker is allowed to revise marks until the marking deadline is reached \n(usually set to be the day before the start of the next lab session). At any time a tutor can \nview a given script together with the currently associated marks and comments (Figure 7, \nleft). Other administration pages can show the state of progress on the peer assessment by\nvarious tables (Figure 7, right). Test questions are authored using other dynamic pages. \nEvaluation and discussion \nOASYS was commissioned early in 1999, ready for the teaching of the first year Design of \nInformation Structures module in the spring term. Unfortunately, parts of the systems \nwere not ready until the end of term, which had a detrimental effect on its perception, as \nwe discuss below. Each student was required to attend four two-hour lab sessions in total \nover six weeks following online worksheets and problem-solving using template Java \nprograms and experimenting with various data structures and related algorithms (arrays, \nstacks, lists, trees, hash-tables, searching and sorting, etc.). Each session was with a group \nof approximately forty students, attended by four postgraduate utors. Students were \nencouraged towork in pairs during the lab but an individual thirty-minute st was taken \nat the end of the session consisting of MCQ and open-ended questions. The total credit \nfrom the lab tests amounted to 10 per cent of the module. \nPaper Tests OASYS OASYS \nAcademic year 1998-9 1999-2000 2000--I \nTutors 6 6 6 \nStudents 212 240 275 \nStaff time (person hours) 170 230 78 \nStudent time (person hours) 1600 1800 2200 \nAverage mark (stdev) 48% (2.6) 42% (0.2) \nBest case feedback time 2 weeks I hour I hour \nWorst case feedback time 4 weeks 3 weeks I week \nResources 2400 paper sheets 83Mb disk space 83Mb disk space \nTable t: A comparison of resources required for the paper-based testing system compared with \nOASYS. The staff time for 1999-2000 includes roughly four people~weeks of development and \nmarking in addition to about seventy hours of tutoring. The staff time for 2000-1 is estimated to \nindude a small amount of moderation and administration time (sixteen hours). The system will handle \na total of I, I O0 test scripts. \nThe first evaluation we conducted was to compare the impact hat OASYS had on resource \nutilization with estimated figures for the paper-based assessment system we had used in the \nprevious academic year (1998-9). The results are summarized in Table 1. The staff time for \n1999-2000 includes about four people\/weeks of system development in addition to about \nsixty hours of contact hours with students during the labs. The resource benefits of the \nnew system will only become apparent next year (2000-1) when the staff time will require a \nsmall amount of time for script moderation and administration tasks plus the tutoring \ntime. We stimate that students will spend about five to ten minutes marking each script \n34 \nALT-] Volume 9 Number I \n(twelve to mark in total), so overall, one to two hours outside the lab sessions. Intriguingly, \nthe average mark stayed about he same when peer marked, but the marks were less spread. \nThe minimum feedback time is now potentially zero (in fact, it is zero for the MCQ \nquestions which are automatically marked). The maximum feedback time was poor in \n1999-2000 due to the late implementation f the feedback interface, but next year it should \nbe at a theoretical maximum of one week (dependent simply on the timetable logistics). \nAt the end of the lab sessions, we used a questionnaire to gauge the perception and experi- \nence of the learners toward the online assessment system. Included in the questionnaire \nwas a section on the students' learning style modelled on Entwistle's dimensions of \nApproaches to Learning (Entwistle, 1988). Some of the results of the questionnaire are \ngiven in Table 2. The amount of time students had spent on marking was not as much as \nwe had hoped. However, 90 per cent had reconsidered their answers, which is encouraging \nand certainly better han never looking at the test again. The lack of timely feedback gave a \npredictable r sponse. Anonymity was clearly important to these students, probably as they \nwere all answering the same questions. The more marking they did, the better their own \nresults became, which may in itself justify the work of peer assessment. One of the \ninteresting results of the Entwistle Approach to Study part of the questionnaire was that \n'reproducers' (who might do well on a solely MCQ-based test) did not do so well here and \nexperienced difficulty in marking. \nThe final set of evaluations was conducted after the summer examinations, attempting to\nmake statistical correlations between the students' learning approach, their lab results and \ntheir perception of the labs (from the questionnaire responses) and the overall achievement \nin the final examination. Where possible, these results were compared with correlation \nindices from the previous year. Figure 8 illustrates that the correlations between the exam \n(E), coursework (C) and lab marks (LM) have not changed significantly between the paper \ntests and OASYS, which is encouraging. However there appears to be little correlation \n(significantly ess than 0.5) between the lab participation (LP) and the lab marks (LM) or \nin fact any other index. This is disappointing aswe would have liked to have seen a good \ncorrelation between attainment and participation i the lab sessions reflected in both the \ncoursework and end-of-year exams. We do not believe that this completely negates the \nbenefits of participation i the exercise (answers on the questionnaire support this) and we \nwould hope to improve on this aspect, perhaps by better information about the objectives \nof OASYS, to see a higher positive correlation i  the future. \n'In total, I spent this amount of time marking' \/~ -- 65 mins (5 mins per script), \na -- 40.7 \n'When marking, I realised mistakes I had made in \nmy own answers' Yes 90%, No 10% \n'1 received speedy feedback on my work in the tests' Agree 15%, Indifferent 27%, Disagree 56% \n(Unanswered 2%) \n'Anonymous marking of the tests is important o me' Agree 53%, Indifferent 41%, Disagree 7% \nTable 2: Some findings of the post-lab session questionnaire. Learners who stated that they marked \nmore scripts tended to receive a higher final mark themselves. Learners with a strong 'reproducing' \norientation tended to receive lower marks and found markdng difficult. \n35 \nAbhir Bhalerao and Ashley Ward Towards electronically assisted peer assessment: a case study \nE = exam \nC = coursework \nL = labs \nLM = lab mark \nLP = lab participation \n1998 \/ 1999 \n(N=73) \nE \nC 0.45 L \nFigure 8: Correlations found in module results. \n1999 \/ 2000 \n(N-188) \nE \n\/0 .102  \nLP \nConclusions \nWe have presented our experiences of implementing and using a computer-assisted peer \nassessment system. The aim of this study was to build a system that would give learners \neffective and timely feedback on the results of laboratory tests. Our system, OASYS, \nachieves this by the automated document management of test scripts. After taking a test, \neach learner is required to mark three other scripts. OASYS controls the distribution of \nthese scripts so that they are anonymized and ranked on the results of MCQ answers. A key \nfeature of our system is the inclusion of open-ended-questions nvolving problem-solving in \nthe tests. The 'automated' marking of these questions i possible since the students mark \neach other's work and are given example answers and a marking scheme. If necessary, the \nmarks and comments can be moderated by tutors before they are fed back to the learner. \nOverall, the system has been successful in reducing the resource requirement for \nadministering and marking the laboratory tests. The system has been well accepted by the \nstudents and, in general, most students have been willing participants and have felt in \ngreater control of the testing process. Our evaluations have shown that peer assessment has \nbeen beneficial to the learners in improving their critical and analytical abilities in \nprogramming and problem-solving. The delay in completing the system last year caused a\ndelay before students could receive their marks and comments. We do not anticipate these \nproblems in this academic year and aim to profit further from reusing the same system. \nWe believe that OASYS is sufficiently flexible to be adopted in disciplines other than \nComputer Science. For example, simply by authoring a set of new tests, for example, for \nPsychology labs, the system could be reused immediately elsewhere. However, small \nmodifications to the imposed constraints on the sequencing of the tests and marking \nwould have to be made for non-labs pecific testing. Use in a social sciences etting for peer \nreview of complex discursive works, such as essays, would require more extensive changes \nto the learner interface and the a priori ranking and script distribution algorithms. None of \nthese proposals, we think, is too far beyond the scope of our work. \n36 \nALT-J Volume 9 Number I \nReferences \nBeevers, C. E., Youngson, M. A., McGuire, G. R., Wild, D. G., and Fiddes, D. J. (1999), \n~Issues of partial credit in mathematical assessment bycomputer', ALT-J, 7 (1), 26-32. \nBoud, D. (1988), Developing Student Autonomy in Learning, London: Kogan Page. \nBrown, S. (ed.) (1998), Peer Assessment in Practice, Staff and Educational Development \nAssociation. \nDonaldson, A. J. M. and Topping, K. J. (1996), 'Promoting peer assisted learning amongst \nstudents in higher and further ducation', Staff and Educational Development Association, \nSEDA Annual Conference, Paper 96. \nDuBois, E (1999), MySQL, CA, USA: New Riders Publishing. \nEntwistle, N. J. (1988), Styles of Learning and Teaching.\" An Integrated Outline of \nEducational Psychology, Fulton. \nFarthing, D. W (1998), 'Best practice in non-subjective assessment', Monitor (Journal of \nCTI Centre of Computing), 24-6. \nHilton, C., Willis, J., and Borud, B. (1999), Building Database Applications on the Web \nUsing PHP3, New York: Addison-Wesley Longman Inc., USA. \nJoy, M. S. and Luck, M. (1998), 'The BOSS system for online submission and assessment \nmonitor', Journal of the CTI Centre for Computing, 10, 27-9. \nJoy, M. S. and Luck, M. (1999), 'Plagiarism in programming assignments', IEEE \nTransaction on Education, 42 (2), 129-33. \nKolb, D. A. (1984), Experiential Learning: Experience as the Source of Learning and \nDevelopment, Englewood Cliffs, N J\/London: Prentice Hall. \nMedinets, D. (1999), PHP3: Programming Browser-Based Applications, McGraw-Hill. \nPerry, G. (1988), 'Different worlds in the same classroom', in P. Ramsden (ed.), improving \nLearning: New Perspectives, London: Kogan Page, 145-61. \nRobinson, J. (1999), 'Electronically-assisted p er review', in S. Brown, P. Race and J. Bull \n(eds.), Computer-Assisted Assessment in Higher Education, Staff and Educational \nDevelopment Series, SEDA. \nThelwall, M. (1998), 'A unique style of computer-assisted assessment', ALT-J, 6 (2), 49-57. \nTopping, K. and Ehly, S. (eds.) (1998), Peer-assisted Learning, Mahwah, NJL: Lawrence \nErlbaum Associates. \nYarger, R. J., Reese, G., and King, T. (1999), MySQL andmSQE, O'Reilly and Associates. \n37 \n"}