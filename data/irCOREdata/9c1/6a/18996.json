{"doi":"10.1080\/17437270701614790","coreId":"18996","oai":"oai:eprints.bham.ac.uk:599","identifiers":["oai:eprints.bham.ac.uk:599","10.1080\/17437270701614790"],"title":"Where does good evidence come from?","authors":["Gorard, Stephen","Cook, Thomas"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2007","abstract":"This paper started as a debate between the two authors. Both authors present a series of propositions about quality standards in education research. Cook\u2019s propositions, as might be expected, concern the importance of experimental trials for establishing the security of causal evidence, but they also include some important practical and acceptable alternatives such as regression discontinuity analysis. Gorard\u2019s propositions, again as might be expected, tend to place experimental trials within a larger mixed method sequence of research activities, treating them as important but without giving them primacy. The paper concludes with a synthesis of these ideas, summarising the many areas of agreement and clarifying the few areas of disagreement. The latter include what proportion of available research funds should be devoted to trials, how urgent the need for more trials is, and whether the call for more truly mixed methods work requires a major shift in the community","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"Taylor & Francis","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.bham.ac.uk:599<\/identifier><datestamp>\n      2011-12-20T11:22:15Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D48:4831<\/setSpec><setSpec>\n      7375626A656374733D4C:4C31<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Where does good evidence come from?<\/dc:title><dc:creator>\n        Gorard, Stephen<\/dc:creator><dc:creator>\n        Cook, Thomas<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        H Social Sciences (General)<\/dc:subject><dc:subject>\n        L Education (General)<\/dc:subject><dc:description>\n        This paper started as a debate between the two authors. Both authors present a series of propositions about quality standards in education research. Cook\u2019s propositions, as might be expected, concern the importance of experimental trials for establishing the security of causal evidence, but they also include some important practical and acceptable alternatives such as regression discontinuity analysis. Gorard\u2019s propositions, again as might be expected, tend to place experimental trials within a larger mixed method sequence of research activities, treating them as important but without giving them primacy. The paper concludes with a synthesis of these ideas, summarising the many areas of agreement and clarifying the few areas of disagreement. The latter include what proportion of available research funds should be devoted to trials, how urgent the need for more trials is, and whether the call for more truly mixed methods work requires a major shift in the community.<\/dc:description><dc:publisher>\n        Taylor & Francis<\/dc:publisher><dc:date>\n        2007<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.bham.ac.uk\/599\/1\/IJRME_special_issue_paper2.pdf<\/dc:identifier><dc:relation>\n        public<\/dc:relation><dc:relation>\n        http:\/\/eprints.bham.ac.uk\/599\/1.hassmallThumbnailVersion\/IJRME_special_issue_paper2.pdf<\/dc:relation><dc:relation>\n        http:\/\/dx.doi.org\/10.1080\/17437270701614790<\/dc:relation><dc:identifier>\n        Gorard, Stephen and Cook, Thomas (2007) Where does good evidence come from? International Journal of Research & Method in Education, 30 (3). pp. 307-323. ISSN 1743-727x<\/dc:identifier><dc:relation>\n        http:\/\/eprints.bham.ac.uk\/599\/<\/dc:relation><dc:language>\n        English<\/dc:language><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["public","http:\/\/eprints.bham.ac.uk\/599\/1.hassmallThumbnailVersion\/IJRME_special_issue_paper2.pdf","http:\/\/dx.doi.org\/10.1080\/17437270701614790","http:\/\/eprints.bham.ac.uk\/599\/"],"year":2007,"topics":["LB Theory and practice of education","H Social Sciences (General)","L Education (General)"],"subject":["Article","PeerReviewed"],"fullText":" 1 \n'This is an electronic post-print version of an article published in International \nJournal of Research and Method in Education Vol. 30, No. 3 (2007): 307-323. \nInternational Journal of Research and Method in Education is available online at: \nhttp:\/\/www.informaworld.com\/smpp\/title~content=t713727792.    \nURL to published version: http:\/\/dx.doi.org\/10.1080\/17437270701614790.   \n \n \nWhere does good evidence come from? \n \n \nStephen Gorard \nSchool of Education \nUniversity of Birmingham \ns.gorard@bham.ac.uk \n \n \nThomas Cook \nNorthwestern University \n \n \nAbstract \n \nThis paper started as a debate between the two authors. Both authors present a series of \npropositions about quality standards in education research. Cook\u2019s propositions, as might \nbe expected, concern the importance of experimental trials for establishing the security of \ncausal evidence, but they also include some important practical and acceptable \nalternatives such as regression discontinuity analysis. Gorard\u2019s propositions, again as \nmight be expected, tend to place experimental trials within a larger mixed method \nsequence of research activities, treating them as important but without giving them \nprimacy. The paper concludes with a synthesis of these ideas, summarising the many \nareas of agreement and clarifying the few areas of disagreement. The latter include what \nproportion of available research funds should be devoted to trials, how urgent the need \nfor more trials is, and whether the call for more truly mixed methods work requires a \nmajor shift in the community. \n \n \n \nIntroduction \n \nThis paper is unusual in that it has some of the characteristics of a debate between its two \nauthors. The debate concerns what we consider policy-makers and practitioners require in \nthe form of high quality education research evidence. Should education research be \npredominantly experimental or based on mixed-method studies? The main issue we \naddress in this brief paper is the place of experimental design in evidence-based policy-\n 2 \nmaking and practice.  One of us, Thomas Cook, has written widely on randomised \ncontrol experiments in education (Cook & Campbell 1979, Cook 2002), detailing their \nmerits, outlining the assumptions and threats to validity associated with them, and listing \nand refuting most objections usually raised to doing them. The other of us, Stephen \nGorard, is probably better known for writing about the value of mixing methods of \ndifferent kinds (Gorard 2001, Gorard with Taylor 2004). Our difference in research \nemphasis speaks to a vexing issue of method choice that currently bedevils the \neducational research community as it seeks to ground education policy decisions in better \nevidence. As will become clear, our positions are quite similar when we address general \npropositions such as the question forming the title to this paper. But we differ in some \nparticulars of great importance for deciding which kinds of education research to \ncommission in order to improve the policy yield of education research. Learning where \nwe agree may help readers identify where they can be relatively confident about method \nchoice. Learning where we disagree may help them identify which method choice \ndecisions remain problematic and where maximal caution is required in evaluating claims \nabout new knowledge for improving the outcomes from education. Each of us will first \nindividually present some research principles and propositions, and we then later draw \ntogether our areas of agreement and difference. The emphasis throughout is on clarity of \nexpression. \n \n \nThomas Cook \n \n1. Educational policy speaks to many different kinds of issue and question, most \nassociated with different method preferences. So, comprehensive \u201cevidence-based \nresearch must be multi-method.  \n \nAmong other issues, educational policy has to be concerned with \u201cwho gets what?\u201d; \n\u201cwhat does a given educational service cost?\u201d; \u201cwhat is classroom life like?\u201d; \u201chow well \nare students performing?\u201d; \u201chow are teachers trained?\u201d; and \u201cwhat works to improve \nstudent performance?\u201d The majority of these questions are descriptive; only the last is \nexplicitly causal. Theorists of method in the social sciences broadly agree that the best \nmethods for dealing with non-casual issues require theory, ethnography, interviews and \nsurveys, among other methods. Experiments hardly help. If educational research is to \nspeak to the comprehensive knowledge needs of the education policy community, it can, \nshould and must involve multiple methods. Framing the issue as a choice between \nexperimental or mixed methods is silly. Even questions that seem purely causal at first \nglance are embedded within contexts where we also need to know: \u201cWho gets the new \neducational practice under evaluation?\u201d \u201cWhat does the program cost?\u201d \u201cWhich social \nvalues does the intervention speak to?\u201d, and so on. Even the major institutional advocate \nof experiments today, the Institute for Educational Sciences of the United States \nDepartment of Education, routinely commissions experimental evaluations that also \ninclude theoretical analysis of the program under review and observational measures of \nprogram implementation. It also funds many, and some very large, non-experimental \nsurveys of educational resources and performance, like the National Assessment of \nEducational Progress (NAEP). Arguing for mixed method research is anodyne, given the \n 3 \nheterogeneity of knowledge needs in education and the research design practices of even \nthe most passionate advocates of experiments. The debate needs to be framed differently-\n-about (1) the priority to give to causal versus non-causal issues in educational research \ntoday; and (2) when causal questions are central\u2014and only then--the priority that should \nbe given to randomised control experiments versus other causal methods. I address these \ntwo basic themes in the points below. \n \n2. Causal questions have a special importance in educational policy research.  \n \nMy rationale for this assertion is that policy-makers are selected or elected to make \ndecisions. These decisions often touch on how to change schools and colleges to raise the \nperformance of teachers and students. This is always a pressing concern, but especially in \nnations where comparative studies like PISA indicate disappointing levels of average \nperformance. But even in nations currently doing well, novel ideas are needed if they are \nto maintain their relatively high standing. Where are these ideas to come from, and how \nshould they be tested before being implemented on a broad scale? I believe many \ndescriptive issues are important in education; but identifying \u201cwhat works\u2019 deserves a \nspecial status among the concerns of those accountable for the quality of educational \nperformance, as does learning about \u201cwhat works\u201d in the most secure ways. Moreover, I \nalso believe that the need to learn what works is especially acute right now, raising even \nmore the priority of gaining accurate causal knowledge in education. The main reason for \nbelieving this is immediately below. \n \n3. The causal knowledge now being generated in education is inadequate for providing a \nsecure stock of knowledge about effective educational practice. \n \nEmpirically based causal assertions are rampant in today\u2019s educational research, very few \nof them the product of experiments. How valid are they in general? No definitive answer \nis possible, given that an answer depends on the very standards of evidence that are in \ncontention among educational researchers today. But in the countries I know best--the \nUSA, UK, France and Germany--no secure body of literature exists that policy makers \ncan rely upon to learn what should be changed in schools in order to improve student \nachievement and social behavior. Cacophonous claims about effective practices abound. \nBut we will later see that their technical warrant is generally weak when evaluated by the \nmost widely accepted causal methods in statistics and across the social sciences as a \nwhole, as opposed to the standards currently operating in large parts of the educational \nresearch community. When the fundamental values buttressing policy choices are at \nissue, all educational policy-makers should welcome active dispute since contention \nabout values is the mother\u2019s milk of democracy. But to welcome dispute about the effects \nof discrete educational practices is another matter. Evidence-based policy depends on a \nreasonably clear research-based consensus about effective practices as one central input \ninto decision-making, though all decision-makers realize that total consensus is \nimpossible. Yet typically decision-makers do not get even an approximation to \nconsensus. Some decisions have been endorsed by education researchers in the past and \nwere widely disseminated without much quality research evidence to back them up. \nSome of these turned out to be quite disastrous--e.g., new math and whole language \n 4 \nreading instruction. I believe that causal issues are central to educational policy and that \nthe causal knowledge generated by educational researchers to date has generally not been \ntrustworthy. So the key is to learn more about what works in education. One proposal to \ndo this involves radically increasing the incidence of random assignment experiments, \nsince in Cook\u2019s (2002) review of the relevant literature they constitute from 1% to 5% of \nall the educational research studies that claimed a causal finding. Why stress \nexperiments? \n \n4. For answering causal questions, the randomised experiment is well warranted \ntheoretically and empirically. \n \nThe theoretical warrant for experiments comes from a minor variant on the same \nstatistical theory that undergirds the highly successful survey research industry. This \nminor variant uses statistical theory to create, not a single sample that formally represents \nthe population from which it was drawn, but two or more samples that represent the same \npopulation, whatever it might be. Since the groups so created are initially identical on \nexpectations, any final difference between them must be due to whatever intervention one \ngroup had that the other (or others) did not. However, this is not the only warrant for \nexperiments. Over the last decades we have had considerable experience implementing \nthem in sectors other than education and even some experience in education, albeit \nprimarily in the USA. We are fast learning how to improve their implementation in order \nto regularly meet all the assumptions the method requires. The survey research industry \ncould not exist without both a statistical theory and decades of wisdom (much from \nsmall-scale experiments) about how to implement surveys so as to reduce bias. The \nneeded statistical theory already exists for experiments, and knowledge is being quickly \naccumulated about how to implement them more often and better (Cook, 2002). I do not \nwant to argue that experiments are perfect, only that they are superior to their current \nalternatives. Their imperfections are of several kinds. \n \n5. The valid causal interpretation of experiments depends on assumptions being met.  \n \nTo produce unbiased causal results experiments require several assumptions that are \nroutinely described in method texts. The major ones are that a correct random assignment \nprocedure is chosen; that it is correctly implemented; that no differential attrition occurs \nacross the groups being compared; and that contamination of the intervention details from \none group to another is minimal. Also, the analysis of experiments depends on standard \nstatistical assumptions being met, as do other causal studies too. Each of these \nassumptions can be violated, but methodologists know about them and about how to \navoid or limit their influence in complex settings like schools and colleges. However, \nwhile many educational researchers know about the necessary statistical theory, far fewer \nof them are experienced in implementing experiments so that their assumptions are \ndemonstrably met in school-based research, and on a quasi-automatic basis (Cook & \nForay, in press). Experiments are only sufficient for unbiased causal knowledge when the \nabove assumptions are demonstrably met, and meeting them is not difficult for those with \nexperience conducting experiments. \n \n 5 \n6. Being limited in their capacity to generalize causal findings, experiments do not \nalways answer the question of greatest policy relevance. \n \nMany experiments are limited to those schools, teachers or students that agree to \nwhatever treatment they are assigned by chance (Cook, 1991). The causal findings so \ngenerated will be bias-free, but only apply to those who volunteer for a random \nassignment study. Other types of causal study will also depend on volunteers, but not \nnecessarily volunteers of the same kind. Experiments have other restrictions to their \ngenerality. They do not guarantee that any obtained effects will hold in the future; and the \neffects of an intervention may change if it is implemented on a much broader scale that \nleads to different causal processes being involved in the smaller experiment than in the \nextrapolation to, say, an entire nation. Once again, though, these restrictions apply to \nvarying degrees to other kinds of causal study too. The limited generalization of findings \nfrom single experiments helps explain why advocates of experiments prefer policy to \ndepend on multiple experimental studies, each with a different population of persons, \nsettings and times as well as on different ways of instantiating the intervention and \nmeasuring the outcome. Alternative causal methodologies are also limited in their \ncapacity to generalize, although not all in the same ways as experiments. What are these \nalternatives? And how good are they? We must answer this to support the claim that \nexperiments are marginally superior to their alternatives, albeit not perfect. \n \n7. In human history, valid causal knowledge has often come from non-experimental and \nnon-quantitative sources.  \n \nIt would be preposterous to maintain that experiments are necessary for causal \nknowledge. Our ancestors learned about the causal effects of making fires millennia \nbefore there was formal experimentation. And scholars knew that out-group threats \nusually cause in-group cohesion long before R.A. Fisher created the first formalization of \nexperimental design. The case for experiments is that they are needed for detecting \neffects that are smaller than many of the others humans have learned about in the past. \nWe have learned from studies of educational performance net of various student \nbackground characteristics that, within the limits of the models used, schooling effects \nare indeed very small and swamped by individual differences, particularly familial and \npsychological ones, not to speak of the genetic ones still to be examined in detail \n(*Coleman, 1966; Jencks, 1972). This may be why the Institute for Educational Sciences \ndesigns its evaluations to detect achievement gains of 1\/5th of a standard deviation--\ntypically over several years and thus equivalent to a total of about one year\u2019s change in \ngrowth over these years. As important as such effect sizes are, they are not obviously \n\u201clarge\u201d and are manifestly far from transformational. Experiments are also needed \nbecause many educational practices that might be effective are enmeshed in real-world \nschool or college life within complex systems involving many other variables. This \nmakes it difficult to identify the unique causal role of any one educational practice, or set \nof practices, unless these practices have first been isolated and then systematically varied. \n \n8. In social science, experiments are not the only method known from theory to be \ncapable of generating unbiased causal knowledge.  \n 6 \n \nFour alternatives to the experiment are known to generate unbiased casual inferences \nunder certain conditions. (1) From statistical theory and comparative empirical research \nreviewed in Cook (2007), we know that regression-discontinuity studies can produce the \nsame causal estimates as experiments. These studies depend on an educational resource \nbeing distributed according to an eligibility score along some quantitative continuum, \noften a specific level of need or merit but sometimes a specific date of birth or order of \napplying for the service under review. The key is that everyone on one side of the \neligibility score receives the service and those on the other side do not. (2) We also know \nfrom theory that instrumental variable approaches can result in unbiased causal inference \nwhen an instrument is found that is correlated with the treatment but not with errors in the \noutcome (Angrist,   ). We also know that casual inferences are unbiased if (3) the process \nof assignment to treatment is perfectly known or (4) the outcome is perfectly predicted \n(Cronbach, 1982).  \n \n9. These theoretically unbiased alternatives have assumptions that cannot be as clearly \nmet in actual research practice, making them technically inferior to the experiment. \n \nRegression-discontinuity has less statistical power to detect effects than the experiment \n(Trochim, 1984), and it depends on strong assumptions about the functional form of the \nrelationship between the assignment variable and outcome (Rubin, 1977). As for \ninstrumental variables, it has proven very difficult to find many of them that meet the \nrequirement of being uncorrelated with the outcome\u2014the ironic exceptions being random \nassignment (Angrist, Imbrens & Rubin, 1996) and regression-discontinuity (Hahn, Todd \n& VanderKlauuwe, 2002). Most causal claims to date using such instruments, \nparticularly in economics, have been hotly contested and thus limit our confidence that an \ninstrumental variable approach can be widely used to promote causal inference. Both \nrandom assignment and regression-discontinuity derive their intellectual warrant from the \nfact that the process of assignment into the different treatment conditions is completely \nknown and hence easily modeled. This is not the case with quasi-experiments or non-\nexperiments where attempts are made to model the treatment assignment process. \nEmpirical research on attempts to do this via selection models (Heckman, 1979) and \npropensity scores (Rosenbaum & Rubin, 1984) show that these statistical tools nearly \nalways fail to recreate the results of experiments that share the same intervention group \nand so vary only in how their control group is formed\u2014at random or not (LaLonde, \n1984; Glazerman, Levy & Myers, 2002; Cook, Shadish & Wong, 2007). So full \nknowledge of the treatment assignment process has not yet turned out to be a viable and \npractical causal tool. And it is almost always impossible in actual research practice to \ntotally predict any educational outcome, even when schools are the unit of study. The \nforegoing implies that the main case for preferring experiments is that they are practically \nsuperior to the other causal methods known from theory to be unbiased. \n \n10. Many other methods are also currently used for supporting claims about what works \nin education, but they are generally inferior because they do not enjoy an independent \ntheoretical or empirical warrant as unbiased.  \n \n 7 \nA great array of other methods is used to justify causal claims in education. They range \nfrom site visits to countries that are performing well in PISA through to highly statistical \ndifference-in-differences or causal modeling studies. Also included are ethnographic \naccounts, secondary analysis of survey data, and quasi-experiments. None of them enjoys \nan independent and theoretically infallible warrant sufficient to justify the causal \nknowledge gained. The shortfalls are many and vary by method. Suffice it to note here \nthat Campbell and Stanley (1963) and its successors (Cook & Campbell, 1979; Shadish, \nCook & Campbell, 2002) have detailed many threats to the validity of causal conclusions \nthat are associated with even the better of these study types. Moreover, Glazerman et al. \n(2002) have documented how practice among economists, including some who work in \neducation, regularly fails to produce the same results as experiments that share the same \ntreatment group. The absence of both a theoretical and empirical warrant for the many \ntypes of study from which causal conclusions are regularly drawn in education today \ncould well be a major reason why so many causal claims have failed to stand up to hard \nscrutiny and have not led to clear cumulative learning about what works.   \n \n11. In many sectors where policy is currently made, experiments enjoy more credibility \nthan other kinds of causal study.  \n \nThis is the case in health, public health, agriculture, the prevention sciences, criminal \njustice, and legal studies of compliance with gender- and race-based hiring laws. And \neven in survey research, improvements to practice have often depended on experiments. \nThey are also common in research on early childhood education in the USA where \nCongress requested that its largest national program, Head Start, be evaluated \nexperimentally. Also, the pre-school studies regularly cited to promote the \u201cuniversal \npreschool\u201d policy in the USA are held in such high regard because they are experimental \nand involve decade-long effects on children\u2019s lives (Weikart,   Reynolds,   Ramey,    ). To \nadvocate against randomised experiments requires a compelling argument that schools \nare systematically different from institutions in other sectors in ways that either make \nexperimentation infeasible or bias the results obtained. Such advocates also have to \nexplain why experiments are common both in pre-schools and in school-based research \nwith prevention rather than academic achievement outcomes. It is important to note that  \nexperimentation does not exist in a vacuum.  \n \n12. Any single experiment assumes prior knowledge that need not itself be the product of \nexperiments.  \n \nExperiments require prior substantive theory and the experience of persons \nknowledgeable about what is feasible in school life. They also require the availability of \ngood measures of the preferred outcomes, or the ability to construct such measures. \nFurther, they require at least local political and administrative support for the study. And \nfinally, they depend on prior causal studies. These can be experiments, but need not be so \nin order to confer marginal advantages for constructing future experiments. For instance, \nstatistical power calculations depend on variance estimates from other studies, as do \nbigger picture issues like how an intervention is conceptualized, chosen and \nimplemented. All experiments build on the shoulders of prior scholars in theoretical and \n 8 \napplied fields. They do not exist in a methodological vacuum, and experimenters are not \na new priesthood that can afford to declare itself independent of educational research\u2019 \npast. \n \n13. Having information from experiments does not guarantee that this information will be \nused in policy debates, and certainly not used to form a decision. \n \nAlthough experiments give a marginally superior causal answer compared to other \nmethods, this does not guarantee that these results will be more often used in debates \nabout educational change. And when evidence from experiments is used, it certainly does \nnot mean that they will alone shape policy decisions. The history of educational research \nis replete with examples of study results not apparently used; and in democracies \ndecision-making does, and should, depend on many factors other than scientific \nknowledge alone. \n \n14. But having scientific information from experiments probably increases the odds of the \ninformation being used in policy debates.  \n \nIt is difficult to argue this point for education today, given the recent history of school-\nbased experiments with random assignment. However, in other fields of study, causal \nresults from experiments are routinely preferred over the results from other kinds of \nstudy. This is especially true in medical, public health and prevention contexts, and also \nwhen the results from multiple studies are synthesized in search of an effective policy \noption. Indeed, it is standard practice in meta-analyses to analyze the results from \nexperiments separately and to add non-experimental results to the review only if their \naverage effect size does not differ from that from experiments (Cook, Cooper,       ). This \nis even the case in those rare educational instances where a very large number of studies \nof an intervention exist, creating enough experiments to analyze separately even if they \nare but a tiny fraction of the whole corpus of studies\u2014 for two instance in early \nchildhood reading, see Ehri (2001, a,b). In more qualitative review contexts, at least in \nthe USA, expert panels commissioned to review the literature for a governmental agency \noften pay special attention to the experiments in formulating conclusions for policy \nconsideration within a government agency, deliberately giving them more weight than the \nnon-experimental evidence. \n \nIn conclusion, the argument is that learning \u201cwhat works\u201d is crucial in educational policy-\nmaking, and that it is especially a problem today. This is because we have failed over the \nlast 30 years to accumulate a secure body of knowledge about effective educational \npractices. So I believe that the case for more causal research is clear--that is, relative to \nother kinds of study with a claim on educational research funds. To do more experiments \ndoes not mean that only experiments are valuable and that only they should be funded. \nBut it does mean that they deserve, at least temporarily, a higher profile than they \nreceived over the last 30 years or so. But only if the causal studies provide more secure \ncausal knowledge of what works, and the best method for achieving this involves doing \nexperiments, given their independent warrant in statistical theory and also in past practice \nin sectors outside of school-based education. Experiments are not perfect. But no other \n 9 \nmethod currently exists that does as well, and this is broadly acknowledged in sectors \nother than education. But it is also acknowledged in two sectors with close links to \ntraditional education\u2014in research on cognitive outcomes in pre-schools and on \nprevention outcomes in research in schools. Experimentation is not a novelty in school-\nbased research; merely something whose sphere of application needs to be extended to \nmeet a commitment to learn more about what works in a context of international crisis \nabout educational performance levels in many larger countries. \n \n \nStephen Gorard \n \nLike Tom Cook, I shall set out a number of summary propositions. Interested readers can \ntrace the further basis for these propositions in my research writings \u2013 examples of which \nare provided. In my own writing I am concerned with education as an area of public \npolicy, including pre-school, post-compulsory, and adult, provision, whereas Tom Cook \nwrites for the context of schools. I see no reason why this difference should affect our \nmethods approach. \n \n1. A key ethical concern for those conducting or using publicly-funded education \nresearch ought to be the quality of the research, and so the robustness of the findings, \nand the security of the conclusions drawn. \n \nUntil recently, very little of the writing on the ethics of education research has been \nconcerned with quality. The concern has been largely for the participants in the research \nprocess, which is perfectly proper, but this emphasis may have blinded researchers to \ntheir responsibility to those not participating in the research process. The tax-payers and \ncharity-givers who fund the research, and the general public who use the resulting \neducation service, have the right to expect that the research is conducted in such a way \nthat it is possible for the researcher to test and answer the questions asked. Generating \nsecure findings for widespread use in public policy could involve a variety of factors \nincluding care and attention, sceptical consideration of plausible alternatives, independent \nreplication, transparent prior criteria for success and failure, use of multiple \ncomplementary methods, and explicit testing of theoretical explanations through \nrandomised controlled trials or similar experimental designs (Gorard 2002a). \n \n2. It is helpful to consider the research enterprise as a cycle of complementary phases \nand activities, because this illustrates how all methods can have an appropriate place in \nthe full cycle of research.  \n \nExperimental designs, like in-depth work or secondary analysis, have an appropriate \nplace in the cycle of research from initial idea to development of the results. The main \nreason to emphasise experiments at this point in time is not because they are more \nimportant than other phases in the cycle, but because they represent a stage of work that \nis largely absent in education research. If nearly all of education research were currently \nconducted as laboratory experiments then I would be one of the commentators pleading \nfor more and better in-depth work or secondary analysis, for example. Other weak points \n 10 \nin the cycle are currently the systematic synthesis of what we already know in an area of \nwork, the design or engineering of what we already know into usable products for policy \nand practice, and the longer-term monitoring of the real-world utility of these products \n(Gorard with Taylor 2004, Gorard et al. 2004). \n \n3. Working towards an experimental design can be an important part of any research \nenterprise, even where an experiment is not envisaged or even possible. \n \nSometimes a true experiment, such as a large randomised controlled trial, is not \nnecessary, and sometimes it is not possible. An experiment is not necessary in a variety \nof research situations, including where the research question does not demand it, and \nwhere a proposed intervention presents no prime facie case for extended trialling. An \nexperiment may also not be possible in a variety of research situations, including where \nthe intervention has complete coverage, or has already been implemented for a long time, \nand where it would be impossible to allocate cases at random. However, a \u2018thought \nexperiment\u2019 is always possible, in which the researcher considers no practical or ethical \nconstraints except answering the research question as clearly as possible. In then having \nto compromise from this \u2018ideal\u2019 to conduct the actual research, the researcher may come \nto realise how much more they could be doing. There might then be more natural \nexperimental designs, more practitioner experiments, and surely more studies with \nappropriate comparison groups rather than no explicit comparison at all (a situation \nwhich reviews show is the norm for UK academic research in education). There might \nalso be more humility about the quality of the findings emanating from the compromise \ndesign (Gorard 2002b, 2003a). \n \n4. Part of the problem of research quality lies in traditional research methods training \nand \u2018experts\u2019. \n \nIn the UK, traditional methods training for new researchers in university departments of \neducation generally starts by introducing students to differences between types of \nresearch, and emphasising the purportedly incommensurable values underlying the \nvariety of approaches to discovery. Most obviously, researchers are introduced to a \nsupposed paradigmatic division between \u2018qualitative\u2019 and \u2018quantitative\u2019 studies in a way \nthat encourages methods identities based on a choice of only one of these \u2018paradigms\u2019. \nThis leads many of us to indulge in paradigmatic strife, or write off entire fields of \nendeavor \u2013 as being \u2018positivist\u2019, for example. Some commentators try to heal these \nschisms after they have been created, but there is a shortage of texts and training \nresources that take the far superior approach of assuming that there is a universal \nunderlying logic to all research. Such an approach leads from the outset of training to a \nfocus on the craft of research, thus bringing design, data collection, analysis, and \nwarranting results to the fore, leaving little or no place for paradigms (Gorard 2003b, \n2004a). \n \n5. Part of the problem of research quality lies in a lack of appropriate use of numbers. \n \n 11 \nOne of the main reasons why there is not more mixed methods education research is \nclearly that there are few researchers willing and able to work with numbers. Since \nexperimental designs are seen by many, incorrectly, to be \u2018quantitative\u2019 in nature, this \ncould also be part of the reason for the lack of experimental work. There may be a \nnumber of influences at play here, including poor maths teaching in schools, lower ability \nof social science students in comparison to other disciplines both in terms of maths and \nperhaps also overall, the selection of methods courses by students in terms of perceived \nease, and the widespread misunderstanding that being a \u2018qualitative\u2019 researcher means \nnever having to deal with numbers. However, I am coming increasingly to the view that a \nmajor share of the blame lies with \u2018quantitative\u2019 researchers. They seem to prefer \ndevising more and more complex methods of analysis rather than devoting their energy to \ncreating higher quality datasets that are easier to analyse. They often present their \nresearch in exclusive and unnecessarily technical ways. They generally assume, \nincorrectly, that numbering is the same as measuring, that reliability is the same as \nvalidity, that probabilistic statistics can be used with purposive samples or even with \npopulation figures, and that any use of numbers must be based on sampling theory. This \nis not the way forward (Gorard 2006a, 2006b). \n \n6. Part of the problem of research quality lies in an unwillingness to test our cherished \ntheories. \n \nAnother element of the methods crisis stems from our love of specific theories, and our \nconsequent unwillingness to test them for failure. A typical piece of evaluation in UK \neducation is either commissioned by, or conducted by, those responsible for the \nprogramme being evaluated. There may then be pressure from funders to \u2018finesse\u2019 the \nresults. I have certainly been contacted by evaluators seeking some new kind of analysis \nthat will gainsay the surface findings, and which will support instead their underlying \nbelief that the programme must be being effective. This is no different, in principle, to the \ndredging of data that goes on shamelessly post hoc in other forms of research as well. I \nhave also experienced far too many cases in which researchers simply make up or distort \ndata in order to help preserve their prior beliefs. Some methods experts actually advise \nresearchers to \u2018take sides\u2019 before conducting research, and not to publish negative or \notherwise unhelpful results. Of course, it remains true that the evidence-based approach \nto policy-making and practice is itself untested in education, and still far from fully \nsatisfactory in fields such as health sciences. But this is a reason to test it, not to reject it \nout of hand (Gorard 2004b, Gorard and Fitz 2006). \n \n7. Much of the solution lies in greater scepticism, because the problem is not really one \nof methods at all. \n \nSome of the criticism of education research during the 1990s was concerned with \nrelevance. But education is a very applied field of research. I do not find much published \nresearch that has no relevance to some important or useful component of education. The \ncriticism is more properly about the poor quality of much research, so that even though \nthe findings may have relevance they still cannot be used safely. In response, capacity-\nbuilding activities have tended to focus on solutions in terms of methods, such as having \n 12 \nmore complex quantitative work, more systematic reviews, or more experiments. These, \nto my mind, are not the answer in themselves. A more general change is needed in the \nculture of research. The answer for me lies in genuine curiosity, coupled with outright \nscepticism. These characteristics lead a researcher to suit methods to purpose, try \ndifferent approaches, replicate and triangulate, and attempt to falsify their findings. It \nleads them to consider carefully the logic and hidden assumptions on the path from \nevidence to conclusions, automatically generating caveats and multiple plausible \ninterpretations from the standard query \u2013 \u2018if my conclusions are actually incorrect, then \nhow else could I explain what I have found?\u2019. Some improvement may come from \nresearcher development, but, somewhat pessimistically for an educator, I have come to \nbelieve that the role of capacity-building is limited here. Some people appear genuinely \ncurious and sceptical anyway. Some, on the other hand, tend to be devoted \u2018believers\u2019 of \nthings, and their development may involve simply a change of the subject of those beliefs \nas when a committed religious person becomes an enthusiastic Marxist, or when a \n\u2018qualitative\u2019 researcher turns heavily \u2018quantitative\u2019 (Gorard 2002c, 2005). In a sense, \nwhat we need for evidence-based policy making and practice is more real research, where \nthe researcher is genuinely trying to find something out. From this, all else will likely \nfollow \u2013 including more and better experiments for many of the reasons advanced by \nboth authors in this paper so far. \n \n \nAgreements and Disagreements \n \nIntriguingly, having written out our opening positions independently, it seems that we are \nmostly in agreement, though there are differences of emphasis we will mention. We agree \nthat all commonly used methods have a valid purpose and a place in the larger cycle of \neducation research. Our capacity-building should, therefore, focus on filling in the \nexisting gaps within the cycle so as to create the needed expertise and practices, on trying \nto overcome mono-method identities where researchers reject the use of all but one type \nof evidence, and on teaching respect for all methods in their place, as difficult as it is to \nidentify these places.  \n \nWe also agree that the full research cycle represented in Figure 1 presents a simplified \nand stylized, but useful, model of the research cycle. In this cycle, reviews and secondary \nanalyses might appear in Phase 1, theory-building and small-scale fieldwork in Phase 2, \net cetera, with smaller experiments being part of Phase 5 and a full randomised controlled \ntrial only appearing once, in Phase 6. We agree that experimental designs are not \nprivileged for all of these phases and that other means are preferable, especially for the \nfirst four phases. We also agree that experiments are currently lacking in education \nresearch practice writ large, and that most education research gets stuck in phases 1 to 4. \nIn other words, it is stuck working towards a randomised trial that hardly ever gets done.  \n \nFigure 1 \u2013 An outline of the full cycle of education research  \n 13 \n \n \nWe further agree that it is important to answer descriptive questions such as \u2018Who gets \nwhat?\u2019 or \u2018How are teachers trained?\u2019. But these questions are no sooner broached than \nwe usually also want to learn how to improve things in these domains and causal \nquestions then arise, like: \u2018How can we train better teachers?\u2019 or \u2018How can we better \nshare out resources? Thus, a complete programme of education research will generally \nlead to a need to make causal claims, and so to an ethical need for researchers to use \nsomething like a randomised controlled trial to make these claims responsibly. \n \nImportant consequences follow from our agreement that most education research gets \nstuck in phases 1 to 4 and that experiments have a special role to play in the \nunderrepresented phases 5 through 7. For a fixed research budget, doing more \nexperiments in the later phases will entail fewer resources for those researchers working \non phases 1 through 4, this being the vast majority of education researchers. So these \nindividuals will not, and do not, like increasing the priority accorded to causal questions \nand methods. This priority is deeply threatening to them intellectually and instrumentally, \nhence their lack of support for the call to conduct more school-based experiments  \n \n 14 \nDrawing attention to the neglected later phases of the research cycle indirectly serves to \nraise the priority accorded to them. After all, there is little point to a model that rarely \nmeets its ultimate goals! Without explicit or implicit priorities, Figure 1 is conservative in \nits implications. It is a recipe for more of the same since so few education researchers \nwant to work on the later phases, or even know how to do so if experiments are required. \nThey might want to argue that phases 1 through 4 are necessary for the later phases, thus \njustifying much more work on the earlier than the later phases, especially since the figure \npresumes a winnowing process - only some modest fraction of the ideas initially \ngenerated ever get to have a randomised experiment devoted to them later. However, we \nboth agree that the early phases are not necessary conditions for the later ones, as \nadvantageous as it is to have them. Indeed, many educational practices that are currently \nwidespread have never been through even the first four phases of Figure 1. They are \nwidely implemented despite theory that is weak or even non-existent and, if any studies \nsupport these practices at all, they are not strong in terms of internal or external validity, \nhaving mostly been conducted in contrived settings or tested in a few schools and with \nfew classrooms or children. In the past, we have been accepting of educational reforms \nthat have hardly benefited from phases 1 through 4, let alone 5 and 6. Even in logic, there \nis no need for potential school reforms to have gone through a multi-year testing process \nbefore being implemented in schools. \n \nAlso pushing towards conservatism is that an un-prioritized Figure 1 leaves the funders \nof education research with total freedom of action. They never need take stands about \npriorities, and so they need not fear alienating their constituencies in universities and \nministries. In many policy environments, setting priorities is a political headache one \nwould like to avoid if possible. Figure 1 may be a good normative description of some \nPlatonic research cycle, but it will only change education research practice if it is linked \nto acknowledging two things we both agree on concerning its last phases\u2014that they are: \n(1) indispensable to evidence-based policy research since much of policy is about \nimproving educational performance; and (2) they are neglected in current education \nresearch practice, making secure knowledge about what works in education a current gap \nof some significance.  \n \nWhere we may differ more is on the urgency of the need to fill this gap and hence on the \nextent to which experiments are needed. Tom Cook is more worried that current \neducation research rarely gets to a point where it reliably tests its ideas in the hurly-burly \nof school life, and that so few organizations responsible for education and research on \neducation are fazed by this. He believes that those commissioning education research \nhave a responsibility for hurrying along the research cycle and for short-circuiting it on a \nregular basis by jumping quickly to stages 5 and 6. He argues that the last phases in \nFigure 1 are the sine qua non of evidence-based education research. Without them, \npolicy-makers do not have secure causal evidence, arguably the most relevant of all kinds \nof evidence for forming policy. Consequently, policy-makers cannot truly meet their \naccountability obligation to tax-payers. Of course, there are always researchers willing to \noffer policy-makers causal knowledge; but without experiments they cannot offer causal \nknowledge that is known to be secure because it results from a valid statistical theory \nbased on random assignment and from the wisdom about implementing experiments that \n 15 \nhas accumulated from doing them in complex settings in the past, including even from \nrandomised experiments on doing randomised experiments (e.g., Shadish, Clark & \nLuellen, 2007). \n \nStephen Gorard sees the need for more causal studies at the end of the research cycle in \nFigure 1, and also the need for more experiments in Phase 6. Indeed, he has supported \nboth as Director of the ESRC-funded Research Capacity-building Network in the UK. \nThis helped convince him of the difficulty of shifting the culture in UK higher education \nresearch, though he nevertheless continues to take on the task and is currently leading an \nESRC-funded Researcher Development Initiative designed to promote the use and \nunderstanding of randomised controlled trials (http:\/\/trials-pp.co.uk\/). However, he is less \nworried about the shortage of knowledge about effective educational practices than Tom \nCook is; and he is also less sure of the size of the premium that experiments deserve \nwhen causal knowledge is needed. So he does not use the rhetoric of crisis and, if we \nwere to re-assign some hypothetical education research budget, he might not assign as \nmuch money to experiments as Tom Cook would. However, this is a difference of degree \nrather than a fundamental difference about the relative importance of causal questions \nand experimental methods. \n \nHowever, we do disagree on whether calling for more genuinely mixed methods is \n\u2018anodyne\u2019, as Tom Cook terms it. Stephen sees the dominance of qualitative studies in \nUK education journals and regrets the number of researchers who fail to accept the \nprinciple that different kinds of questions (phases) require different (multiple) \napproaches. Tom Cook sees different kinds of questions requiring different methods, but \nnot each kind of question requiring multiple methods. For a given kind of question, one \nmethod is often superior to another. It is only across all of education research with its \nmany different kinds of question that multiple methods are needed. And we both agree on \nthis last proposition. However, Tom Cook sees it as so obvious that it is not worth \nclaiming as an intellectual principle. In this sense, it is anodyne for him, however \ngripping the need for mixed methods may be as part of a political battle between research \nfactions that struggle to be at the table for prestige, funds and self-vindication. But the \nmain point is that we both agree that randomised controlled trials are the best available \nprimary method for answering causal questions. We both want to know, therefore: How \ncan we get more of them done and done well? \n \n \nReferences: \n \nAngrist, J.D., Imbens, G. W., & Rubin, D. B. (1996). Identification of Causal Effects \nusing Instrumental Variables. Journal of the American Statistical Association, 91, \n444-472. \nCampbell, D.T., & Stanley, J.C. (1963). Experimental and quasi-experimental designs for \nresearch.  Chicago: Rand-McNally. \nColeman, J.S. (1966). Equality of Educational Opportunity.  \nCook, T.D. (1991). Clarifying the warrant for generalized causal inferences in quasi-\nexperimentation. In M.W. McLaughlin & D.C. Phillips (Eds.), Evaluation and \n 16 \neducation: At quarter-century (pp. 115-144). Chicago: National Society for the Study \nof Education. \nCook, T.D. (2002). Randomised Experiments in Educational Policy Research: A Critical \nExamination of the Reasons the Educational Evaluation Community has Offered for \nNot Doing Them. Educational Evaluation and Policy Analysis, 24(3), 175-199. \nCook, T.D. (in press). \u201cWaiting for Life to happen\u201d; History of the Regression \nDiscontinuity Design in Psychology, Statistics and Economics. Journal of \nEconometrics.  \nCook, T.D., & Campbell, D.T. (1979). Quasi-Experimentation: Design and Analysis \nIssues for Field Settings. Chicago: Rand-McNally. \nCook, T.D., Cooper, H., Cordray, D.S., Hartmann, H., Hedges, L.V., Light, R.J., Louis, \nT.A., & Mosteller, F. (Eds.). (1992). Meta-analysis for explanation: A casebook. New \nYork: Russell Sage Foundation. \nCook, T.D., & Foray, D. (in press). Building the Capacity to Experiment in Schools: A \nCase Study of the Institute of Educational Sciences in the U.S. Department of \nEducation. Economics of Innovation and New Technology. \nCook, T.D., Shadish, W.J., & Wong, V.C. (2007). When Non-experimental and \nExperimental Effect Size Estimates do and do not differ: A Review of the Within-\nStudy Comparisons Literature. Institute for Policy Research, Northwestern \nUniversity, Evanston, Ill. \nCronbach, L.J. (1982). Designing Evaluations of Educational and Social Programs. San \nFrancisco: Jossey-Bass. \nEhri, L., Nunes, S., Stahl, S., & Willows, D. (2001). Systematic phonics instruction helps \nchildren learn to read: Evidence from the National Reading Panel\u2019s meta-analysis. \nReview of Educational Research, 3, 393\u2013447. \nEhri, L., Nunes, S., Willows, D., Schuster, B., Yaghoub-Zadeh, Z., & Shanahan, T. \n(2001). Phonemic awareness instruction helps children learn to read: Evidence from \nthe National Reading Panel\u2019s meta-analysis. Reading Research Quarterly, 36, 250\u2013\n287. \nGlazerman, S., Levy, D. M., & Myers, D. (2003). Nonexperimental versus Experimental \nEstimates of Earnings Impacts. The Annals of the American Academy, 589, 63-93. \nGorard, S. (2001) Quantitative Methods in Educational Research: The role of numbers \nmade easy, London: Continuum. \nGorard, S. (2002a) Ethics and equity: pursuing the perspective of non-participants, Social \nResearch Update, 39, 1-4. \nGorard, S. (2002b) The role of causal models in education as a social science, Evaluation \nand Research in Education, 16, 1, 51-65. \nGorard, S. (2002c) Fostering scepticism: the importance of warranting claims, Evaluation \nand Research in Education, 16, 3, 136-149. \nGorard, S. (2003a) Quantitative methods in social science: the role of numbers made \neasy, London: Continuum. \nGorard, S. (2003b) Understanding probabilities and re-considering traditional research \nmethods training, Sociological Research Online, 8,1, 12 pages. \nGorard, S. (2004a) Scepticism or clericalism? Theory as a barrier to combining methods, \nJournal of Educational Enquiry, 5, 1, 1-21. \n 17 \nGorard, S. (2004b) Three abuses of \u2018theory\u2019: an engagement with Nash, Journal of \nEducational Enquiry, 5, 2, 19-29. \nGorard, S. (2005) Current contexts for research in educational leadership and \nmanagement, Educational Management Administration and Leadership, 33, 2, 155-\n164. \nGorard, S. (2006a) Using everyday numbers effectively in research, London: Continuum.  \nGorard, S. (2006b) Towards a judgement-based statistical analysis, British Journal of \nSociology of Education, 27, 1, 67-80. \nGorard, S. and Fitz, J. (2006) What counts as evidence in the school choice debate?, \nBritish Educational Research Journal, 32, 6, 797-816. \nGorard, S., Rushforth, K. and Taylor, C. (2004) Is there a shortage of quantitative work in \neducation research?, Oxford Review of Education, 30, 3, 371-395. \nGorard, S., with Taylor, C. (2004) Combining methods in educational and social \nresearch, London: Open University Press. \nHahn, J., Todd, P., & VanderKlaauw, W. (2001). Identification and estimation of \ntreatment effects with a regression-discontinuity design. Econometrica, 69(1), 201-\n209. \nHeckman, J.J. (1979). Sample selection bias as a specification error. Econometrica, 47, \n153-161. \nJencks, C. In F. Mosteller & D.P. Moynihan (Eds.), On equality of educational \nopportunity. New York: Random House. \nLaLonde, R. (1986). Evaluating the Econometric Evaluations of Training with \nExperimental Data. The American Economic Review, 76(4), 604-620. \nRosenbaum, P., & Rubin, D.B. (1984). Reducing bias is observational studies using \nsubclassification on the propensity score. Journal of the American Statistical \nAssociation, 79, 516-524.  \nRubin, D. B. (1977). Assignment to Treatment Group on the Basis of a Covariate. \nJournal of Educational Statistics, 2(1), 1-26. \nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental Quasi-\nExperimental Designs for Generalized Causal Inference. Boston: Houghton Mifflin \nCompany. \nShadish, W.R., Luellen, J.K. & Clark, M.H. Propensity scores and quasi-experiments: A \ntestimony to the practical side of Less Sechrest. In R. R.Boootzin (Ed.). \nMeasurement, Methods and Evaluation. Washington, D.C.: American Psychological \nAssociation Press. \nTrochim, W.M.K. (1984) Research Design for Evaluation. Beverly Hills, Ca.: Sage \nPublications. \n \n \n \n \n"}